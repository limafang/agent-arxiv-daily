## Updated on 2025.04.06
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2025-04-03**|**Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions**|PeiJie Yu et.al.|[2504.02623](http://arxiv.org/abs/2504.02623)|**[link](https://github.com/yupeijei1997/MMTB)**|**大型语言模型（LLMs）因其卓越的理解和规划能力，在作为工具调用代理方面展现出巨大潜力。用户越来越多地依赖基于LLM的代理通过迭代交互来解决复杂任务。然而，现有的基准测试主要评估单一任务场景下的代理，未能捕捉现实世界的复杂性。为此，我们提出了多任务工具基准（Multi-Mission Tool Bench）。在该基准中，每个测试案例包含多个相互关联的任务。这种设计要求代理能够动态适应不断变化的需求。此外，所提出的基准探索了固定任务数量下所有可能的任务切换模式。具体而言，我们提出了一种多代理数据生成框架来构建该基准。我们还提出了一种新颖的方法，利用动态决策树来评估代理决策的准确性和效率。对多种开源和闭源LLMs的实验揭示了影响代理鲁棒性的关键因素，并为工具调用社区提供了可操作的见解。**|
|**2025-04-02**|**On Simulation-Guided LLM-based Code Generation for Safe Autonomous Driving Software**|Ali Nouri et.al.|[2504.02141](http://arxiv.org/abs/2504.02141)|null|自动驾驶系统（ADS）是一种负责解释车辆环境并据此做出决策的安全关键软件系统。由于驾驶环境中复杂性和不可预见事件的无限性，持续改进成为必要，这通常通过迭代的DevOps过程实现。然而，DevOps过程本身的复杂性使得这些改进既耗时又耗费资源。大型语言模型（LLM）在ADS代码生成中的自动化是解决这一挑战的一种潜在方法。然而，ADS的开发需要严格的流程来验证、确认、评估和验证代码，然后才能将其部署到车辆中使用。在这项研究中，我们在工业环境中开发并评估了一个自动代码生成和评估的原型，该原型采用了一条由LLM代理、仿真模型和基于规则的反馈生成器组成的管道。LLM生成的代码会在仿真模型中针对多个关键交通场景进行自动评估，并提供评估报告作为反馈给LLM以进行修改或调试。我们报告了使用Codellama:34b、DeepSeek (r1:32b 和 Coder:33b)、CodeGemma:7b、Mistral:7b 和 GPT4针对自适应巡航控制（ACC）和无监督碰撞避免机动（CAEM）的实验结果。最后，我们通过两家原始设备制造商（OEM）的11位专家进行了访谈研究来评估该工具|
|**2025-04-02**|**Building Knowledge from Interactions: An LLM-Based Architecture for Adaptive Tutoring and Social Reasoning**|Luca Garello et.al.|[2504.01588](http://arxiv.org/abs/2504.01588)|null|将大型语言模型集成到日常场景如辅导或体能训练中，需要机器人具备适应性、社交性和目标导向的交互能力。尽管大型语言模型在类人交流方面显示出潜力，但其独立使用受到记忆限制和上下文不连贯性的阻碍。本文提出了一种多模态、认知启发的框架，以增强基于大型语言模型的自主决策在社交和任务导向的人机交互中的应用。具体而言，我们开发了一个基于大型语言模型的机器人教练代理，平衡了社交对话与任务指导以及目标驱动的动机。为进一步增强自主性和个性化，我们引入了一种记忆系统，用于选择、存储和检索经验，从而促进基于跨不同交互构建的知识进行泛化推理。初步的人机交互用户研究和离线实验验证了我们的方法，证明了该系统的复杂交互管理能力、自主驱动训练任务的能力以及构建和检索上下文记忆的能力，推动了社会智能机器人技术的发展。|
|**2025-04-01**|**Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks**|Naimul Haque et.al.|[2504.01241](http://arxiv.org/abs/2504.01241)|null|大型语言模型（LLMs）在自然语言处理（NLP）领域尤其是自然语言理解（NLU）任务上取得了显著进展。随着我们迈向具有自主性的世界，LLM 基于的代理将在没有人工干预的情况下自主处理专业化任务，因此对于这些模型来说，在适应新任务的同时不忘记之前学到的信息变得至关重要，这一挑战被称为灾难性遗忘。本研究评估了不同开源 LLM 的连续微调，重点是参数规模小于 10 亿的模型，在 GLUE 基准测试中的关键 NLU 任务，包括 SST-2、MRPC、CoLA 和 MNLI。通过采用提示工程和任务特定调整，我们评估并比较了这些模型在保留先前知识的同时学习新任务的能力。我们的结果表明，像 Phi-3.5-mini 这样的模型在最小化遗忘的同时保持了强大的学习能力，使其非常适合连续学习环境。此外，Orca-2-7b 和 Qwen2.5-7B 等模型在微调后也表现出令人印象深刻的性能和学习能力。这项工作有助于理解 LLM 中的灾难性遗忘问题，并突出了提示工程在优化模型性能以适应连续学习场景中的作用。|
|**2025-04-01**|**Personality-Driven Decision-Making in LLM-Based Autonomous Agents**|Lewis Newsham et.al.|[2504.00727](http://arxiv.org/abs/2504.00727)|null|大型语言模型（LLMs）嵌入到自主代理中的应用正在快速发展，这使得动态、可配置的行为成为可能，而无需进行广泛的领域特定训练。在我们之前的工作中，我们介绍了SANDMAN，这是一种利用五因素OCEAN人格模型的欺骗性代理架构，展示了人格诱导对代理任务规划有显著影响。在此研究基础上，本研究提出了一种新的方法来衡量和评估诱导的人格特质如何影响基于LLM的代理的任务选择过程——特别是规划、调度和决策。我们的结果显示，与诱导的OCEAN属性一致的任务选择模式明显存在，强调了设计高度可信的欺骗性代理以实现主动网络防御策略的可行性。|
|**2025-04-01**|**GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments**|Enjun Du et.al.|[2504.00711](http://arxiv.org/abs/2504.00711)|null|基础模型的时代已经彻底改变了人工智能研究，然而图基础模型（GFMs）仍然受到大规模图数据集稀缺的限制。传统的图数据合成技术主要集中在简单的结构操作上，缺乏生成具有有意义文本属性的语义丰富节点的能力：这是现实世界应用中的一个关键局限性。尽管大型语言模型（LLMs）展示了出色的文本生成能力，但它们直接应用于图合成时受到上下文窗口限制、幻觉现象和结构一致性挑战的阻碍。为了解决这些问题，我们引入了GraphMaster，这是首个专门为数据受限环境下的图数据合成设计的多智能体框架。GraphMaster通过四个专门的LLM智能体（管理者、感知器、增强器和评估器）协同优化合成过程，通过迭代改进确保语义连贯性和结构完整性。为了严格评估我们的方法，我们创建了六个标准图基准的新“子”变体，专门设计用于在现实约束下测试合成能力。此外，我们开发了一种新的可解释性评估框架，结合了人类评估与基于Grassmannian流形的分析，提供了语义连贯性的定性和定量度量。实验结果表明，GraphMaster在多个数据集上显著优于传统合成方法，为在数据稀缺环境中推进GFMs奠定了坚实的基础。|
|**2025-04-01**|**AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems**|Yingxuan Yang et.al.|[2504.00587](http://arxiv.org/abs/2504.00587)|null|大规模语言模型（LLMs）的快速发展推动了多智能体系统的兴起，其中多个基于LLM的智能体协作以解决复杂任务。然而，现有的系统大多依赖于集中式协调，这带来了可扩展性瓶颈、适应性限制以及单点故障的风险。此外，隐私问题和对专有知识共享的担忧阻碍了跨组织的合作，导致专业知识的孤立。为了解决这些挑战，我们提出了AgentNet，这是一种去中心化的、基于检索增强生成（RAG）的框架，使基于LLM的智能体能够在有向无环图（DAG）结构网络中自主进化其能力并高效协作。与传统多智能体系统不同，AgentNet允许智能体动态专业化、调整连接，并在不依赖预定义工作流的情况下路由任务。AgentNet的核心设计基于几个关键创新：（1）完全去中心化范式：移除中央协调器，使智能体能够自主协调和专业化，促进容错性和涌现的集体智能。（2）动态演化的图拓扑结构：根据任务需求实时调整智能体之间的连接，确保可扩展性和弹性。（3）自适应学习以优化专业技能：一种基于检索的记忆系统，使智能体能够持续更新和优化其专业化技能。通过消除集中控制，AgentNet增强了容错性，促进了可扩展的专业化，并实现了跨组织的隐私保护协作。通过去中心化协调和最小的数据交换，智能体可以利用多样化的知识源，同时保护敏感信息。|
|**2025-04-01**|**Automated detection of atomicity violations in large-scale systems**|Hang He et.al.|[2504.00521](http://arxiv.org/abs/2504.00521)|null|原子性违规在中断驱动程序中对关键系统的软件安全性构成了重大威胁。这些违规行为发生在共享资源的操作执行序列被异步中断打断时。由于庞大的程序状态空间、应用级代码依赖关系以及复杂的领域特定知识，检测原子性违规具有挑战性。我们提出了Clover，这是一种混合框架，集成了静态分析与大型语言模型（LLM）代理以检测真实世界程序中的原子性违规。Clover首先执行静态分析以提取关键代码片段和操作信息。然后启动多代理过程，其中专家代理利用领域特定知识检测原子性违规，随后由法官代理进行验证。在RaceBench 2.1、SV-COMP和RWIP上的评估表明，Clover的精确度/召回率达到了92.3%/86.6%，在F1分数上比现有方法高出27.4-118.2%|
|**2025-04-01**|**When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)**|Mahak Agarwal et.al.|[2504.00374](http://arxiv.org/abs/2504.00374)|null|在许多现实场景中，单一的大规模语言模型（LLM）可能会遇到相互矛盾的主张——有些准确，有些则坚决错误——并必须判断哪一个是真实的。我们研究了这种风险在一个单轮、多智能体辩论框架中：一个基于LLM的代理从TruthfulQA提供事实答案，另一个坚定地捍卫虚假信息，而相同的LLM架构则充当裁判。我们引入了置信加权说服力覆盖率（CW-POR），它不仅衡量裁判被误导的频率，还反映其对错误选择的信心强度。我们的实验在五个开源LLM上进行（参数量为3B到14B），我们系统性地改变智能体的冗长程度（30至300词），结果表明即使较小的模型也能构建有说服力的论点，从而推翻真实答案——通常还伴随着高度的信心。这些发现强调了稳健校准和对抗性测试的重要性，以防止LLM自信地支持错误信息|
|**2025-03-31**|**Large Language Models in Numberland: A Quick Test of Their Numerical Reasoning Abilities**|Roussel Rahman et.al.|[2504.00226](http://arxiv.org/abs/2504.00226)|null|人类数学推理的一个基本要素是我们的数感——一种抽象的数字及其关系的理解能力——它使我们能够在有限的计算资源下解决涉及庞大数空间的问题。大型语言模型（LLMs）的数学推理通常通过高水平的问题（如奥林匹克挑战、几何、应用题和谜题）进行测试，但它们在低层次数感方面的表现仍较少被探索。我们引入了“数字国度”，这是一个包含100个问题的测试，用于评估基于LLM的代理的数值推理能力。这些任务包括基本运算、高级计算（如指数运算、复数）、质数检查以及24点游戏，旨在测试基础技能及其在解决复杂和不确定问题中的整合能力。我们评估了五种基于LLM的代理：OpenAI的o1和o1-mini、谷歌Gemini、微软Copilot和Anthropic Claude。在允许确定性步骤求解的前三个任务中，它们的得分在74%-95%之间。在需要尝试错误搜索的24点游戏中，性能下降到10%-73%。我们将24点游戏的顶级求解器（得分为73%的o1）在25个更难的问题上进行了测试，其得分降至27%，这证实了搜索是一个瓶颈。这些结果以及错误类型表明LLMs的数感较为脆弱，这与其在具有挑战性的基准测试中表现出的能力形成对比。LLMs数值推理的局限性突显了简单且有针对性的测试的重要性，以评估和解释LLMs的数学技能，确保其安全使用。|
|**2025-03-31**|**Get the Agents Drunk: Memory Perturbations in Autonomous Agent-based Recommender Systems**|Shiyi Yang et.al.|[2503.23804](http://arxiv.org/abs/2503.23804)|null|基于大型语言模型的推荐系统代理（Agent4RSs）越来越多地用于实现个性化行为建模。具体来说，Agent4RSs引入了记忆机制，使代理能够从现实世界交互中自主学习和自我进化。然而，据我们所知，关于Agent4RSs的鲁棒性尚未被探索。因此，在本文中，我们提出了首个针对Agent4RSs的攻击研究，通过扰动代理的记忆来揭示其局限性，并增强其安全性和鲁棒性，以确保更安全可靠的AI代理的发展。鉴于安全和隐私问题，黑盒环境下的攻击更为实用，其中无法轻易获取受害模型的准确知识。此外，实际攻击通常是隐蔽的以最大化影响。为此，我们提出了一种新颖的实用攻击框架名为DrunkAgent。DrunkAgent由生成模块、策略模块和代理模块组成。生成模块旨在生成有效的且连贯的对抗文本触发器，这些触发器可用于实现如推广目标项目等攻击目标。策略模块旨在“让目标代理喝醉”，从而使它们在交互过程中无法有效更新记忆。这样，触发器就能发挥最佳作用。这两个模块都在代理模块上进行优化，以提高攻击的可迁移性和不可察觉性。通过识别和分析漏洞，我们的工作提供了关键见解，为构建更安全和更有弹性的Agent4RSs铺平了道路。在各种真实数据集上的广泛实验证明了DrunkAgent的有效性。|
|**2025-03-30**|**An Analysis of Decoding Methods for LLM-based Agents for Faithful Multi-Hop Question Answering**|Alexander Murphy et.al.|[2503.23415](http://arxiv.org/abs/2503.23415)|null|大型语言模型（LLMs）经常产生事实性错误的输出，这种现象被称为幻觉，这限制了它们在知识密集型自然语言处理任务中的准确性。通过检索增强生成和具有推理与行动（ReAct）框架等方法可以解决这一问题，使模型能够访问外部知识。然而，LLMs往往无法忠实地遵循检索到的信息。缓解这一问题是至关重要的，特别是当LLMs需要根据检索到的信息进行推理时。最近的研究探索了无需训练的解码策略来提高模型生成的忠实度。我们对结合ReAct框架和解码策略（即DeCoRe、DoLa和CAD）如何影响LLM生成答案的忠实度进行了系统分析。我们的结果显示，在下游多跳问答任务中，结合知识检索的主动框架与提升忠实度的解码方法可以提高准确性。例如，我们在HotpotQA上的F1分数从19.5提高到了32.6，当使用ReAct和DoLa时。|
|**2025-03-29**|**CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis**|Anjiang Wei et.al.|[2503.23145](http://arxiv.org/abs/2503.23145)|null|归纳程序合成（或称编程示例）需要从输入输出示例中合成函数，并推广到未见过的输入。尽管大型语言模型代理在自然语言指导下展示了在编程任务中的潜力，但它们在进行归纳程序合成的能力尚未被充分探索。现有的评估协议依赖于静态的示例集和保留测试，当合成的函数不正确时无法提供反馈，且未能反映如逆向工程等真实世界场景。我们提出了CodeARC（代码抽象与推理挑战），这是一个新的评估框架，其中代理通过查询隐藏的目标函数来与之交互，合成候选函数，并利用差分测试oracle迭代地完善其解决方案。这种交互式设置鼓励代理基于反馈进行函数调用和自我修正。我们构建了第一个大规模基准用于通用归纳程序合成，包含1114个函数。在评估的18个模型中，o3-mini表现最佳，成功率为52.7%，突显了这项任务的难度。对LLaMA-3.1-8B-Instruct进行微调后可获得高达31%的相对性能提升。CodeARC提供了一个更现实和更具挑战性的平台来评估基于LLM的程序合成和归纳推理能力。|
|**2025-03-28**|**Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models**|Bruno Coelho et.al.|[2503.22877](http://arxiv.org/abs/2503.22877)|null|事实核查是大型语言模型（LLMs）对抗日益增长的虚假信息传播的潜在有用应用。然而，LLMs在不同地理区域的表现各不相同。在本文中，我们评估了开放模型和私有模型在一组多样化地区和场景中的事实准确性。使用包含600条经过事实核查的陈述并平衡来自六个全球地区的数据集，我们考察了三种事实核查设置：（1）仅提供陈述时；（2）使用具有维基百科访问权限的LLM代理时；以及（3）作为最佳情况，使用提供了官方事实核查的检索增强生成（RAG）系统时。我们的研究结果表明，无论场景和LLM类型如何，包括GPT-4、Claude Sonnet和LLaMA，来自北方国家的陈述表现明显优于来自南方国家的陈述。此外，在更现实的维基百科代理系统情况下，这一差距进一步扩大，这表明过于通用的知识库在解决地区特定细微差别方面能力有限。这些结果强调了改善数据集平衡和强化检索策略以提升LLM事实核查能力，特别是在地理多样性背景下的紧迫需求。|
|**2025-03-28**|**WorkTeam: Constructing Workflows from Natural Language with Multi-Agents**|Hanchao Liu et.al.|[2503.22473](http://arxiv.org/abs/2503.22473)|null|工作流在通过协调多个工具或组件来提升企业效率方面发挥着关键作用。然而，手工构建工作流需要专家知识，这带来了显著的技术障碍。近期，大型语言模型（LLMs）的进步提升了从自然语言指令生成工作流的能力（即NL2Workflow）。然而，现有的单一LLM代理方法在复杂任务上由于需要专门知识和任务切换的压力而出现性能下降。为了解决这些挑战，我们提出了WorkTeam，这是一种多代理NL2Workflow框架，由监督者、协调者和填充者代理组成，每个代理具有不同的角色，共同增强转换过程。由于目前没有公开可用的NL2Workflow基准数据集，我们还引入了HW-NL2Workflow数据集，其中包括3695个现实业务样本用于训练和评估。实验结果表明，我们的方法显著提高了工作流构建的成功率，提供了一种新颖有效的解决方案，以支持企业的NL2Workflow服务。|
|**2025-03-28**|**Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey**|Shengyue Guan et.al.|[2503.22458](http://arxiv.org/abs/2503.22458)|null|本调查研究了基于大型语言模型（LLM）的代理在多轮对话设置中的评估方法。采用PRISMA启发式的框架，我们系统地回顾了近250篇学术资源，涵盖了各种发表渠道，为我们的分析奠定了坚实的基础。本研究通过构建两个相互关联的分类系统提供了一种结构化的方法：一个定义了“评估什么”，另一个解释了“如何评估”。第一个分类系统确定了LLM驱动代理在多轮对话中的关键组成部分及其评估维度，包括任务完成、响应质量、用户体验、记忆和上下文保留以及规划与工具集成。这些组件确保对话代理的表现能够以全面且有意义的方式进行评估。第二个分类系统则聚焦于评估方法学。它将方法分为基于注释的评估、自动指标、结合人工评估与定量测量的混合策略以及利用LLM的自评估方法。此框架不仅涵盖了源自语言理解的传统指标（如BLEU和ROUGE分数），还融入了反映多轮对话动态交互性质的先进技术。|
|**2025-03-27**|**Debate-Driven Multi-Agent LLMs for Phishing Email Detection**|Ngoc Tuong Vy Nguyen et.al.|[2503.22038](http://arxiv.org/abs/2503.22038)|null|网络钓鱼攻击仍然是一个关键的网络安全威胁。攻击者不断改进其方法，使得网络钓鱼电子邮件更难被检测到。传统的检测方法，包括基于规则的系统和有监督的机器学习模型，要么依赖于预定义的模式（如黑名单），这些模式可以通过轻微修改来规避，要么需要大量数据进行训练，但仍会产生误报和漏报。在这项工作中，我们提出了一种多代理大型语言模型（LLM）提示技术，该技术通过模拟代理之间的辩论来检测电子邮件内容是否为网络钓鱼。我们的方法使用两个LLM代理分别提供支持或反对分类任务的论点，由法官代理根据提供的推理质量做出最终裁决。这种辩论机制使模型能够批判性地分析文本中的上下文线索和欺骗模式，从而提高分类准确性。所提出的框架在多个网络钓鱼电子邮件数据集上进行了评估，并证明混合代理配置始终优于同质配置。结果还表明，辩论结构本身足以得出准确的决策，而无需额外的提示策略。|
|**2025-03-27**|**MemInsight: Autonomous Memory Augmentation for LLM Agents**|Rana Salama et.al.|[2503.21760](http://arxiv.org/abs/2503.21760)|null|大型语言模型（LLM）代理已发展出智能处理信息、做出决策并与用户或工具交互的能力。一个关键能力是集成长期记忆功能，使这些代理能够借鉴历史互动和知识。然而，内存大小的增长和语义结构的需求带来了重大挑战。在这项工作中，我们提出了一种自主记忆增强方法MemInsight，以增强语义数据表示和检索机制。通过利用对历史交互的自主增强，LLM代理被证明可以提供更准确和情境化的响应。我们在三个任务场景中实证验证了所提出方法的有效性：会话推荐、问答和事件总结。在LLM-REDIAL数据集上，MemInsight将推荐的说服力提高了多达14%。此外，它在LoCoMo检索中比RAG基线高出34%的召回率。我们的实证结果展示了MemInsight在多个任务中增强LLM代理上下文性能的潜力。|
|**2025-03-27**|**GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics**|Arsham Gholamzadeh Khoee et.al.|[2503.21735](http://arxiv.org/abs/2503.21735)|null|确保软件发布决策的可靠性和有效性在汽车等安全关键领域尤为重要。传统方法依赖于对大量测试数据和验证指标进行人工分析，这种方法容易导致延迟和高成本。大型语言模型（LLMs）提供了一种有前景的替代方案，但它们在分析推理、上下文理解、处理超出范围的查询以及一致地处理结构化测试数据方面存在挑战，这些限制阻碍了它们在安全关键场景中的直接应用。本文介绍了一种基于LLM的工具GateLens，用于分析汽车领域的表格数据。GateLens将自然语言查询转换为关系代数（RA）表达式，并生成优化的Python代码。它在基准数据集上优于基线系统，具有更高的F1分数，并且能够更稳健地处理复杂的模糊查询。消融研究证实了关系代数模块的关键作用，当省略该模块时性能会急剧下降。工业评估显示GateLens将分析时间减少了80％以上，同时保持了高准确率和可靠性。通过展示结果表明GateLens在不依赖少量示例的情况下实现了高性能，展示了其在各种查询类型和不同公司角色中的强大泛化能力。部署GateLens与一家合作伙伴汽车公司的经验提供了关于如何将AI集成到发布验证等关键工作流程中的实用指导。结果显示，通过自动化测试结果分析，GateLens使发布决策更快、更明智、更可靠，从而推动了汽车系统的软件可扩展性和可靠性。|
|**2025-03-27**|**debug-gym: A Text-Based Environment for Interactive Debugging**|Xingdi Yuan et.al.|[2503.21557](http://arxiv.org/abs/2503.21557)|null|大型语言模型（LLMs）越来越多地被用于编码任务，但在大多数情况下假设所有相关信息都可以在上下文中访问或与训练数据匹配。我们认为LLMs可以从与代码库的交互式探索中受益以获取与其任务相关的信息。为此，我们提出了一个文本环境debug-gym，用于开发基于LLM的代理在交互式编码环境中工作。我们的环境轻量级且提供了一些有用的工具，例如Python调试器（pdb），旨在促进基于LLM的代理的交互式调试。除了编码和调试任务外，这种方法还可以推广到其他需要LLM代理信息寻求行为的任务。|
|**2025-03-27**|**Large Language Model Agent: A Survey on Methodology, Applications and Challenges**|Junyu Luo et.al.|[2503.21460](http://arxiv.org/abs/2503.21460)|**[link](https://github.com/luo-junyu/awesome-agent-papers)**|**智能代理的时代已经到来，这得益于大型语言模型的革命性进步。大型语言模型（LLM）代理具有目标驱动的行为和动态适应能力，可能代表了通往通用人工智能的关键路径。本综述通过以方法为中心的分类法系统地解构了LLM代理系统，连接了架构基础、协作机制和演化路径。我们通过揭示代理设计原则与它们在复杂环境中涌现行为之间的基本联系，统一了零散的研究线索。我们的工作提供了统一的架构视角，考察了代理如何构建、如何协作以及如何随着时间推移而演变，同时涉及评估方法、工具应用、实际挑战和多样化的应用领域。通过综述这一快速发展的领域的最新进展，我们为研究人员提供了一个理解LLM代理的结构化分类法，并确定了未来研究的有前景方向。该合集可在https://github.com/luo-junyu/Awesome-Agent-Papers获取。**|
|**2025-03-28**|**EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues**|Yuhan Liu et.al.|[2503.21080](http://arxiv.org/abs/2503.21080)|null|虽然基于大型语言模型（LLM）的聊天机器人在信贷对话中的有效互动方面取得了进展，但它们在动态情感表达方面的能力仍然有限。目前的代理主要依赖被动共情而非情感推理。例如，当面对客户的持续消极情绪时，代理应采用战略性的情感适应，通过表达适度的愤怒来遏制破坏性行为，并引导对话走向解决方案。这种情境感知的情感调节对于模仿人类谈判者的细微决策过程至关重要。本文介绍了一种EQ谈判者，它结合了来自预训练语言模型（PLMs）的情绪感知与基于博弈论和隐马尔可夫模型的情感推理。它考虑了客户当前和历史情绪，以更好地管理和应对交互过程中的负面情绪。通过在公共情绪数据集上微调预训练语言模型（PLMs）并验证信贷对话数据集，我们的方法使基于LLM的代理能够有效捕捉客户情绪的变化，并根据我们的情感动机策略在现实世界的金融谈判中动态调整其响应语气。这种EQ谈判者还可以帮助信用机构培养积极的客户关系，提升信贷服务满意度。|
|**2025-03-26**|**Operating Room Workflow Analysis via Reasoning Segmentation over Digital Twins**|Yiqing Shen et.al.|[2503.21054](http://arxiv.org/abs/2503.21054)|null|分析手术室（OR）工作流程以获得关于OR效率的定量洞察对医院来说对于最大化患者护理和财务可持续性非常重要。以往关于OR层面工作流程分析的研究主要依赖于端到端深度神经网络。虽然这些方法在约束条件下表现良好但它们缺乏适应不同OR场景需求（例如大型学术中心与农村提供商）所需的灵活性而无需进行数据收集注释和重新训练。基于推理分割（RS）的Foundation模型提供了这种灵活性通过仅依靠与感兴趣对象相关的隐式文本查询即可自动分析OR工作流程。然而由于当前RS方法依赖于大型语言模型（LLM）微调因此在推理语义/空间关系方面存在困难并且由于视觉特征变化和领域特定术语的限制表现出有限的泛化能力。为了解决这些局限性我们首先提出了一种新的数字孪生（DT）表示形式该表示形式保留了OR各个组成部分之间的语义和空间关系。在此基础上我们提出了ORDiRS（基于数字孪生表示的推理分割），这是一种无需LLM微调的RS框架它将RS重新定义为“推理检索合成”范式。最后我们介绍了ORDiRS-Agent一种基于LLM的代理该代理将OR工作流程分析查询分解为可管理的RS子查询并通过结合详细的文本解释和支持性的RS可视化证据来生成响应。实验结果表明在内部数据集和公共OR数据集上我们的ORDiRS相比现有最先进的技术实现了cIoU提升6.12%-9.74%|
|**2025-03-27**|**Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs**|Yuxuan Lu et.al.|[2503.20749](http://arxiv.org/abs/2503.20749)|null|近期研究表明，大型语言模型（LLMs）可以通过仅使用提示的方法模拟出“可信”的人类行为以驱动LLM代理。在本研究中，我们聚焦于评估和提升LLMs在网页动作生成任务中的“准确性”，而非主观的“可信性”，并利用从在线购物的人类行为中收集的大规模真实世界数据集进行评估。我们对最先进的LLMs（如DeepSeek-R1、Llama和Claude）在网页动作生成任务上的表现进行了首次全面的定量评估。结果表明，对LLMs进行基于真实行为数据的微调显著提升了它们生成动作的能力，相比仅使用提示的方法效果更好。此外，将合成的推理轨迹纳入模型训练中进一步提高了性能，这表明显式推理在行为建模中的价值。本研究建立了一个新的基准来评估LLMs在行为模拟中的表现，并提供了如何通过真实动作数据和推理增强来提高LLM代理保真度的可行见解。|
|**2025-03-25**|**BugCraft: End-to-End Crash Bug Reproduction Using LLM Agents in Minecraft**|Eray Yapağcı et.al.|[2503.20036](http://arxiv.org/abs/2503.20036)|null|在不断发展的游戏如Minecraft中重现游戏漏洞，特别是崩溃漏洞，是一个手动、耗时且难以自动化的挑战。尽管大型语言模型（LLMs）驱动的错误重现技术在其他软件领域取得了成功，但具有复杂交互环境的游戏领域仍未得到充分解决。本文介绍了一种名为BugCraft的新颖端到端框架，旨在直接从用户提交的错误报告中自动化重现Minecraft中的崩溃错误，填补了游戏错误重现自动化的重要空白。BugCraft采用两阶段方法：首先，步骤合成器结合了LLMs和Minecraft Wiki知识，将错误报告转化为高质量的结构化重现步骤（S2R）。其次，动作模型由基于视觉的LLM代理（GPT-4o）和自定义宏API驱动，在Minecraft内执行这些S2R步骤以触发报告的崩溃。为了便于评估，我们引入了BugCraft-Bench，这是一个精心策划的Minecraft崩溃错误报告数据集。在BugCraft-Bench上的评估显示，我们的框架成功实现了30.23%的崩溃错误端到端重现。步骤合成器在生成正确的错误重现计划方面达到了66.28%的准确率，展示了其在解释和结构化错误报告信息方面的有效性。BugCraft证明了使用LLMs自动化复杂游戏环境中崩溃错误重现的可行性，为游戏测试和开发开辟了有前景的研究方向。该框架及其BugCraft-Bench数据集为未来游戏错误分析的自动化研究铺平了道路，并具有推广到其他互动游戏平台的潜力。最后，我们将代码开源于https://bugcraft2025.github.io/|
|**2025-03-24**|**A Survey of Large Language Model Agents for Question Answering**|Murong Yue et.al.|[2503.19213](http://arxiv.org/abs/2503.19213)|null|本文综述了基于大型语言模型（LLM）的代理在问答（QA）中的发展。传统代理面临显著限制，包括大量数据需求和难以泛化到新环境的问题。基于LLM的代理通过利用LLMs作为核心推理引擎解决了这些挑战。这些代理在QA任务中的表现优于传统的QA流水线和朴素的LLM QA系统，方法是通过与外部环境进行交互实现。我们系统地回顾了LLM代理在QA任务中的设计，围绕规划、问题理解、信息检索和答案生成等关键阶段展开讨论。此外，本文还指出了当前存在的挑战，并探讨了未来的研究方向以提升LLM代理QA系统的性能。|
|**2025-03-24**|**EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments**|Sara Fish et.al.|[2503.18825](http://arxiv.org/abs/2503.18825)|null|我们开发了针对大型语言模型（LLM）代理的基准测试，这些代理在未知环境中行动、学习并制定策略，而环境的具体规格需要通过有目的的探索来逐步学习。我们的基准测试由来自经济学关键问题的决策任务组成，并且这些任务是根据可扩展难度水平合成生成的。此外，我们提出了“石蕊测试”，这是一种衡量LLM和LLM代理的新定量方法。与基准测试不同，石蕊测试通过考虑它们在面对权衡（例如效率与平等）时的行为来量化LLM和LLM代理之间的特性、价值观和倾向差异，在这种情况下没有客观上正确或错误的行为。总体而言，我们的基准测试和石蕊测试评估了LLM代理在解决复杂经济问题方面的能力和倾向，涉及从采购、调度、任务分配到定价等多样化场景的应用——随着这些代理进一步融入经济，这些应用的重要性将日益增加。|
|**2025-03-24**|**Defeating Prompt Injections by Design**|Edoardo Debenedetti et.al.|[2503.18813](http://arxiv.org/abs/2503.18813)|null|大型语言模型（LLMs）越来越多地部署在与外部环境交互的智能体系统中。然而，当处理不可信数据时，LLM 智能体容易受到提示注入攻击的影响。本文提出了一种名为 CaMeL 的鲁棒性防御方法，它在 LLM 周围创建了一个保护层，即使底层模型可能易受攻击，也能确保其安全性。为了实现这一点，CaMeL 明确提取了来自可信查询的控制流和数据流；因此，LLM 检索到的不可信数据永远不会影响程序流程。为了进一步提高安全性，CaMeL 借助能力的概念防止通过未经授权的数据流泄露私人数据。我们通过在 AgentDojo [NeurIPS 2024] 上进行的实验表明，CaMeL 在 67% 的任务中实现了可证明的安全性。|
|**2025-03-24**|**AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents**|Haoyu Wang et.al.|[2503.18666](http://arxiv.org/abs/2503.18666)|null|基于大型语言模型（LLM）构建的代理正越来越多地部署在各个领域，自动化复杂的决策和任务执行。然而，它们的自主性带来了安全风险，包括安全漏洞、法律违规和意外有害行为。现有的缓解方法，如基于模型的安全保障和早期执行策略，在鲁棒性、可解释性和适应性方面存在不足。为了解决这些挑战，我们提出了AgentSpec，这是一种轻量级的领域特定语言，用于在运行时指定和强制执行对LLM代理的约束。通过AgentSpec，用户可以定义包含触发器、谓词和执行机制的结构化规则，确保代理在预定义的安全边界内运行。我们在多个领域实施了AgentSpec，包括代码执行、具身代理和自动驾驶，展示了其适应性和有效性。我们的评估显示，AgentSpec在超过90%的代码代理案例中成功防止了不安全执行，消除了具身代理任务中的所有危险行为，并使自动驾驶汽车（AVs）达到了100%的合规性。尽管具有强大的安全保障，AgentSpec仍然计算高效，开销仅为毫秒级别。通过结合可解释性、模块化和效率，AgentSpec为跨多样化应用的LLM代理安全提供了一个实用且可扩展的解决方案。我们还使用LLMs自动生成规则并评估其效果。我们的评估表明，由OpenAI生成的o1规则在具身代理中达到了95.56%的精确度和70.96%的召回率，成功识别了87.26%的风险代码，并在5个场景中防止了AVs违反法律。|
|**2025-03-23**|**AgentRxiv: Towards Collaborative Autonomous Research**|Samuel Schmidgall et.al.|[2503.18102](http://arxiv.org/abs/2503.18102)|null|科学发现的进步很少是单个“尤里卡时刻”的结果，而是数百名科学家共同努力朝着共同目标迈进的产物。尽管现有的代理工作流能够独立自主地产生研究，但它们无法持续改进之前的研究成果。为了解决这些挑战，我们引入了AgentRxiv框架，该框架允许大型语言模型（LLM）代理实验室上传和检索共享预印本服务器上的报告，以便协作、分享见解并迭代地建立在彼此的研究成果之上。我们将代理实验室的任务设定为开发新的推理和提示技术，并发现拥有访问其先前研究成果的代理相比孤立运作的代理实现了更高的性能提升（在MATH-500基准测试中相对提高了11.4%）。我们发现，表现最佳的策略可以推广到其他领域的基准测试中（平均提高了3.3%）。多个代理实验室通过AgentRxiv共享研究能够更快速地朝着共同目标努力，整体准确性更高（在MATH-500基准测试中相对提高了13.7%）。这些发现表明，自主代理可能在人类的协助下设计未来的AI系统中发挥作用。我们希望AgentRxiv能够让代理协作实现研究目标，并帮助研究人员加速发现进程。|
|**2025-03-23**|**Metaphor-based Jailbreaking Attacks on Text-to-Image Models**|Chenyu Zhang et.al.|[2503.17987](http://arxiv.org/abs/2503.17987)|null|为了减轻滥用风险，文本到图像（T2I）模型通常会引入安全过滤器以防止生成敏感图像。然而，最近的越狱攻击方法利用大型语言模型（LLMs）生成对抗性提示，成功绕过安全过滤器并生成敏感图像，揭示了T2I模型的安全漏洞。但是，现有的基于LLM的攻击方法缺乏明确指导，需要大量的查询才能实现成功的攻击，这限制了它们在现实场景中的实用性。在这项工作中，我们提出了MJA，这是一种受禁忌游戏启发的基于隐喻的越狱攻击（metaphor-based jailbreaking attack）方法，旨在通过生成基于隐喻的对抗性提示来平衡攻击效果和查询效率。具体而言，MJA由两个模块组成：基于LLM的多智能体生成模块（MLAG）和对抗性提示优化模块（APO）。MLAG 将基于隐喻的对抗性提示生成分解为三个子任务：隐喻检索、上下文匹配和对抗性提示生成。随后，MLAG 协调三个基于LLM的智能体，通过探索各种隐喻和上下文来生成多样化的对抗性提示。为了提高攻击效率，APO首先训练一个代理模型来预测对抗性提示的攻击结果，然后设计一种获取策略以自适应地识别最优的对抗性提示。实验表明，与基线方法相比，MJA在更高的攻击有效性的同时减少了所需的查询次数。此外，我们的对抗性提示在各种开源和商业T2I模型之间表现出强大的可迁移性。需要注意的是，本文包含模型生成的内容，其中可能包含冒犯性或令人不安的材料。|
|**2025-03-23**|**An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models**|Riya Naik et.al.|[2503.17936](http://arxiv.org/abs/2503.17936)|null|自然语言作为人机交互的媒介长期以来备受期待，随着大规模语言模型（LLMs）的出现发生了巨大变化，这些模型在处理和生成语言方面展现出惊人的能力。如今，我们许多人将LLMs视为现代的神谕，向其提出几乎任何类型的问题。与它的德尔斐前辈不同，咨询LLM不一定是一次性的活动（提问、回答、离开）；而且——也不同于皮提亚——人们普遍承认LLM的回答可以通过提供更多上下文来改进。在这篇论文中，我们旨在研究何时需要与LLMs进行多轮交互才能成功回答问题；或者得出问题无法回答的结论。我们提出了一个神经符号框架，用于模拟人类与LLM代理之间的交互。通过该框架，我们将问题中的不完整性和模糊性定义为可以从交互过程中交换的消息中推断出的属性，并提供了基准问题的结果，在这些问题中，答案的正确性取决于问题是否表现出我们所识别的不完整性和模糊性。我们的结果显示，对于具有较高比例不完整或模糊问题的数据集，通常需要多轮交互；并且增加交互长度可以减少不完整性和模糊性。结果还表明，我们对不完整性和模糊性的度量可以成为表征与LLM在问答问题上交互的有用工具。|
|**2025-03-22**|**CP-AgentNet: Autonomous and Explainable Communication Protocol Design Using Generative Agents**|Dae Cheol Kwon et.al.|[2503.17850](http://arxiv.org/abs/2503.17850)|null|尽管深度强化学习（DRL）已成为优于现有手工设计通信协议的有力工具，但它面临显著的局限性：1）选择合适的神经网络架构和设置超参数对于实现期望的性能水平至关重要，这需要领域专业知识。2）DRL模型中的决策过程通常是不透明的，常被称为“黑箱”。3）DRL模型是数据饥渴型的。为应对这些挑战，我们提出了CP-AgentNet，这是首个利用生成式代理开发通信网络协议的框架。该方法通过创建一个自主的协议设计系统，大大减少了人为努力。我们开发了LLMA（基于LLM-agents的多址接入）和CPTCP（基于CP-Agent的TCP）以适应异构环境。我们的全面模拟展示了LLMA和CPTCP在使用不同类型协议的节点之间高效共存，并增强了可解释性。|
|**2025-03-22**|**Can LLMs Automate Fact-Checking Article Writing?**|Dhruv Sahnan et.al.|[2503.17684](http://arxiv.org/abs/2503.17684)|null|自动事实核查旨在通过提供工具来支持专业事实核查员以加速手动事实核查。然而，现有的框架未能解决关键步骤——生成适合向公众广泛传播的输出：虽然人类事实核查员通过事实核查文章传达他们的发现，但自动系统通常只生成很少或根本不生成对其评估的解释。在这里，我们旨在弥合这一差距。我们通过与领先的事实核查组织的专家进行一系列访谈来确定此类文章的关键需求。然后，我们开发了QRAFT，这是一种基于LLM的主动式框架，模仿了人类事实核查员的写作流程。最后，我们通过专业人士对QRAFT的评估来衡量其实际实用性。我们的评估显示，尽管QRAFT在性能上优于先前提出的几种文本生成方法，但它与专家撰写的文章相比仍有很大差距。我们希望我们的工作能够推动在这个新且重要的方向上的进一步研究|
|**2025-03-21**|**Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent**|Humza Nusrat et.al.|[2503.17553](http://arxiv.org/abs/2503.17553)|null|放射治疗计划是一个复杂且耗时的过程，常常受到规划者差异性和主观决策的影响。为了解决这些挑战，我们引入了剂量优化语言代理（DOLA），这是一种基于大型语言模型（LLM）的自主代理，旨在优化放射治疗计划的同时严格保护患者隐私。DOLA直接与商业治疗计划系统集成，利用思维链提示、检索增强生成（RAG）和强化学习（RL）。该代理完全在安全的本地基础设施内运行，消除了外部数据共享。我们通过回顾性队列研究评估了DOLA，该队列包括18名接受60 Gy分20次照射的前列腺癌患者，比较了不同模型规模（80亿参数 vs 700亿参数）和优化策略（无RAG、RAG 和 RAG+RL）在10个规划迭代中的表现。结果显示700亿参数模型显著提高了性能，在最终评分上比80亿参数模型高出约16.4%。RAG方法比无RAG基线提升了19.8%，而加入RL加速了收敛过程，展示了检索记忆和强化学习协同作用的优势。最优温度超参数分析表明0.4提供了探索和利用的最佳平衡。这项概念验证研究表明，这是首次成功部署本地托管的LLM代理用于商业放射治疗计划系统内的自主优化。通过扩展人机交互并通过可解释的自然语言推理，DOLA提供了一个可扩展且注重隐私的框架，具有重要的临床实施和工作流程改进潜力。|
|**2025-03-21**|**CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities**|Yuxuan Zhu et.al.|[2503.17332](http://arxiv.org/abs/2503.17332)|**[link](https://github.com/uiuc-kang-lab/cve-bench)**|**大型语言模型（LLM）代理能够自主开展网络攻击，对现有应用程序构成了重大威胁。这一日益增长的风险凸显了建立现实世界基准以评估LLM代理利用Web应用程序漏洞能力的迫切需求。然而，现有的基准测试存在局限性，它们要么局限于抽象的夺旗竞赛，要么缺乏全面覆盖。构建针对真实世界漏洞的基准测试既需要专门知识来重现漏洞利用，也需要系统方法来评估不可预测的威胁。为应对这一挑战，我们引入了CVE-Bench，这是一个基于高危Common Vulnerabilities and Exposures（CVE）的现实世界网络安全基准。在CVE-Bench中，我们设计了一个沙箱框架，使LLM代理能够在模拟真实世界条件的情景下利用易受攻击的Web应用程序，同时有效评估其利用效果。我们的评估显示，最先进的代理框架可以解决高达13%的漏洞。**|
|**2025-03-21**|**A-IDE : Agent-Integrated Denoising Experts**|Uihyun Cho et.al.|[2503.16780](http://arxiv.org/abs/2503.16780)|null|近年来基于深度学习的去噪方法在提高低剂量CT图像质量方面取得了显著进展。然而，由于不同HU分布和多样的解剖特征，单一模型往往难以在多种解剖结构中实现良好的泛化能力。为了解决这一局限性，我们提出了名为Agent-Integrated Denoising Experts (A-IDE) 的框架，该框架通过一个决策导向的大语言模型（LLM）代理管理三个针对特定解剖区域的RED-CNN模型。代理通过分析来自BiomedCLIP的语义线索，动态地将输入的低剂量CT扫描路由到最合适的专家模型。我们强调了这种方法的三大优势：A-IDE 在异构且数据稀缺的环境中表现出色；通过任务分配给多个专家模型，该框架自动防止过拟合；最后，我们的LLM驱动的自主管道消除了对人工干预的需求。在Mayo-2016数据集上的实验评估表明，与单一统一去噪器相比，A-IDE 在RMSE、PSNR 和 SSIM 方面实现了更优的性能。|
|**2025-03-20**|**Towards Agentic Recommender Systems in the Era of Multimodal Large Language Models**|Chengkai Huang et.al.|[2503.16734](http://arxiv.org/abs/2503.16734)|null|大型语言模型（LLMs）的最新突破催生了具有自主能力的人工智能系统，这些系统超越了独立模型的能力。通过赋予LLMs感知外部环境、整合多模态信息并与各种工具交互的能力，这些自主系统在复杂任务中展现出更高的自治性和适应性。这一演变给推荐系统带来了新的机遇：基于LLM的自主推荐系统（LLM-ARS）可以提供更具互动性、上下文感知和主动性的推荐，可能重塑用户体验并扩大推荐系统的应用范围。尽管取得了早期的积极成果，但该领域仍面临诸多根本性挑战，包括如何有效整合外部知识、平衡自治与可控性以及在动态、多模态环境中评估性能。在这篇视角论文中，我们首先对LLM-ARS进行了系统分析：（1）明确核心概念和架构；（2）强调规划、记忆和多模态推理等自主能力如何提升推荐质量；（3）概述安全、效率和终身个性化等领域的重要研究问题。我们还讨论了开放性问题和未来方向，认为LLM-ARS将推动推荐系统创新的下一波浪潮。最终，我们预见了一种向智能、自主和协作推荐体验的范式转变，这种体验更紧密地契合用户不断变化的需求和复杂的决策过程。|
|**2025-03-20**|**Survey on Evaluation of LLM-based Agents**|Asaf Yehudai et.al.|[2503.16416](http://arxiv.org/abs/2503.16416)|null|LLM 基础模型代理的出现代表了人工智能的一个范式转变，使自主系统能够在与动态环境交互时进行规划、推理、使用工具和保持记忆。本文提供了对这些日益强大的代理评估方法的第一份综合调查。我们系统地分析了评估基准和框架在四个关键维度上：（1）基础代理能力，包括规划、工具使用、自我反思和记忆；（2）针对网络、软件工程、科学和对话代理的应用特定基准；（3）通用代理的基准；以及（4）评估代理的框架。我们的分析揭示了新兴趋势，包括向更现实、更具挑战性的评估转变，以及连续更新的基准。我们还确定了未来研究必须解决的关键差距——特别是在评估成本效率、安全性和稳健性方面，以及开发细致入微且可扩展的评估方法方面。本调查描绘了代理评估快速发展的格局，揭示了该领域的新兴趋势，指出了当前的局限性，并提出了未来研究的方向。|
|**2025-03-20**|**Issue2Test: Generating Reproducing Test Cases from Issue Reports**|Noor Nashid et.al.|[2503.16320](http://arxiv.org/abs/2503.16320)|null|自动解决GitHub问题的工具正受到研究人员和从业者的广泛关注，例如以基础模型和LLM（大型语言模型）为基础的提示式代理。成功解决问题的关键一步是创建一个能够准确重现问题的测试用例。这样的测试用例可以指导寻找合适的补丁，并帮助验证补丁是否符合问题的意图。然而，现有的问题重现技术仅取得了中等的成功率。本文提出了一种名为Issue2Test的基于LLM的技术，用于自动生成给定问题报告的重现测试用例。与旨在生成通过测试的自动化回归测试生成器不同，我们的方法旨在生成失败的测试，且该失败具体针对问题描述中的原因。为此，Issue2Test执行三个步骤：（1）理解问题并收集与重现相关的上下文信息（如相关文件和项目特定指南）；（2）生成候选测试用例；以及（3）基于编译和运行时反馈迭代地优化测试用例，直到它失败并且失败原因与问题描述一致。我们在SWT-bench-lite数据集上评估了Issue2Test，结果显示其成功重现了30.4%的问题，相对现有最佳技术提升了40.1%。我们的评估还表明，Issue2Test重现了七种先前技术未能解决的28个问题，总计贡献了所有工具可重现问题中的68.3%。我们希望本方法能推动自动解决GitHub问题这一重要任务的整体进展。|
|**2025-03-20**|**The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement**|Ruihan Yang et.al.|[2503.16024](http://arxiv.org/abs/2503.16024)|null|大型语言模型（LLMs）最近从文本助手发展为具备规划、推理和迭代改进能力的自主代理。虽然数值奖励信号和验证器可以有效地对候选行动进行排名，但它们通常提供的上下文指导有限。相比之下，自然语言反馈更好地与LLMs的生成能力相匹配，提供更丰富且可操作的建议。然而，有效解析和实施这些反馈对于基于LLM的代理来说可能具有挑战性。在这项工作中，我们引入了批评指导改进（CGI），这是一种新颖的双人框架，包括一个探索环境的演员模型和一个生成详细自然语言反馈的评论家模型。通过训练评论家生成细粒度评估和可操作修订，以及演员利用这些批评，我们的方法促进了对替代策略的更稳健探索，同时避免了局部最优。在三个交互式环境中进行的实验表明，CGI大幅超越现有基线。值得注意的是，即使是小型评论家模型也超过了GPT-4的反馈质量。由此产生的演员实现了最先进的性能，展示了显式迭代指导增强LLM代理决策制定的能力。|
|**2025-03-18**|**Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection**|Tharindu Kumarage et.al.|[2503.15552](http://arxiv.org/abs/2503.15552)|null|快速发展的会话代理，特别是由大型语言模型（LLM）驱动的聊天机器人，在社会媒体平台上带来了显著的社会工程（SE）攻击风险。与单次实例检测相比，多轮对话中的SE检测由于其动态特性而更加复杂。减轻这一威胁的关键在于理解SE攻击的操作机制，具体来说是攻击者如何利用漏洞以及受害者的人格特质如何影响其易感性。在这项工作中，我们提出了一个LLM驱动的框架SE-VSim，通过生成多轮对话来模拟SE攻击机制。我们构建了具有不同人格特质的受害者代理，以评估心理档案如何影响对操纵的易感性。基于超过1000段模拟对话的数据集，我们研究了攻击者伪装成招聘人员、资助机构和记者等情景下的攻击策略，试图提取敏感信息。根据这项分析，我们展示了SE-OmniGuard的概念验证，它通过利用受害者的先前知识、评估攻击策略并监控对话中的信息交换来为用户提供个性化保护|
|**2025-03-19**|**SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks**|Yifei Zhou et.al.|[2503.15478](http://arxiv.org/abs/2503.15478)|**[link](https://github.com/facebookresearch/sweet_rl)**|**大型语言模型（LLM）代理需要在真实世界任务中进行多轮交互。然而，现有的用于优化LLM代理的多轮强化学习算法无法在利用LLMs泛化能力的同时进行有效的跨轮归因，目前尚不清楚如何开发此类算法。为研究这一问题，我们首先引入了一个新的基准ColBench，在该基准中，LLM代理与人类合作者进行多轮交互以解决后端编程和前端设计等现实任务。基于此基准，我们提出了一种新颖的RL算法SWEET-RL（通过训练时信息实现的分步评估的RL），该算法使用精心设计的优化目标来训练一个具有访问额外训练时信息权限的评论家模型。评论家为改进策略模型提供分步奖励。我们的实验表明，SWEET-RL在ColBench上的成功率和胜率绝对提高了6%，使Llama-3.1-8B的表现达到了或超过了GPT4-o在现实协作内容创建中的表现。**|
|**2025-03-19**|**Exploring Large Language Models for Word Games:Who is the Spy?**|Chentian Wei et.al.|[2503.15235](http://arxiv.org/abs/2503.15235)|**[link](https://github.com/ct-wei/who-is-the-spy)**|**词语游戏因其基于规则和情境的特性，在自然语言处理（NLP）、博弈论及相关领域具有重要的研究价值。本研究探讨了大型语言模型（LLMs）在词语游戏中的有效参与方式，并提出了一种无需训练的框架。以“谁是卧底”这一经典词语游戏为例，我们引入了一种基于思维链（CoT）的调度框架，使LLMs能够在推断角色词汇和伪装身份等任务中表现出色。我们通过游戏成功率以及LLM代理分析结果的准确性来评估该框架的表现。实验结果证实了该框架的有效性，展示了其在多个数据集上显著提升LLM性能的能力。这项工作凸显了LLMs在掌握结构化游戏环境中情境推理和社会互动方面的潜力。我们的代码已公开发布于https://github.com/ct-wei/Who-is-The-Spy。**|
|**2025-03-19**|**Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models**|Man Fai Wong et.al.|[2503.15129](http://arxiv.org/abs/2503.15129)|null|本文研究了AI辅助编程和大型语言模型（LLM）如何通过GitHub Copilot和Amazon CodeWhisperer等AI工具提升软件开发人员的能力，同时结合人类反馈增强强化学习（RLHF），并通过众包计算来优化文本到代码的生成。此外，我们展示了我们的贝叶斯优化框架如何支持代码生成中的AI对齐，通过分担负反馈收集的任务，强调了收集高质量人类反馈的价值。我们的实证评估证明了这种方法的有效性，展示了LLM代理在改进文本到代码生成方面的潜力。我们的贝叶斯优化框架可以针对特定领域语言进行设计，推动大型语言模型能力与人类反馈在AI辅助编程中的对齐，用于代码生成。|
|**2025-03-19**|**ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents**|Hao Liang et.al.|[2503.14948](http://arxiv.org/abs/2503.14948)|null|协同感知因其能够通过与周围车辆代理的信息交换来增强单个车辆的感知能力而受到广泛关注。然而，现有的协同感知系统受限于用户交互效率低下以及多摄像机照片级真实感可视化方面的挑战。为了解决这些挑战，本文介绍了ChatStitch，首个能够通过自然语言命令结合外部数字资产揭示被遮挡盲区信息的协同感知系统。为了妥善处理复杂的或抽象的命令，ChatStitch采用了一种基于大型语言模型的多代理协作框架。为实现最直观的人类感知，ChatStitch提出了SV-UDIS，这是首个在非全局重叠条件下进行环绕视图无监督深度图像拼接的方法。我们在UDIS-D、MCOV-SLAM公开数据集以及我们自己的现实世界数据集上进行了广泛的实验。具体而言，我们的SV-UDIS方法在UDIS-D数据集上的3、4和5幅图像拼接任务中达到了最先进的性能，PSNR分别提高了9%、17%和21%，SSIM分别提高了8%、18%和26%。|
|**2025-03-18**|**Gricean Norms as a Basis for Effective Collaboration**|Fardin Saad et.al.|[2503.14484](http://arxiv.org/abs/2503.14484)|**[link](https://github.com/fardinsaad/gricean-norms)**|**有效的人机-AI协作不仅依赖于AI代理遵循明确指令的能力，还取决于其处理模糊、不完整、无效或无关交流的能力。Gricean会话和推理规范通过协调模糊指令与合作原则来促进协作。我们提出了一种规范框架，将Gricean规范与认知框架（共同基础、相关性理论和心智理论）整合到基于大型语言模型（LLM）的代理中。该规范框架采用Gricean准则中的数量、质量、关系和方式，以及推理作为Gricean规范来解释不清楚的指令：模糊、不完整、无效或无关。在此框架下，我们引入了Lamoids，即由GPT-4驱动的代理，旨在与人类协作。为了评估Gricean规范在人机-AI协作中的影响，我们评估了两种版本的Lamoid：一种带有规范，另一种没有。在实验中，Lamoid与人类在一个网格世界（门、钥匙和宝石）中合作，通过解释清晰和不清晰的自然语言指令来实现共同目标。我们的结果显示，带有Gricean规范的Lamoid在任务准确性上更高，并且生成更清晰、更准确且上下文相关的响应。这种改进源于规范框架，它增强了代理的实用推理能力，促进了有效的人机-AI协作，并使基于LLM的代理能够进行上下文感知的通信**|
|**2025-03-18**|**PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play**|Wei Fang et.al.|[2503.14432](http://arxiv.org/abs/2503.14432)|null|大型语言模型（LLMs）越来越多地与专业外部工具集成，但许多任务需要零样本工具使用且文档有限或噪声较大。现有的解决方案依赖于手动重写或标记数据进行验证，这使得它们在真正的零样本设置下不适用。为了解决这些挑战，我们提出了PLAY2PROMPT，这是一种自动框架，通过与每个工具的迭代试错过程系统地“玩耍”来探索其输入-输出行为。通过这一过程，PLAY2PROMPT可以完善工具文档并生成使用示例，而无需任何标记数据。这些示例不仅指导LLM推理，还作为验证手段进一步提升工具利用效果。广泛的实验证明，在真实任务中PLAY2PROMPT显著提高了开放和闭源模型的零样本工具性能，提供了一种可扩展且有效的领域特定工具集成解决方案。|
|**2025-03-18**|**MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG and Multi-Agent LLM Collaboration**|Yisen Xu et.al.|[2503.14340](http://arxiv.org/abs/2503.14340)|null|维护和扩展软件系统高度依赖于有效的代码重构，但这一过程仍然劳动密集，需要开发人员仔细分析现有代码库并防止引入新的缺陷。尽管最近的进展利用了大型语言模型（LLMs）来自动化重构任务，但当前的解决方案在范围上受到限制，并缺乏保证代码可编译性和成功测试执行的机制。在这项工作中，我们介绍了MANTRA，这是一种全面的LLM代理框架，用于自动化方法级重构。MANTRA集成了上下文感知检索增强生成、协调的多代理协作以及语言强化学习，以模拟人类在重构过程中的决策，同时保持代码的正确性和可读性。我们的实证研究基于从10个代表性Java项目中提取的703个“纯重构”实例（即仅涉及结构改进的代码更改），涵盖了六种最常见的重构操作。实验结果表明，MANTRA显著超越了基线LLM模型（RawGPT），其生成的代码能够编译并通过所有测试的成功率为82.8%（582/703），而RawGPT仅为8.7%（61/703）。此外，与IntelliJ的LLM驱动的重构工具（EM-Assist）相比，MANTRA在生成提取方法转换方面表现出50%的提升。一项涉及37名专业开发人员的可用性研究表明，由MANTRA执行的重构被感知为与人工编写的代码一样具有可读性和可重用性，在某些情况下甚至更受欢迎。这些结果突显了MANTRA的实际优势，并强调了LLM为基础的系统在推进软件重构任务自动化方面的潜力|
|**2025-03-17**|**Why Do Multi-Agent LLM Systems Fail?**|Mert Cemri et.al.|[2503.13657](http://arxiv.org/abs/2503.13657)|**[link](https://github.com/multi-agent-systems-failure-taxonomy/MASFT)**|尽管多智能体系统（MAS）领域日益受到关注，但多个大型语言模型（LLM）代理协作完成任务的表现相较于单代理框架在流行基准测试中的表现提升有限。这一差距凸显了分析阻碍MAS效果的挑战的必要性。本文呈现了对MAS挑战的首次全面研究。我们分析了五个流行的MAS框架下的超过150项任务，涉及六位专家人类注释员。我们识别出14种独特的失败模式，并提出了一种适用于各种MAS框架的综合分类法。该分类法从三位专家注释员每项研究的共识中迭代产生，Cohen's Kappa评分为0.88。这些细化的失败模式被归类为三类：(i) 规格和系统设计失败、(ii) 代理间不一致、(iii) 任务验证与终止。为了支持可扩展评估，我们将MASFT与LLM作为裁判集成。我们还探讨了所识别的失败是否可以通过提出两种干预措施轻松避免：改进代理角色的规格和增强协调策略。我们的发现表明，这些识别出的失败需要更复杂的解决方案，这为未来的研究提供了明确的方向。我们将数据集和LLM注释员开源。|
|**2025-03-17**|**DAgent: A Relational Database-Driven Data Analysis Report Generation Agent**|Wenyi Xu et.al.|[2503.13269](http://arxiv.org/abs/2503.13269)|null|关系数据库驱动的数据分析（RDB-DA）报告生成旨在通过查询关系数据库生成数据分析报告，已在金融和医疗等领域广泛应用。通常，这些任务由数据科学家手动完成，导致过程非常耗时且劳动密集，显示出对自动化的需求。尽管已有方法（如表格问答或文本到SQL）被提出以减少人为依赖，但它们无法处理需要多步推理、跨表关联以及将见解整合到报告中的复杂分析任务。此外，尚无针对自动RDB-DA报告生成的数据集可用。为填补这一空白，本文提出了一个用于RDB-DA报告生成任务的LLM代理系统，称为DAgent；同时构建了一个自动数据分析报告生成基准，其中包括一个新的数据集DA-Dataset和评估指标。DAgent集成了规划工具和记忆模块，可将自然语言问题分解为逻辑上独立的子查询，从关系数据库中准确检索关键信息，并通过多步推理和有效数据集成生成满足完整性、正确性和简洁性要求的分析报告。在DA-Dataset上的实验分析表明，DAgent在检索性能和分析报告生成质量方面表现出色，展示了其在处理复杂数据库分析报告生成任务方面的强大潜力。|
|**2025-03-16**|**VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures**|Yoo Yeon Sung et.al.|[2503.12651](http://arxiv.org/abs/2503.12651)|null|AI从业者越来越多地在复杂的推理任务中使用大型语言模型（LLM）代理来构建复合AI系统。然而，这些代理的执行往往无法达到人类的标准，导致错误的发生，从而损害系统的整体性能。由于代理的不透明推理过程、与人类期望的不一致、代理之间复杂依赖性以及人工检查的高成本，通过人工干预解决这些失败变得极具挑战性。因此，本文介绍了一种以人类为中心的评估框架——VeriLA（Verifying LLM Agent Failures），该框架系统性地评估代理的失败情况，以减少人类的努力并使这些失败对人类更具可解释性。该框架首先通过精心设计的人类制定的代理标准明确每个代理的预期行为。然后，开发了一个与人类标准对齐的代理验证器模块，并用人类黄金标准进行训练，以评估每个代理的执行输出。这种方法能够从人类标准的角度对每个代理的表现进行细致评估，提供明确的修订指南，同时减轻人类的认知负担。我们的案例研究结果表明，VeriLA既具有可解释性又高效，有助于从业者更有效地与系统互动。通过在人机协作中坚持问责制，VeriLA为构建更值得信赖和与人类对齐的复合AI系统铺平了道路。|
|**2025-03-16**|**A Survey on the Optimization of Large Language Model-based Agents**|Shangheng Du et.al.|[2503.12434](http://arxiv.org/abs/2503.12434)|null|随着大型语言模型（LLMs）的快速发展，基于LLMs的智能体已在各个领域得到广泛应用，成为自主决策和交互任务的重要工具。然而，目前的研究通常依赖于对原始LLMs进行提示设计或微调策略，这往往导致在复杂智能体相关环境中的效果有限或性能不佳。尽管LLM优化技术可以提高模型在许多通用任务上的表现，但它们缺乏针对智能体关键功能（如长期规划、动态环境交互和复杂决策制定）的专业化优化。虽然近期有许多研究探索了多种优化基于LLMs的智能体以应对复杂智能体任务的策略，但从整体视角对这些方法进行全面总结与比较的工作仍然不足。本文提供了一个全面综述，涵盖了基于LLMs的智能体优化方法，将其分为参数驱动和无参优化两大类。首先聚焦于参数驱动优化，包括基于微调的优化、基于强化学习的优化以及混合策略，分析了诸如轨迹数据构建、微调技术、奖励函数设计及优化算法等关键方面。此外，我们简要讨论了通过提示工程和外部知识检索来优化智能体行为的无参策略。最后，我们总结了用于评估和调整的基准数据集，回顾了LLM智能体的关键应用，并探讨了主要挑战和有前景的未来方向。我们的相关参考文献仓库地址为https://github.com/YoungDubbyDu/LLM-Agent-Optimization。|
|**2025-03-15**|**TFHE-Coder: Evaluating LLM-agentic Fully Homomorphic Encryption Code Generation**|Mayank Kumar et.al.|[2503.12217](http://arxiv.org/abs/2503.12217)|null|全同态加密（TFHE）在密文上执行计算而不解密的能力使其成为安全和保密计算的核心技术。尽管它在隐私保护机器学习、安全多方计算、私有区块链交易和安全医疗诊断等领域具有潜力，但其应用仍受到限制，主要由于加密的复杂性和可用性挑战。虽然存在各种TFHE库和编译器，但实用的代码生成仍然是一个障碍。我们提出了一种集成编译器框架来评估大型语言模型（LLM）推理和基于代理的优化在TFHE代码生成中的应用，重点关注逻辑门和ReLU激活函数。我们的方法评估了错误率、可编译性和结构相似性，涵盖了开源和闭源LLMs。结果表明，现成模型存在显著局限性，而基于代理的优化如检索增强生成（RAG）和少量提示可以减少错误并提高代码保真度。这项工作建立了首个TFHE代码生成基准，展示了当LLMs与领域特定反馈结合使用时，如何弥合FHE代码生成方面的专业知识差距。|
|**2025-03-15**|**Multi-Agent Systems Execute Arbitrary Malicious Code**|Harold Triedman et.al.|[2503.12188](http://arxiv.org/abs/2503.12188)|null|多智能体系统通过基于大型语言模型的智能体协同工作以完成用户任务。在实际应用中，多智能体系统不可避免地会与不受信任的输入进行交互，例如恶意网页内容、文件、电子邮件附件等。我们以几个最近提出的多智能体框架为例，展示了攻击者可以通过这些不受信任的输入劫持系统的控制和通信机制，从而调用不安全的智能体和功能。这可能导致严重的安全漏洞，包括在用户的设备上执行任意恶意代码或从用户的容器化环境中窃取敏感数据。我们证明，即使单个智能体本身不易受直接或间接提示注入攻击，并且拒绝执行有害操作，这种控制流劫持攻击仍然可以成功。|
|**2025-03-15**|**AgentDroid: A Multi-Agent Framework for Detecting Fraudulent Android Applications**|Ruwei Pan et.al.|[2503.12163](http://arxiv.org/abs/2503.12163)|null|随着假冒和恶意Android应用程序等欺诈性应用的日益增多，高精度且具有适应性的检测方法变得至关重要。本文介绍了一种名为AgentDroid的新框架，该框架基于多模态分析和多智能体系统用于Android欺诈应用程序检测。AgentDroid克服了传统检测方法的局限性，例如无法处理多模态数据以及高误报率等问题。它对Android应用程序进行处理并提取一系列多模态数据以供分析。多个基于大型语言模型（LLM）的智能体承担特定角色，它们分析相关数据并协作以有效检测复杂欺诈行为。我们在包含各种类别欺诈应用和合法应用的数据集上构建并验证了我们的框架。实验结果表明，我们基于GPT-4o的多智能体框架达到了91.7%的准确率和91.68%的F1分数，显示出相较于基准方法的检测准确性提升。|
|**2025-03-15**|**Is Multi-Agent Debate (MAD) the Silver Bullet? An Empirical Analysis of MAD in Code Summarization and Translation**|Jina Chun et.al.|[2503.12029](http://arxiv.org/abs/2503.12029)|null|大型语言模型（LLMs）在自主代理的规划和决策方面取得了进展，但在需要多样化专业知识和多步骤推理的复杂任务上仍然存在困难。多代理辩论（MAD）系统通过引入NLP研究中的结构化辩论机制解决了这一问题，通过使基于LLMs的代理之间进行动态交互和迭代精进来完善解决方案。MAD促进了发散性思维，通过角色特定的代理、动态互动和结构化决策实现。鉴于软件工程（SE）与协作人类解决问题之间的相似之处，本研究调查了MAD系统在两个SE任务中的有效性。我们对MAD系统进行了适应性调整，分析了代理之间的互动以评估共识构建和迭代改进，并针对观察到的弱点提出了两种增强方法。我们的研究结果表明，结构化的辩论和协作改善了解决问题的能力，并在某些情况下表现出了强大的性能，这凸显了MAD在SE自动化中的潜力，同时指出了需要进一步探索的领域。|
|**2025-03-18**|**SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning**|Edward Y. Chang et.al.|[2503.11951](http://arxiv.org/abs/2503.11951)|**[link](https://github.com/genglongling/SagaLLM)**|近年来基于大型语言模型（LLM）的代理框架在任务委派和工作流编排方面展示了令人印象深刻的性能，但面临保持上下文意识和确保规划一致性的重大挑战。本文提出SagaLLM，这是一种结构化的多代理框架，解决了当前LLM方法中的四个基本限制：自我验证不足、上下文缩小、缺乏事务属性以及代理间协调不足。通过实施专门的上下文管理代理和验证协议，SagaLLM在整个复杂的规划过程中保留了关键约束和状态信息，即使在出现中断的情况下也能实现稳健且一致的决策。我们使用REALM基准中的选定问题来评估我们的方法，重点关注挑战上下文保留和适应性推理的顺序和反应式规划场景。我们的实验使用最先进的LLMs，包括Claude 3.7、DeepSeek R1、GPT-4o和GPT-o1，表明尽管这些模型在推理能力上表现出色，但在复杂规划任务中尤其是在适应意外变化时，它们难以保持全局约束意识。相比之下，SagaLLM的分布式认知架构在各种场景中在规划一致性、约束执行和对中断的适应性方面显示出显著改进。|
|**2025-03-15**|**End-to-End Edge AI Service Provisioning Framework in 6G ORAN**|Yun Tang et.al.|[2503.11933](http://arxiv.org/abs/2503.11933)|null|随着6G的到来，开放无线电接入网络（O-RAN）架构正在演进以支持智能、自适应和自动化的网络编排。本文提出了一种新的边缘人工智能和网络服务编排框架，该框架利用部署为O-RAN rApp的大语言模型（LLM）代理。所提出的LLM代理驱动系统通过将用户的用例描述转换为可部署的人工智能服务和相应的网络配置，实现了交互式和直观的编排。LLM代理自动化多个任务，包括从存储库（例如Hugging Face）选择AI模型、服务部署、网络适配以及通过xApp进行实时监控。我们使用开源O-RAN项目（OpenAirInterface和FlexRIC）实现了一个原型，以展示我们框架的功能性和可行性。我们的演示展示了从用户交互到网络适配的端到端人工智能服务编排流程，确保服务质量（QoS）合规性。这项工作强调了将LLM驱动的自动化集成到6G O-RAN生态系统中的潜力，为更易于访问和高效的边缘人工智能生态系统铺平了道路。|
|**2025-03-14**|**CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control**|Zirui Yuan et.al.|[2503.11739](http://arxiv.org/abs/2503.11739)|**[link](https://github.com/usail-hkust/CoLLMLight)**|交通信号控制（TSC）在城市交通管理中起着至关重要的作用，通过优化交通流和缓解拥堵来提升道路效率。尽管大型语言模型（LLMs）最近作为解决TSC问题的有前景工具崭露头角，但由于其缺乏对多智能体协作的关注，现有方法在实现网络级优化方面存在局限性。为了解决这一问题，我们提出了CoLLMLight，这是一种用于TSC的协同LLM代理框架。具体来说，我们首先构建了一个结构化的时空图，以捕捉实时交通动态和相邻交叉口之间的空间关系，使LLM能够推理复杂的交通交互。此外，我们引入了一种基于复杂度的推理机制，该机制根据实时交通状况动态调整推理深度，确保在不牺牲决策质量的前提下实现最优的计算效率。除此之外，我们还提出了一种迭代仿真驱动的数据收集和环境反馈策略，用以构建轻量级LLM，使其适用于协同TSC任务。在合成数据集和真实世界数据集上的大量实验表明，CoLLMLight在各种交通场景中均优于最先进的方法，展示了其有效性、可扩展性和鲁棒性。|
|**2025-03-14**|**Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment, Distribution, and Discovery**|Balaji Rama et.al.|[2503.11444](http://arxiv.org/abs/2503.11444)|**[link](https://github.com/agiresearch/cerebrum)**|自主LLM（大规模语言模型）驱动的智能体在复杂任务执行方面展现出了强大的能力，然而该领域缺乏标准化的工具来支持智能体的开发、部署、分发和发现。我们提出了Cerebrum，这是AIOS平台的一个智能体SDK，旨在通过三个关键组件解决这一问题：（1）一个全面的SDK，具备模块化的四层架构，用于智能体开发，涵盖了LLM、记忆、存储和工具管理；（2）一个社区驱动的智能体中心，用于共享和发现智能体，并提供版本控制和依赖管理功能；（3）一个交互式网页界面，用于测试和评估智能体。该平台的有效性通过多种智能体架构的实现得到了验证，包括链式思维（CoT）、ReAct以及工具使用型智能体。Cerebrum通过提供统一框架，在标准化智能体开发的同时，为研究人员和开发者提供了创新和分发智能体的灵活性。在线网站地址为https://app.aios.foundation，代码仓库位于https://github.com/agiresearch/Cerebrum，视频演示可在https://app.aios.foundation/video-demo查看。|
|**2025-03-14**|**API Agents vs. GUI Agents: Divergence and Convergence**|Chaoyun Zhang et.al.|[2503.11069](http://arxiv.org/abs/2503.11069)|null|大型语言模型（LLMs）已经超越了简单的文本生成，发展成为能够直接将自然语言指令转化为具体操作的软件代理。虽然基于API的LLM代理最初因其强大的自动化能力和与程序化端点的无缝集成而备受瞩目，但最近多模态LLM研究的进步使得基于GUI的LLM代理得以实现，这些代理以类人的方式与图形用户界面进行交互。尽管这两种范式都旨在实现由LLM驱动的任务自动化，但在架构复杂性、开发工作流程和用户交互模型上存在显著差异。本文对基于API的和基于GUI的LLM代理进行了首次全面的对比研究，系统地分析了它们的分歧与潜在的融合。我们考察了关键维度，并指出了在哪些场景下混合方法可以发挥它们互补的优势。通过提出明确的选择标准并展示实际用例，我们希望指导从业者和研究人员在选择、结合或过渡这些范式时有所依据。最终，我们认为LLM驱动的自动化持续创新有望模糊基于API和基于GUI的代理之间的界限，为广泛的实际应用铺平道路，带来更加灵活和适应性强的解决方案。|
|**2025-03-14**|**BannerAgency: Advertising Banner Design with Multimodal LLM Agents**|Heng Wang et.al.|[2503.11060](http://arxiv.org/abs/2503.11060)|null|广告横幅对于吸引用户注意力和提升广告活动效果至关重要。然而，由于涉及多个设计元素的搜索空间较大且设计过程具有迭代性和主观性，创建美观且能传达广告信息的横幅设计极具挑战性。此外，为了适应不同尺寸的显示设备以及针对不同受众群体，广告商通常需要多种版本的横幅设计。尽管目前已有模型作为人类设计师的助手参与了各种设计任务，但它们通常只能处理创意设计过程中的部分环节，或者生成像素级输出从而限制了可编辑性。本文介绍了一个无需训练的框架，用于全自动的广告横幅设计创作，使前沿的多模态大型语言模型（MLLM）能够在最少人工干预的情况下，在多样化的营销背景下优化广告效果。我们提出了BannerAgency系统，该系统通过与广告商协作理解其品牌标识和横幅目标，生成匹配的背景图像，设计前景元素的蓝图，并以Figma或SVG格式渲染最终的可编辑组件，而非静态像素。为促进评估和未来研究，我们推出了BannerRequest400基准数据集，其中包括100个独特的标志与400个不同的横幅需求。通过定量和定性评估，我们展示了该框架的有效性，重点在于生成横幅设计的质量、其对多样化横幅需求的适应性以及通过这种基于组件的方法实现的强大可编辑性。|
|**2025-03-13**|**Teamwork makes the dream work: LLMs-Based Agents for GitHub README.MD Summarization**|Duc S. H. Nguyen et.al.|[2503.10876](http://arxiv.org/abs/2503.10876)|null|大型语言模型（LLMs）近年来的普及在各个领域实现了许多应用。通过从各种来源训练大量数据，LLMs 可以被部署来解决不同的任务，包括软件工程（SE）。尽管它们已被广泛应用，但使用多个 LLMs 进行协作的潜力尚未得到充分研究。在本文中，我们提出了 Metagente 作为一种新颖的方法来增强多个 LLMs 的协同作用。Metagente 是一个基于多个 LLMs 的多智能体框架，通过评估、反馈和不同专业智能体之间的合作来自我优化系统。这种框架创建了一个环境，其中多个智能体迭代地从不同角度优化提示。这些探索的结果随后由教师智能体进行审查和聚合。为了研究其性能，我们用软件工程任务（即 README.MD 文件的总结）评估了 Metagente，并将其与三个已建立的基线（即 GitSum、LLaMA-2 和 GPT-4o）进行了比较。结果表明，我们提出的方法效率高且效果显著，仅需少量数据进行微调即可获得高精度，从而大大优于基线。与最相关的基准 GitSum 相比，性能提升范围从 27.63% 到 60.43%。更重要的是，与仅使用一个 LLM 相比，Metagente 将准确性提高到多倍。|
|**2025-03-13**|**Capturing Semantic Flow of ML-based Systems**|Shin Yoo et.al.|[2503.10310](http://arxiv.org/abs/2503.10310)|null|基于机器学习的系统是整合了机器学习组件（如深度神经网络DNN或大型语言模型LLM）的软件系统。这类系统能够实现诸如高性能计算机视觉、自然语言处理和代码生成等先进功能，但其内部行为对于传统动态分析方法（例如测试）来说仍然很大程度上是不透明的：现有的分析通常仅关注从外部可观察到的内容，如输入相似性或类别标签的变化。我们提出了语义流的概念，旨在捕捉基于机器学习系统的内部行为，并为传统动态分析技术适应于此类系统提供平台。语义流结合了控制流的思想以及来自ML系统执行的内部状态，例如DNN特定层的激活值或LLM代理在特定推理步骤中的嵌入值。由此产生的表示形式，总结为语义流图，可以捕获在传统的ML系统控制流中未明确表示的内部决策。我们介绍了语义流的概念，并通过DNN和LLM代理的两个示例进行说明，最后概述了其特性以及如何将其用于适应现有动态分析技术以应用于基于机器学习的软件系统。|
|**2025-03-13**|**LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns**|Idan Horowitz et.al.|[2503.10248](http://arxiv.org/abs/2503.10248)|null|我们研究了大型语言模型（LLMs）在涉及重复选择和从反馈中学习的决策任务中的选择模式，并将其行为与人类参与者进行比较。我们发现，在总体上，LLMs的行为似乎与人类表现出相似的偏差：两者都对罕见事件赋予较低权重并出现相关效应。然而，更细致的分析显示，这种现象的原因截然不同。LLMs表现出强烈的近因偏差，而人类则以更复杂的方式作出反应。尽管这两种过程可能导致平均行为相似，但基于最近事件的选择模式在两组之间存在巨大差异。具体而言，“意外触发变化”和“罕见事件的波动近因效应”等现象在人类中稳健存在，但在LLMs中完全不存在。我们的研究结果揭示了使用LLMs模拟和预测人类在学习环境中的行为时的局限性，并强调了在调查其是否复制人类决策倾向时需要进行更精细的行为分析。|
|**2025-03-13**|**Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop Framework Using LLM**|Mohd Ariful Haque et.al.|[2503.10071](http://arxiv.org/abs/2503.10071)|null|LLM代理与外部工具的结合使模型能够解决超出其知识库的复杂任务。人为设计的工具缺乏灵活性，局限于专家预先创建的解决方案范围。为了解决这个问题，我们提出了ATLASS，这是一种先进的工具学习和选择系统，设计为一个闭环框架。它使LLM能够在需要时动态生成外部工具。在这个框架中，代理在工具选择、执行和优化中发挥关键作用，确保适应性问题解决能力。ATLASS的操作分为三个阶段：第一阶段是理解工具需求，代理确定是否需要工具并指定其功能；第二阶段是工具检索/生成，代理根据工具的可用性检索或生成工具；第三阶段是任务解决，涉及组合所有必要的组件工具以完成初始任务。工具数据集存储生成的工具，确保可重用性并最小化推理成本。当前基于LLM的工具生成系统在创建需要API或外部包的复杂工具方面存在困难。在ATLASS中，我们通过自动设置环境、在线获取相关API文档以及使用Python解释器来创建可靠且通用的工具，使其适用于更广泛的情况。我们使用OpenAI GPT-4.0作为LLM代理，并通过人类反馈处理生成代码执行前的安全和伦理问题。通过解决预定义工具集的局限性并增强适应性，ATLASS作为一个现实世界的解决方案，为用户提供动态生成的工具以解决复杂问题。|
|**2025-03-13**|**OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model**|Bowen Zhang et.al.|[2503.10009](http://arxiv.org/abs/2503.10009)|**[link](https://github.com/bwz96sco/or_llm_agent)**|**运筹学（OR）已在资源分配、生产计划和供应链管理等诸多领域得到广泛应用。然而，解决现实世界中的OR问题需要OR专家进行数学建模，同时程序员开发求解算法。这种传统方法高度依赖专家，成本高昂且开发周期长，严重限制了OR技术的广泛应用。目前鲜有研究考虑利用人工智能（AI）替代专业人士以实现OR问题的全自动解决方案。我们提出了OR-LLM-Agent，这是首个能够端到端自动化解决现实世界OR问题的AI代理。OR-LLM-Agent利用大型语言模型（LLMs）的链式思维（CoT）推理能力，将自然语言问题描述转化为正式的数学模型，并自动生成Gurobi求解器代码。在OR-LLM-Agent中，OR-CodeAgent被设计用于在沙盒环境中自动执行和修复代码，从而推导出最终解决方案。由于缺乏专门的基准数据集来评估OR问题的自动化求解，我们构建了一个包含83个现实世界OR问题的基准数据集，这些问题用自然语言描述。我们通过与最先进的推理LLMs（包括GPT-o3-mini DeepSeek-R1和Gemini 2.0 Flash Thinking）进行对比实验。结果表明，OR-LLM-Agent达到了100%的最高通过率和85%的最高求解准确率，证明了全自动求解OR问题的可行性。数据和代码已公开发布于https://github.com/bwz96sco/or_llm_agent**|
|**2025-03-12**|**A Survey on Trustworthy LLM Agents: Threats and Countermeasures**|Miao Yu et.al.|[2503.09648](http://arxiv.org/abs/2503.09648)|**[link](https://github.com/Ymm-cll/TrustAgent)**|随着大规模语言模型（LLMs）的快速发展，基于LLM的代理和多智能体系统（MAS）显著扩展了LLM生态系统的功能。这一进步源于为LLM赋予更多模块，例如记忆、工具、环境以及甚至其他代理。然而，这种发展也引入了更为复杂的可信性问题，以往仅关注LLM的研究无法涵盖这些问题。在这篇综述中，我们提出了TrustAgent框架，这是一项关于代理可信性的全面研究，其特点在于模块化分类、多维度内涵和技术实现。通过深入调查和总结新出现的攻击、防御措施以及评估方法，我们将可信的大规模语言模型的概念扩展到新兴的可信代理范式。在TrustAgent中，我们首先分解并介绍代理和MAS的各种组件。然后，我们将它们的可信性分为内在（大脑、记忆和工具）和外在（用户、代理和环境）方面。随后，我们详细阐述可信性的多层面含义，并深入探讨与这些内部和外部模块相关的现有研究的技术实现。最后，我们提出对该领域的见解和展望，旨在为未来的研究提供指导。|
|**2025-03-12**|**Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks**|Lutfi Eren Erdogan et.al.|[2503.09572](http://arxiv.org/abs/2503.09572)|null|大型语言模型（LLMs）在使语言代理处理简单任务方面表现出显著的进步。然而，将其应用于复杂、多步、长时程的任务仍然是一项挑战。近期的研究通过将高级规划与低级执行分离，成功地使模型能够有效地平衡高级规划目标和低级执行细节。然而，生成准确的计划仍然很困难，因为LLMs并非专门为此任务而训练的。为了解决这个问题，我们提出了Plan-and-Act，这是一种新颖的框架，它将明确的规划纳入基于LLM的代理，并引入了一种可扩展的方法来通过一种新的合成数据生成方法增强计划生成。Plan-and-Act由一个规划器模型和一个执行器模型组成，其中规划器模型生成结构化的高级计划以实现用户目标，而执行器模型则将这些计划转化为特定环境中的操作。为了有效训练规划器，我们引入了一种合成数据生成方法，该方法使用真实轨迹标注真实的可行计划，并辅以多样化和广泛的示例以增强泛化能力。我们在网络导航作为代表性长时程规划环境上评估了Plan-and-Act，结果表明其在WebArena-Lite基准测试上的成功率达到了最先进的54%。|
|**2025-03-12**|**COLA: A Scalable Multi-Agent Framework For Windows UI Task Automation**|Di Zhao et.al.|[2503.09263](http://arxiv.org/abs/2503.09263)|**[link](https://github.com/alokia/cola-demo)**|**随着大型语言模型（LLMs）的快速发展，越来越多的研究利用LLMs作为智能体的认知核心来解决复杂的任务决策挑战。特别是近期研究展示了基于LLM的智能体在自动化Windows图形用户界面（GUI）操作方面的潜力。然而，现有的方法表现出两个关键挑战：（1）静态智能体架构无法动态适应操作系统级任务的异构需求，导致场景泛化能力不足；（2）智能体工作流缺乏容错机制，需要对UI智能体的决策错误进行整个流程的重新执行。为了解决这些局限性，我们引入了COLA（Collaborative Multi-Agent Framework for Automating Windows UI Operations），这是一个协作多智能体框架，用于自动化Windows用户界面操作。在此框架中，场景感知的任务调度器将任务需求分解为原子化的功能单元，动态选择最优智能体从决策智能体池中响应多样化场景的能力需求。决策智能体池支持即插即用扩展以增强灵活性。此外，我们为所有智能体设计了一个记忆单元，用于其自我进化。另外，我们开发了一种交互式回溯机制，使人工干预能够触发状态回滚，从而实现无损的过程修复。我们在GAIA基准上的实验结果表明，COLA框架达到了最先进的性能，平均得分为31.89%，显著优于没有Web API集成的基线方法。消融研究进一步验证了动态调度的个体贡献。代码可在https://github.com/Alokia/COLA-demo获取。**|
|**2025-03-12**|**LocAgent: Graph-Guided LLM Agents for Code Localization**|Zhaoling Chen et.al.|[2503.09089](http://arxiv.org/abs/2503.09089)|**[link](https://github.com/gersteinlab/locagent)**|**代码定位——即准确识别代码库中需要进行更改的具体位置——是软件维护中一项基础但具有挑战性的任务。现有的方法在处理复杂代码库时往往难以高效地定位相关的代码部分。这一挑战在于如何将自然语言的问题描述与适当的代码元素联系起来，通常需要跨越层次结构和多个依赖关系进行推理。我们引入了LocAgent框架，通过基于图的方法来解决代码定位问题。LocAgent将代码库解析为有向异构图，创建了一种轻量级的表示形式，该形式捕获了代码结构（文件、类、函数）及其依赖关系（导入、调用、继承），使大型语言模型代理能够通过强大的多跳推理有效地搜索和定位相关实体。实验证明，在真实世界的基准测试中，我们的方法显著提高了代码定位的准确性。特别是，使用经过微调的Qwen-2.5-Coder-Instruct-32B模型的方法达到了与最先进的专有模型相当的结果，但成本大大降低（约减少了86%），在文件级定位上达到了高达92.7%的准确率，并且在多次尝试（Pass@10）的情况下将GitHub问题解决的成功率提高了12%。我们的代码可以在https://github.com/gersteinlab/LocAgent获取。**|
|**2025-03-11**|**ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews**|Xian Gao et.al.|[2503.08506](http://arxiv.org/abs/2503.08506)|null|学术论文评审是研究社区中的关键但耗时的任务。随着学术发表数量的增加，自动化评审过程已成为一个重大挑战。主要问题在于生成的评审意见需要全面、准确且推理一致，并与人类评审员的判断相一致。在本文中，我们通过提出ReviewAgents框架来应对这一挑战，该框架利用大型语言模型（LLMs）生成学术论文评审。首先，我们介绍了一个新的数据集Review-CoT，包含142k条评审评论，用于训练LLM代理。此数据集模拟了人类评审员的结构化推理过程——总结论文、引用相关工作、识别优点和缺点以及生成评审结论。在此基础上，我们使用相关论文感知的训练方法训练能够进行结构化推理的LLM评审代理。此外，我们构建了ReviewAgents，一个多角色多LLM代理评审框架，以增强评审意见生成过程。同时，我们提出了ReviewBench，一个用于评估LLMs生成的评审意见的基准。我们在ReviewBench上的实验结果表明，现有的LLMs在一定程度上展示了自动化的评审过程的潜力，但与人类生成的评审相比仍存在差距。此外，我们的ReviewAgents框架进一步缩小了这一差距，在生成评审意见方面优于先进的LLMs。|
|**2025-03-10**|**DynTaskMAS: A Dynamic Task Graph-driven Framework for Asynchronous and Parallel LLM-based Multi-Agent Systems**|Junwei Yu et.al.|[2503.07675](http://arxiv.org/abs/2503.07675)|null|大型语言模型（LLMs）在多智能体系统（MAS）中的出现开启了人工智能的新可能性，但当前的实现面临着资源管理、任务协调和系统效率方面的重大挑战。虽然现有的框架展示了基于LLM的代理在协作问题解决方面的潜力，但它们通常缺乏复杂的并行执行和动态任务管理机制。本文介绍了一种名为DynTaskMAS的新型框架，该框架通过动态任务图在基于LLM的MAS中协调异步和并行操作。该框架有四个关键创新：（1）动态任务图生成器能够智能地分解复杂任务同时保持逻辑依赖关系；（2）异步并行执行引擎通过高效的任务调度优化资源利用率；（3）语义感知上下文管理系统使代理之间能够高效的信息共享；（4）自适应工作流管理器能够动态优化系统性能。实验评估表明，与传统方法相比，DynTaskMAS实现了显著改进：在不同复杂度的任务中执行时间减少了21-33%（对于更复杂的任务，提升更为显著），资源利用率提高了35.4%（从65%提高到88%），并且在多达16个并发代理的情况下接近线性吞吐量扩展（4倍代理时提升了3.47倍）。我们的框架为构建可扩展且高性能的基于LLM的多智能体系统奠定了基础，使其能够高效处理复杂且动态的任务。|
|**2025-03-10**|**LLMs syntactically adapt their language use to their conversational partner**|Florian Kandra et.al.|[2503.07457](http://arxiv.org/abs/2503.07457)|null|人们常常观察到人类在对话过程中会调整自己的语言使用方式。本文通过实证研究探讨大型语言模型（LLMs）是否也表现出类似的对话适应行为。我们构建了一个大型语言模型之间的对话语料库，发现两个LLM代理在对话过程中确实会逐渐做出更相似的句法选择，这表明现代LLMs以一种初步的方式适应了它们的对话伙伴的语言使用方式。|
|**2025-03-10**|**Experimental Exploration: Investigating Cooperative Interaction Behavior Between Humans and Large Language Model Agents**|Guanxuan Jiang et.al.|[2503.07320](http://arxiv.org/abs/2503.07320)|null|随着大型语言模型（LLMs）的兴起，作为自主决策者的AI代理在人类与AI协作中的角色带来了显著的机会和挑战。尽管许多研究已经探讨了人类与AI工具的合作，但关于LLM增强的自主代理在竞争性合作互动中的作用仍缺乏深入研究。本研究调查了人类的合作行为，涉及30名参与者与具有不同特征（拟人化为人类、拟人化为基于规则的AI代理以及LLM代理）的LLM代理在重复囚徒困境博弈中的交互。研究结果显示，人类的合作行为因代理的拟人化特征而有显著差异，并且参与者性别与代理拟人化特征之间存在交互效应。此外，我们分析了人类的响应模式，包括游戏完成时间、主动有利行为以及对修复努力的接受程度。这些见解为理解人类与LLM代理在竞争合作情境下的互动提供了新的视角，例如虚拟化身或未来的物理实体。研究强调了理解人类对AI代理的偏见的重要性，以及观察到的行为如何影响未来的人类与AI合作动态。|
|**2025-03-10**|**Automated Movie Generation via Multi-Agent CoT Planning**|Weijia Wu et.al.|[2503.07314](http://arxiv.org/abs/2503.07314)|**[link](https://github.com/showlab/movieagent)**|现有的长视频生成框架缺乏自动化规划功能，需要人工输入故事线、场景、摄影和角色互动等内容，导致成本高昂且效率低下。为了解决这些挑战，我们提出了MovieAgent，这是一种通过多智能体链式思维（CoT）规划实现的自动电影生成方法。MovieAgent具有两大优势：首先，我们首次探索并定义了自动电影/长视频生成的范式。给定剧本和角色库，MovieAgent能够生成包含多个场景和镜头的长视频，保持叙事连贯性，同时确保角色一致性、字幕同步以及音频在整个影片中的稳定性。其次，MovieAgent引入了一种分层的基于CoT的推理过程，以自动生成场景结构、摄像设置和电影摄影，大大减少了人力投入。通过采用多个LLM智能体来模拟导演、编剧、故事板艺术家和场地经理的角色，MovieAgent优化了生产流程。实验表明，MovieAgent在剧本忠实度、角色一致性和叙事连贯性方面达到了新的技术水平。我们的分层框架向前迈进了一步，并为全自动电影生成提供了新的见解。代码和项目网站可在以下链接获取：https://github.com/showlab/MovieAgent 和 https://weijiawu.github.io/MovieAgent。|
|**2025-03-10**|**DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data Science**|Ziming You et.al.|[2503.07044](http://arxiv.org/abs/2503.07044)|null|数据科学任务是多方面的、动态的，并且通常具有领域特定性。现有的基于大型语言模型的方法主要集中在孤立的阶段，忽视了许多数据科学任务之间的相互依赖性，限制了其全面端到端支持的能力。我们提出了DatawiseAgent，这是一种以笔记本为中心的大型语言模型代理框架，通过markdown和可执行代码单元格统一用户、代理和计算环境之间的交互，支持灵活且适应性强的数据科学自动化。DatawiseAgent基于有限状态转换器（FST），协调四个阶段，包括类似于DSF的规划、增量执行、自我调试和后过滤。具体而言，DFS样式的规划阶段系统地探索了解决方案空间，而增量执行利用实时反馈并适应大型语言模型的有限能力逐步完成任务。自我调试和后过滤模块通过诊断和纠正错误以及修剪冗余信息进一步增强了可靠性。在包括数据分析、可视化和数据建模在内的多样化任务上的广泛实验表明，DatawiseAgent在多个模型设置下始终优于或匹配最先进的方法。这些结果突显了其在跨数据科学场景中泛化的能力，并为更高效、全自动的工作流程奠定了基础。|
|**2025-03-10**|**ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation**|Kaiyuan Liu et.al.|[2503.07010](http://arxiv.org/abs/2503.07010)|null|近年来，大型语言模型（LLM）在提升编程能力方面取得了快速进展。然而，现有的基准测试缺乏从用户视角自动评估的能力，也缺乏对LLM代码生成能力结果的可解释性。因此，我们引入了ProjectEval，这是一个新的基准测试，用于自动化评估LLM在项目级代码生成方面的表现，通过模拟用户交互进行评估。ProjectEval由LLM与人工审查共同构建，包含三种不同层次的自然语言或代码骨架输入。它可以通过用户交互模拟执行以及通过现有客观指标的代码相似性来评估生成的项目。通过ProjectEval，我们发现系统化的工程项目的代码、整体项目理解以及综合分析能力是LLM实现实际项目的关键。我们的研究结果和基准测试为开发更有效的编程代理提供了宝贵的见解，这些代理可以在未来的实际生产环境中部署|
|**2025-03-10**|**Beyond Code Generation: LLM-supported Exploration of the Program Design Space**|J. D. Zamfirescu-Pereira et.al.|[2503.06911](http://arxiv.org/abs/2503.06911)|null|在本文中，我们探讨了大型语言模型（LLM）支持计算机程序迭代设计的明确方法。程序设计与其他设计活动一样，以探索替代问题表述及其相关解决方案的空间为特征，并以迭代方式进行。尽管LLMs可能是强大的工具来帮助这种探索，但默认情况下，代码生成LLMs提供的代码仅代表一个特定的点解决方案，这掩盖了更大的可能替代方案空间，其中许多方案可能比LLMs的默认解释及其生成的代码更可取。我们贡献了一个集成开发环境（IDE），通过生成和展示新的问题表述方式以及替代解决方案、跟踪设计决策并识别程序员或LLMs所做的隐式决策来支持程序设计。在用户研究中，我们发现使用我们的IDE，用户可以结合并并行化设计阶段以探索更广泛的设计空间——但也难以跟上由LLMs引发的代码变化以及其他信息过载的问题。这些发现表明未来支持程序设计的IDE面临的一个核心挑战：仔细管理注意力并决定LLM代理应该向程序员呈现哪些信息以及何时呈现。|
|**2025-03-09**|**Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets**|Tommaso Bendinelli et.al.|[2503.06664](http://arxiv.org/abs/2503.06664)|null|高质量、无错误的数据集是构建可靠、准确且无偏机器学习模型的关键。然而，现实世界中的数据集往往由于传感器故障、数据录入错误或数据整合不当而出现各种问题，这些问题会严重影响模型性能。检测和纠正这些错误通常需要定制化的解决方案，并且需要大量的领域专业知识。因此，自动化过程面临挑战，使得这一过程变得耗时且繁琐。在本研究中，我们探讨了大型语言模型（LLMs）是否能够减轻手动数据清理的负担。我们在实验中让LLM与Python结合，用于清理训练数据集以提高学习算法的性能，但不允许其修改训练管道或进行任何特征工程。我们使用多个在Kaggle上故意添加错误的数据集进行此实验。我们的结果显示，LLMs可以通过利用同一行内其他特征的上下文信息以及从前几次迭代中获得的反馈来识别并纠正错误条目，例如不合逻辑的值或异常值。然而，它们难以检测更复杂的错误，这些错误需要理解跨多行的数据分布，如趋势和偏差。|
|**2025-03-09**|**Performant LLM Agentic Framework for Conversational AI**|Alex Casella et.al.|[2503.06410](http://arxiv.org/abs/2503.06410)|null|大型语言模型（LLMs）在处理由节点和边组成的图逻辑工作流时面临挑战，例如复杂工作流中的对齐错误和由于上下文过大导致的幻觉。为了解决这些局限性，我们引入了执行高效的自主框架（PAF），这是一种新颖的系统，通过结合LLMs推理与数学基础的向量评分机制，帮助其在遍历复杂图时选择合适的节点并按顺序执行操作。我们的方法动态平衡了严格遵循预定义路径与灵活的节点跳转，以高效处理各种用户输入。实验表明，PAF显著优于基线方法，为复杂业务环境中的可扩展、实时会话式人工智能系统铺平了道路。|
|**2025-03-08**|**Towards Conversational AI for Disease Management**|Anil Palepu et.al.|[2503.06074](http://arxiv.org/abs/2503.06074)|null|尽管大型语言模型（LLMs）在诊断对话方面显示出潜力，但它们在有效管理推理方面的表现，包括疾病进展、治疗反应和安全药物处方等方面仍需深入研究。我们通过一种新的基于LLM的主动系统推进了Articulate Medical Intelligence Explorer (AMIE)之前展示出的诊断能力，该系统优化用于临床管理和对话，涵盖了疾病演变、多次患者就诊情况、治疗反应以及在药物处方中的专业能力。为了使其推理基于权威的临床知识，AMIE利用了Gemini的长上下文处理能力，结合了上下文化检索与结构化推理，以确保其输出符合相关且最新的临床实践指南和药物处方集。在一项随机、盲法虚拟客观结构化临床考试（OSCE）研究中，AMIE与21名全科医生（PCPs）就100个多访问病例场景进行了比较，这些场景旨在反映英国NICE指导和BMJ最佳实践指南。由专科医生评估的管理推理结果显示，AMIE不逊于全科医生，并且在治疗和检查的精确性以及与临床指南的一致性和基础方面得分更高。为了基准测试药物推理，我们开发了RxQA，这是一个来自两个国家药物处方集（美国、英国）的多选题基准，并由董事会认证的药剂师验证。虽然AMIE和全科医生都从能够访问外部药物信息中受益，但在较高难度的问题上，AMIE的表现优于全科医生。尽管还需要进一步的研究才能实现实际应用，但AMIE在各项评估中的优异表现标志着会话式AI作为疾病管理工具的重要一步。|
|**2025-03-08**|**DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments**|Wenjie Tang et.al.|[2503.06047](http://arxiv.org/abs/2503.06047)|**[link](https://github.com/decibrain-group/dsgbench)**|大型语言模型（LLM）代理在解决复杂和动态任务方面越来越受欢迎，但需要适当的评估系统来评估它们的能力。然而，现有的基准通常要么专注于单目标任务，要么使用过于宽泛的评估指标，未能全面检查LLM代理在复杂决策任务中的实际能力。为了解决这些问题，我们引入了DSGBench，这是一个更严格的策略决策评估平台。首先，它包含了六个复杂的策略游戏，这些游戏由于其长期和多维度的决策需求以及定制各种难度级别或多个目标任务的灵活性，成为了理想的测试平台。其次，DSGBench采用了一种细粒度的评分系统，通过考察五个特定维度的性能提供全面的评估。此外，DSGBench还引入了自动决策跟踪机制，这使得对代理行为模式和策略变化的深入分析成为可能。我们通过将其应用于多种流行的LLM代理来展示DSGBench的进步之处，结果表明DSGBench提供了有价值的见解，有助于选择LLM代理以及改进它们的未来发展。DSGBench可在https://github.com/DeciBrain-Group/DSGBench获取。|
|**2025-03-07**|**A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval**|Yu Zhang et.al.|[2503.05659](http://arxiv.org/abs/2503.05659)|**[link](https://github.com/tsinghua-fib-lab/llm-agent-for-recommendation-and-search)**|信息技术极大地改变了人类处理信息的方式。在线创建、分享和传播的大量内容使得获取相关的信息变得越来越困难。在过去的二十年里，搜索和推荐系统（统称为信息检索系统）为了应对这些挑战经历了显著的发展。大型语言模型（LLMs）的最新进展展示了其在各种语言任务上超越人类表现的能力，并表现出一般理解、推理和决策能力。本文探讨了大型语言模型代理在增强搜索和推荐系统方面的变革潜力。我们讨论了LLM代理的动机和作用，并建立了一个分类框架来阐述现有的研究。我们强调了LLM代理在解决当前搜索和推荐系统中的挑战方面的巨大潜力，并提供了对未来研究方向的见解。本文首次系统地回顾并分类了这些领域中关于LLM代理的研究，为利用这一先进的AI技术进行信息检索提供了新的视角。为了帮助理解现有研究，我们在以下链接列出了基于代理的使用大型语言模型的现有论文：https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search。|
|**2025-03-07**|**ORANSight-2.0: Foundational LLMs for O-RAN**|Pranshav Gajjar et.al.|[2503.05200](http://arxiv.org/abs/2503.05200)|null|尽管大型语言模型（LLMs）在医疗保健、客户服务和商业营销等关键领域产生了变革性影响，但将其集成到开放无线接入网络（O-RAN）中的应用仍然有限。这一差距主要由于缺乏特定领域的基础模型，现有的解决方案通常依赖于通用的LLMs，这些模型无法解决O-RAN特有的挑战和技术复杂性。为弥合这一差距，我们推出了ORANSight-2.0（O-RAN洞见），这是一个旨在开发专门针对O-RAN的定制化基础LLMs的开创性项目。基于18个跨越五个开源LLM框架的模型，ORANSight-2.0对从1B到70B参数范围内的模型进行微调，显著减少了对专有闭源模型的依赖，同时提升了O-RAN环境下的性能。ORANSight-2.0的核心是RANSTRUCT，这是一种基于检索增强生成（RAG）的指令调优框架，使用两个LLM代理来创建高质量的指令调优数据集。生成的数据集随后用于通过QLoRA微调18个预训练的开源LLMs。为了评估ORANSight-2.0，我们引入了srsRANBench，这是一个针对广泛使用的5G O-RAN堆栈srsRAN的代码生成和代码库理解设计的新基准。我们还利用ORANBench13K，一个现有用于评估O-RAN特定知识的基准。我们的全面评估表明，ORANSight-2.0模型在ORANBench上比通用和闭源模型（如ChatGPT-4o和Gemini）高出5.421%，在srsRANBench上高出18.465%，实现了卓越的性能，同时保持了更低的计算和能源成本。我们还尝试了ORANSight-2.0 LLMs的RAG增强变体，并对其能量特性进行了彻底评估，展示了训练、标准推理和RAG增强推理的成本。|
|**2025-03-06**|**SafeArena: Evaluating the Safety of Autonomous Web Agents**|Ada Defne Tur et.al.|[2503.04957](http://arxiv.org/abs/2503.04957)|null|LLM基于的代理在解决基于网络的任务方面变得越来越熟练。随着这种能力的提升，也带来了更大的滥用风险，例如在在线论坛上发布错误信息或在网站上销售非法物质。为了评估这些风险，我们提出了SafeArena，这是首个专注于网络代理故意误用的安全基准。SafeArena包含四个网站上的250个安全任务和250个有害任务。我们将有害任务分类为五种危害类别——错误信息、非法活动、骚扰、网络犯罪和社会偏见，旨在评估网络代理的实际误用情况。我们评估了领先的LLM基于的网络代理，包括GPT-4o、Claude-3.5 Sonnet、Qwen-2-VL 72B和Llama-3.2 90B，在我们的基准测试中的表现。为了系统地评估它们对有害任务的易感性，我们引入了代理风险评估框架，该框架将代理行为分为四个风险级别。我们发现代理对恶意请求的遵从度令人惊讶地高，其中GPT-4o和Qwen-2分别完成了34.7%和27.3%的有害请求。我们的研究结果突显了网络代理安全对齐程序的迫切需求。我们的基准可以在https://safearena.github.io获取。|
|**2025-03-05**|**Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents**|Jingying Zeng et.al.|[2503.04830](http://arxiv.org/abs/2503.04830)|null|随着对话大型语言模型（LLMs）的发展，已经开发出了几种基于LLMs的购物代理（CSA），以帮助客户回答问题并简化其在电子商务领域的购物旅程。构建可信赖的CSA的主要目标是确保代理的回复准确且有事实依据，这对于建立客户信任和鼓励持续互动至关重要。然而，仍然存在两个挑战。首先，LLMs会产生虚构或未得到支持的声明。此类不准确性可能会传播错误信息并削弱客户的信任。其次，如果不提供知识来源归属，客户很难验证LLM生成的信息。为了解决这些挑战，我们提出了一种易于生产化的方法，该方法利用上下文学习（ICL）和多用户体验推理（MUI）来生成带有引用以归因于原始来源的回复，而不会干扰其他现有的用户体验功能。通过适当的用户体验设计，这些引用标记可以链接到相关的产品信息并显示给我们的客户。在这项工作中，我们还建立了自动指标和可扩展基准来全面评估LLM的归因能力。我们的实验表明，引入这种引用生成范式可以在真实数据上显著提高LLM回复的准确性13.83%。因此，我们的解决方案不仅解决了LLM准确性问题，而且还增加了对话式人工智能的透明度。|
|**2025-03-06**|**ToolFuzz -- Automated Agent Tool Testing**|Ivan Milev et.al.|[2503.04479](http://arxiv.org/abs/2503.04479)|null|大型语言模型（LLM）代理利用LLM的高级推理能力在实际应用中。为了与环境进行交互，这些代理通常依赖于工具，例如网络搜索或数据库API。当代理提供工具文档和用户查询时，文档的完整性和正确性至关重要。然而，工具文档常常过度、不足或描述不当，影响了代理的准确性。尽管标准软件测试方法难以识别这些错误，因为它们用自然语言表达，因此目前尚无自动化方法来测试代理的工具文档。为了解决这一问题，我们提出了ToolFuzz，这是首个用于自动化测试工具文档的方法。ToolFuzz旨在发现两类错误：（1）用户查询导致工具运行时错误；（2）用户查询导致代理响应不正确。ToolFuzz可以生成大量且多样的自然输入，有效地以低误报率找到工具描述错误。此外，我们提出了两种简单的提示工程方法。我们在32个常见的LangChain工具和35个新创建的自定义工具上评估了这三种工具测试方法，并使用两个新的基准进一步加强评估。我们发现许多公开可用的工具存在描述不足的问题。具体而言，我们表明ToolFuzz比提示工程方法识别出的错误输入多20倍，使其成为构建可靠AI代理的关键组成部分。|
|**2025-03-06**|**Measuring temporal effects of agent knowledge by date-controlled tool use**|R. Patrick Xian et.al.|[2503.04188](http://arxiv.org/abs/2503.04188)|null|时间进程是知识积累和更新的重要组成部分。网络搜索经常被用作代理知识的基础，但其配置不当会影响代理响应的质量。我们构建了一个基于工具的样本外测试框架，以衡量来自不同日期控制工具的大型语言模型（LLM）代理的知识变异性。我们展示了作为写作助手的LLM代理的时间效应，该代理可以使用网络搜索来帮助完成科学出版物摘要。我们表明，搜索引擎的时间效应转化为与工具相关的代理性能，但可以通过基础模型选择和显式推理指令（如思维链提示）来缓解。我们的结果表明，代理评估应采取动态观点，并考虑工具的时间影响以及外部资源的更新。|
|**2025-03-06**|**InterChat: Enhancing Generative Visual Analytics using Multimodal Interactions**|Juntong Chen et.al.|[2503.04110](http://arxiv.org/abs/2503.04110)|null|大型语言模型（LLMs）和生成式可视分析系统的兴起已经改变了数据驱动的洞察力，然而，在准确解释用户分析和交互意图方面仍然存在显著挑战。虽然语言输入提供了灵活性，但它们往往缺乏精确性，使得复杂意图的表达低效、容易出错且耗时。为了应对这些局限性，我们通过文献回顾和试点头脑风暴会议探讨了生成式可视分析的多模态交互设计空间。在此基础上，我们引入了一个高度可扩展的工作流程，该工作流程集成了多个LLM代理用于意图推理和可视化生成。我们开发了InterChat，这是一种结合了直接操作可视化元素和自然语言输入的生成式可视分析系统。这种集成能够实现精确的意图沟通，并支持渐进式的、以视觉为导向的探索性数据分析。通过采用有效的提示工程、上下文交互链接以及直观的可视化和交互设计，InterChat弥合了用户交互与LLM驱动的可视化之间的差距，提升了可解释性和可用性。广泛的评估，包括两个使用场景、一项用户研究和专家反馈，证明了InterChat的有效性。结果显示，在处理复杂的可视分析任务方面有显著的准确性与效率提升，这突显了多模态交互在重新定义生成式可视分析中的用户参与度和分析深度方面的潜力。|
|**2025-03-07**|**A Practical Memory Injection Attack against LLM Agents**|Shen Dong et.al.|[2503.03704](http://arxiv.org/abs/2503.03704)|null|基于大型语言模型（LLM）的代理在广泛的复杂现实世界应用中展示了强大的能力。然而，当用于演示的过往记录被恶意篡改时，具有被篡改记忆库的LLM代理可能会轻易产生有害输出。在这篇论文中，我们提出了一种新颖的记忆注入攻击（MINJA），它仅通过与代理进行查询和输出观察交互即可向记忆库中注入恶意记录。这些恶意记录旨在诱发一系列导致不期望的代理行为的恶意推理步骤，当执行受害用户的查询时尤其如此。具体来说，我们引入了一系列桥接步骤来连接受害查询和恶意推理步骤。在注入恶意记录的过程中，我们提出了一个指示提示来引导代理自主生成我们设计的桥接步骤。我们还提出了一种逐步缩短策略，逐渐移除指示提示，使得在处理受害查询时恶意记录更容易被检索到。我们的广泛实验跨越了不同的代理，证明了MINJA在破坏代理记忆方面的有效性。由于执行所需的条件最少，MINJA使任何用户都能影响代理的记忆，突显了LLM代理的实际风险。|
|**2025-03-05**|**Benchmarking LLMs and LLM-based Agents in Practical Vulnerability Detection for Code Repositories**|Alperen Yildiz et.al.|[2503.03586](http://arxiv.org/abs/2503.03586)|null|大型语言模型（LLMs）在软件漏洞检测方面显示出潜力，特别是在函数级基准测试如Devign和BigVul上。然而，实际的漏洞检测需要进行过程间分析，因为漏洞通常通过多跳函数调用而非孤立的函数出现。虽然像ReposVul和VulEval这样的仓库级基准测试引入了过程间上下文，但它们仍然计算成本高昂，缺乏对漏洞修复的成对评估，并且探索的上下文检索有限，限制了其实用性。我们引入了JitVul，这是一个JIT漏洞检测基准，它将每个函数与其引入漏洞和修复提交链接起来。JitVul由879个CVE组成，涵盖91种不同的漏洞类型，使全面评估检测能力成为可能。我们的结果显示，采用思维-行动-观察和过程间上下文的ReAct Agent的表现优于LLMs，在区分易受攻击代码和良性代码方面表现更好。虽然像Chain-of-Thought这样的提示策略有助于LLMs，但ReAct Agent仍需进一步改进。两种方法都表现出不一致性，要么误识别漏洞，要么过度分析安全防护措施，这表明有很大的改进空间。|
|**2025-03-04**|**MPO: Boosting LLM Agents with Meta Plan Optimization**|Weimin Xiong et.al.|[2503.02682](http://arxiv.org/abs/2503.02682)|**[link](https://github.com/weiminxiong/mpo)**|近期大型语言模型（LLMs）的进展使得基于LLMs的代理能够在交互式规划任务中取得成功。然而，尽管取得了这些成功，现有的方法通常会遇到规划幻觉的问题，并且需要针对每个新代理进行重新训练。为了解决这些挑战，我们提出了元计划优化（MPO）框架，该框架通过直接纳入明确指导来增强代理的规划能力。与之前的依赖复杂知识的方法不同——这些方法要么需要大量的人力投入，要么缺乏质量保证，MPO通过利用高层通用指导的元计划来协助代理规划，并能够根据代理任务执行的反馈持续优化这些元计划。我们的实验在两个代表性任务上进行，结果表明MPO显著优于现有基线。此外，我们的分析表明，MPO提供了一个即插即用的解决方案，增强了以前未见过场景中的任务完成效率和泛化能力。|
|**2025-03-04**|**Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent**|Xingzuo Li et.al.|[2503.02519](http://arxiv.org/abs/2503.02519)|**[link](https://github.com/wisper12933/ga-rollback)**|大型语言模型（LLM）代理通常采用逐步推理框架，在该框架中它们交替进行思考和行动以完成给定任务。然而，这种范式面临一个根深蒂固的单次通过问题，即每次生成的中间想法都会被插入到轨迹中，而不管其正确与否，这可能导致不可逆的错误传播。为了解决这个问题，本文提出了一种名为生成器辅助逐步回滚（GA-Rollback）的新框架，以促进LLM代理更好的决策。特别地，GA-Rollback利用一个生成器与环境交互，以及一个助手检查生成器产生的每个动作，当助手检测到错误的动作时触发回滚操作。此外，我们为回滚场景引入了两种额外的策略以进一步提高其有效性。大量的实验表明，GA-Rollback在三个广泛使用的基准测试中显著优于几个强大的基线方法。我们的分析进一步揭示，GA-Rollback可以作为一个强大的即插即用模块，与其他方法无缝集成。|
|**2025-03-04**|**AppAgentX: Evolving GUI Agents as Proficient Smartphone Users**|Wenjia Jiang et.al.|[2503.02268](http://arxiv.org/abs/2503.02268)|null|近期大型语言模型（LLMs）的发展催生了能够与图形用户界面（GUIs）交互的智能LLM代理。这些代理展现出强大的推理和适应能力，使其能够执行那些传统上需要预定义规则的复杂任务。然而，LLM代理对逐步推理的依赖常常导致效率低下，尤其是在处理例行任务时。相比之下，传统的基于规则的系统在效率方面表现出色，但缺乏适应新颖场景的智能和灵活性。为了解决这一挑战，我们提出了一种新的针对GUI代理的进化框架，旨在提高操作效率的同时保持智能性和灵活性。我们的方法引入了一个记忆机制来记录代理的任务执行历史。通过分析这段历史，代理能够识别重复的动作序列，并进化出高层次的动作作为捷径，取代这些低层次的操作，从而提高效率。这使代理能够专注于需要更复杂推理的任务，同时简化例行操作。在多个基准任务上的实验结果表明，我们的方法在效率和准确性方面显著超越现有方法。代码将开源以支持进一步的研究。|
|**2025-03-04**|**Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient and Feasible Multitasking with Time Constraints Between Actions**|Zirui Wu et.al.|[2503.02238](http://arxiv.org/abs/2503.02238)|**[link](https://github.com/williamzr/recipe2plan)**|尽管基于大型语言模型的代理在任务完成方面取得了显著进展，现有的评估基准往往过分强调单一任务的表现，而对实际场景中所需的多任务规划和执行效率关注不足。为了弥补这一差距，我们提出了Recipe2Plan，这是一个基于真实烹饪场景的新颖基准框架。与传统基准不同，Recipe2Plan挑战代理通过并行任务执行来优化烹饪时间，同时遵守时间约束，即特定操作需要在前续步骤后的特定时间段内执行。过于激进的局部并行化可能会扰乱这种约束，从而可能损害整个烹饪过程。这种严格的动作之间的时间约束为代理提出了一个独特的挑战，即在最大化并发操作的同时遵守关键的时间约束。广泛的实验显示了最先进的模型在这两者之间的平衡上面临的挑战。结果突显了大型语言模型在时间感知和全局多任务处理能力方面的改进需求。我们将基准和代码开源在https://github.com/WilliamZR/Recipe2Plan。|
|**2025-03-04**|**ATLaS: Agent Tuning via Learning Critical Steps**|Zhixun Chen et.al.|[2503.02197](http://arxiv.org/abs/2503.02197)|null|大型语言模型（LLM）代理在多领域任务中展示了显著的泛化能力。现有的代理调优方法通常采用在整个专家轨迹上的监督微调。然而，完全行为克隆整个轨迹会引入专家偏差，并削弱对未被专家数据覆盖的状态的泛化能力。此外，诸如规划、复杂推理以解决中间子任务以及策略决策等关键步骤对于代理任务的成功至关重要，因此学习这些步骤是提高LLM代理的关键。为了实现更有效和高效的代理调优，我们提出了ATLaS，该方法识别专家轨迹中的关键步骤，并仅针对这些步骤对LLMs进行微调，从而降低成本。通过将训练重点放在少数关键步骤上，我们的方法降低了过度拟合整个轨迹的风险，并促进了不同环境和任务之间的泛化。在广泛的实验中，仅使用由ATLaS选出的30%关键步骤微调的LLM优于所有步骤微调的LLM和最近的开源LLM代理。ATLaS保持并提升了基础LLM作为与多种环境交互的通用代理的技能。|
|**2025-03-03**|**Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions**|Angana Borah et.al.|[2503.02038](http://arxiv.org/abs/2503.02038)|null|现有的误信息暴露和易感性挑战在不同的人群中各不相同，因为某些人口群体比其他群体更容易受到误信息的影响。大型语言模型（LLMs）通过其大规模生成有说服力内容的能力以及强化现有偏见的特性，引入了这些挑战的新维度。本研究调查了LLMs与人类在接触误导性内容时的双向说服动态。我们使用人类立场数据集分析人类对LLM的影响，并通过生成基于LLM的有说服力的论点来评估LLM对人类的影响。此外，我们使用一个多代理LLM框架来分析在说服作用下误信息的传播情况，并针对面向不同人口群体的LLM代理进行分析。我们的研究结果显示，人口统计因素影响着LLMs对误信息的易感性，这与人类在误信息易感性方面的人口统计模式紧密相关。我们还发现，类似于人类人口群体，多代理LLMs表现出回音室行为。本研究探讨了人类与LLMs之间的相互作用，在误信息背景下突出了人口统计差异，并为未来的干预措施提供了见解。|
|**2025-03-03**|**MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents**|Kunlun Zhu et.al.|[2503.01935](http://arxiv.org/abs/2503.01935)|**[link](https://github.com/multiagentbench/marble)**|大型语言模型（LLMs）作为自主代理已经展示了惊人的能力，然而现有的基准测试要么专注于单代理任务，要么局限于狭窄的领域，未能捕捉到多代理协调和竞争的动态。在本文中，我们介绍了MultiAgentBench，这是一个全面的基准测试平台，旨在评估基于LLM的多代理系统在各种交互场景中的表现。我们的框架不仅衡量任务完成情况，还通过新的里程碑式关键性能指标来衡量协作和竞争的质量。此外，我们评估了各种协调协议（包括星形、链形、树形和图结构拓扑）以及诸如小组讨论和认知规划等创新策略。值得注意的是，gpt-4o-mini达到了平均最高的任务得分，图结构在研究场景中表现出最好的协调协议效果，而认知规划提高了3%的里程碑达成率。代码和数据集可在https://github.com/MultiagentBench/MARBLE公开获取。|
|**2025-03-03**|**Can (A)I Change Your Mind?**|Miriam Havin et.al.|[2503.01844](http://arxiv.org/abs/2503.01844)|null|不断增加的大语言模型（LLM）基于对话代理的整合引发了关于其影响人类观点的潜在认知和社会问题。尽管之前的研究表明，基于LLM的代理可以生成有说服力的内容，但这些研究通常涉及受控且使用英语的环境。为了解决这一问题，我们的一项预先注册的研究探讨了LLM在更生态、不受限制的情景中的说服能力，考察了静态（书面段落）和动态（通过Telegram进行对话）交互类型。该研究完全使用希伯来语进行了200名参与者的实验，评估了LLM和人类对话者在有争议的公民政策话题上的说服效果。结果表明，参与者对LLM和人类的观点采纳相似，在所有条件下都出现了显著的意见变化，无论对话者类型或交互模式如何。除了静态LLM交互外，大多数情况下信心水平显著增加。这些发现证明了LLM基代理在不同来源和设置下的强大说服能力，突显了它们在塑造公众意见方面的潜在影响。|
|**2025-03-03**|**Student engagement in collaborative learning with AI agents in an LLM-empowered learning environment: A cluster analysis**|Zhanxin Hao et.al.|[2503.01694](http://arxiv.org/abs/2503.01694)|null|在教育实践中整合大型语言模型（LLM）可以促进个性化学习，因为它能够适应不同学习者类型的各种行为模式。本研究旨在探讨在一个新颖的互动环境中这些学习者类型，并对其独特的特征和互动动态进行详细分析。研究涉及中国一所大学的110名学生，在一个由LLM赋能的学习环境中与多个LLM代理互动，完成了六个模块的课程作业。收集并分析了学生们的非认知特征、课程参与度以及与AI的互动模式数据。使用分层聚类分析，学生们被归类为三个不同的群体：积极提问者、响应导航者和沉默倾听者。随后应用认识网络分析进一步阐述不同类型学习者的互动概况和认知参与度。研究结果强调了不同类型的学习者如何与人机互动学习进行互动，并为自适应教育系统的开发提供了实用启示。|
|**2025-03-02**|**Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity**|Yupu Hao et.al.|[2503.00771](http://arxiv.org/abs/2503.00771)|**[link](https://github.com/hypasd-art/etapp)**|**个性化工具利用对于在各种工具交互场景中与用户偏好对齐大型语言模型（LLMs）至关重要。然而，当前大多数基准主要集中在文本生成的个性化或直接工具使用上，而没有同时考虑两者。在这项工作中，我们引入了一个新的基准ETAPP来评估个性化的工具调用，建立了一个沙盒环境，并提供了一个涵盖多样化用户档案的全面测试案例数据集，共计800个测试案例。为了提高评估的准确性，我们提出了一种基于关键点的LLM评估方法，通过手动标注每个测试案例的关键点并将其作为参考提供给LLM，从而减轻了LLM作为法官系统中的偏差。此外，我们评估了优秀的LLMs并进行了深入分析。进一步地，我们研究了不同的工具调用策略对LLMs个性化性能的影响以及在我们任务中的微调效果。我们的偏好设定和基于关键点的评估方法的有效性也得到了验证。我们的研究结果为改进个性化LLM代理提供了见解。我们的代码可在https://github.com/hypasd-art/ETAPP获取。**|
|**2025-02-28**|**ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph Environments**|Pedro Gimenes et.al.|[2502.21208](http://arxiv.org/abs/2502.21208)|null|近期的研究表明，通过扩展测试时的计算能力可以增强大型语言模型（LLM）在推理任务上的表现。一种有前景的方法，尤其是在处理可分解问题时，涉及将中间解决方案排列成一个图，在其上执行变换以探索解空间。然而，先前的工作依赖于预设的任务特定变换计划，这些计划受到一组搜索超参数的限制。在这项工作中，我们将思维图变换视为马尔可夫决策过程中的动作，并实现策略代理来引导基础推理LLM代理的有效行动策略。特别是，我们研究了另一个LLM作为策略代理在思维图环境中的能力，并介绍了ARIES，这是一种用于LLM推理的多代理架构。在ARIES中，推理LLM代理解决分解的小问题，而策略LLM代理则保持对思维图状态的可见性，并动态地调整问题解决策略。通过广泛的实验，我们观察到使用现成的LLM作为策略代理，在无需监督微调的情况下，相对于静态变换计划，可以在HumanEval上获得高达29%的更高准确率，同时减少35%的推理成本，并且避免任何搜索需求。我们还对观察到的失败模式进行了彻底分析，强调LLM大小的限制和问题分解的深度可以被视为扩大LLM指导推理的挑战。|
|**2025-02-28**|**PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured Data with Text and Relational Information**|Hansi Yang et.al.|[2502.21087](http://arxiv.org/abs/2502.21087)|null|大型语言模型（LLMs）在各个领域的问答方面展现出了 impressive 的能力，但在需要专业和最新知识的问题上经常遇到幻觉问题。为了应对这一局限性，提出了检索增强生成（RAG）技术，该技术从外部资源检索相关信息以提供答案。然而，现有的 RAG 方法通常只关注单一类型的外部数据，如向量化文本数据库或知识图谱，而无法很好地处理包含文本和关系信息的半结构化数据的真实问题。为了解决这一差距，我们介绍了 PASemiQA，这是一种新颖的方法，它联合利用半结构化数据中的文本和关系信息来回答问题。PASemiQA 首先生成一个计划，以确定回答半结构化数据中的问题所需的相关文本和关系信息，然后使用 LLM 代理遍历半结构化数据并提取必要信息。我们的实证结果展示了 PASemiQA 在来自不同领域的各种半结构化数据集上的有效性，展示了其提高半结构化数据问答系统准确性和可靠性的潜力。|
|**2025-02-28**|**The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents**|Yifan Duan et.al.|[2502.20859](http://arxiv.org/abs/2502.20859)|null|大型语言模型（LLMs）在封闭任务（包括问题解决和代码生成）和开放任务（包括创意写作）方面表现出色，但现有对它们能力的解释缺乏与现实世界人类智能的联系。为了填补这一空白，本文通过“人类模拟”的视角系统地研究了LLM的智能，探讨了三个核心问题：（1）人格特质如何影响封闭任务中的问题解决？（2）特质如何塑造开放任务中的创造力？（3）单个代理的表现如何影响多代理协作？通过将大五人格特质分配给LLM代理，并评估他们在单个代理和多代理设置下的表现，我们揭示了特定的人格特质显著影响推理准确性（封闭任务）和创造性输出（开放任务）。此外，多代理系统表现出的集体智能不同于个体能力，由独特的人格特质组合驱动。我们证明LLMs通过下一个令牌预测内在地模拟人类行为，反映了人类的语言、决策和协作动态。|
|**2025-02-28**|**Digital Player: Evaluating Large Language Models based Human-like Agent in Games**|Jiawei Wang et.al.|[2502.20807](http://arxiv.org/abs/2502.20807)|**[link](https://github.com/fuxiailab/civagent)**|随着大型语言模型（LLM）的快速发展，基于LLM的自主代理在数字员工领域展现出潜力，例如数字分析师、教师和程序员。在本文中，我们开发了一个基于开源策略游戏“Unciv”的应用程序级测试平台，该平台拥有数百万活跃玩家，以便研究人员构建一个“数据飞轮”来研究人类化的代理在“数字玩家”任务中的表现。这种类似于“文明”（Civilization）的游戏具有广泛的决策空间以及丰富的语言交互，如外交谈判和欺骗行为，这对基于LLM的代理在数值推理和长期规划方面提出了重大挑战。此外，“数字玩家”还需要生成类似人类的响应来进行社交互动、协作和与人类玩家进行谈判。“数字玩家”项目的开源项目可以在<https://github.com/fuxiAIlab/CivAgent>找到。|
|**2025-02-27**|**Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents**|Haochen Sun et.al.|[2502.20073](http://arxiv.org/abs/2502.20073)|**[link](https://github.com/yusaemeow/collab-overcooked)**|**大型语言模型（LLMs）驱动的多智能体系统（LLM-MAS）在实际应用中已经超越了传统的自然语言处理任务。本文提出了一种新的LLM驱动的多智能体系统基准测试——Collab-Overcooked，该测试基于流行的Overcooked-AI游戏，并在交互环境中提供了更多适用且具有挑战性的任务。Collab-Overcooked从两个新颖的角度扩展了现有的基准测试。首先，它提供了一个支持多样任务和目标的多智能体框架，并通过自然语言通信鼓励协作。其次，它引入了一系列过程导向的评估指标来评估不同LLM智能体在细粒度协作能力方面的表现，这一维度在先前的研究中常常被忽略。我们对10种流行的LLMs进行了广泛的实验，结果显示尽管这些LLMs在目标解释方面表现出强大的能力，但在积极协作和持续适应方面存在显著差异，而这些是高效完成复杂任务的关键。值得注意的是，我们指出了LLM-MAS的优势和不足，并为在统一且开源的基准上改进和评估LLM-MAS提供了见解。环境、30个开放式任务以及一个集成的评估包现已在https://github.com/YusaeMeow/Collab-Overcooked公开可用。**|
|**2025-02-27**|**MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue**|Yujia Chen et.al.|[2502.19860](http://arxiv.org/abs/2502.19860)|null|心理健康问题在当今竞争激烈的社会中日益严重，如抑郁和焦虑。传统的治疗方法如心理咨询和聊天机器人难以有效吸引患者，它们通常提供泛泛的回应，缺乏情感深度。虽然大型语言模型（LLM）有潜力创造更人性化的人机交互，但它们仍然难以捕捉微妙的情感。这需要LLM具备类似人类的适应性和温暖感。为填补这一空白，我们提出了MIND（多代理内心对话）这一新范式，旨在提供更具沉浸式的心理疗愈环境。考虑到LLM代理强大的生成能力和角色扮演能力，我们预定义了一个互动疗愈框架，并在该框架内为LLM代理分配不同的角色，与用户进行互动的内心对话，从而提供沉浸式的疗愈体验。我们在各种现实世界的疗愈维度中进行了广泛的真人实验，结果表明MIND比传统模式提供了更友好的用户体验。这证明了MIND有效地利用了LLM在心理疗愈方面的巨大潜力。|
|**2025-02-26**|**Language-Driven Opinion Dynamics in Agent-Based Simulations with LLMs**|Erica Cau et.al.|[2502.19098](http://arxiv.org/abs/2502.19098)|null|理解意见如何演变对于解决诸如极化、激进化和共识等社会系统中的问题至关重要。尽管已有大量研究集中在识别影响意见变化的因素上，但语言和论证谬误的作用仍未得到充分探索。本文旨在填补这一空白，通过LODAS（一种基于语言的意见动态模型）来探讨语言以及社会动态如何通过模拟围绕“忒修斯之船”悖论的辩论来影响意见演变。该模型模拟了带有离散意见的代理之间的互动，并通过接受、拒绝或忽略提出的论点使这些意见发生变化。我们研究了三种不同的场景：平衡、两极分化和不平衡的意见分布。友好性和谄媚性成为LLM代理的两个主要特征，在几乎任何设定下都形成了对所陈述观点的一致意见。此外，由于这些AI代理在试图说服同伴时常常会产生谬误性的论点，并且由于它们的顺从性，它们也极易受到基于逻辑谬误的论点的影响。这些结果不仅突显了这一框架在模拟社会动态方面的潜力，还从另一个角度揭示了LLM的偏见和不足之处，这可能会影响它们与人类的互动。|
|**2025-02-26**|**Letters from Future Self: Augmenting the Letter-Exchange Exercise with LLM-based Agents to Enhance Young Adults' Career Exploration**|Hayeon Jeon et.al.|[2502.18881](http://arxiv.org/abs/2502.18881)|null|年轻成年人在职业探索过程中经常遇到挑战。自我引导干预措施，如书信交换练习，其中参与者设想并采纳其未来自我的视角通过与他们想象中的未来自我交换信件，可以支持职业发展。然而，如果没有结构化的指导，这种干预措施的广泛采用可能会受到限制。为了解决这一问题，我们将基于大型语言模型（LLM）的代理整合到书信交换练习中，这些代理模拟参与者的未来自我，并评估了其有效性。一项为期一周的实验（N=36）比较了三种条件：（1）参与者手动撰写回复给自己，从他们未来自我的角度（基线），（2）未来自我代理向参与者发送信件，以及（3）未来自我代理与参与者进行聊天对话。结果表明，与未来自我代理交换信件增强了参与者在练习中的参与度，而干预措施对未来发展、职业自我概念和心理支持的整体效果在各条件下基本相同。我们讨论了设计AI增强干预措施的启示，以支持年轻成年人的职业探索。|
|**2025-02-26**|**AgentSociety Challenge: Designing LLM Agents for User Modeling and Recommendation on Web Platforms**|Yuwei Yan et.al.|[2502.18754](http://arxiv.org/abs/2502.18754)|**[link](https://github.com/tsinghua-fib-lab/agentsocietychallenge)**|AgentSociety挑战赛是网络会议上的首个竞赛，旨在探索大型语言模型（LLM）代理在建模用户行为和增强网络平台推荐系统方面的潜力。该挑战赛包括两个赛道：用户建模赛道和推荐赛道。参与者被要求利用来自Yelp、Amazon和Goodreads的综合数据集以及交互式环境模拟器来开发创新的LLM代理。该挑战吸引了全球295支团队，在为期37天的官方比赛日程中共收到了超过1400份提交。参赛者在发展阶段对赛道1和赛道2分别提高了21.9%和20.3%，在最终阶段分别提高了9.1%和15.9%，这代表了显著的成就。本文讨论了挑战赛的详细设计，分析了成果，并强调了最成功的LLM代理设计。为了支持进一步的研究和发展，我们已将基准环境开源在https://tsinghua-fib-lab.github.io/AgentSocietyChallenge。|
|**2025-02-25**|**Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support**|Kevin Pu et.al.|[2502.18658](http://arxiv.org/abs/2502.18658)|null|AI编程工具实现了强大的代码生成功能，最近的原型尝试通过主动的AI代理来减少用户的工作量，但它们对编程工作流程的影响尚未被探索。我们介绍了并评估了Codellaborator，这是一种基于编辑器活动和任务上下文启动编程辅助的设计探针LLM代理。我们探讨了三种界面变体以评估越来越显著的AI支持之间的权衡：仅提示、主动代理和具有存在感及交互上下文的主动代理（Codellaborator）。在一项被试内研究（N=18）中，我们发现主动代理与仅提示范式相比提高了效率，但也引起了工作流程中断。然而，存在指示器和交互上下文支持减轻了这些中断并提高了用户对AI过程的意识。我们强调Codellaborator在用户控制、所有权和代码理解方面的权衡，强调需要根据编程过程调整主动性。我们的研究有助于探索和评估主动AI系统的设计，并提出了关于AI集成编程工作流程的设计启示。|
|**2025-02-25**|**IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Agents**|Eric Xue et.al.|[2502.18530](http://arxiv.org/abs/2502.18530)|null|计算机视觉是许多实际应用中的关键组件，包括农业中的植物监测和数字系统中的手写文字分类。然而，传统上开发高性能的计算机视觉模型需要机器学习（ML）专业知识和特定领域的知识，这使得过程成本高昂、劳动密集且难以触及。大型语言模型（LLM）代理已成为自动化这一工作流程的一个有希望的解决方案，但大多数现有方法有一个共同的局限：它们试图在一个步骤中优化整个管道并在之后进行评估，这使得很难将改进归因于具体的变化。这种缺乏粒度性导致了不稳定的优化和较慢的收敛，限制了它们的有效性。为了解决这个问题，我们引入了一种名为迭代细化的新策略，这是一种受人类ML专家逐步完善模型启发的LLM驱动的ML管道设计方法，该方法一次专注于一个组件而不是一次性做出大范围的改变。通过基于真实训练反馈系统地更新各个组件，迭代细化提高了稳定性、可解释性和整体模型性能。我们在IMPROVE框架中实现了这一策略，这是一个端到端的LLM代理框架，用于自动优化对象分类管道。通过在不同大小和领域的数据集上的广泛评估，包括标准基准和Kaggle竞赛数据集，我们证明了迭代细化使IMPROVE能够始终如一地实现比现有零样本LLM方法更好的性能。这些发现确立了迭代细化作为一种有效的LLM驱动的ML自动化新策略，并将IMPROVE定位为无需ML专业知识即可构建高质量计算机视觉模型的便捷解决方案。|
|**2025-02-25**|**AgentRM: Enhancing Agent Generalization with Reward Modeling**|Yu Xia et.al.|[2502.18407](http://arxiv.org/abs/2502.18407)|null|现有的基于大型语言模型（LLM）的代理在保留任务上表现出色，但它们对未见过任务的泛化能力仍然较差。因此，一些最近的工作集中在通过更多样化的任务微调策略模型以提高其泛化能力。在这项工作中，我们发现通过微调奖励模型来指导策略模型比直接微调策略模型更为稳健。基于这一发现，我们提出了AgentRM，一个通用的奖励模型，用于指导策略模型进行有效的测试时间搜索。我们全面研究了构建奖励模型的三种方法，包括显式奖励建模、隐式奖励建模和LLM作为裁判。然后我们使用AgentRM来指导答案生成，并采用Best-of-N采样和步级束搜索。在四种类型的九个代理任务中，AgentRM将基础策略模型提升了平均8.8分，超过了顶级通用代理4.0分。此外，它展示了从弱到强的泛化能力，在LLaMA-3-70B策略模型上提升了更大的12.6分。至于特殊化能力，AgentRM还可以提升经过微调的策略模型，在三个保留任务上比顶级专用代理高出11.4分。进一步分析验证了其在测试时扩展的有效性。代码将会发布以便促进该领域的研究。|
|**2025-02-25**|**RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction**|Jianhao Yan et.al.|[2502.18308](http://arxiv.org/abs/2502.18308)|null|在多轮交互方案中，大型语言模型（LLM）可以利用用户反馈来提高其响应的质量和相关性。然而，评估LLM整合用户反驳反馈的能力至关重要却具有挑战性。在这项研究中，我们介绍了RefuteBench 2.0，它显著扩展了原始的RefuteBench，通过引入LLM代理作为反驳者和评估者，这允许进行灵活且全面的评估。我们设计了具有不同有效期限的瞬态和持久反驳指令。元评估表明，基于LLM的反驳者可以生成更像人类的反驳，而评估者可以给出与人类高度相关的评分。各种LLM的实验结果表明，当前模型能够有效地满足反驳但无法记住反驳信息。有趣的是，我们还观察到初始任务的表现随着反驳的增加而下降。注意力分数的分析进一步显示了当前LLM的一个潜在弱点：它们在长对话上下文中难以保持和正确使用先前的信息。|
|**2025-02-25**|**LAG: LLM agents for Leaderboard Auto Generation on Demanding**|Jian Wu et.al.|[2502.18209](http://arxiv.org/abs/2502.18209)|null|本文介绍了一种名为Leaderboard Auto Generation（LAG）的新颖且系统化的框架，用于在人工智能（AI）等快速发展的领域中自动生成特定研究主题的排行榜。面对每天更新的大批AI论文，研究人员难以追踪每篇论文提出的方法、实验结果和设置，因此迫切需要高效的自动排行榜构建方法。虽然大型语言模型（LLMs）为自动化这一过程提供了希望，但诸如多文档总结、排行榜生成和实验公平比较等挑战仍然有待探索。LAG通过系统的方法解决了这些挑战，该方法包括论文收集、实验结果提取与整合、排行榜生成以及质量评估。我们的贡献包括全面解决排行榜构建问题的一种解决方案、一种可靠的评估方法以及显示排行榜高质量的实验结果。|
|**2025-02-25**|**LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena**|Tianmi Ma et.al.|[2502.17967](http://arxiv.org/abs/2502.17967)|**[link](https://github.com/wekjsdvnm/agent-trading-arena)**|**近期大型语言模型（LLMs）在自然语言处理任务中的性能有了显著提升。然而，它们在数值推理方面泛化到动态、未见过的任务的能力仍然是一个挑战。现有的基准测试主要评估LLMs在具有预定义最优解的问题上的表现，这可能与现实世界场景不符，在这些场景中不存在明确的答案。为了弥合这一差距，我们设计了Agent Trading Arena，这是一个虚拟的数值游戏，通过零和游戏模拟复杂的经济系统，其中代理投资股票组合。我们的实验表明，包括GPT-4o在内的LLMs在处理纯文本股票数据时在代数推理方面存在困难，通常关注局部细节而非全局趋势。相比之下，当面对视觉数据如散点图或K线图时，LLMs在几何推理方面的表现要好得多，这表明视觉表示可以增强数值推理能力。通过结合反射模块，这种能力得到了进一步提高，该模块有助于分析和解释复杂数据。我们在NASDAQ股票数据集上验证了我们的发现，在该数据集中，LLMs在处理视觉数据时表现出比文本更强的推理能力。我们的代码和数据可在https://github.com/wekjsdvnm/Agent-Trading-Arena.git公开获取。**|
|**2025-02-25**|**Towards Enhanced Immersion and Agency for LLM-based Interactive Drama**|Hongqiu Wu et.al.|[2502.17878](http://arxiv.org/abs/2502.17878)|**[link](https://github.com/gingasan/interactive-drama)**|LLM基交互戏剧是一种新颖的基于AI的对话场景，在其中用户（即玩家）扮演故事中的一个角色，与由LLM代理扮演的角色进行对话，并经历一个不断展开的故事。本文从沉浸感和能动性两个方面理解交互戏剧：沉浸感是玩家在故事中的存在感，而能动性是玩家对故事世界产生影响的能力。这两个方面对于创造令人愉悦的互动体验至关重要，但在之前的研究中却探索不足。为了增强这两个方面，我们首先提出了一种名为剧本指导生成的新方法，该方法帮助LLM创作具有显著改进的结构和叙事质量的戏剧故事。此外，我们引入了基于情节的反思，使LLM代理能够根据玩家的意图来调整其反应。我们的评估依赖于人类的判断，以评估我们的方法在沉浸感和能动性方面的增益。|
|**2025-02-24**|**Aligning Compound AI Systems via System-level DPO**|Xiangwen Wang et.al.|[2502.17721](http://arxiv.org/abs/2502.17721)|null|复合人工智能系统由多个相互作用的组件如大语言模型代理和外部工具组成，在各种任务中展示了最先进的结果。因此，确保系统内组件的一致性以匹配人类期望至关重要。然而，传统的对齐方法，例如直接偏好优化（DPO），并不直接适用于复合人工智能系统。这些挑战包括组件之间非可微分的交互，使得端到端梯度优化不可行。此外，系统级偏好不能直接转化为组件级偏好，这进一步复杂化了对齐过程。我们通过将复合人工智能系统构造成有向无环图（DAGs）来解决这些问题，捕捉代理之间的连接以及数据生成过程。我们提出了一种系统级DPO（SysDPO）来通过适应DPO在这些DAG上进行联合对齐。我们研究了大语言模型和扩散模型的联合对齐，以展示我们的方法的有效性。我们的探索提供了对复合人工智能系统对齐的见解，并为进一步发展奠定了基础。|
|**2025-02-24**|**A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis**|Yuzhi Hao et.al.|[2502.16879](http://arxiv.org/abs/2502.16879)|null|本文开创了一种利用多个大型语言模型（LLMs）作为异质性人工智能经济主体进行经济和公共政策分析的新方法。我们首先评估了五个LLMs在解决两期消费分配问题方面的经济决策能力，在两种不同的场景下：一种是有明确的效用函数，另一种是基于直觉推理。虽然以往的研究通常通过仅改变提示来模拟异质性，但我们的方法利用了不同LLMs之间内在的分析能力差异来模拟具有不同认知特征的主体。在此基础上，我们构建了一个多LLM主体基础（MLAB）框架，并将这些LLMs映射到特定的教育群体和相应的收入区间。以利息所得税为例，我们展示了MLAB框架如何模拟政策对异质主体的影响，为经济和公共政策分析提供了一个有前景的新方向，充分利用了LLMs的人类式推理能力和计算能力。|
|**2025-02-24**|**Grounded Persuasive Language Generation for Automated Marketing**|Jibang Wu et.al.|[2502.16810](http://arxiv.org/abs/2502.16810)|null|本文开发了一个以大型语言模型（LLMs）为基础的主动框架，用于自动生成有说服力且基于事实的营销内容，以房地产列表描述作为我们的重点应用领域。我们的方法旨在使生成的内容与用户偏好保持一致，同时突出有用的客观属性。该代理由三个关键模块组成：(1) 基线模块，模拟专家人类行为来预测具有市场吸引力的特征；(2) 个性化模块，使内容与用户偏好相匹配；(3) 营销模块，确保事实准确性和包含地方性特征。我们在房地产营销领域进行了系统的受试者实验，重点关注潜在购房者。结果表明，通过我们方法生成的营销描述比由人类专家编写的更受欢迎。我们的研究结果表明，基于LLM的主动框架有望实现大规模目标市场营销的自动化，同时仅使用事实确保负责任的生成。|
|**2025-02-24**|**Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances**|Yaozu Wu et.al.|[2502.16804](http://arxiv.org/abs/2502.16804)|null|自动驾驶系统（ADS）正在通过减少人类干预、提高操作效率和增强安全性来革新交通运输。大型语言模型（LLM），以其出色的规划和推理能力而闻名，已被整合到ADS中以协助驾驶决策。然而，基于LLM的单Agent ADS面临着三大挑战：有限的感知能力、不足的合作以及高计算需求。为了应对这些问题，近期在基于LLM的多Agent ADS方面的研究重点在于改进Agent间的通信与合作。本文提供了一项关于基于LLM的多Agent ADS的前沿调查。我们首先介绍相关概念的背景，然后根据不同的Agent交互模式对现有的基于LLM的方法进行分类。接下来，我们将讨论在LLM-based Agent与人类互动的场景中的Agent-人交互。最后，我们总结了该领域的关键应用、数据集和挑战，以支持未来的研究。|
|**2025-02-24**|**AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay**|Ziyi Tang et.al.|[2502.16789](http://arxiv.org/abs/2502.16789)|null|阿尔法挖掘是量化投资中的一个关键组成部分，旨在发现未来资产回报的预测信号，但在日益复杂的金融市场中，阿尔法衰减的问题给阿尔法挖掘带来了巨大挑战。传统方法如遗传规划由于过拟合和复杂性而迅速衰减，而大型语言模型（LLM）驱动的方法虽然有其前景，但往往过于依赖现有知识，导致生成同质化因子，加剧了拥挤并加速了衰减。为了解决这一挑战，我们提出了AlphaAgent，这是一种自主框架，有效集成了具有特定正则化的LLM代理来挖掘抗衰减的阿尔法因子。AlphaAgent采用三个关键机制：(i) 通过基于抽象语法树（AST）的相似性度量对现有阿尔法进行原创性约束，(ii) 通过LLM评估市场假设与生成因子之间的语义一致性实现假设-因子对齐，以及(iii) 通过基于AST的结构约束控制复杂性，防止过度设计的构造，这些构造容易过拟合。这些机制共同引导阿尔法生成过程以平衡原创性、金融合理性以及适应不断变化的市场条件的能力，从而降低阿尔法衰减的风险。广泛的评估表明，AlphaAgent在多头和空头市场中均优于传统和基于LLM的方法，在过去四年中在中国CSI 500和美国S&P 500市场中持续产生显著的阿尔法。值得注意的是，AlphaAgent展示了对抗阿尔法衰减的显著能力，提升了产生强大因子的潜力。|
|**2025-02-23**|**Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System**|Saikat Barua et.al.|[2502.16750](http://arxiv.org/abs/2502.16750)|**[link](https://github.com/GitsSaikat/Guardians-Preventing-Jail-Break-Prompts)**|自主AI代理使用大型语言模型可以在社会的所有领域创造无可争议的价值，但它们面临着从对手那里产生的安全威胁，这需要立即采取保护措施，因为信任和安全问题随之出现。考虑到许多高级攻击，如多次越狱和欺骗性对齐，这些攻击无法通过在监督训练期间使用的静态护栏来缓解，这指出了现实世界鲁棒性的重要研究重点。静态护栏在动态多智能体系统中的结合未能防御这些攻击。我们希望通过开发新的评估框架来增强基于LLM的代理的安全性，这些框架能够识别并应对安全操作部署的威胁。我们的工作使用三种检查方法通过逆向图灵测试检测流氓代理，并通过多智能体模拟分析欺骗性对齐，并通过工具中介的对抗场景开发反越狱系统。我们对GEMINI 1.5 pro和llama-3.3-70B、deepseek r1模型进行了测试。检测能力很强，例如GEMINI 1.5 pro的准确率达到94%，但当受到长时间攻击时，系统存在持续的漏洞，随着提示长度的增加，攻击成功率（ASR）和多样性指标变得无效，同时揭示了多个复杂的系统故障。研究结果表明，采用灵活的安全系统是必要的，这种系统可以由代理自身进行主动监控，并且可以通过系统管理员进行适应性干预，因为当前的模型可能会产生导致不可靠和脆弱系统的漏洞。因此，在我们的工作中，我们试图解决这种情况并提出一个综合框架以应对安全问题。|
|**2025-02-23**|**RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents**|Sho Nakatani et.al.|[2502.16730](http://arxiv.org/abs/2502.16730)|null|我们提出了RapidPen，这是一个全自动的渗透测试框架，旨在解决从IP地址到获取shell过程中的初始立足点问题，且无需人工干预。与之前主要关注于渗透测试后期工作或需要人工参与的方法不同，RapidPen利用大型语言模型（LLM）自主发现并利用漏洞，从单个IP地址开始。通过整合先进的ReAct风格任务规划（Re）与检索增强型成功攻击案例知识库，以及命令生成和直接执行反馈循环（Act），RapidPen系统地扫描服务，识别可行的攻击路径，并以全自动方式执行针对性的攻击。  在针对Hack The Box平台上的一个易受攻击的目标进行评估时，RapidPen在200至400秒内获得了shell访问权限，每次运行的成本约为0.3至0.6美元，在重用先前“成功案例”数据的情况下达到了60%的成功率。这些结果突显了真正自动化的渗透测试对于安全新手和资深专家的潜力。没有专职安全团队的组织可以使用RapidPen快速识别关键漏洞，而专业的渗透测试人员可以将重复性任务交给RapidPen处理，专注于更复杂的挑战。最终，我们的工作旨在使渗透测试更加便捷和成本效益更高，从而提升现代软件生态系统的整体安全性。|
|**2025-02-23**|**BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning**|Haiteng Zhao et.al.|[2502.16660](http://arxiv.org/abs/2502.16660)|**[link](https://github.com/zhao-ht/biomaze)**|**大型语言模型（LLMs）在各个生物领域的应用最近得到了探索，但它们在复杂生物系统如通路中的推理能力仍缺乏研究，这对预测生物现象、提出假设和设计实验至关重要。这项工作探讨了LLMs在通路推理方面的潜力。我们介绍了BioMaze，这是一个包含5.1K个源自真实研究的复杂通路问题的数据集，涵盖了包括自然动态变化、扰动、附加干预条件以及多尺度研究目标在内的各种生物学背景。我们对方法如CoT和图增强推理的评估表明，LLMs在通路推理方面存在困难，尤其是在受扰系统中。为了解决这个问题，我们提出了PathSeeker，这是一种通过基于子图导航增强推理的LLM代理，能够更有效地处理生物学系统中的复杂性，并以科学合理的方式进行推理。数据集和代码可在https://github.com/zhao-ht/BioMaze获取。**|
|**2025-02-21**|**Position: Standard Benchmarks Fail -- LLM Agents Present Overlooked Risks for Financial Applications**|Zichen Chen et.al.|[2502.15865](http://arxiv.org/abs/2502.15865)|null|当前的金融领域大型语言模型（LLM）代理基准测试是不充分的。它们优先考虑任务性能而忽略了根本性的安全风险。诸如幻觉、时间错位和对抗性漏洞等威胁在高风险金融环境中构成了系统性风险，但现有的评估框架未能捕捉到这些风险。我们坚定地认为：传统的基准测试不足以确保金融领域LLM代理的可靠性。为了解决这个问题，我们分析了现有的金融LLM代理基准测试，发现了安全方面的不足，并引入了十种风险感知评估指标。通过API基础和开源权重LLM代理的实证评估，我们揭示了隐藏的漏洞，这些漏洞未被常规评估所发现。为了推动该领域的发展，我们提出了安全感知评估代理（SAEA），它基于一个三级评估框架，从模型层面（内在能力）、工作流程层面（多步骤过程的可靠性）以及系统层面（集成鲁棒性）评估代理。我们的研究结果强调了重新定义LLM代理评估标准的紧迫性，即将重点从原始性能转移到安全性、鲁棒性和现实世界中的韧性上。|
|**2025-02-21**|**WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM Agents**|Xinhang Liu et.al.|[2502.15601](http://arxiv.org/abs/2502.15601)|null|构建逼真的虚拟世界在各个领域都有应用，但通常需要高度专业训练的人员使用传统的三维建模软件进行大量劳动。为了使这一过程民主化，我们引入了WorldCraft系统，该系统利用程序生成让大型语言模型（LLM）代理创建带有物体的室内和室外场景，允许用户通过直观的自然语言命令控制单个物体属性和场景布局。在我们的框架中，一个协调者代理管理整个过程，并与两个专业的LLM代理合作完成场景创建：ForgeIt通过集成不断增长的手动验证来实现精确的单一物体定制，ArrangeIt则制定层次优化问题以实现平衡人体工程学和美学考虑的布局。此外，我们的流程还包括一个轨迹控制代理，允许用户通过自然语言互动来动画化场景并操作摄像机。我们的系统还兼容现成的深度三维生成器以丰富场景资产。通过评估并与最先进的方法进行比较，我们展示了WorldCraft的多功能性，从单一物体定制到复杂的大型室内和室外场景设计。该系统使非专业人士能够将自己的创意愿景变为现实。|
|**2025-02-21**|**Construction and Evaluation of LLM-based agents for Semi-Autonomous penetration testing**|Masaya Kobayashi et.al.|[2502.15506](http://arxiv.org/abs/2502.15506)|null|随着高性能的大语言模型（LLMs）如GPT、Claude和Gemini的出现，任务的自主和半自主执行在各个领域都取得了显著进展。然而，在网络安全等高度专业化领域，完全自治仍是一个挑战。这一难题主要源于大语言模型在推理能力和领域特定知识方面的局限性。我们提出了一种系统，通过采用多个LLM模块来制定攻击策略、生成命令和分析结果，从而实现复杂网络安全工作流程的半自主执行，以此解决上述问题。我们的实验使用了Hack The Box虚拟机，确认该系统能够自主构建攻击策略、发出适当命令并自动化某些过程，从而减少手动干预的需求。|
|**2025-02-21**|**Textual-to-Visual Iterative Self-Verification for Slide Generation**|Yunqing Xu et.al.|[2502.15412](http://arxiv.org/abs/2502.15412)|null|生成演示文稿幻灯片是一项耗时的任务，迫切需要自动化。由于现有自主大型语言模型（LLM）代理的灵活性有限且缺乏自动精炼机制，它们在现实世界中的适用性受到限制。我们将生成缺失演示文稿幻灯片的任务分解为两个关键组成部分：内容生成和布局生成，这与创建学术幻灯片的典型过程相一致。首先，我们介绍了一种内容生成方法，通过结合周围幻灯片的上下文并利用部分检索策略来增强连贯性和相关性。对于布局生成，我们提出了一种文本到视觉的自验证过程，使用基于LLM的审阅者+优化器工作流程，将复杂的文本布局转换为直观的视觉格式。这种模态转换简化了任务，使得准确且类似人类的审阅和优化成为可能。实验表明，我们的方法在对齐、逻辑流程、视觉吸引力和可读性方面显著优于基线方法。|
|**2025-02-21**|**ARS: Automatic Routing Solver with Large Language Models**|Kai Li et.al.|[2502.15359](http://arxiv.org/abs/2502.15359)|**[link](https://github.com/Ahalikai/ARS-Routbench)**|现实世界的车辆路径问题（VRP）的特点是存在各种实际约束条件，使得手动设计求解器既耗时又耗知识。尽管自动设计路由算法的兴趣日益增加，现有的研究仅探索了有限的VRP变体，并未能充分解决现实世界中遇到的复杂且普遍存在的约束条件。为填补这一空白，本文介绍了RoutBench，这是一个由24个属性衍生出的1000个VRP变体的基准，用于评估自动路由求解器在处理复杂约束方面的有效性。同时，我们提出了自动路由求解器（ARS），它利用大规模语言模型（LLM）代理通过自动生成基于问题描述和从数据库中选择的几个代表性约束的约束感知启发式代码，来增强一个基础算法框架。我们的实验表明，ARS的表现优于最先进的基于LLM的方法和常用的求解器，能够自动解决91.67%的常见VRP，并在所有基准测试中至少提高30%。|
|**2025-02-21**|**Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs**|Tingting Chen et.al.|[2502.15224](http://arxiv.org/abs/2502.15224)|null|鉴于大型语言模型（LLMs）的卓越表现，一个重要的问题出现了：LLMs能否进行类似人类的科学研究并发现新知识，充当AI科学家？科学发现是一个迭代过程，需要高效的更新和编码知识。它涉及理解环境、识别新的假设以及推理行动；然而，目前没有专门设计用于评估LLM代理在科学发现方面的标准化基准。针对这些局限性，我们引入了一个名为\textit{Auto-Bench}的新基准，该基准涵盖了评估LLMs在自然科学和社会科学中进行科学发现所需的关键方面。我们的基准基于因果图发现的原则。它挑战模型揭示隐藏的结构并做出最优决策，这包括生成有效的理由。通过与一个oracle进行交互，模型通过战略性干预逐步完善对潜在相互作用的理解，包括化学和社会互动。我们评估了最先进的LLMs，包括GPT-4、Gemini、Qwen、Claude和Llama，并观察到随着问题复杂性的增加，性能显著下降，这表明机器智能和人类智能之间存在一个重要差距，未来LLMs的发展需要考虑这一点。|
|**2025-02-20**|**Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models**|Yeonjun In et.al.|[2502.15086](http://arxiv.org/abs/2502.15086)|**[link](https://github.com/yeonjun-in/u-safebench)**|随着大型语言模型（LLM）代理的使用日益增多，其安全漏洞也变得越来越明显。现有的广泛基准测试通过定义安全标准主要依赖于通用标准，而忽视了用户特定的标准。然而，LLM的安全标准可能因用户特定的配置文件而异，而不是在所有用户之间保持一致。这就提出了一个重要研究问题：当考虑用户特定的安全标准时，LLM代理是否安全地行动？尽管这对LLM的安全使用至关重要，但目前尚不存在评估LLM用户特定安全性的基准数据集。为了解决这一差距，我们介绍了U-SAFEBENCH，这是首个旨在评估LLM安全性的用户特定方面基准。我们对18个广泛使用的LLM进行的评估表明，当前的LLM在考虑用户特定安全标准时未能安全地行动，这在该领域是一个新的发现。为了应对这一漏洞，我们提出了一种基于思维链的简单补救措施，并证明了其在提高用户特定安全性方面的有效性。我们的基准和代码可在https://github.com/yeonjun-in/U-SafeBench获取。|
|**2025-02-21**|**I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search**|Zujie Liang et.al.|[2502.14693](http://arxiv.org/abs/2502.14693)|null|近期大型语言模型（LLMs）在自动化机器学习任务方面展示了显著的潜力。然而，现有的基于LLMs的代理通常难以生成具有低多样性和次优性的代码。尽管最近的研究引入了蒙特卡洛树搜索（MCTS）来解决这些问题，但在生成的思维质量和多样性以及用于节点选择的标量值反馈机制方面仍然存在局限性。在这项研究中，我们介绍了一种名为内省蒙特卡洛树搜索（I-MCTS）的新方法，该方法通过一个内省过程逐步扩展树节点，通过仔细分析父节点和兄弟节点的解决方案和结果，从而对搜索树中的节点进行持续细化，进而提升整体决策过程。此外，我们整合了一个基于大型语言模型（LLM）的价值模型，以便在进行全面计算展开之前直接评估每个节点的解决方案。采用混合奖励机制以实现从LLM估算分数到实际性能分数的无缝转换。这使得质量更高的节点能够被更早地探索。应用于各种机器学习任务时，我们的方法相较于强大的开源自动机器学习代理，在性能上绝对提升了6%，证明了其在增强自主机器学习系统方面的有效性。资源可在<https://github.com/jokieleung/I-MCTS>获取。|
|**2025-02-20**|**InstructAgent: Building User Controllable Recommender via LLM Agent**|Wujiang Xu et.al.|[2502.14662](http://arxiv.org/abs/2502.14662)|**[link](https://github.com/wujiangxu/iagent)**|**传统的推荐系统通常采用用户-平台范式，用户直接暴露在平台的推荐算法控制下。然而，这种范式下的推荐算法缺陷可能会使用户处于非常脆弱的位置。首先，许多复杂的模型往往以商业目标为导向，侧重于平台的利益，这可能阻碍它们保护和捕捉用户真实兴趣的能力。其次，这些模型通常使用所有用户的数据显示优化，可能会忽略个别用户的偏好。由于这些缺点，用户在传统的用户-平台直接暴露范式下可能会遇到几个不利因素，如对推荐系统的控制不足、可能被平台操纵、回音室效应，或者由于活跃用户在协同学习中的主导地位导致不活跃用户缺乏个性化推荐。因此，迫切需要开发一种新范式来保护用户利益并缓解这些问题。最近，一些研究人员引入了LLM代理来模拟用户行为，这些方法主要旨在优化平台性能，但核心问题在推荐系统中仍未解决。为了解决这些局限性，我们提出了一种新的用户-代理-平台范式，在这种范式中，代理作为用户与推荐系统之间的保护屏障，实现了间接曝光。为此，我们首先构建了四个推荐数据集，分别记作 $\dataset$ ，并为每个记录提供了用户指令。**|
|**2025-02-20**|**Plan-over-Graph: Towards Parallelable LLM Agent Schedule**|Shiqi Zhang et.al.|[2502.14563](http://arxiv.org/abs/2502.14563)|**[link](https://github.com/zsq259/plan-over-graph)**|**大型语言模型（LLMs）在任务规划的推理方面展现出了卓越的能力。然而，对于并行日程安排的挑战仍未得到充分探索。本文介绍了一种新的范式，即plan-over-graph，在这种范式下，模型首先将现实生活中的文本任务分解成可执行的子任务，并构建一个抽象的任务图。然后，该模型将此任务图作为输入并生成一个并行执行的计划。为了增强复杂、可扩展图的规划能力，我们设计了一个自动化且可控的流程来生成合成图，并提出了一种两阶段训练方案。实验结果表明，我们的plan-over-graph方法显著提高了基于API的LLMs和可训练开源LLMs的任务性能。通过将复杂任务规范化为图形，我们的方法自然支持并行执行，展示了全局效率。代码和数据可在<https://github.com/zsq259/Plan-over-Graph>获取。**|
|**2025-02-20**|**MLGym: A New Framework and Benchmark for Advancing AI Research Agents**|Deepak Nathani et.al.|[2502.14499](http://arxiv.org/abs/2502.14499)|null|我们介绍了Meta MLGym和MLGym-Bench，这是一个新的框架和基准，用于评估和发展在AI研究任务上的大型语言模型（LLM）代理。这是第一个针对机器学习（ML）任务的Gym环境，使研究者能够研究强化学习（RL）算法来训练这些代理。MLGym-Bench包含了来自计算机视觉、自然语言处理、强化学习和博弈论等多个领域的13个多样化且开放式的AI研究任务。解决这些任务需要实际的AI研究技能，如生成新想法和假设、创建和处理数据、实现ML方法、训练模型、运行实验、分析结果，并通过迭代这一过程来改进给定任务的表现。我们在这些基准上评估了一些前沿的大型语言模型（LLMs），例如Claude-3.5-Sonnet、Llama-3.1 405B、GPT-4o、o1-preview和Gemini-1.5 Pro。我们的MLGym框架易于添加新任务、整合并评估模型或代理、大规模生成合成数据以及开发用于训练代理的新学习算法以应对AI研究任务。我们发现当前的前沿模型可以在给定的基线上有所提升，通常通过找到更好的超参数实现这一点，但它们并未生成新颖的假设、算法、架构或显著的改进。我们开源了我们的框架和基准，以促进未来的研究，推动LLM代理的AI研究能力的发展。|
|**2025-02-20**|**Enhancing Language Multi-Agent Learning with Multi-Agent Credit Re-Assignment for Interactive Environment Generalization**|Zhitao He et.al.|[2502.14496](http://arxiv.org/abs/2502.14496)|**[link](https://github.com/THUNLP-MT/CollabUIAgents)**|LLM基的代理在诸如移动操作和网页浏览等交互环境中取得了显著进展，并且在使用计算机之外的其他领域也表现出色。当前多代理系统在性能上普遍优于单个代理，但由于预定义的角色和不充分的策略来推广语言代理，这些系统在不同环境中的泛化能力较弱。实现强大的性能和良好的泛化能力一直是多代理系统发展的瓶颈。为了解决这些问题，我们提出了CollabUIAgents，这是一种具有新颖多代理信用重分配（CR）策略的多代理强化学习框架。该策略通过语言模型而不是特定于环境的奖励来分配过程奖励，并通过合成偏好数据进行学习，以促进无角色代理策略之间的可泛化的协作行为。实验结果表明，我们的框架提高了多代理系统的性能和跨环境的泛化能力。此外，我们的70亿参数系统在结果上与或超过了强大的闭源模型，并且指导CR的语言模型也表现良好。我们还提供了有效利用细粒度CR奖励进行环境泛化的见解，并适应训练好的语言模型在多代理系统中的应用。|
|**2025-02-20**|**FlowAgent: Achieving Compliance and Flexibility for Workflow Agents**|Yuchen Shi et.al.|[2502.14345](http://arxiv.org/abs/2502.14345)|**[link](https://github.com/lightblues/flowagent)**|在大型语言模型（LLM）的工作流集成中，LLM驱动的代理能够执行预定义的过程，从而增强现实世界应用中的自动化。传统基于规则的方法倾向于限制LLM的内在灵活性，因为它们预定义的执行路径限制了模型的动作空间，尤其是在遇到超出工作流（OOW）的查询时。相反，基于提示的方法允许LLM完全控制流程，这可能导致程序合规性的减弱。为了解决这些挑战，我们引入了FlowAgent，这是一种新颖的代理框架，旨在保持合规性和灵活性。我们提出了程序描述语言（PDL），它结合了自然语言的适应性和代码的精确性来制定工作流。在此基础上，我们开发了一个全面的框架，使LLM能够有效地管理OOW查询，同时在一套控制器的监督下保持执行路径。此外，我们提出了一种新的评估方法，以严格评估LLM代理处理OOW场景的能力，超越了现有基准测试中例行流程合规性的测试。在三个数据集上的实验表明，FlowAgent不仅遵守工作流，还能有效管理OOW查询，突显其在合规性和灵活性方面的双重优势。代码可在https://github.com/Lightblues/FlowAgent获取。|
|**2025-02-20**|**STeCa: Step-level Trajectory Calibration for LLM Agent Learning**|Hanlin Wang et.al.|[2502.14276](http://arxiv.org/abs/2502.14276)|**[link](https://github.com/WangHanLinHenry/STeCa)**|**大型语言模型（LLM）代理在通过与环境的动态交互解决复杂任务方面显示出潜力。现有工作主要集中在从专家演示中进行行为克隆和通过探索性轨迹采样进行偏好学习。然而，这些方法在长期任务中往往难以应对，其中次优动作逐步累积，导致代理偏离正确的任务轨迹。为了解决这一问题，我们强调了及时校准的重要性，并提出了自动构建校准轨迹以训练代理的需求。我们提出了一种名为Step-Level Trajectory Calibration (STeCa)的新框架，用于LLM代理学习。具体而言，STeCa通过探索期间的步骤级奖励比较来识别次优动作。它利用基于LLM的反思构建校准轨迹，使代理能够从改进的决策过程中学习。这些校准轨迹与成功轨迹数据一起用于强化训练。广泛的实验表明，STeCa显著优于现有方法。进一步分析表明，步骤级校准使代理能够更稳健地完成任务。我们的代码和数据可在<https://github.com/WangHanLinHenry/STeCa>获取。**|
|**2025-02-19**|**Investigating Non-Transitivity in LLM-as-a-Judge**|Yi Xu et.al.|[2502.14074](http://arxiv.org/abs/2502.14074)|null|基于大型语言模型（LLM）的自动评估方法正成为评估LLM基代理指令跟随能力的标准工具。在这一范式中最常用的方法是与基线模型进行成对比较，但这种方法严重依赖于偏好传递性的假设。然而，这种假设的有效性尚未得到充分探索。在这项研究中，我们调查了AlpacaEval框架内非传递性的存在，并分析了其对模型排名的影响。我们发现LLM评审员表现出非传递性的偏好，导致排名对基线模型的选择敏感。为了缓解这个问题，我们展示了通过使用布拉德利-特里模型结合循环赛可以产生更可靠的排名。值得注意的是，我们的方法提高了与Chatbot Arena的Spearman相关性和Kendall相关性（从95.0%提高到96.4%，从82.1%提高到86.3%）。为了解决循环赛计算成本高的问题，我们提出了瑞士智迭代匹配（Swim）比赛，采用动态匹配策略来获得循环赛的好处同时保持计算效率。|
|**2025-02-19**|**Autellix: An Efficient Serving Engine for LLM Agents as General Programs**|Michael Luo et.al.|[2502.13965](http://arxiv.org/abs/2502.13965)|null|大型语言模型（LLM）的应用正在超越简单的聊天机器人，发展成为能够推理、探索和解决复杂任务的动态通用代理程序。然而，现有的LLM服务系统忽略了程序之间以及调用之间的依赖关系，从而错失了显著的优化机会。我们的分析表明，提交给LLM服务引擎的程序经历了较长的累积等待时间，这主要是由于单个LLM请求和程序层面的队首阻塞造成的。为了解决这一问题，我们引入了Autellix，这是一种将程序视为第一类实体以最小化其端到端延迟的LLM服务系统。Autellix拦截程序提交的LLM调用，并向调度器提供程序级别的上下文。我们提出了两种调度算法——适用于单线程和分布式程序的算法——这些算法根据程序先前完成的调用来抢占和优先处理LLM调用。评估表明，在各种LLM和代理工作负载下，与最先进的系统（如vLLM）相比，Autellix在相同延迟的情况下将程序吞吐量提高了4到15倍。|
|**2025-02-19**|**DataSciBench: An LLM Agent Benchmark for Data Science**|Dan Zhang et.al.|[2502.13897](http://arxiv.org/abs/2502.13897)|**[link](https://github.com/thudm/datascibench)**|**本文介绍了DataSciBench，这是一个全面的基准测试，用于评估大型语言模型（LLM）在数据科学中的能力。最近的相关基准测试主要集中在单一任务、容易获得的ground truth和简单的评估指标上，这限制了可评估任务的范围。相比之下，DataSciBench基于一个更全面且精心策划的自然且具有挑战性的提示集合，这些提示具有不确定的ground truth和评估指标。我们开发了一个半自动化的流程来生成ground truth（GT）并验证评估指标。该流程利用并实现了一种基于LLM的自一致性及人工验证策略，通过利用收集到的提示、预定义的任务类型以及聚合函数（指标）来生成准确的GT。此外，我们提出了一种创新的任务-函数-代码（TFC）框架，根据精确定义的指标和编程规则来评估每个代码执行结果。我们的实验框架涉及使用我们收集的一系列提示测试6个基于API的模型、8个开源通用模型和9个开源代码生成模型。这种方法旨在提供对LLM在数据科学中的更全面和严格的评估，揭示它们的优势和劣势。实验结果显示，基于API的模型在所有指标上均优于开源模型，而Deepseek-Coder-33B-Instruct在开源模型中得分最高。我们将在https://github.com/THUDM/DataSciBench发布所有代码和数据。**|
|**2025-02-19**|**AI Software Engineer: Programming with Trust**|Abhik Roychoudhury et.al.|[2502.13767](http://arxiv.org/abs/2502.13767)|null|大型语言模型（LLMs）在生成代码片段方面展现出了惊人的能力，有望通过人工智能（AI）自动化大量的软件工程工作。我们认为，成功部署AI软件工程师需要建立的信任水平应等于甚至大于通过人为驱动的软件工程实践所建立的信任水平。最近的趋势是朝着LLM代理发展，这为将LLMs的强大功能用于创建新代码与分析工具的功能相结合以增加对代码的信任提供了途径。本文档评论了LLM代理是否可能在未来主导软件工程工作流程，以及编程的重点是否会从大规模编程转向基于信任的编程。|
|**2025-02-19**|**An LLM-based Agent for Reliable Docker Environment Configuration**|Ruida Hu et.al.|[2502.13681](http://arxiv.org/abs/2502.13681)|**[link](https://github.com/bytedance/repo2run)**|环境配置是软件开发中的关键步骤，尤其是在处理不熟悉的代码仓库时。尽管大语言模型（LLMs）展示了完成软件工程任务的潜力，但现有的环境配置方法通常依赖于手动操作或脆弱的脚本，导致效率低下且结果不可靠。我们介绍了Repo2Run，这是首个基于LLM的代理，旨在完全自动化任意Python代码仓库的环境配置并生成可执行的Dockerfile。我们解决了两个主要挑战：（1）使LLM代理能够在隔离的Docker容器内配置环境；（2）确保成功配置过程被准确记录并转移到Dockerfile中而不会出错。为此，我们提出了原子配置合成，采用双环境架构（内部和外部环境）以及回滚机制来防止失败命令导致的环境“污染”，保证原子执行（完全执行或根本不执行），并提供Dockerfile生成器以将成功的配置步骤转化为可运行的Dockerfile。我们在我们提出的包含420个近期Python代码仓库的基准测试上评估了Repo2Run，并通过单元测试验证其性能，结果显示其成功率为86.0%，比最好的基线高出63.9%。|
|**2025-02-18**|**Demonstrating specification gaming in reasoning models**|Alexander Bondarenko et.al.|[2502.13295](http://arxiv.org/abs/2502.13295)|null|我们展示了通过指导模型对抗国际象棋引擎来赢得比赛的方式来说明大型语言模型（LLM）代理的规格博弈行为。我们发现，像o1预览和DeepSeek-R1这样的推理模型通常会默认通过黑客手段来破解基准测试，而像GPT-4o和Claude 3.5 Sonnet这样的语言模型则需要被告知正常玩法行不通才能进行破解。我们的研究改进了之前的工作（如Hubinger等人，2024；Meinke等人，2024；Weij等人，2024），通过使用现实的任务提示并避免过度引导。我们的结果显示，面对难题时，推理模型可能会诉诸于黑客手段来解决问题，这与OpenAI（2024）在网络安全能力测试中的o1 Docker逃脱观察相一致。|
|**2025-02-18**|**Interactive Agents to Overcome Ambiguity in Software Engineering**|Sanidhya Vijayvargiya et.al.|[2502.13069](http://arxiv.org/abs/2502.13069)|**[link](https://github.com/sani903/interactivesweagents)**|AI代理越来越多地被部署来自动化任务，而这些任务通常基于模糊和不明确的用户指令。由于假设不当或未能提出澄清问题而导致的结果不佳、因工具误用带来的安全风险以及浪费的计算资源都是常见问题。在本研究中，我们探讨了大型语言模型（LLM）代理在交互式代码生成场景中处理模糊指令的能力，通过评估专有模型和开源模型在三个关键步骤上的表现：(a) 利用交互性改善模糊情况下的性能，(b) 检测模糊性，以及(c) 提出有针对性的问题。我们的发现揭示，模型难以区分指令是否明确。然而，当模型对指令不明确时进行交互时，它们能够有效地从用户那里获得重要信息，从而显著提高性能，并强调了有效交互的价值。本研究指出了当前最先进的模型在处理复杂软件工程任务中的模糊性方面的关键差距，并将评估结构化为不同的步骤，以便进行有针对性的改进。|
|**2025-02-18**|**Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents**|Chaoran Chen et.al.|[2502.13012](http://arxiv.org/abs/2502.13012)|null|角色扮演代理（RPA）是一种越来越受欢迎的基于大型语言模型（LLM）的代理，能够在各种任务中模拟类似人类的行为。然而，由于任务需求和代理设计的多样性，评估RPA具有挑战性。本文通过系统地回顾2021年1月至2024年12月期间发表的1,676篇论文，提出了一个基于证据、可操作且普适的LLM基础RPA评估设计指南。我们的分析从现有文献中识别出六种代理属性、七种任务属性以及七种评估指标。基于这些发现，我们提出了RPA评估设计指南，以帮助研究人员开发更加系统和一致的评估方法。|
|**2025-02-18**|**Towards more Contextual Agents: An extractor-Generator Optimization Framework**|Mourad Aouini et.al.|[2502.12926](http://arxiv.org/abs/2502.12926)|null|大型语言模型（LLM）为基础的代理在解决广泛的一般性应用中的复杂任务时已经取得了显著的成功。然而，在特定领域的场景中，比如专业行业或研究领域，由于缺乏领域相关的知识，它们的表现往往会下降，导致结果不够精确或不理想。为了解决这一挑战，我们的工作介绍了一种系统的方法来增强基于LLM的代理的上下文适应能力，通过优化其底层的关键组件——提示词，这些提示词控制着代理的行为、角色和交互。手动为特定上下文的任务优化提示词是一项劳动密集型、容易出错且缺乏可扩展性的任务。在这项工作中，我们引入了一个提取生成框架，旨在自动化优化上下文相关的基于LLM的代理。我们的方法通过两个关键阶段进行操作：(i) 从一组黄金标准输入输出示例中提取特征，以及(ii) 通过高级优化策略生成提示词，该策略迭代地识别表现不佳的情况并应用自我改进技术。此框架通过使多样化的输入更加精确的一致性推广，大大提高了提示词的适应能力，特别是在特定上下文的任务中，保持语义一致性并最小化错误传播对于可靠性能至关重要。尽管该方法主要针对单阶段工作流程开发，但它自然可以扩展到多阶段工作流程，为各种基于代理的系统提供广泛的应用。实证评估表明，我们的框架显著提升了经过提示词优化的代理的性能，提供了一种结构化且高效的方法来处理基于LLM的上下文代理。|
|**2025-02-18**|**Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols**|Kathrin Seßler et.al.|[2502.12842](http://arxiv.org/abs/2502.12842)|null|有效的反馈对于促进学生在科学探究中的成功至关重要。随着人工智能的发展，大型语言模型（LLMs）为即时和适应性反馈提供了新的可能性。然而，这种反馈通常缺乏由现实世界从业者提供的教学验证。为了解决这一局限性，我们的研究评估并比较了LLM代理与教师和科学教育专家对学生撰写实验方案的反馈质量。四位匿名评审者，均为科学探究和科学教育领域的专业人士，使用五点李克特量表根据有效反馈的六个标准（包括Feed Up、Feed Back、Feed Forward、建设性语气、语言清晰度和技术术语）对LLM代理生成的反馈文本、教师的反馈以及科学教育专家的反馈进行了评价。我们的结果显示，LLM生成的反馈在总体质量上与教师和专家的反馈没有显著差异。然而，在涉及识别和解释学生作品上下文中错误的Feed Back维度中，LLM代理的表现较差。定性分析突显了LLM代理在上下文理解以及明确传达特定错误方面的局限性。我们的研究结果表明，将LLM生成的反馈与人类专业知识相结合可以增强教育实践，利用LLM的效率和教育工作者的细微理解。|
|**2025-02-18**|**DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent**|Pengyu Zhu et.al.|[2502.12575](http://arxiv.org/abs/2502.12575)|**[link](https://github.com/whfelingyu/demonagent)**|随着基于大语言模型的代理越来越普遍，用户查询或环境反馈可以植入代理中的后门引发了关于安全漏洞的关键问题。然而，后门攻击通常可以通过分析代理推理过程的安全审计来检测。为此，我们提出了一种新的后门植入策略，称为“动态加密多后门植入攻击”。具体而言，我们引入了动态加密，它将后门映射到良性内容上，有效地规避了安全审计。为了增强隐蔽性，我们将后门进一步分解为多个子后门片段。基于这些改进，后门得以显著地规避安全审计。此外，我们提出了AgentBackdoorEval，这是一个用于全面评估代理后门攻击的数据集。在多个数据集上的实验结果表明，我们的方法达到了接近100%的攻击成功率，同时保持了0%的检测率，展示了其在规避安全审计方面的有效性。我们的研究结果突显了现有安全机制在检测高级攻击方面的局限性，强调了对更强大的防御措施以应对后门威胁的迫切需求。代码和数据可在<https://github.com/whfeLingYu/DemonAgent>获取。|
|**2025-02-18**|**UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design**|Yuxuan Lu et.al.|[2502.12561](http://arxiv.org/abs/2502.12561)|**[link](https://github.com/neuhai/uxagent)**|可用性测试是用户体验（UX）研究者评估网页设计的一种基本但具有挑战性的方法（例如，难以迭代研究设计中的缺陷并且很难招募到研究参与者）。近期大型语言模型模拟代理（LLM-Agent）研究的进步启发我们设计了UXAgent，以支持UX研究者在进行真实的人类主题研究之前评估并反复推敲其可用性测试研究设计。我们的系统包括一个LLM-Agent模块和一个通用浏览器连接器模块，使UX研究者能够自动生成数千名模拟用户来测试目标网站。结果以定性（例如，采访代理如何思考）、定量（例如，操作次数）以及视频记录的形式呈现，供UX研究者分析。通过五位UX研究者的启发式用户评估，参与者赞扬了我们系统的创新性但也表达了对LLM代理辅助的UX研究未来的担忧。|
|**2025-02-18**|**CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space**|Yong Zhao et.al.|[2502.12532](http://arxiv.org/abs/2502.12532)|**[link](https://github.com/tsinghua-fib-lab/CityEQA)**|Embodied Question Answering（EQA）主要集中在室内环境中，对于涵盖环境、动作和感知的复杂城市环境探索较少。为了填补这一空白，我们引入了CityEQA，这是一个新任务，在动态的城市空间中，通过主动探索让具身智能体回答开放式问题。为了支持这个任务，我们推出了CityEQA-EC，这是首个基准数据集，包含了六个类别的1,412个人工标注任务，并且基于一个现实的三维城市模拟器。此外，我们提出了一种名为规划者-管理者-执行者（Planner-Manager-Actor，PMA）的新颖智能体，专门用于CityEQA。PMA能够实现长时间范围内的规划和分层任务执行：规划者将回答问题分解成子任务，管理者在过程控制期间维护以对象为中心的认知地图进行空间推理，而专门的执行者处理导航、探索和收集等子任务。实验表明，PMA达到了人类水平回答准确性的60.7%，显著优于前沿基准。尽管前景可观，与人类相比的表现差距突显了在CityEQA中增强视觉推理的需求。这项工作为未来在城市空间智能方面的进步铺平了道路。数据集和代码可在<https://github.com/BiluYong/CityEQA.git>获取。|
|**2025-02-18**|**EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness**|Yunxiao Zhang et.al.|[2502.12494](http://arxiv.org/abs/2502.12494)|null|大型语言模型（LLMs）作为AI代理展示了显著的能力。然而，现有增强LLM代理能力的方法往往缺乏对数据质量的关注，导致在微调和提示工程过程中效率低下且结果次优。为了解决这一问题，我们引入了EDGE，这是一种新颖的方法，用于识别具有信息量的样本而无需黄金答案。我们提出了指导方针有效性（GE）指标，通过测量多轮交互任务中人类提供的指导方针的影响来选择具有挑战性的样本。低GE分数表明样本所需的专家知识没有包含在指导方针中，使得该样本更具信息量。通过选择具有低GE分数的样本，我们可以提高提示工程和微调过程的效率及结果。大量的实验验证了我们方法的有效性。我们的方法在HotpotQA和WebShop数据集上实现了竞争性的结果，分别只需要较少的数据量（75%和50%），同时超越了现有方法。我们还提供了对LLM代理微调数据质量的新视角。|
|**2025-02-18**|**EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning**|Xiaoqian Liu et.al.|[2502.12486](http://arxiv.org/abs/2502.12486)|null|大型语言模型（LLMs）在具有明确解决方案的定义良好的问题上，如数学和编码方面展现出了令人印象深刻的推理能力。然而，它们在复杂的真实世界场景中仍然存在困难，比如商业谈判，这需要战略推理——一种能够在动态环境中对齐长期目标的能力。现有的战略推理方法面临着适应性、可扩展性和将策略转移到新背景下的挑战。为了解决这些问题，我们提出了显式策略优化（EPO），这是一种针对LLM的战略推理方法，它可以在开放式的行动空间中提供策略，并可以插入到任意的LLM代理中以激发目标导向的行为。为了提高适应性和策略的可转移性，我们通过多回合强化学习（RL）使用过程奖励和迭代自我博弈来训练战略推理模型，而不使用监督微调（SFT）作为初步步骤。实验涵盖了社交和物理领域，展示了EPO在通过增强战略推理实现长期目标对齐方面的能力，在社交对话和网页导航任务上达到了最先进的性能。我们的研究揭示了EPO中出现的各种协作推理机制及其在生成新策略方面的有效性，突显了其在真实世界应用中的战略推理潜力。|
|**2025-02-18**|**Investigating and Extending Homans' Social Exchange Theory with Large Language Model based Agents**|Lei Wang et.al.|[2502.12450](http://arxiv.org/abs/2502.12450)|**[link](https://github.com/paitesanshi/set)**|Homans的社会交换理论（SET）被广泛认为是理解人类文明和社会结构形成和发展的基本框架。在社会科学中，这一理论通常基于简单的模拟实验或真实的人类研究来学习，但这些方法要么缺乏真实性，要么过于昂贵而难以控制。在人工智能领域，大型语言模型（LLM）的最新进展展示了在模拟人类行为方面的巨大潜力。受此启发，我们采用跨学科的研究视角，提出使用基于LLM的代理来研究Homans的SET。具体而言，我们构建了一个由三个LLM代理组成的虚拟社会，并让他们参与一个社会交换游戏以观察其行为。通过广泛的实验，我们发现Homans的SET在我们的代理社会中得到了很好的验证，显示了代理行为与人类行为之间的一致性。在此基础上，我们故意改变代理社会的设置，扩展了传统的Homans的SET，使其更加全面和详细。据我们所知，本文标志着首次使用基于LLM的代理来研究Homans的SET。更重要的是，它介绍了一种新的可行的研究范式，通过基于LLM的代理将社会科学和计算机科学领域联系起来。代码可在<https://github.com/Paitesanshi/SET>获取。|
|**2025-02-17**|**HARBOR: Exploring Persona Dynamics in Multi-Agent Competition**|Kenan Jiang et.al.|[2502.12149](http://arxiv.org/abs/2502.12149)|null|我们研究了影响大型语言模型（LLM）代理在竞争性多智能体环境中的成功因素，使用拍卖作为测试平台，在这里代理竞价以最大化利润。这些代理配备了投标领域的知识、反映物品偏好的不同人格特征，并且具有对拍卖历史的记忆。我们的工作通过创建一个现实的环境扩展了经典的拍卖场景，在这个环境中，多个代理竞标房屋，考虑诸如大小、位置和预算等因素，以确保获得最理想的房屋并以最低价格购得。特别是，我们探讨了三个关键问题：(a) 人格如何影响代理在竞争环境中的行为？(b) 代理能否有效分析其竞争对手在拍卖中的行为？(c) 如何利用人格分析来利用策略优势，例如心智理论？通过一系列实验，我们分析了LLM代理的行为，并揭示了新的发现。我们的测试平台称为HARBOR，为深入理解竞争环境中多智能体工作流程提供了有价值的平台。|
|**2025-02-17**|**Scaling Autonomous Agents via Automatic Reward Modeling And Planning**|Zhenfang Chen et.al.|[2502.12130](http://arxiv.org/abs/2502.12130)|null|大型语言模型（LLMs）在各种文本生成任务中展示了显著的能力。然而，LLMs在需要多步决策和环境反馈的问题上仍然存在困难，例如在线购物、科学推理和数学问题解决。与纯文本数据不同，收集大规模决策数据具有挑战性。此外，许多强大的LLMs仅通过API访问，这由于成本和复杂性限制了它们在代理任务中的微调。为了应对LLM代理的局限性，我们提出了一种框架，可以从环境中自动学习奖励模型而无需人工注释。该模型可以用于评估LLM代理的动作轨迹，并为任务规划提供启发式方法。具体而言，我们的方法涉及使用基于LLM的代理随机导航环境，从而生成多样化的动作轨迹。随后，利用另一个LLM分配任务意图并合成每个轨迹的负面响应和正确响应。这些三元组（任务意图、正面响应和负面响应）被用作训练数据以优化能够评分动作轨迹的奖励模型。通过在不同的代理基准上进行的评估证明了我们框架的有效性和通用性。总之，我们提出的框架代表了增强LLM代理决策能力的重要进展。通过自动化奖励模型的学习，我们克服了数据稀缺性和API限制的挑战，可能革新LLMs在复杂和交互式环境中的应用。这项研究为开发能够解决广泛现实世界问题的更复杂AI代理铺平了道路。|
|**2025-02-17**|**A-MEM: Agentic Memory for LLM Agents**|Wujiang Xu et.al.|[2502.12110](http://arxiv.org/abs/2502.12110)|**[link](https://github.com/wujiangxu/agenticmemory)**|虽然大型语言模型（LLM）代理可以有效地使用外部工具来处理复杂的现实任务，但它们需要记忆系统来利用历史经验。当前的记忆系统仅能实现基本的存储和检索功能，尽管最近尝试引入了图数据库，但仍然缺乏复杂记忆组织。此外，这些系统的固定操作和结构限制了它们在多样任务中的适应性。为了解决这一局限，本文提出了一种新颖的代理记忆系统，该系统可以动态地以代理方式组织记忆。遵循Zettelkasten方法的基本原则，我们设计了记忆系统，通过动态索引和链接创建相互关联的知识网络。当新增记忆时，我们生成包含多个结构化属性的综合笔记，包括上下文描述、关键词和标签。系统随后分析历史记忆以识别相关连接，在有意义的相似之处建立联系。此外，这一过程还实现了记忆演化——随着新记忆的整合，它们可以触发对现有历史记忆的上下文表示和属性的更新，从而使记忆网络能够持续优化其理解。我们的方法结合了Zettelkasten的结构化组织原则与代理驱动决策的灵活性，实现了更适应和上下文感知的记忆管理。在六个基础模型上的实验证明了相对于现有最先进的基线方法的优越改进。源代码可在https://github.com/WujiangXu/AgenticMemory获取。|
|**2025-02-17**|**Can LLM Agents Maintain a Persona in Discourse?**|Pranav Bhandari et.al.|[2502.11843](http://arxiv.org/abs/2502.11843)|null|大型语言模型（LLMs）作为对话代理被广泛应用于教育、法律、医学等多个领域。然而，这些模型经常表现出上下文转换的行为，导致缺乏一致且符合其个性的交互。对心理特征的遵循缺乏全面分析，特别是在双人对话的情况下。我们从两个角度探讨了这一挑战：首先使用两个对话代理针对某一主题进行对话，并根据OCEAN框架（开放性、尽责性、外向性、亲和力和神经质）分配高/低性格特质。随后，使用多个裁判代理来推断最初分配的性格特质，以探索预测一致性、模型间的一致性和与分配个性的对齐情况。我们的研究结果表明，虽然LLMs可以在个性驱动的对话中得到引导，但它们保持性格特征的能力在很大程度上取决于模型组合和对话设置。这些不一致性突显了在LLMs中实现稳定且可解释的个性对齐交互所面临的挑战。|
|**2025-02-17**|**LLM Agents Making Agent Tools**|Georg Wölflein et.al.|[2502.11705](http://arxiv.org/abs/2502.11705)|null|工具的使用已经将大型语言模型（LLMs）转变为能够通过动态利用外部软件组件来执行复杂多步任务的强大代理。然而，这些工具必须由人类开发者提前实现，这限制了LLM代理在需要大量高度专业化工具的领域中的应用，如生命科学和医学。受科学研究越来越多地伴随公共代码仓库的趋势启发，我们提出了ToolMaker，这是一种新颖的自主框架，能够将带有代码的论文转化为LLM兼容的工具。给定一个简短的任务描述和一个代码库URL，ToolMaker能够自主安装所需的依赖项并生成执行该任务的代码，使用闭环自我修正机制来迭代诊断和修复错误。为了评估我们的方法，我们引入了一个包含15个多样化且复杂的计算任务的基准测试，这些任务跨越了医学和非医学领域，并有超过100个单元测试以客观评估工具的正确性和鲁棒性。ToolMaker正确实现了80%的任务，显著优于当前最先进的软件工程代理。因此，ToolMaker是向全自主代理科学工作流程迈出的一步。|
|**2025-02-17**|**Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation**|Amin Qasmi et.al.|[2502.11649](http://arxiv.org/abs/2502.11649)|null|我们引入了一种新颖的非合作博弈模型来分析意见形成和抗性，融入了社会心理学中的确认偏误、资源限制和影响力惩罚等原则。我们的模拟展示了大型语言模型（LLM）代理在竞争影响人群时的情况，并对生成传播或反驳虚假信息的消息的行为施加了惩罚。此框架将资源优化整合到了代理的决策过程中。研究结果表明，较高的确认偏误虽然增强了群体内部的意见一致性，但也加剧了总体上的两极分化。相反，较低的确认偏误导致意见碎片化且个体信念的变化有限。大量投资于高资源辟谣策略最初可以使人群与辟谣代理保持一致，但存在快速耗尽资源和长期影响力减弱的风险。|
|**2025-02-17**|**AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection**|Weidi Luo et.al.|[2502.11448](http://arxiv.org/abs/2502.11448)|null|大型语言模型（LLMs）的快速发展使得它们能够被部署为处理动态环境中复杂任务的自主代理。这些LLMs展示了强大的问题解决能力和对多方面场景的适应性。然而，它们作为代理的使用也引入了显著的风险，包括任务特定风险和系统风险。任务特定风险由代理管理员基于特定任务需求和限制来识别，而系统风险源于其设计或交互中的漏洞，可能导致信息的机密性、完整性和可用性（CIA）受到损害，并引发安全风险。现有的防御机构未能适应性地有效缓解这些风险。在本文中，我们提出了AGrail，一种终身代理护栏，以增强LLM代理的安全性，该方法具有自适应安全检查生成、有效的安全检查优化以及工具兼容性和灵活性。广泛的实验表明，AGrail不仅在应对任务特定和系统风险方面表现出色，而且在不同LLM代理的任务之间还表现出可转移性。|
|**2025-02-17**|**SMART: Self-Aware Agent for Tool Overuse Mitigation**|Cheng Qian et.al.|[2502.11435](http://arxiv.org/abs/2502.11435)|**[link](https://github.com/qiancheng0/open-smartagent)**|当前的大语言模型（LLM）代理在推理和工具使用方面表现出色，但往往缺乏自我意识，无法有效地平衡这两种方法。这种不平衡导致了工具过度使用的问题，即模型在可以通过参数知识解决的任务中不必要地依赖外部工具，从而增加了计算开销。受人类元认知的启发，我们引入了SMART（战略性模型认知推理与工具使用）范式，以增强代理的自我意识，优化任务处理并减少工具过度使用。为了支持这一范式，我们引入了SMART-ER数据集，该数据集涵盖三个领域，在推理过程中交替使用参数知识和工具依赖步骤，并且每个步骤都附有解释何时需要使用工具的理由。通过监督训练，我们开发了SMARTAgent，这是一系列能够动态平衡参数知识和工具使用的模型。评估显示，SMARTAgent减少了24%的工具使用量，同时提高了超过37%的性能，使得70亿规模的模型能够匹配其700亿对应版本以及GPT-4o的表现。此外，SMARTAgent在分布外测试数据如GSM8K和MINTQA上也表现良好，保持了准确性，仅需五分之一的工具调用次数。这些结果突显了战略性工具使用在增强推理、缓解过度使用以及缩小模型规模与性能之间差距方面的潜力，推动了智能且资源高效的代理设计的发展。|
|**2025-02-17**|**\textsc{FLAG-Trader}: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading**|Guojun Xiong et.al.|[2502.11433](http://arxiv.org/abs/2502.11433)|null|大型语言模型（LLMs）经过多模态金融数据的微调，在各种金融任务中展示了出色的推理能力。然而，它们在交互式金融市场中的多步骤、目标导向场景中常常表现不佳，例如在需要复杂代理方法来改进决策的交易中。为了解决这一问题，我们提出了\textsc{FLAG-Trader}，这是一种统一架构，集成了语言处理（通过LLMs）和基于梯度的强化学习（RL）策略优化，其中部分微调的LLM充当策略网络，利用预训练知识的同时通过参数高效微调适应金融领域。通过基于交易奖励的策略梯度优化，我们的框架不仅提高了LLMs在交易中的表现，还在其他金融领域任务中提升了结果。我们提供了广泛的实证证据来验证这些改进。|
|**2025-02-17**|**TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents**|Geon Lee et.al.|[2502.11418](http://arxiv.org/abs/2502.11418)|null|时间序列数据在各种应用中至关重要，包括气候建模、医疗监测和金融分析。理解与真实世界时间序列数据相关的上下文信息对于准确可靠的事件预测通常是必不可少的。在本文中，我们介绍了TimeCAP，这是一种时间序列处理框架，它创造性地利用了大型语言模型（LLM）作为时间序列数据的上下文生成器，扩展了它们通常用作预测器的应用。TimeCAP包含两个独立的LLM代理：一个生成捕捉时间序列上下文的文本摘要，另一个则使用这个丰富的摘要来进行更明智的预测。此外，TimeCAP采用了一种多模态编码器，它与LLM代理协同工作，通过利用上下文示例来增强输入的相互增效作用，从而提高预测性能。实验结果表明，在真实世界的数据集上，TimeCAP在事件预测方面优于最先进的方法，包括那些利用LLM作为预测器的方法，并且在F1分数上平均提高了28.75%。|
|**2025-02-14**|**Process Reward Models for LLM Agents: Practical Framework and Directions**|Sanjiban Choudhury et.al.|[2502.10325](http://arxiv.org/abs/2502.10325)|**[link](https://github.com/sanjibanc/agent_prm)**|我们介绍了Agent Process Reward Models (AgentPRM)，这是一种简单且可扩展的框架，用于通过互动不断训练大型语言模型（LLM）代理以提升性能。AgentPRM遵循轻量级的动作-批评范式，使用蒙特卡罗滚动来计算奖励目标并优化策略。它对现有的强化学习从人类反馈（RLHF）管道所需的修改最小，使其易于大规模集成。除了AgentPRM之外，我们还提出了InversePRM，该方法直接从演示中学习过程奖励，而无需显式的成果监督。我们还探讨了关键挑战和机遇，包括探索、过程奖励塑造以及模型预测推理。我们在ALFWorld基准上进行了评估，结果显示使用AgentPRM和InversePRM训练的小型3B模型优于强大的GPT-4o基线，并分析了测试时扩展性、奖励操纵等问题。我们的代码可在以下地址获取：https://github.com/sanjibanc/agent_prm。|
|**2025-02-14**|**Automated Hypothesis Validation with Agentic Sequential Falsifications**|Kexin Huang et.al.|[2502.09858](http://arxiv.org/abs/2502.09858)|**[link](https://github.com/snap-stanford/POPPER)**|假设在信息获取、决策和发现中起着核心作用。然而，许多现实世界的假设都是抽象的、高层次的陈述，难以直接验证。这一挑战因大型语言模型（LLMs）生成假设而进一步加剧，这些模型容易产生幻觉，并且生成的假设数量使得手动验证变得不切实际。在这里，我们提出了Popper，这是一种代理框架，用于对自由形式的假设进行严格的自动化验证。受卡尔·波普尔证伪原则的指导，Popper使用LLM代理设计并执行针对其可测量推论的证伪实验。一种新颖的序贯检验框架确保严格控制I类错误的同时，积极地从多样化的观察中收集证据，无论是来自现有数据还是新进行的程序。我们在包括生物学、经济学和社会学在内的六个领域展示了Popper的应用。Popper提供了稳健的错误控制、高功效和可扩展性。此外，与人类科学家相比，Popper在验证复杂生物假设方面实现了相当的性能，同时减少了90%的时间，提供了一种可扩展且严谨的假设验证解决方案。|
|**2025-02-13**|**AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration**|Jizhou Chen et.al.|[2502.09809](http://arxiv.org/abs/2502.09809)|null|工具使用在大型语言模型（LLMs）中的集成使得具有实际影响力的自主系统成为可能。同时，与独立的LLMs相比，这些具备工具使用能力的代理系统如果被恶意利用，可能会执行造成更大影响的有害工作流程。我们提出了一种名为AgentGuard的框架，旨在自主发现并验证不安全的工作流程，随后生成安全约束以限制代理的行为，从而在部署时确保基本的安全保障。AgentGuard利用了LLM协调器的固有能力——了解工具功能、可扩展且现实的工作流程生成以及工具执行权限——作为其自身的安全评估者。该框架通过四个阶段运作：识别不安全的工作流程、在真实环境中验证它们、生成安全约束并验证约束的有效性。输出的评估报告包括不安全的工作流程、测试用例和经过验证的约束，能够支持多种安全应用。我们通过实验展示了AgentGuard的可行性。通过这项探索性工作，我们希望激发标准化测试和强化程序的建立，以增强LLM代理在实际应用中的可信度。|
|**2025-02-13**|**MDCrow: Automating Molecular Dynamics Workflows with Large Language Models**|Quintina Campbell et.al.|[2502.09565](http://arxiv.org/abs/2502.09565)|**[link](https://github.com/ur-whitelab/MDCrow)**|**分子动力学（MD）模拟对于理解生物分子系统至关重要，但实现自动化仍然具有挑战性。近期大型语言模型（LLM）在使用基于LLM的代理自动执行复杂科学任务方面取得了成功。本文介绍了一种名为MDCrow的代理型LLM助手，能够自动化MD工作流程。MDCrow通过超过40个专家设计的工具链式思考，处理和加工文件，设置模拟，分析模拟输出，并从文献和数据库中检索相关信息。我们在25项不同子任务和难度的任务上评估了MDCrow的表现，并评估了该代理对难度和提示风格的鲁棒性。\texttt{gpt-4o}能够在低方差下完成复杂任务，紧随其后的是\texttt{llama3-405b}，这是一个引人注目的开源模型。虽然最佳模型的性能不受提示风格的影响，但它对较小的模型有显著影响。**|
|**2025-02-14**|**RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage**|Peter Yong Zhong et.al.|[2502.08966](http://arxiv.org/abs/2502.08966)|null|基于工具的代理系统（TBAS）允许语言模型（LM）使用外部工具执行超出其独立能力的任务，如网站搜索、航班预订或金融交易。然而，这些工具极大地增加了提示注入攻击的风险，其中恶意内容劫持语言模型代理以泄露机密数据或触发有害操作。现有的防御措施（如OpenAI GPTs）要求用户在每次调用工具前进行确认，给用户带来沉重负担。我们引入了鲁棒TBAS（RTBAS），它能够自动检测和执行那些能够保持完整性和保密性的工具调用，仅在无法确保这些保障时才需要用户确认。RTBAS针对TBAS特有的挑战调整了信息流控制方法。我们提出了两种新的依赖筛选器，即利用LM作为裁判和基于注意力的显著性方法，以克服这些挑战。实验结果表明，在AgentDojo提示注入基准测试中，RTBAS能够防止所有针对性攻击，并且在遭受攻击时仅损失了2%的任务效用。进一步的测试证实了其在检测微妙和直接隐私泄露方面接近最优性能。|
|**2025-02-12**|**If Multi-Agent Debate is the Answer, What is the Question?**|Hangfan Zhang et.al.|[2502.08788](http://arxiv.org/abs/2502.08788)|null|多智能体辩论（MAD）作为一种有前景的方法，通过在推理过程中让多个智能体进行迭代讨论，以增强大型语言模型（LLMs）的事实准确性及推理质量。尽管具有潜力，我们认为当前的MAD研究在评估实践中存在关键缺陷，包括有限的数据集重叠和不一致的基线，这引发了关于其泛化能力的重大关切。相应地，本文系统性地评估了五种代表性的MAD方法，在九个基准数据集上使用了四种基础模型。令人惊讶的是，我们的研究发现，即使消耗额外的推理时间计算，MAD方法也未能可靠地超越简单的单智能体基线，如Chain-of-Thought和Self-Consistency。通过分析我们发现，模型异质性可以显著提升MAD框架的表现。我们提出了Heter-MAD，使单一LLM智能体能够访问来自异构基础模型的输出，从而提升现有MAD框架的性能。最后，我们概述了推进MAD的潜在方向，旨在引发更广泛的讨论并激发该领域未来的研究。|
|**2025-02-12**|**SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent**|Keyeun Lee et.al.|[2502.08599](http://arxiv.org/abs/2502.08599)|**[link](https://github.com/keyeun/spectrum-framework-llm)**|现有的模拟个体身份的方法往往过于简化人类的复杂性，可能导致不完整或扁平化的表征。为了解决这个问题，我们引入了SPeCtrum，这是一种基于框架的构造LLM代理人格的方法，通过整合个人的多维自我概念来实现真实的人格构建。SPeCtrum整合了三个核心组成部分：社会身份（S）、个人身份（P）和个人生活背景（C），每个部分都贡献了身份的不同但相互关联的方面。为了评估SPeCtrum在身份表征方面的有效性，我们进行了自动化和人工评估。自动化评估使用流行的戏剧角色表明，来自对偏好和日常生活的简短文章分析所衍生出的个人生活背景（C）比单独的社会身份（S）和个人身份（P）更能有效地模拟角色的身份，并且与完整的SPC组合表现相当。然而，在涉及现实世界个体的人工评估中发现，完整的SPC组合提供了比C单独提供更全面的自我概念表征。我们的研究结果表明，虽然C单独可能足以用于基本的身份模拟，但整合S、P和C可以增强现实世界身份表征的真实性和准确性。总体而言，SPeCtrum为LLM代理中的个体模拟提供了一种结构化方法，使得人机交互更加个性化，并提高了基于模拟的行为研究的真实性。|
|**2025-02-12**|**Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks**|Ang Li et.al.|[2502.08586](http://arxiv.org/abs/2502.08586)|null|近年来，许多机器学习安全文献集中于针对对齐的大规模语言模型（LLMs）的攻击。这些攻击可能提取私人信息或将模型胁迫成生成有害输出。在实际部署中，LLMs通常作为更大代理管道的一部分，包括记忆系统、检索、网络访问和API调用。这些额外组件引入了漏洞，使这些LLM驱动的代理比孤立的LLMs更容易受到攻击，但与此类LLM代理的安全性相关的研究相对较少。在本文中，我们分析了仅限于LLM代理的安全和隐私漏洞。我们首先提供了一种按威胁行为者、目标、入口点、攻击者可观测性、攻击策略以及代理管道固有漏洞分类的攻击 taxonomy。然后，我们对流行的开源和商业代理进行了一系列说明性攻击，展示了其漏洞的直接实际影响。值得注意的是，我们的攻击易于实施且无需理解机器学习。|
|**2025-02-13**|**Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation**|Mahnaz Koupaee et.al.|[2502.08514](http://arxiv.org/abs/2502.08514)|**[link](https://github.com/amazon-science/madisse)**|**基于大型语言模型（LLM）的忠实性评估器往往被文本的流畅性所迷惑，并且难以识别总结中的错误。我们提出了一种在总结忠实性评估中的方法，其中多个基于LLM的代理被分配初始立场（无论它们的实际信念如何），并被迫提出理由来证明强加的信念，从而进行多轮辩论以达成一致。均匀分布的初始分配导致了更大的立场多样性，这使得辩论更有意义，并最终能够识别出更多的错误。此外，通过分析最近的忠实性评估数据集，我们观察到实际上并非所有总结要么完全忠实于源文档要么不忠实。因此，我们引入了一个新的维度——模糊性，并提出了一个详细的分类法来识别这些特殊情况。实验表明，我们的方法有助于识别模糊性，并且在非模糊性总结上表现更佳。**|
|**2025-02-11**|**Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs**|Ruichen Zhang et.al.|[2502.07942](http://arxiv.org/abs/2502.07942)|null|基于大型语言模型（LLMs）的网络浏览代理在自动化复杂网页任务方面显示出巨大的潜力。现有方法通常依赖于大型LLM（例如GPT-4）来探索网页环境并生成轨迹数据，这些数据随后用于演示检索（针对大型LLM）或以与探索分离的方式进行小LLM（例如Llama3）的提炼。在这篇论文中，我们提出了AgentSymbiotic，这是一种迭代框架，将数据合成与任务执行相结合，实现了大型和小型LLM的“共生改进”。我们的研究揭示了LLM类型之间的互补动态：虽然大型LLM擅长生成高质量的轨迹用于提炼，但提炼出的小型LLM由于其独特的推理能力，往往会选择与大型LLM不同的行动。这种差异驱动了新轨迹的探索，从而丰富了合成的数据。然而，我们也观察到小型LLM的性能成为这一迭代增强过程中的瓶颈。为了解决这个问题，我们提出了两种LLM提炼的创新方法：一种推测性数据合成策略，以减轻离策略偏差，以及一种多任务学习方法，旨在提升学生LLM的推理能力。此外，我们引入了一种混合模式以解决用户隐私问题。在WEBARENA基准测试中，AgentSymbiotic实现了最先进的性能。我们的最佳大型LLM代理达到了52%，超过了之前的最佳成绩45%，而我们的8B精炼模型也表现出色，达到49%，超过了之前的最佳成绩28%。代码将在接受后发布。|
|**2025-02-12**|**MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces**|Loris Gaven et.al.|[2502.07709](http://arxiv.org/abs/2502.07709)|**[link](https://github.com/LorisGaven/MAGELLAN)**|**开放性学习代理必须在广阔的可能性空间中高效地优先考虑目标，专注于那些最大化学习进度（LP）的目标。当这种由在线RL训练的LLM代理在高维和不断演变的目标空间中实现自我导向探索时，预测LP的一个关键挑战是建模自己的能力，这是一种元认知监控的形式。传统方法要么需要大量的采样，要么依赖于脆弱的专家定义的目标分组。我们介绍了MAGELLAN，一个元认知框架，使LLM代理能够在线学习预测它们的能力和LP。通过捕捉目标之间的语义关系，MAGELLAN实现了样本高效的LP估计，并通过泛化动态适应不断变化的目标空间。在一个交互式学习环境中，我们展示了MAGELLAN如何提高LP预测效率和目标优先级排序，它是唯一能使代理完全掌握大型和不断演变的目标空间的方法。这些结果表明，增强LLM代理的元认知能力以预测LP可以有效地将课程学习扩展到开放性的目标空间。**|
|**2025-02-11**|**Approximating Human Strategic Reasoning with LLM-Enhanced Recursive Reasoners Leveraging Multi-agent Hypergames**|Vince Trencsenyi et.al.|[2502.07443](http://arxiv.org/abs/2502.07443)|null|LLM驱动的多智能体仿真近年来备受关注，应用范围涵盖了博弈论和社会模拟。尽管大多数实现旨在利用或评估LLM的智能推理能力，但通常采用的是较为薄弱的代理概念和简化的架构。我们实施了一种基于角色的多智能体战略互动框架，专门针对高级递归推理者设计，提供了系统深入开发和评估战略推理的方法。我们的游戏环境由裁判管理，负责从匹配到移动验证再到环境管理的游戏全过程。玩家在其决策机制中整合了最先进的LLM，并依赖于基于形式化超博弈的层级信念模型。我们使用一次性的两人美丽赛局来评估最新LLM的递归推理能力，提供了一个与经济学中的基准模型及人类实验数据进行比较的机会。此外，我们还提出了一个替代的语义推理层次测量方法，而不只是k级理论。实验结果表明，人工智能推理者在逼近人类行为和达到最优解方面可以超越基准模型。|
|**2025-02-11**|**Graph RAG-Tool Fusion**|Elias Lumer et.al.|[2502.07223](http://arxiv.org/abs/2502.07223)|**[link](https://github.com/eliaslumer/graph-rag-tool-fusion-toollinkos)**|**近期在检索增强生成（RAG）领域的进展使得大型语言模型（LLM）代理能够扩展其对外部工具、API或作为工具的代理的复杂调用能力至数百甚至数千个。然而，传统的基于RAG的工具检索方法未能捕捉到工具之间的结构化依赖关系，这限制了对所检索工具的依赖关系的准确性。例如，在一个向量数据库中的工具里，“获取股票价格”API需要从“获取股票代码”API中获得“股票代码”参数，并且两者都依赖于操作系统级别的互联网连接工具。在本文中，我们通过引入Graph RAG-Tool融合这一新颖的即插即用方法来解决此局限性，该方法结合了基于向量的检索和高效的图遍历的优点，以捕获预定义工具知识图谱中所有相关的工具（节点）及其任何嵌套依赖关系（边）。我们还提出了ToolLinkOS，这是一个新的工具选择基准，包含了573个虚构工具，跨越15个行业，每个工具平均有6.3个工具依赖关系。我们证明Graph RAG-Tool Fusion在ToolLinkOS和ToolSandbox基准测试上分别比简单的RAG方法绝对提升了71.7%和22.1%（mAP@10）。ToolLinkOS数据集可在<https://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS>获取**|
|**2025-02-11**|**Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification**|Peipei Wei et.al.|[2502.07165](http://arxiv.org/abs/2502.07165)|null|我们提出了基于原则的提示方法，这是一种简单但有效的多代理提示策略，用于文本分类。该方法首先让多个大型语言模型（LLM）代理根据带标签或不带标签的示例样本独立生成候选原则，然后通过最终确定者代理将这些原则整合成最终原则，最后将这些原则发送给分类器代理以执行下游分类任务。我们在二元和多类分类数据集上进行了大量实验，这些数据集具有不同大小的LLM，结果表明我们的方法不仅在宏F1得分上比零样本提示获得了显著的性能提升（1.55% - 19.37%），而且在性能上也超过了其他强大的基线方法（如CoT和后退提示）。我们的方法生成的原则帮助LLMs在两个私有数据集上的分类任务中表现优于人工编写的原则。与基于演示的少量样本提示方法相比，我们的多代理基于原则的提示方法在性能上相当甚至更好，同时推理成本显著降低。消融研究显示标签信息和多代理合作的LLM框架在生成高质量原则以促进下游分类任务方面发挥了重要作用。|
|**2025-02-10**|**Interactive Data Harmonization with LLM Agents**|Aécio Santos et.al.|[2502.07132](http://arxiv.org/abs/2502.07132)|null|数据整合是一项将来自不同来源的数据集进行集成的重要任务。尽管这一领域已经进行了多年的研究，但由于模式不匹配、术语差异以及数据收集方法的不同，它仍然是一项耗时且具有挑战性的任务。本文提出了代理式数据整合作为一种手段，旨在既能让专家整合自己的数据，又能简化该过程。我们介绍了Harmonia系统，该系统结合了基于大型语言模型的推理、交互式用户界面和数据整合基元库，以自动化合成数据整合管道。我们通过一个临床数据整合场景展示了Harmonia，展示了如何互动地创建可重用的管道来将数据集映射到标准格式。最后，我们讨论了挑战和未解决问题，并提出了研究方向以推进我们的愿景。|
|**2025-02-10**|**Repository-level Code Search with Neural Retrieval Methods**|Siddharth Gandhi et.al.|[2502.07067](http://arxiv.org/abs/2502.07067)|**[link](https://github.com/Siddharth-Gandhi/ds)**|本文提出了一种多阶段重排序系统，用于仓库级代码搜索。该系统利用大型开源仓库中广泛可用的提交历史记录来辅助错误修复。我们将仓库级代码搜索定义为检索代码仓库当前状态下与用户问题或错误最相关的文件集。所提出的方案结合了基于BM25的消息检索和使用CodeBERT进行神经重排序以识别最重要文件的方法。通过从不同的仓库及其提交历史记录中学习模式，该系统可以展示出与任务相关的重要文件。该系统利用提交消息和源代码进行相关性匹配，并在正常和oracle设置下进行了评估。实验在一个由7个流行的开源仓库创建的新数据集上进行，结果显示相较于BM25基线，在各种查询下，该方法在MAP、MRR和P@1指标上的提升高达80%，证明了此方法的有效性。我们希望这项工作能够作为工具帮助LLM代理实现更好的代码搜索和理解。我们的代码和获得的结果是公开可获取的。|
|**2025-02-10**|**SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering**|Xuehang Guo et.al.|[2502.06994](http://arxiv.org/abs/2502.06994)|null|软件工程（SE）越来越依赖协作，开发人员需要在共享的复杂代码库上协同工作。有效的协作要求参与者——无论是人类还是AI代理——随着环境的变化保持对当前状态的理解一致。当参与者的理解与当前状态不一致时——我们称之为不同步挑战——其操作可能会失败，导致集成问题。在这项工作中，我们引入了SyncMind框架，该框架系统地定义了大型语言模型（LLM）代理在协作软件工程（CSE）中面临的不同步问题。基于SyncMind，我们创建了SyncBench基准，其中包括来自21个流行的GitHub仓库的24,332个实际CSE中的代理不同步场景，并带有可执行验证测试。对SyncBench的实验揭示了现有LLM代理的能力和局限性的关键见解。除了代理之间的显著性能差距（从Llama-3.1代理 <= 3.33%到Claude-3.5-Sonnet >= 28.18%），它们合作意愿的一致性较低（<= 4.86%）表明现有LLM在CSE中的根本局限性。然而，当合作发生时，它与不同步恢复的成功正相关。代理在资源感知不同步恢复中的微小性能差异进一步揭示了它们在资源意识和适应性方面的重大不足，这为未来的资源高效协作系统提供了启示。代码和数据可在我们的项目网站上公开获取：https://xhguo7.github.io/SyncMind/|
|**2025-02-10**|**Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents**|Mathis Pink et.al.|[2502.06975](http://arxiv.org/abs/2502.06975)|null|随着大型语言模型（LLMs）从文本完成工具发展成为在动态环境中运作的完整代理，它们必须应对持续学习和保留长期知识的挑战。许多生物系统通过情景记忆解决了这些挑战，该记忆支持一次性学习实例特定上下文的能力。受此启发，我们提出了一个针对LLM代理的情景记忆框架，围绕着情景记忆支撑适应性和上下文敏感行为的五个关键属性。尽管已有多个研究工作部分覆盖了这些属性，但本文认为现在是时候对情景记忆进行明确且集中的关注，以促进长期代理的发展。为此，我们概述了一条路线图，将多个研究方向联合起来，旨在支持所有五个情景记忆属性，从而实现更高效的长期LLM代理。|
|**2025-02-10**|**Visual Agentic AI for Spatial Reasoning with a Dynamic API**|Damiano Marsili et.al.|[2502.06787](http://arxiv.org/abs/2502.06787)|null|视觉推理——即解读视觉世界的能力——对于在三维场景中运作的具身代理至关重要。尽管人工智能的进步使得视觉和语言模型能够根据图像回答问题，但它们在处理三维空间推理任务时表现欠佳。为了解决这类推理问题的复杂性，我们引入了一种基于主体程序合成的方法，其中大型语言模型（LLM）代理协同生成一个Python风格的API，用以解决常见的子问题。我们的方法克服了依赖静态、人类定义的API的传统方法的局限性，使其能够应对更广泛的问题。为了评估AI在三维理解方面的能力，我们提出了一组新的查询，涉及多步骤的定位和推理。我们展示了该方法在三维视觉推理任务中优于先前的零样本模型，并通过实证验证了我们基于主体的框架在三维空间推理任务中的有效性。项目网站：https://glab-caltech.github.io/vadar/|
|**2025-02-10**|**Towards Internet-Scale Training For Agents**|Brandon Trabucco et.al.|[2502.06776](http://arxiv.org/abs/2502.06776)|null|我们开发了一种管道来促进无需繁琐人工标注的互联网规模训练。首先，大规模语言模型为15万多个不同的网站生成任务。接下来，这些模型代理完成任务并生成轨迹。最后，另一个大规模语言模型审查这些轨迹并判断其成功与否。语言模型在检测和过滤有害内容方面准确率达到97%，生成可行任务的成功率为89%，判断成功的轨迹准确率为82.6%。通过扩展这个管道，基于Llama 3.1 70B的大规模语言模型代理能够解决15万个网站上16.7%的任务。使用我们管道生成的数据进行训练与使用人类演示数据进行训练效果相当。在来自Mind2Web和WebLINX的数据受限设置下，我们改进了步骤准确性，分别提高了+89.5%和+122.1%。当使用基准测试中所有可用的人类数据训练代理时，它们无法泛化到各种实际网站，而添加我们的数据可以提高它们的泛化能力，对于WebLINX和Mind2Web分别提高了+149.0%和+156.3%。代码将在data-for-agents.github.io获取。|
|**2025-02-10**|**Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training**|Yuchen Zhuang et.al.|[2502.06589](http://arxiv.org/abs/2502.06589)|null|由于面向代理的预训练数据稀缺，基于大语言模型（LLM）的自主代理通常依赖于复杂的提示或广泛的微调，这往往无法在保持强大泛化能力的同时引入新功能。我们介绍了Hephaestus-Forge，这是首个大规模预训练语料库，旨在增强LLM代理在API函数调用、内在推理和规划以及适应环境反馈方面的基础能力。Hephaestus-Forge包含1030亿个代理特定数据，涵盖了76,537个API，包括工具文档以引入API功能的知识，以及功能调用轨迹以加强内在推理。为了探索有效的训练协议，我们研究了缩放法则以确定数据混合比率的最佳配方。通过持续在Hephaestus-Forge上进行预训练，Hephaestus在三个代理基准测试中表现出色，超越了小型到中型的开源LLM，并且可以与商业LLM相媲美，证明了我们的预训练语料库在增强LLM的基础代理能力和任务或环境泛化能力方面的有效性。|
|**2025-02-10**|**CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories**|Yijia Xiao et.al.|[2502.06111](http://arxiv.org/abs/2502.06111)|null|不断增加的计算机科学研究项目的复杂性要求更有效的工具来部署代码仓库。大型语言模型（LLMs），如Anthropic Claude和Meta Llama，在计算机科学研究的各个领域，包括自动化各种软件工程任务方面，已经展现出了显著的进步。为了评估LLMs在处理研究项目中的复杂代码开发任务方面的有效性，特别是针对NLP/CV/AI/ML/DM领域的任务，我们引入了CSR-Bench，这是一个计算机科学研究基准。该基准从准确度、效率和部署脚本质量等多个方面评估LLMs，旨在探索它们在自主进行计算机科学研究方面的潜力。我们还介绍了一个新的框架CSR-Agents，它利用多个LLM代理来自动化计算机科学研究项目GitHub代码仓库的部署。具体来说，通过检查自述文件中的指令并解释仓库结构，该模型生成并迭代改进bash命令，以设置实验环境并部署代码来进行研究任务。CSR-Bench的初步结果显示，LLM代理可以显著增强仓库部署的工作流程，从而提高开发者的生产力并改善开发工作流的管理。|
|**2025-02-09**|**HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents**|Mohammad Amin Abbasi et.al.|[2502.05982](http://arxiv.org/abs/2502.05982)|null|本文介绍了HamRaz，这是一个为以大型语言模型（LLMs）为基础的人格中心疗法（PCT）设计的新型波斯语心理健康数据集。尽管LLMs在AI驱动的心理咨询中的应用日益增长，现有的数据集大多侧重于西方和东亚背景，忽略了对有效波斯语治疗至关重要的文化和语言细微差别。为了解决这一差距，HamRaz结合了基于剧本的对话与适应性LLM角色扮演，确保连贯且动态的治疗互动。我们还介绍了HamRazEval，这是一种双评价框架，使用通用对话指标和Barrett-Lennard关系量表（BLRI）来衡量对话质量和治疗效果。实验结果表明，HamRaz在产生更具同理心、上下文感知和现实的治疗课程方面优于传统的剧本模式和双代理模式。通过发布HamRaz，我们贡献了一个文化适应的、由LLM驱动的资源，以推进多元社区中的AI驱动心理治疗研究。|
|**2025-02-09**|**MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents**|Jiabin Tang et.al.|[2502.05957](http://arxiv.org/abs/2502.05957)|null|大型语言模型（LLM）代理在任务自动化和智能决策方面展示了显著的能力，推动了像LangChain和AutoGen这样的代理开发框架的广泛应用。然而，这些框架主要服务于具有丰富技术专长的开发者——这是一个重要的限制，因为全球只有0.03%的人口具备必要的编程技能。这种明显的可访问性差距引发了基本问题：我们能否让每个人，无论其技术水平如何，仅通过自然语言就能构建自己的LLM代理？为了解决这一挑战，我们介绍了MetaChain——一个全自动且高度自我发展的框架，使用户能够仅通过自然语言创建和部署LLM代理。作为一款自主代理操作系统，MetaChain包含四个关键组件：i）代理系统实用程序，ii）基于LLM的操作引擎，iii）自我管理文件系统，以及iv）自我调整代理定制模块。这个轻量级但功能强大的系统能够在无需编码或人工干预的情况下高效地动态创建和修改工具、代理和工作流程。除了无代码代理开发能力外，MetaChain还作为一个多功能多代理系统，用于通用AI助手。在GAIA基准上的综合评估表明，MetaChain在通用多代理任务上表现出色，超越了现有的最先进方法。此外，MetaChain在与检索增强生成（RAG）相关的能力方面也表现出持续优越的性能，超过了众多基于LLM的解决方案。|
|**2025-02-07**|**MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison**|Kaijie Zhu et.al.|[2502.05174](http://arxiv.org/abs/2502.05174)|null|近期的研究表明大型语言模型（LLM）代理容易受到间接提示注入（IPI）攻击的威胁，其中工具检索到的信息中嵌入的恶意任务可以引导代理采取未经授权的操作。现有的IPI防御方法存在显著局限：要么需要重要的模型训练资源，要么对复杂的攻击效果不佳，或者会损害正常的实用性。我们提出了MELON（掩码重执行和工具比较），这是一种新的IPI防御方法。我们的方法基于这样的观察：在成功的攻击下，代理的下一个动作对用户任务的依赖性降低，而对恶意任务的依赖性增加。根据这一点，我们设计了MELON通过使用经过掩码函数修改的掩码用户提示来重新执行代理的行为轨迹以检测攻击。如果原始执行和掩码执行生成的动作相似，则识别为攻击。我们还设计了三个关键功能以减少潜在的误报和漏报。我们在IPI基准测试AgentDojo上的广泛评估表明，MELON在攻击预防和实用性保持方面都优于现有技术（SOTA）的防御措施。此外，我们将MELON与一种SOTA提示增强防御相结合（表示为MELON-Aug）进一步提升了其性能。我们还进行了详细的消融研究以验证我们设计的关键要素。|
|**2025-02-07**|**Learning Strategic Language Agents in the Werewolf Game with Iterative Latent Space Policy Optimization**|Zelai Xu et.al.|[2502.04686](http://arxiv.org/abs/2502.04686)|null|大型语言模型（LLM）代理在开放式对话和多步骤决策制定等领域已经取得了显著的进展。然而，将这些代理应用于需要战略决策和自由形式语言互动的社会推理游戏（如狼人杀）仍然具有挑战性。传统方法基于反事实遗憾最小化（CFR）或强化学习（RL），通常依赖于预定义的动作空间，这使得它们不适合具有不受限文本动作空间的语言游戏。同时，纯LLM代理常常受到内在偏差的影响，并且需要庞大的数据集进行微调。我们提出了潜在空间策略优化（LSPO），这是一种迭代框架，通过首先将自由形式文本映射到离散潜在空间来解决这些问题，在这个空间中，CFR和RL可以更有效地学习战略政策。然后，我们将学到的策略转换回自然语言对话，用于通过直接偏好优化（DPO）微调LLM。通过在这些阶段之间交替迭代，我们的LSPO代理逐步提升战略推理和语言沟通能力。实验结果表明，在狼人杀游戏中，我们的方法在每次迭代中都提高了代理的表现，并优于现有的狼人杀代理，这凸显了其在自由形式语言决策制定中的潜力。|
|**2025-02-07**|**Self-Regulation and Requesting Interventions**|So Yeon Min et.al.|[2502.04576](http://arxiv.org/abs/2502.04576)|null|人类智能涉及元认知能力，如自我调节、认识局限以及仅在必要时寻求帮助。尽管大型语言模型（LLM）代理在许多领域表现出色，但它们通常缺乏这种意识。过于自信的代理可能会导致灾难性失败，而过度寻求帮助的代理则会阻碍效率。一个关键挑战是，在有限干预预算 $C$ 的情况下，如何决定何时请求帮助。在这篇论文中，我们提出了一种离线框架，通过结合基于LLM的过程奖励模型（PRMs）与表格化强化学习来训练一个“辅助”策略，以请求干预，例如使用更强大的模型或测试时间计算。利用离线收集的状态转换，我们使用PRMs对最优干预时机进行评分，并在此标记轨迹上训练辅助模型。这种方法显著减少了训练期间昂贵的干预调用。此外，PRMs与表格化RL的结合增强了对非策略数据的鲁棒性，同时避免了深度RL的低效问题。我们的实证研究表明，该方法能够实现最优的辅助行为。|
|**2025-02-06**|**Multi-Agent Reinforcement Learning with Focal Diversity Optimization**|Selim Furkan Tekin et.al.|[2502.04492](http://arxiv.org/abs/2502.04492)|**[link](https://github.com/sftekin/rl-focal)**|大型语言模型（LLMs）及其微调策略的发展引发了对多智能体强化学习的重新关注。本文介绍了一种焦点多样性优化的多智能体强化学习方法，称为MARL-Focal，具有三个独特特征。首先，我们开发了一个代理融合框架，以鼓励基于多个LLM的代理协作产生每个LLM查询的最终推理输出。其次，我们开发了一种焦点-多样性优化的代理选择算法，可以根据它们如何互补生成查询输出来选择可用代理的一个小子集。最后，我们设计了一种冲突解决方法，用于检测多个代理之间的输出不一致，并通过奖励感知和策略自适应推理融合产生我们的MARL-Focal输出。在五个基准上的广泛评估表明，MARL-Focal是成本高效的且对抗稳健的。我们的多智能体融合模型比最佳的单个LLM代理提高了5.51%的性能，并在TruthfulQA基准上提供了更强的鲁棒性。代码可在<https://github.com/sftekin/rl-focal>获取。|
|**2025-02-06**|**Active Task Disambiguation with LLMs**|Katarzyna Kobalczyk et.al.|[2502.04485](http://arxiv.org/abs/2502.04485)|**[link](https://github.com/kasia-kobalczyk/active-task-disambiguation)**|尽管大型语言模型（LLMs）在各种基准测试中表现出色，但它们解决现实世界交互中常见的模糊问题的能力仍缺乏探索。为了解决这一差距，我们引入了任务模糊性的正式定义，并通过贝叶斯实验设计的视角将任务消歧问题形式化。通过提出澄清性问题，LLM代理可以获取额外的任务规范，逐步缩小可行解决方案的空间，从而降低生成不令人满意的输出的风险。然而，生成有效的澄清性问题要求LLM代理进行一种元认知推理，而这种能力LLMs目前可能还不具备。我们提出的主动任务消歧方法使LLM代理能够生成最大化信息增益的针对性问题。实际上，这种方法将推理负担从隐式转移到对可行解决方案空间的显式推理上。实证结果表明，这种问题选择方式比仅在问题空间内进行推理的方法更能有效地进行任务消歧。|
|**2025-02-04**|**Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives**|Elliot Meyerson et.al.|[2502.04358](http://arxiv.org/abs/2502.04358)|null|在将复杂问题分解为子问题时，这些问题通常会变得更加容易和高效地解决。随着大型语言模型（LLM）跨越越来越多能力的关键可靠性阈值，人们越来越努力将系统分解为一组基于LLM的代理，每个代理都可以被委派子任务。然而，即使这种分解是自动化的，它也往往是直观的，例如基于人类可能分配给团队成员的角色。这些角色分解离最优有多近？这篇论文认为需要使用LLM原语进行渐近分析来推断此类分解系统的效率，并且从这种分析中获得的见解将解锁扩展它们的机会。通过将LLM前向传递视为计算成本的基本单位，可以将特定LLM的（通常是不透明的）内部工作与一组LLM执行硬任务的固有效率区分开来。换句话说，如果我们希望将LLM的部署扩展到极限，而不是拟人化LLM，应使用渐近分析与LLM原语来推理并开发更强大的大规模问题分解为LLM代理的方法。|
|**2025-02-06**|**ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization**|Yinjie Wang et.al.|[2502.04306](http://arxiv.org/abs/2502.04306)|**[link](https://github.com/gen-verse/scoreflow)**|近期的研究利用大规模语言模型的多智能体系统解决复杂问题，同时试图减少构建这些系统的手动工作量，推动了自动化智能体工作流程优化方法的发展。然而，现有方法由于表示能力有限、缺乏适应性以及在依赖离散优化技术时可扩展性差而显得不够灵活。我们通过ScoreFlow解决了这些挑战，ScoreFlow是一个简单但高性能的框架，利用连续空间中的高效梯度优化。ScoreFlow包含了Score-DPO，这是一种直接偏好优化方法的新变种，考虑到了定量反馈。在涵盖问答、编码和数学推理的六个基准测试中，ScoreFlow比现有基线提高了8.2%。此外，它还使较小的模型能够在较低的推理成本下超越较大的模型。项目：https://github.com/Gen-Verse/ScoreFlow|
|**2025-02-06**|**PsyPlay: Personality-Infused Role-Playing Conversational Agents**|Tao Yang et.al.|[2502.03821](http://arxiv.org/abs/2502.03821)|null|当前关于角色扮演对话代理（RPCAs）与大型语言模型（LLMs）的研究主要集中在模仿特定的说话风格和利用角色背景上，而忽略了对更深层次个性特征的描绘。在这项研究中，我们引入了人格特征注入的角色扮演方法，以鼓励代理在对话中准确地展现其指定的人格特质。我们提出了PsyPlay，这是一种对话生成框架，旨在促进多个LLM代理之间丰富人格特征的表达。具体来说，PsyPlay使代理能够承担具有不同人格特质的角色，并围绕特定主题进行讨论，在整个互动过程中始终表现出其指定的人格特质。验证生成的对话数据表明，PsyPlay能够准确地展现预期的人格特质，在GPT-3.5上的总体成功率达到了80.31%。值得注意的是，我们观察到与积极价值观一致的LLMs在展现积极人格角色方面比展现消极人格角色更为成功。此外，我们构建了一个用于人格特征注入的角色扮演游戏对话语料库，称为PsyPlay-Bench。该语料库由4745个使用PsyPlay正确展现的对话实例组成，旨在进一步促进个性化角色扮演和对话人格检测研究。|
|**2025-02-06**|**MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers**|Nicole Cho et.al.|[2502.03711](http://arxiv.org/abs/2502.03711)|null|一个在大型语言模型（LLMs）机构采用过程中关键的挑战源自于它们在生成回答时倾向于产生幻觉。为了解决这个问题，我们提出了MultiQ&A，这是一种系统的方法，用于评估LLM生成答案的稳健性和一致性。我们展示了MultiQ&A能够通过大规模地众包问题扰动及其相应的答案来实现这一点。我们的实验最终检查了190万个问题扰动和230万个答案。此外，MultiQ&A表明集成的LLM，如gpt-3.5-turbo，在扰动下仍然相对稳健和一致。MultiQ&A在响应生成领域提供了清晰度，提供了一种有效的方法来检查分歧和变异性。因此，我们的系统为机构采用LLM提供了一个潜在框架，具备衡量信心、一致性和量化幻觉的能力。|
|**2025-02-05**|**A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene Graphs with Large-Language-Models (LLMs)**|Yiye Chen et.al.|[2502.03450](http://arxiv.org/abs/2502.03450)|null|场景图已成为大型语言模型（LLMs）进行接地空间推理的结构化和可序列化环境表示。在这项工作中，我们提出了SG-RwR，这是一种针对场景图的Schema引导式检索-推理框架。我们的方法采用了两个合作的、编写代码的LLM代理：一个（1）推理器用于任务规划和信息查询生成，以及一个（2）检索器用于根据查询提取相应的图信息。这两个代理迭代地协同工作，使顺序推理和自适应关注图信息成为可能。与之前的工作不同，两个代理仅使用场景图模式而不是完整的图数据进行提示，这通过限制输入令牌减少了幻觉，并促使推理器抽象地生成推理跟踪。根据跟踪，检索器基于模式理解程序化地查询场景图数据，允许对图进行动态和全局的关注，从而增强推理和检索之间的对齐。通过在多个模拟环境中的实验，我们展示了我们的框架在数值问答和规划任务中超越了现有的基于LLM的方法，并且可以从任务级别的少量示例中受益，即使没有代理级别的演示。项目代码将会发布。|
|**2025-02-04**|**Adaptive Self-improvement LLM Agentic System for ML Library Development**|Genghan Zhang et.al.|[2502.02534](http://arxiv.org/abs/2502.02534)|**[link](https://github.com/zhang677/pcl-lite)**|**ML库通常使用针对特定架构的编程语言（ASPL）编写，这些语言面向特定领域的架构，对于高效的ML系统至关重要。然而，编写这些高性能的ML库具有挑战性，因为它需要对ML算法和ASPL有专业知识。另一方面，大型语言模型（LLMs）展示了通用编码能力。然而，当使用LLMs生成使用ASPL的ML库时仍然存在挑战，因为这项任务即使对于经验丰富的程序员来说也很复杂，并且由于ASPL的深奥和不断发展的性质，代码示例有限。因此，LLMs需要在数据有限的情况下进行复杂的推理才能完成此任务。为了解决这些挑战，我们引入了一种自适应自我改进的智能系统。为了评估我们系统的有效性，我们在典型的ML库基准上构建了一个测试，并使用开放和闭源的LLMs在这个基准上生成了ASPL代码。我们的结果显示，与单一LLM基线相比，性能提高了多达3.9倍。**|
|**2025-02-03**|**Firewalls to Secure Dynamic LLM Agentic Networks**|Sahar Abdelnabi et.al.|[2502.01822](http://arxiv.org/abs/2502.01822)|null|未来的大型语言模型（LLM）代理很可能会代表用户与其他实体代理进行通信，以完成涉及长期计划和相互依赖目标的任务。目前的工作并未关注这样的代理网络及其所面临的挑战。因此，我们首先确定了代理间通信所需的特点，这些特点应该是主动的且具有适应性。通信需要满足以下要求：1）隐私性：代理不应分享超过任务所需的个人信息；2）安全性：通信必须保持完整性，并在面对自私实体时保持实用性。我们设计了一个使用案例（旅行规划）作为测试平台，以体现这些要求，并展示了如果处理不当可能会出现的问题。接下来，我们提出了一种实用的设计方案，该方案借鉴了已建立的网络安全原则，旨在为受限的LLM代理网络平衡适应性、安全性和隐私性。我们的框架能够自动构建和更新特定任务的规则，从而建立防火墙。我们提供了多层防御措施，包括：1）将自由格式输入转换为特定任务的协议；2）动态地将用户数据抽象到特定任务的许可程度；3）自我纠正代理的行为轨迹。|
|**2025-02-03**|**Position: Towards a Responsible LLM-empowered Multi-Agent Systems**|Jinwei Hu et.al.|[2502.01714](http://arxiv.org/abs/2502.01714)|null|Agent AI和基于大型语言模型的多智能体系统（LLM-MAS）的兴起突显了实现系统操作的可靠性和责任性的需求。LangChain等工具和基于检索的生成方法扩展了LLM的功能，通过增强的知识检索和推理，实现了LLM在MAS中的更深层次集成。然而，这些进展带来了关键挑战：LLM代理表现出固有的不可预测性，其输出中的不确定性可能在交互过程中累积，威胁到系统的稳定性。为应对这些风险，需要采用以人为中心的设计方法和主动动态监管。这种方法通过促进智能体间的有效沟通和系统治理来增强传统的被动监督，从而更高效地实现MAS的预期目标。|
|**2025-02-03**|**TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues**|Yubin Ge et.al.|[2502.01630](http://arxiv.org/abs/2502.01630)|null|时间推理在多会话对话中提出了一个重要的挑战，这一领域在之前的时序推理基准测试中研究不足。为了填补这一空白，我们提出了一项新的评估任务，用于多会话对话中的时序推理，并介绍了一种通过增强LoCoMo对话数据集并创建多选题的方式来构建新基准的方法。此外，我们提出了TReMu框架，旨在提升LLM代理在这种情境下的时序推理能力。具体而言，该框架通过时间线摘要实现“时间感知记忆”，通过对每次对话会话中的事件及其推断日期进行总结来生成可检索的记忆。此外，我们整合了“神经符号时序推理”，其中LLM生成Python代码来进行时序计算并选择答案。对流行LLM的实验评估表明，我们的基准具有挑战性，而所提出的框架显著提升了时序推理性能，相较于标准提示方法（GPT-4o上的29.83）提升至我们的方法下的77.67，这突显了其在解决多会话对话中的时序推理问题方面的有效性。|
|**2025-02-04**|**Reinforcement Learning for Long-Horizon Interactive LLM Agents**|Kevin Chen et.al.|[2502.01600](http://arxiv.org/abs/2502.01600)|null|交互式数字代理（IDAs）利用应用程序编程接口（APIs）在响应用户请求时执行任务。尽管由经过指令调优的大语言模型（LLMs）驱动的IDAs可以在多步骤交互中对界面调用的反馈做出反应，但它们并未在其目标环境中进行训练。此前的方法在复杂的基准测试如AppWorld中完成的任务不足一半。我们提出了一种强化学习（RL）方法，直接在目标环境中训练IDAs。我们将这种训练形式化为一个部分可观测马尔可夫决策过程，并推导出LOOP，这是一种无需价值网络且内存效率极高的近端策略优化变体。LOOP仅在内存中保持单一LLM的副本，使其实现简单且内存使用效率与单次微调LLM相同。在AppWorld环境中训练的一个320亿参数的代理通过LOOP比规模更大的OpenAI o1代理高出9个百分点（相对提高15%）。据我们所知，这是首次将RL应用于通过直接API调用与状态多域多应用环境互动的IDAs的报告应用。我们的分析揭示了RL在此领域的有效性，表明该代理学会了查阅API文档，避免不必要的假设，减少虚构，并从挫折中恢复。|
|**2025-02-03**|**Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints Internalization**|Minttu Alakuijala et.al.|[2502.01562](http://arxiv.org/abs/2502.01562)|null|随着人工智能（AI）代理的通用能力不断进化，它们通过经验掌握多个复杂任务的能力仍然是一个关键挑战。当前的大语言模型（LLM）代理，尤其是那些基于专有语言模型的代理，通常依赖于提示来整合关于目标任务的知识。这种方法不允许代理内化这些信息，而是依赖于不断扩大的提示来在不同场景中维持其功能，这类似于患有顺行性遗忘症的人使用笔记系统，即无法形成新记忆的人。在这篇论文中，我们提出了一种新颖的方法，使AI代理能够在不需要繁琐的笔记系统或高质量的演示数据的情况下整合知识和技能以应对多个任务。我们的方法采用了一个迭代过程，在这个过程中，代理收集新的经验，从人类那里获得纠正性反馈的形式为提示，并通过上下文蒸馏训练程序将这些反馈融入其权重。我们通过在一个基于Llama-3的代理上实现这一方法来证明其有效性，该代理仅经过几次反馈循环后，就在需要正确顺序的信息检索、工具使用和问答的任务集中表现优于先进的模型GPT-4o和DeepSeek-V3。|
|**2025-02-05**|**TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets**|Yuzhe Yang et.al.|[2502.01506](http://arxiv.org/abs/2502.01506)|**[link](https://github.com/tobyyang7/twinmarket)**|社会涌现现象的研究长期以来一直是社会科学的中心课题。传统的建模方法，如基于规则的多智能体模型（ABMs），在捕捉人类行为的多样性和复杂性方面存在困难，特别是强调行为经济学中的非理性因素。近年来，大型语言模型（LLM）代理作为模拟人类行为的社会科学和角色扮演应用的工具得到了广泛关注。研究表明，LLMs能够解释认知偏差、情绪波动以及其他非理性影响，从而实现对社会经济动态的更现实的模拟。在这项工作中，我们引入了TwinMarket，这是一种利用LLMs模拟社会经济系统的新型多智能体框架。具体而言，我们研究了个体行为如何通过互动和反馈机制产生集体动力学和涌现现象。通过在一个模拟股票市场环境中进行实验，我们展示了个体行为如何触发群体行为，导致金融泡沫和经济衰退等涌现结果。我们的方法为理解个体决策与集体社会经济模式之间的复杂相互作用提供了有价值的见解。|
|**2025-02-03**|**Simulating Rumor Spreading in Social Networks using LLM Agents**|Tianrui Hu et.al.|[2502.01450](http://arxiv.org/abs/2502.01450)|**[link](https://github.com/neerajas-group/rumors-in-multi-agent)**|随着社交媒体的兴起，误信息的传播变得日益普遍，这在很大程度上是由谣言的散播所驱动的。本研究探讨了在一种新型框架内使用大型语言模型（LLM）代理来模拟和分析谣言在社交网络中的传播动态。为此，我们设计了多种基于LLM的代理类型，并构建了四种不同的网络结构来进行这些模拟。我们的框架评估了不同网络构造和代理行为在影响谣言传播方面的有效性。结果表明，该框架能够在各种包含一百多代理且有数千条边的网络中模拟谣言传播。评估显示，网络结构、角色和传播方案可以显著影响谣言的传播范围，从完全不传播到影响多达83%的代理，在迭代过程中提供了对社交网络中谣言传播的真实模拟。|
|**2025-02-03**|**Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant**|Gaole He et.al.|[2502.01390](http://arxiv.org/abs/2502.01390)|**[link](https://github.com/richardhgl/chi2025_plan-then-execute_llmagent)**|自从ChatGPT爆炸性地流行以来，大型语言模型（LLMs）继续影响着我们的日常生活。配备专为特定目的设计的外部工具（例如航班预订或闹钟），LLM代理在日常工作中展现出日益增长的能力来辅助人类。尽管LLM代理在作为日常助手方面展示出一个有前景的蓝图，但对于它们如何基于规划和顺序决策能力提供日常协助的理解仍然有限。我们受到最近工作的启发，这些工作强调了‘LLM模组’设置与人机环路相结合在规划任务中的价值。我们进行了一项实证研究（N=248），考察了LLM代理在六个常见任务中作为日常助手的表现，这些任务通常与不同程度的风险相关（例如，航班机票预订和信用卡支付）。为了确保用户对LLM代理的控制权，我们采用了计划后执行的方式，其中代理在一个模拟环境中逐步规划并分步执行。我们分析了每个阶段用户的参与如何影响他们的信任度和协作团队表现。我们的发现表明，LLM代理是一把双刃剑——（1）当高质量的计划和必要的用户执行参与都存在时，它们可以很好地工作；（2）用户很容易对看起来合理的计划产生不信任感。我们综合了关键见解，用于指导如何使用LLM代理作为日常助手以校准用户信任并实现更好的整体任务结果。我们的工作对未来的日常助手设计和LLM代理的人机协作具有重要意义。|
|**2025-02-03**|**ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution**|Kanika Goswami et.al.|[2502.00989](http://arxiv.org/abs/2502.00989)|null|大型语言模型（LLMs）可以执行图表问答任务，但通常会生成未经验证的幻觉性回答。现有的答案归因方法由于受限于有限的视觉语义上下文、复杂的视觉文本对齐要求以及在复杂布局中预测边界框的困难，难以将回答与源图表联系起来。我们提出了ChartCitor，这是一种多代理框架，通过识别图表图像中的支持证据来提供细粒度的边界框引用。该系统协调LLM代理执行图表到表格的提取、回答重构、表格增强、证据检索通过预筛选和重新排序，以及表格到图表的映射。ChartCitor在不同类型的图表上都优于现有基线。定性的用户研究表明，ChartCitor通过为LLM辅助的图表问答提供增强的可解释性来增加用户对生成式人工智能的信任，并使专业人员能够更高效地工作。|
|**2025-01-31**|**Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game**|Mustafa O. Karabag et.al.|[2501.19398](http://arxiv.org/abs/2501.19398)|**[link](https://github.com/mustafakarabag/llmchameleon)**|**大型语言模型（LLM）驱动的智能体在包含非合作方的环境中变得常见。在这些环境中，智能体需要在决策时向对手隐藏信息、向合作者透露信息，并通过推断来识别其他智能体的特征。为了研究LLM是否具备这些信息控制和决策能力，我们让基于LLM的智能体参与一种基于语言的隐身份游戏——变色龙游戏。在游戏中，一组互不认识的非变色龙智能体试图识别变色龙智能体而不泄露秘密。该游戏要求变色龙和非变色龙都具备上述信息控制能力。实证结果表明，虽然非变色龙LLM智能体能够识别变色龙，但它们未能向变色龙隐瞒秘密，其获胜概率远低于甚至是最简单的策略水平。为了正式解释这种行为，我们对从隐瞒到透露的一系列策略进行了理论分析，并提供了非变色龙获胜概率的界限。根据不同策略的实证结果和理论分析，我们得出结论，当代LLM驱动的非变色龙智能体在与未知身份的智能体互动时会泄露过多信息。我们的结果指出了包括GPT-4、GPT-4o、Gemini 1.5和Claude 3.5 Sonnet在内的当代LLM在战略互动中的一个弱点。**|
|**2025-01-30**|**Leveraging LLM Agents for Automated Optimization Modeling for SASP Problems: A Graph-RAG based Approach**|Tianpeng Pan et.al.|[2501.18320](http://arxiv.org/abs/2501.18320)|null|自动化优化建模（AOM）在大型语言模型（LLMs）的快速发展中引起了广泛关注。现有的方法主要依赖于提示工程，利用精心设计的专家响应链或结构化指导。然而，由于缺乏特定领域的知识，基于提示的技术在传感器阵列信号处理（SASP）领域表现不佳。为了解决这个问题，我们提出了一种基于检索增强生成（RAG）技术的自动化建模方法，该方法包括两个主要组成部分：多代理（MA）结构和基于图的RAG（Graph-RAG）过程。MA结构专为架构AOM过程而设计，每个代理都基于人类建模过程的原则进行设计。Graph-RAG过程用于匹配用户查询与特定的SASP建模知识，从而提高建模结果。在十个经典信号处理问题上的结果表明，所提出的MAG-RAG方法优于几种AOM基准方法。|
|**2025-01-31**|**RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing**|Jinyao Guo et.al.|[2501.18160](http://arxiv.org/abs/2501.18160)|null|代码审计是一种旨在发现错误的代码审查过程。大型语言模型（LLMs）在这一任务中展现出显著潜力，能够分析无需编译的程序，并根据指定提示进行定制化的错误检测。然而，将LLMs应用于仓库级别的代码审计时面临显著挑战。LLMs固有的上下文限制和幻觉问题可能导致低质量的错误报告。同时，大型软件仓库引入了显著的时间和令牌成本，阻碍了实际场景中的效率和可扩展性。本研究介绍了一种自主的LLM代理RepoAudit，旨在实现精确且高效的仓库级别代码审计。配备了代理记忆功能，RepoAudit按需探索代码仓库，分析不同可行程序路径上的数据流事实。它还引入验证器来减轻幻觉并检查潜在错误路径的路径条件的可行性，使RepoAudit能够在代码审计过程中排除误报。实验表明，由Claude 3.5 Sonnet驱动的RepoAudit成功在15个现实世界系统中发现了38个真实错误，在每个项目上平均耗时0.44小时，花费2.54美元。|
|**2025-01-29**|**Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant**|Gaole He et.al.|[2501.17546](http://arxiv.org/abs/2501.17546)|**[link](https://github.com/delftcrowd/iui2025_convxai)**|**解释性人工智能（XAI）方法被提出以帮助解释和理解人工智能系统如何得出特定预测。受到先前关于会话式用户界面工作的启发，我们主张通过会话式用户界面增强现有的XAI方法可以提高用户的参与度并增强用户对人工智能系统的理解。在本文中，我们探讨了会话式XAI界面对用户理解人工智能系统、信任程度以及依赖人工智能系统的影响。与XAI仪表板相比，我们发现会话式XAI界面能够使用户更好地理解人工智能系统，并提高用户信任度。然而，无论是使用XAI仪表板还是会话式XAI界面的用户都表现出明显的过度依赖人工智能系统。由大型语言模型（LLM）代理驱动的增强对话加剧了这种过度依赖。根据我们的研究结果，我们认为过度依赖的原因可能是伴随这两种XAI界面出现的解释深度错觉。我们的研究结果对设计有效的会话式XAI界面以促进适当的依赖性和改善人机协作具有重要意义。代码可以在<https://github.com/delftcrowd/IUI2025_ConvXAI>找到**|
|**2025-01-28**|**A sketch of an AI control safety case**|Tomek Korbak et.al.|[2501.17315](http://arxiv.org/abs/2501.17315)|null|随着大型语言模型（LLM）代理造成危害的能力增强，AI开发者可能会越来越多地依赖监控等控制措施来证明它们是安全的。我们概述了开发者如何构建“控制安全性案例”，这是一个结构化的论点，表明模型无法规避控制措施以导致不可接受的结果。作为案例研究，我们概述了一个论点，即在一家人工智能公司内部部署的假设性LLM代理不会泄露敏感信息。该概述基于“控制评估”的证据，在此过程中，红队故意设计模型以在模拟部署环境中泄露数据。这种安全性案例则依赖于几个论点：(1) 红队充分揭示了模型泄露数据的能力，(2) 控制措施在部署时至少同样有效，(3) 开发者保守地推断模型性能以预测在部署中数据泄露的概率。这个安全性案例概述是迈向更具体论证的重要一步，可用于展示一个具有危险能力的LLM代理是安全的可以部署。|
|**2025-01-28**|**Large Language Model Critics for Execution-Free Evaluation of Code Changes**|Aashish Yadavally et.al.|[2501.16655](http://arxiv.org/abs/2501.16655)|**[link](https://github.com/amazon-science/code-agent-eval)**|**大型语言模型（LLMs）为通过多步骤的基于LLM的主动工作流自动化软件工程任务（如错误修复、功能添加等）提供了有前景的方法。然而，现有的评估此类工作流的指标，主要是构建状态和偶尔的日志分析，过于稀疏且有限，无法提供评估所做更改质量所需的信息。在这项工作中，我们设计了基于LLM的评论者来推导出结构良好且严格的中间/步骤级、无执行评估代理，用于代码库级别的代码更改。重要的是，我们假设可以访问问题的黄金测试补丁（即参考感知），以评估生成补丁的语义和可执行性。使用黄金测试补丁作为参考，我们预测所有编辑位置的可执行性，F1得分为91.6%，聚合这些结果，我们可以预测SWE-bench中84.8%实例的构建状态。特别是，这种专注于执行的LLM评论者比其他无参考和有参考的LLM评论者表现高出38.9%到72.5%。此外，我们展示了这种有参考的框架在比较由不同主动工作流生成的补丁方面的有用性。最后，我们将为此项目开发的库开源，该库允许进一步用于其他主动工作流或其他基准测试。源代码可在<https://github.com/amazon-science/code-agent-eval>获取。**|
|**2025-01-27**|**Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma**|Richard Willis et.al.|[2501.16173](http://arxiv.org/abs/2501.16173)|**[link](https://github.com/willis-richard/evollm)**|**随着自主代理在社会中的作用越来越重要，理解他们在战略互动中的集体行为变得至关重要。本研究调查了大型语言模型（LLM）代理系统在社会困境中的新兴合作倾向。与之前的研究不同，我们让最先进的LLM生成重复囚徒困境的完整策略，而不是仅仅输出个体行动。我们使用进化博弈论模拟具有不同战略倾向（好战、合作或中立）的代理人群体，并观察其进化动态。我们的发现揭示了不同的LLM表现出不同的偏见，影响了好战和合作策略的相对成功。本研究为部署的基于LLM的自主代理系统的潜在长期行为提供了见解，并强调了仔细考虑它们所处的战略环境的重要性。**|
|**2025-01-27**|**LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models**|Yuewen Mei et.al.|[2501.15850](http://arxiv.org/abs/2501.15850)|null|确保和提高自动驾驶系统的安全性对于高度自动化车辆的部署至关重要，尤其是在处理安全关键事件时。为了解决稀有性问题，开发了对抗场景生成方法，在这些方法中，交通参与者的操作被操纵以引发安全关键事件。然而，现有方法仍然面临两个局限性。首先，识别对抗参与者直接影响生成的有效性。然而，现实世界场景的复杂性，包括众多参与者和多样的行为，使得识别变得具有挑战性。其次，生成的安全关键场景在持续改进自动驾驶系统（ADS）性能方面的潜力仍有待探索。为了解决这些问题，我们提出了LLM-attacker：一种利用大型语言模型（LLM）的闭环对抗场景生成框架。具体而言，设计并协调多个LLM代理以确定最优攻击者。然后优化攻击者的轨迹以生成对抗场景。这些场景基于ADS的性能进行迭代细化，形成一个反馈回路以提升ADS。实验结果表明，与其它方法相比，LLM-attacker能够创建更具危险性的场景，并且使用其训练的ADS碰撞率仅为使用正常场景训练的一半。这表明LLM-attacker具有测试和增强ADS安全性和鲁棒性的能力。视频演示可访问：<https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view>|
|**2025-01-25**|**Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions**|Naihao Deng et.al.|[2501.15283](http://arxiv.org/abs/2501.15283)|null|随着大型语言模型（LLMs）的能力不断提升，研究人员越来越多地将其用于社会模拟。在本文中，我们研究了LLM代理之间的互动是否类似于人类的互动。具体而言，我们关注领导者和非领导者之间的代词使用差异，考察模拟是否会引导出类似人类的代词使用模式。我们的评估揭示了基于LLM的模拟与人类代词使用的显著差异，提示式或专业化的代理未能展示类似人类的代词使用模式。此外，我们发现即使LLMs理解人类的代词使用模式，它们在实际交互过程中也未能表现出这些模式。我们的研究突显了基于LLM代理的社会模拟的局限性，敦促在从业者决策过程中谨慎使用此类社会模拟。|
|**2025-01-24**|**Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement Learning-based Model Caching and Inference Offloading**|Minrui Xu et.al.|[2501.14205](http://arxiv.org/abs/2501.14205)|null|大型语言模型（LLMs）能够在未见过的任务上进行零样本学习，并在复杂的推理任务上进行少样本学习。然而，资源有限的移动边缘网络在多轮交互过程中难以支持长上下文LLM的服务。与边缘计算中的无状态计算卸载和静态服务卸载不同，优化边缘服务器上的LLM服务具有挑战性，因为LLMs会持续从上下文中学习，这引发了准确率、延迟和资源消耗的动态变化。在这篇论文中，我们提出了一种联合模型缓存和推理卸载框架，该框架利用测试时深度强化学习（T2DRL）来优化长上下文LLM服务的部署和执行策略。在这个框架中，我们分析了性能收敛情况，并设计了一个考虑LLMs中上下文窗口利用的优化问题。此外，T2DRL算法可以在训练阶段和测试阶段进行学习，主动管理缓存的模型和服务请求，并适应执行过程中的上下文变化和使用模式。为了进一步提高资源分配效率，我们提出了一种双荷兰拍卖（DDA）机制，该机制能够动态匹配供需关系，同时最大化社会福利。最后，实验结果表明，T2DRL算法相比基线可以减少至少30%的系统成本，同时保证LLM代理在真实世界感知和推理任务中的性能。|
|**2025-01-24**|**AI Chatbots as Professional Service Agents: Developing a Professional Identity**|Wenwen Li et.al.|[2501.14179](http://arxiv.org/abs/2501.14179)|null|随着大型语言模型（LLM）应用的迅速扩展，LLM基于的AI聊天机器人角色正从仅仅作为通用查询工具转向充当专业服务代理。然而，目前的研究往往忽视了专业服务代理的一个关键方面：以与其专业身份一致的方式进行沟通。这一点在医疗领域尤为重要，在该领域与患者的有效沟通对于实现专业目标至关重要，例如通过鼓励健康行为来促进患者福祉。为了弥补这一差距，我们提出了LAPI（具有专业身份的LLM代理），这是一种设计用于医疗问答服务的专业服务代理的新框架，确保与特定专业身份的一致性。我们的方法包括一个基于理论的任务规划过程，将复杂的专业任务分解为与专业目标一致的可管理子任务，以及一种实用的熵方法，旨在生成具有低不确定性的专业和道德响应。对各种LLM的实验表明，所提出的方法在流畅性、自然性、同理心、以患者为中心和ROUGE-L分数等关键指标上优于基线方法，包括少量提示、思维链提示。此外，消融研究强调了每个组件对整体效果的贡献。|
|**2025-01-23**|**AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback**|Joshua Park et.al.|[2501.13333](http://arxiv.org/abs/2501.13333)|**[link](https://github.com/joshprk/agentrec)**|**多智能体系统必须决定哪个代理最适合执行给定任务。我们提出了一种新颖的架构，通过扩展Sentence-BERT (SBERT)编码模型来推荐在给定自然语言提示的情况下应由哪个大型语言模型代理执行任务。在测试数据上，我们实现了92.2%的前1准确率，每个分类耗时不到300毫秒。与传统分类方法相比，我们的架构计算成本低廉，能够适应新类别，具有可解释性和可控性，并且可以通过强化学习使用任意指标进行调整。通过将自然语言提示编码成句子嵌入，我们的模型捕捉到与推荐代理相关的语义内容。然后，通过对属于同一代理的句子嵌入之间的距离进行最小化并通过人类反馈的强化学习进行对齐，完成了微调。这使得基于最近邻原则通过测量嵌入之间的余弦相似度来对自然语言提示进行分类成为可能。这项工作是通过生成一个用于代理推荐的合成数据集实现的，我们已将该数据集以及AgentRec推荐系统的代码开源给公众，网址为https://github.com/joshprk/agentrec。**|
|**2025-01-23**|**Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents**|Shrinidhi Kumbhar et.al.|[2501.13299](http://arxiv.org/abs/2501.13299)|null|材料发现与设计对于推动各个行业技术进步至关重要，通过开发满足特定应用需求的材料来实现这一目标。近期的研究利用了大规模语言模型（LLMs）来加速这一过程。我们探讨了LLMs在生成可行假设方面的潜力，这些假设一旦经过验证，可以加快材料发现的速度。与材料科学专家合作，我们从最近的期刊出版物中整理了一个新颖的数据集，该数据集涵盖了为设计实际应用而设定的真实目标、约束条件和方法。使用这个数据集，我们测试了基于LLM的代理，它们生成了实现给定目标并符合特定约束条件的假设。为了评估这些假设的相关性和质量，我们提出了一种新的可扩展评估指标，该指标模拟了材料科学家在批判性评估假设时所采用的过程。我们整理的数据集、提出的方法和评估框架旨在推进未来以LLMs加速材料发现与设计的研究。|
|**2025-01-21**|**LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems**|Venkata Sai Aswath Duvvuru et.al.|[2501.11864](http://arxiv.org/abs/2501.11864)|null|彻底的仿真测试对于验证小型无人驾驶航空系统（sUAS）在多种场景下的正确行为至关重要，包括恶劣天气条件（如风和雾）、不同环境（如丘陵地形或城市区域）以及不同的任务配置文件（如监视、跟踪）。尽管存在各种支持开发者的sUAS仿真工具，创建、执行和分析仿真测试的整个过程仍然是一个主要的手动且繁琐的任务。开发者必须确定测试场景、设置仿真环境、将被测系统（SuT）与仿真工具集成、制定任务计划，并收集和分析结果。这些劳动密集型任务限制了开发者进行广泛场景的全面测试的能力。为了解决这个问题，本文提出了一种名为AutoSimTest的大语言模型（LLM）驱动框架，在该框架中多个LLM代理协作以支持sUAS仿真测试过程。这包括：(1) 创建使SuT处于独特环境上下文中的测试场景；(2) 根据测试场景准备仿真环境；(3) 生成供SuT执行的不同sUAS任务；以及(4) 分析仿真结果并提供交互式分析界面。此外，该框架的设计灵活，适用于多种sUAS用例、仿真工具和SuT输入要求。我们通过以下方式评估了我们的方法：(a) 对基于PX4和ArduPilot飞行控制器的SuT进行仿真测试，(b) 分析每个代理的性能，(c) 收集sUAS开发者的反馈。研究结果表明，AutoSimTest显著提高了sUAS测试过程的效率和范围，使得能够进行更全面和多样的场景评估，同时减少了手动工作量。|
|**2025-01-20**|**Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training**|Siyu Yuan et.al.|[2501.11425](http://arxiv.org/abs/2501.11425)|**[link](https://github.com/bytedance/agent-r)**|**大型语言模型（LLMs）代理在处理交互环境中的复杂任务时变得越来越重要。现有的工作主要集中在通过从更强的专家那里进行行为克隆来增强性能，然而这种方法在实际应用中往往难以奏效，主要是因为无法从错误中恢复。然而，逐步骤批评的数据收集既困难又昂贵。因此，自动化和动态构建自我批评数据集对于赋予模型智能代理能力至关重要。在这项工作中，我们提出了一种迭代自我训练框架Agent-R，使语言代理能够即时反思。与传统的基于正确性奖励或惩罚动作的方法不同，Agent-R利用MCTS构建从错误轨迹中恢复正确轨迹的训练数据。代理反思的关键挑战在于需要及时修订而不是等到rollout结束。为了解决这个问题，我们引入了一种模型引导的批评构建机制：行动者模型在其当前能力范围内识别失败轨迹中的第一个错误步骤。从该步骤开始，我们将它与相邻的正确路径拼接在一起，这两个路径在树中共享相同的父节点。这一策略使得模型能够根据其当前策略学习反思，从而提高学习效率。为了进一步探索这种自我改进范式的可扩展性，我们研究了对错误纠正能力和数据集构建的迭代细化。我们的研究结果表明，Agent-R持续提高了模型从错误中恢复的能力，并实现了及时的错误修正。在三个交互环境中进行的实验表明，Agent-R有效地使代理能够纠正错误动作，同时避免循环，相比基线方法性能更优（+5.59%）。**|
|**2025-01-20**|**Towards Advancing Code Generation with Large Language Models: A Research Roadmap**|Haolin Jin et.al.|[2501.11354](http://arxiv.org/abs/2501.11354)|null|近年来，大型语言模型得到了迅速发展，在代码生成的下游任务中展示了出色的性能。然而，尽管这些模型具有潜力，基于LLM的代码生成仍然面临诸多技术和评估挑战，特别是在实际开发环境中的嵌入应用。在本文中，我们提出了当前研究方向的展望，并对这一任务的现有研究进行了深入分析。我们提出了一种六层视图框架，将代码生成过程分类为不同的阶段，即输入阶段、编排阶段、开发阶段和验证阶段。此外，我们概述了我们的视图工作流程，反映了目前普遍存在的框架。我们系统地分析了大型语言模型面临的挑战，包括基于LLM的代理框架在代码生成任务中的挑战。通过这些，我们在该领域提供了各种视角和可操作的建议。我们的目标是为提高基于LLM的代码生成系统的可靠性和实用性提供指导。最终，这项工作旨在解决持久的挑战，并为未来更实用的LLM解决方案提供切实可行的建议。|
|**2025-01-20**|**Large Language Model Agents for Radio Map Generation and Wireless Network Planning**|Hongye Quan et.al.|[2501.11283](http://arxiv.org/abs/2501.11283)|null|使用商用软件进行无线电图生成和无线网络规划通常需要复杂的手动操作，这在可扩展性、适应性和用户友好性方面带来了显著挑战，主要是由于大量的手动操作。为了解决这些问题，我们提出了一种自动化解决方案，该方案采用大型语言模型（LLM）代理。这些代理旨在为指定区域自主生成无线电图并促进无线网络规划，从而最大限度地减少大量手动干预的必要性。为了验证所提出的解决方案的有效性，我们开发了一个集成LLM代理的软件平台。实验结果表明，通过所提出的LLM代理可以节省大量的手动操作，并且自动化解决方案可以在城市环境中实现增强的覆盖范围和信噪比（SINR）。|
|**2025-01-20**|**PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents**|Kanika Goswami et.al.|[2501.11233](http://arxiv.org/abs/2501.11233)|null|图表可视化对于数据解读和交流至关重要，但通常仅以PDF中的图片形式存在，缺乏源数据表格和样式信息。为了使PDF或数字扫描中的图表能够得到有效编辑，我们提出了PlotEdit，这是一种新颖的多代理框架，通过自我反思的大型语言模型（LLM）代理实现自然语言驱动的端到端图表图像编辑。PlotEdit协调五个LLM代理：(1) Chart2Table用于数据表格提取，(2) Chart2Vision用于样式属性识别，(3) Chart2Code用于检索渲染代码，(4) 指令分解代理用于解析用户请求为可执行步骤，以及(5) 多模态编辑代理用于实施复杂的图表组件修改——所有这些都通过多模态反馈进行协调以保持视觉保真度。在ChartCraft数据集上，PlotEdit在样式、布局、格式和数据中心编辑方面优于现有基线，提高了对视障用户的可访问性并提升了初学者的生产力。|
|**2025-01-18**|**Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments**|Hongjin Su et.al.|[2501.10893](http://arxiv.org/abs/2501.10893)|null|自主代理由大型语言模型（LLMs）驱动，有可能增强人类能力，协助执行从发送电子邮件到执行数据分析等数字任务。现有LLMs在这些任务上的能力往往受到缺乏相应环境交互的高质量代理数据的限制。我们提出了一种名为Learn-by-interact的数据中心框架，该框架能够适应任何给定环境而无需人工注释。Learn-by-interact基于文档综合代理与环境交互的轨迹，并通过总结或抽象交互历史来构建指令，这一过程称为逆向构造。我们通过使用合成数据进行基于训练的场景和无训练上下文学习（ICL）评估其质量，在ICL中，我们设计了针对代理优化的创新检索方法。在涵盖现实编码、网络和桌面环境的SWE-bench、WebArena、OSWorld和Spider2-V的广泛实验表明，Learn-by-interact在各种下游代理任务中的有效性——对于使用Claude-3.5的ICL，基线结果提高了多达12.2%，而对于使用Codestral-22B的训练提高了19.5%。我们进一步证明了逆向构造的关键作用，它提供了高达14.0%的训练改进。我们的消融研究展示了合成数据在ICL中的效率以及我们检索管道相对于替代方法如传统检索增强生成（RAG）的优势。我们预计Learn-by-interact将成为LLMs在实际部署于真实环境时的基础性代理数据合成方法。|
|**2025-01-18**|**ML-SceGen: A Multi-level Scenario Generation Framework**|Yicheng Xiao et.al.|[2501.10782](http://arxiv.org/abs/2501.10782)|null|当前的科学研究见证了将大规模语言模型应用于场景生成的各种尝试，但这些研究倾向于只关注全面或危险的场景。在本文中，我们寻求构建一个三阶段框架，不仅让用户重新获得对生成场景的可控性，还能在不受控制的交叉路口环境中生成包含危险因素的全面场景。在第一阶段，大型语言模型代理将有助于将预期场景描述的关键组件转化为功能场景。在第二阶段，我们使用答案集编程（ASP）求解器Clingo来帮助我们在交叉路口内生成全面的逻辑交通。在最后一个阶段，我们使用大型语言模型更新相关参数以提高具体场景的危险程度。|
|**2025-01-17**|**PaSa: An LLM Agent for Comprehensive Academic Paper Search**|Yichen He et.al.|[2501.10120](http://arxiv.org/abs/2501.10120)|**[link](https://github.com/bytedance/pasa)**|**我们介绍了PaSa，一个由大型语言模型驱动的先进论文搜索代理。PaSa能够自主做出一系列决策，包括调用搜索工具、阅读论文和选择相关参考文献，以最终获得复杂学术查询的全面且准确的结果。我们使用合成数据集AutoScholarQuery优化了PaSa，该数据集包含来自顶级AI会议出版物的35,000个细粒度学术查询及其对应的论文。此外，我们开发了RealScholarQuery基准，收集现实世界的学术查询，以评估PaSa在更现实场景中的表现。尽管是在合成数据上训练的，PaSa在RealScholarQuery上的表现显著优于现有的基线系统，包括Google、Google Scholar、对查询进行改写后的Google与GPT-4、chatGPT（可搜索的GPT-4o）、GPT-o1以及通过提示GPT-4o实现的PaSa-GPT-4o。值得注意的是，PaSa-7B在召回@20和召回@50方面分别比最佳的Google基线Google与GPT-4o高出37.78%和39.90%。它还在召回方面比PaSa-GPT-4o高出30.36%，在精确率方面高出4.25%。模型、数据集和代码可在<https://github.com/bytedance/pasa>获取。**|
|**2025-01-17**|**A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks**|Xinzhe Li et.al.|[2501.10069](http://arxiv.org/abs/2501.10069)|**[link](https://github.com/xinzhel/llm-agent-survey)**|LLM测试时计算（或LLM推理）通过搜索已成为一个有前景的研究领域，并且发展迅速。然而，当前的框架通常在三个关键方面（任务定义、LLM配置文件和搜索过程）采取不同的视角，这使得直接比较变得困难。此外，所采用的搜索算法往往偏离了标准实现，而它们的具体特征并未得到充分说明。在这篇调查文章中，我们提供了一个全面的技术回顾，统一了任务定义，并提供了LLM配置文件和搜索过程的模块化定义。这些定义使各种LLM推理框架之间的精确比较成为可能，同时突出了它们与传统搜索算法的差异。我们还讨论了这些方法的应用性、性能和效率。有关更多细节和最新更新，请参阅我们的GitHub仓库：https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md|
|**2025-01-15**|**Leveraging LLM Agents for Translating Network Configurations**|Yunze Wei et.al.|[2501.08760](http://arxiv.org/abs/2501.08760)|null|配置翻译在网络操作中是一项关键且频繁的任务。当网络设备损坏或过时需要更换以维持服务连续性时，这些替换设备可能来自不同的供应商，因此需要进行配置翻译以确保网络操作的无缝运行。然而，手动翻译配置既耗时又容易出错。在本文中，我们提出了一种基于意图的框架，利用大型语言模型（LLM）代理进行网络配置翻译。我们的方法核心是一个基于意图的检索增强生成（IRAG）模块，该模块系统地将配置文件拆分为片段，提取意图，并生成准确的翻译。我们还设计了一个两阶段验证方法来验证翻译配置的语法和语义正确性。我们在真实世界的网络配置上实现了并评估了所提出的方法。实验结果表明，我们的方法达到了97.74%的语法正确性，优于现有技术在翻译准确性方面的表现。|
|**2025-01-14**|**Addressing the sustainable AI trilemma: a case study on LLM agents and RAG**|Hui Wu et.al.|[2501.08262](http://arxiv.org/abs/2501.08262)|null|大型语言模型（LLMs）展示了显著的能力，但其广泛部署和更高级的应用引发了关键的可持续性挑战，特别是在推理能耗方面。我们提出了可持续人工智能三难概念，强调了人工智能能力、数字公平性和环境可持续性之间的紧张关系。通过系统地研究LLM代理和检索增强生成（RAG）的案例研究，我们分析了嵌入在内存模块设计中的能源成本，并引入了新的指标来量化能源消耗与系统性能之间的权衡。我们的实验结果揭示了当前内存增强框架中存在的重大能源效率低下的问题，并表明资源受限环境面临不成比例的效率惩罚。我们的发现挑战了以LLM为中心的设计范式，并为开发更可持续的人工智能系统提供了实用的见解。|
|**2025-01-14**|**Flow: A Modular Approach to Automated Agentic Workflow Generation**|Boye Niu et.al.|[2501.07834](http://arxiv.org/abs/2501.07834)|**[link](https://github.com/tmllab/2025_iclr_flow)**|多智能体框架由大型语言模型（LLM）驱动，在自动化规划和任务执行方面已经取得了显著的成功。然而，有效调整代理工作流程在执行过程中的研究还不够充分。有效的流程调整至关重要，因为在许多现实场景中，初始计划必须实时适应不可预见的挑战和不断变化的条件，以确保复杂任务的高效执行。本文中，我们将工作流程定义为活动顶点（AOV）图。我们通过基于历史表现和先前AOV动态调整任务分配来持续优化工作流程。为了进一步提高系统性能，我们强调基于测量并行性和依赖复杂性的工作流程设计模块化。我们提出的多智能体框架实现了高效的子任务并发执行、目标达成和容错能力。在不同实际任务中的实证结果表明，通过动态更新工作流程和模块化，多智能体框架的效率得到了显著提升。|
|**2025-01-13**|**SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing**|Varun Biyyala et.al.|[2501.07554](http://arxiv.org/abs/2501.07554)|**[link](https://github.com/custommetrics-sst/sst_customevaluationmetrics)**|**视频编辑模型近年来取得了显著进展，但评估其性能仍然具有挑战性。传统的度量标准，如CLIP文本和图像得分，往往不尽如人意：文本得分受限于训练数据的不足和层次依赖关系，而图像得分则无法评估时间一致性。我们提出了SST-EM（语义、空间和时间评价指标），这是一种新颖的评价框架，它利用现代视觉-语言模型（VLM）、目标检测和时间一致性检查。SST-EM包含四个组成部分：（1）使用VLM从帧中提取语义信息，（2）使用目标检测进行主要对象跟踪，（3）通过LLM代理进行关注对象的精炼，以及（4）使用视觉Transformer（ViT）进行时间一致性评估。这些组件被整合到一个统一的度量标准中，权重来自于人类评估和回归分析。SST-EM的名字反映了它在视频评估中对语义、空间和时间方面的关注。SST-EM提供了对视频编辑中语义保真度和时间平滑性的全面评估。源代码可在GitHub仓库<https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git>获取。**|
|**2025-01-13**|**Lifelong Learning of Large Language Model based Agents: A Roadmap**|Junhao Zheng et.al.|[2501.07278](http://arxiv.org/abs/2501.07278)|**[link](https://github.com/qianlima-lab/awesome-lifelong-llm-agent)**|**终身学习，也称为持续或增量学习，是推进人工通用智能（AGI）的关键组成部分，通过使系统能够在动态环境中不断适应。尽管大型语言模型（LLM）在自然语言处理方面展示了令人印象深刻的能力，现有的基于LLM的代理通常设计用于静态系统，并且缺乏随着时间响应新挑战而进行适应的能力。本综述首次系统地总结了将终身学习纳入基于LLM的代理的潜在技术。我们将这些代理的核心组件分为三个模块：感知模块用于多模态输入集成，记忆模块用于存储和检索不断发展的知识，以及动作模块用于与动态环境进行基础交互。我们强调这些支柱如何共同实现连续适应，缓解灾难性遗忘，并提高长期性能。本综述为研究人员和从业人员提供了一张路线图，以开发基于LLM代理的终身学习能力，提供了对新兴趋势、评估指标和应用场景的见解。相关文献和资源可在 https://github.com/qianlima-lab/awesome-lifelong-llm-agent 获取。**|
|**2025-01-12**|**AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds**|Yinfang Chen et.al.|[2501.06706](http://arxiv.org/abs/2501.06706)|null|AI for IT Operations (AIOps)旨在自动化复杂的运维任务，如故障定位和根因分析，以减轻人力负担并减少客户影响。虽然传统的DevOps工具和AIOps算法通常专注于解决孤立的运维任务，但大型语言模型（LLM）和AI代理的最新进展正在通过实现端到端和多任务自动化来革新AIOps。本文展望了一个未来，在这个未来中，AI代理在整个事故生命周期中自主管理运维任务，从而实现自我修复的云系统，我们称这种范式为AgentOps。要实现这一愿景，需要一个全面的框架来指导这些代理的设计、开发和评估。为此，我们提出了AIOPSLAB框架，该框架不仅部署微服务云环境、注入故障、生成工作负载并导出遥测数据，还协调这些组件，并提供与代理交互和评估的接口。我们讨论了此类综合性框架的关键需求，并展示了AIOPSLAB如何促进下一代AIOps代理的评估。通过在AIOPSLAB创建的基准中对最先进的LLM代理进行评估，我们提供了有关其在处理云环境中复杂运维任务的能力和局限性的见解。|
|**2025-01-12**|**DVM: Towards Controllable LLM Agents in Social Deduction Games**|Zheng Zhang et.al.|[2501.06695](http://arxiv.org/abs/2501.06695)|null|大型语言模型（LLMs）在社交推理解谜游戏（SDGs）中的游戏代理能力方面取得了显著进展。这些游戏严重依赖于对话驱动的互动，并要求代理基于此类信息进行推理、决策和表达。尽管这一进步使得社交推理解谜游戏中非玩家角色（NPCs）更加复杂和具有策略性，但仍需要控制这些代理的能力。这种控制不仅确保了NPCs可以在游戏过程中适应不同的难度级别，还为LLM代理的安全性和公平性提供了见解。在本文中，我们提出了一种名为DVM的新框架，用于开发适用于社交推理解谜游戏的可控LLM代理，并在最受欢迎的社交推理解谜游戏之一“狼人杀”中展示了其实现。DVM包含三个主要组件：预测器、决策者和讨论者。通过结合强化学习与以胜率为基础的决策链奖励机制，我们使代理能够动态调整其游戏水平以达到指定的胜率目标。实验表明，DVM不仅在“狼人杀”游戏中优于现有方法，还能成功调节其性能水平以满足预定义的胜率目标。这些结果为LLM代理在社交推理解谜游戏中的自适应和平衡游戏玩法铺平了道路，开启了有关可控游戏代理研究的新领域。|
|**2025-01-10**|**OpenFOAMGPT: a RAG-Augmented LLM Agent for OpenFOAM-Based Computational Fluid Dynamics**|Sandeep Pandey et.al.|[2501.06327](http://arxiv.org/abs/2501.06327)|null|这项工作介绍了一个名为OpenFOAMGPT的大语言模型（LLM）代理，该代理专为以OpenFOAM为中心的计算流体动力学（CFD）模拟而设计，利用了来自OpenAI的两个基础模型：GPT-4o和一个具有思维链（CoT）功能的o1预览模型。这两个代理在多个任务中均表现出色。尽管o1模型的令牌价格是GPT-4o的六倍，但它在处理从零样本案例设置到边界条件修改、湍流模型调整和代码翻译等复杂任务时始终表现出优越的性能。通过迭代校正循环，该代理高效地解决了单相流和多相流、传热、RANS、LES及其他工程场景的问题，通常在有限的迭代次数内以较低的令牌成本收敛。为了嵌入特定领域的知识，我们采用了检索增强生成（RAG）管道，展示了如何利用现有的仿真设置进一步将代理专业化于能源和航空航天等子领域。尽管该代理表现优异，但人类监督对于确保准确性并适应不断变化的环境仍然至关重要。模型性能随时间波动表明，在关键任务应用中需要进行监控。虽然我们的演示集中在OpenFOAM上，但这种框架的可适应性为开发针对广泛求解器和代码的LLM驱动代理打开了大门。通过简化CFD仿真，这种方法有望加速基础研究和工业工程的进步。|
|**2025-01-10**|**Multi-Agent Collaboration Mechanisms: A Survey of LLMs**|Khanh-Tung Tran et.al.|[2501.06322](http://arxiv.org/abs/2501.06322)|null|随着大型语言模型（LLMs）的最新进展，具身人工智能在现实世界的应用变得引人注目，并朝着基于多个LLM的代理方向发展，这些代理能够感知、学习、推理和协作行动。这些基于LLM的多智能体系统（MASs）使一群智能代理能够在规模上协调并集体解决复杂任务，从孤立的模型转向以合作为中心的方法。本文提供了关于MASs协作方面的广泛调查，并介绍了一个可扩展的框架以指导未来的研究。我们的框架根据关键维度对协作机制进行分类：参与者（涉及的代理）、类型（如合作、竞争或coopetition）、结构（如点对点、集中式或分布式）、策略（如基于角色或基于模型）以及协调协议。通过回顾现有方法论，我们的研究结果为阐明和推进LLM-based MASs奠定了基础，使其朝着更智能、更协作的解决方案发展，以应对复杂的实际应用。此外，我们还调查了MASs在不同领域的各种应用，包括5G/6G网络、工业5.0、问答系统以及社会和文化环境，展示了它们的广泛应用及其更广泛的影响。最后，我们确定了MASs在人工集体智能方向上的关键经验教训、开放性挑战和潜在研究方向。|
|**2025-01-09**|**Emergence of human-like polarization among large language model agents**|Jinghua Piao et.al.|[2501.05171](http://arxiv.org/abs/2501.05171)|null|大型语言模型（LLMs）的快速发展使自主代理能够建立社会关系、进行沟通，并对政治问题形成共同或分歧的意见。然而，我们对它们的集体行为和潜在机制的理解仍然不完整，这给社会带来了意想不到的风险。在本文中，我们模拟了一个包含数千个大型语言模型代理的网络系统，发现通过LLM对话引导的社会互动导致了类似人类的两极分化。我们发现这些代理自发地建立了具有人类特性的社交网络，包括同质性聚类，同时也通过现实世界中观察到的回声室效应等机制形成了集体意见。人类与LLM代理之间的相似性——包括行为、机制和涌现现象——引发了关于它们放大社会两极分化的担忧，但同时也有可能作为有价值的测试平台来识别可能缓解两极分化及其后果的策略。|
|**2025-01-09**|**LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models**|Zengqi Peng et.al.|[2501.05057](http://arxiv.org/abs/2501.05057)|null|近期强化学习（RL）在自动驾驶领域的进展展示了其巨大的潜力。尽管前景广阔，但诸如人工设计奖励函数以及在复杂环境中样本效率低等问题仍然阻碍了安全有效的驾驶策略的发展。为了解决这些问题，我们介绍了LearningFlow，这是一种专为城市驾驶定制的创新自动化策略学习工作流程。该框架利用多个大型语言模型（LLM）代理在整个RL训练过程中进行协作。LearningFlow包括课程序列生成过程和奖励生成过程，这两个过程协同工作，通过生成定制化的训练课程和奖励函数来指导RL策略。特别的是，每个过程都有一个分析代理来评估训练进度并为生成代理提供关键见解。通过这些LLM代理的合作努力，LearningFlow能够在一系列复杂的驾驶任务中自动学习策略，并显著减少对人工奖励函数设计的依赖，同时提高样本效率。我们在高保真CARLA模拟器中进行了广泛的实验，并与其他现有方法进行了比较，以证明我们所提出方法的有效性。结果表明，LearningFlow在生成奖励和课程方面表现出色，并且在各种驾驶任务中实现了卓越的性能和稳健的泛化能力，同时也能够很好地适应不同的RL算法。|
|**2025-01-08**|**Agent Laboratory: Using LLM Agents as Research Assistants**|Samuel Schmidgall et.al.|[2501.04227](http://arxiv.org/abs/2501.04227)|null|历史上，科学研究是一个漫长且成本高昂的过程，从初步构想到最终成果需要投入大量时间和资源。为了加速科学研究，降低成本并提高研究质量，我们引入了Agent Laboratory（代理实验室），这是一种基于大型语言模型（LLM）的自主框架，能够完成整个研究过程。该框架接受人类提供的研究想法，并通过三个阶段——文献回顾、实验和报告撰写，生成包括代码仓库和研究报告在内的全面研究成果，同时允许用户在每个阶段提供反馈和指导。我们将Agent Laboratory与各种最先进的LLMs结合部署，并邀请多名研究人员通过参与调查、在研究过程中提供反馈以及评估最终论文来评估其质量。我们发现：(1) 由o1-preview驱动的Agent Laboratory生成的研究成果最佳；(2) 生成的机器学习代码能够达到现有方法中的最先进性能；(3) 人类的介入，在每个阶段提供反馈，显著提高了研究的整体质量；(4) Agent Laboratory显著减少了研究费用，相比之前的自主研究方法降低了84%。我们希望Agent Laboratory能够让研究人员将更多精力投入到创意构思而非低级别的编码和写作上，从而加速科学发现。|
|**2025-01-02**|**Toward Inclusive Educational AI: Auditing Frontier LLMs through a Multiplexity Lens**|Abdullah Mushtaq et.al.|[2501.03259](http://arxiv.org/abs/2501.03259)|null|随着像GPT-4和Llama 3这样的大型语言模型（LLM）在教育环境中的应用日益广泛，人们对这些技术中固有的文化偏见、权力失衡和伦理限制的担忧也在增加。尽管生成式人工智能工具旨在提升学习体验，但它们往往反映了根植于西方、受过教育、工业化、富裕且民主（WEIRD）文化范式的价值观，这可能使多元全球视角边缘化。本文提出了一种通过应用多重复性视角来评估和缓解LLM中文化偏见的框架。多重复性，受到Senturk等人启发，并扎根于伊斯兰及其他智慧传统，强调多种文化观点共存的重要性，支持一种将实证科学与规范价值相结合的多层次认识论。我们的分析揭示，LLM经常表现出文化极化，在明显回应和微妙的上下文线索中都显示出偏见。为了应对内在偏见并融入LLM中的多重复性，我们提出了两种策略：一种是基于上下文实现的多重复性LLM，它直接将多重复性原则嵌入系统提示中，在基础层面影响LLM的输出，并独立于个人提示；另一种是多代理系统（MAS）实现的多重复性LLM，其中多个LLM代理各自代表不同的文化视角，协作生成一个平衡且综合的响应。我们的研究结果表明，随着缓解策略从基于上下文提示发展到MAS实施，文化包容性显著提高，表现在视角分布得分（PDS）显著上升，以及PDS熵从基线的3.25%增加到MAS实施的多重复性LLM的98%。情感分析进一步显示，不同文化之间的正面情感倾向有所增加……|
|**2025-01-10**|**MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning**|Pu Yang et.al.|[2501.01834](http://arxiv.org/abs/2501.01834)|null|图像描述是计算机视觉和自然语言处理交叉领域中的关键任务，在各个领域都有广泛的应用。对于复杂的任务如诊断报告生成，深度学习模型不仅需要特定领域的图像-描述数据集，还需要结合相关通用知识以提供上下文准确性。现有的方法存在固有限制：专门化的模型擅长捕捉特定领域的细节但缺乏泛化能力，而基于大型语言模型（LLM）的视觉-语言模型（VLM）利用通用知识但在特定领域适应方面存在问题。为了解决这些限制，本文提出了一种新颖的增强型模型协作框架，我们称之为MoColl，旨在有效整合特定领域的知识和通用知识。具体来说，我们的方法是将复杂的图像描述任务分解为一系列相互关联的问题-答案子任务。采用可训练的视觉问答（VQA）模型作为专门工具，专注于特定领域的视觉分析，根据图像内容回答特定问题。同时，基于LLM的代理利用通用知识来制定这些问题，并将得到的问题-答案对综合成连贯的描述。除了在利用VQA模型方面的作用外，该代理还指导其自身的训练以增强其特定领域的功能。在放射学报告生成方面的实验结果验证了所提框架的有效性，展示了生成报告质量的显著提升。|
|**2025-01-03**|**SDPO: Segment-Level Direct Preference Optimization for Social Agents**|Aobo Kong et.al.|[2501.01821](http://arxiv.org/abs/2501.01821)|**[link](https://github.com/alibabaresearch/damo-convai)**|**社交代理由大型语言模型（LLMs）驱动，能够模拟人类社会行为，但在处理复杂的目标导向社会对话方面表现不足。直接偏好优化（DPO）已被证明在多种代理任务中有效，能够使LLM的行为与人类偏好对齐。现有的基于DPO的多轮交互方法分为轮次级别和会话级别方法。轮次级别的方法过于细粒度，仅关注个别轮次，而会话级别的方法则过于粗略，通常引入训练噪声。为了解决这些局限性，我们提出了分段级直接偏好优化（SDPO），该方法专注于交互中的特定关键片段来优化多轮代理行为，同时最小化训练噪声。在SOTOPIA基准上的评估表明，SDPO调优的代理在所有现有基于DPO的方法和专有LLM如GPT-4o之上表现出色，突显了SDPO在提升LLM代理社会智能方面的潜力。我们的代码和数据可在<https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO>获取。**|
|**2025-01-03**|**AgentRefine: Enhancing Agent Generalization through Refinement Tuning**|Dayuan Fu et.al.|[2501.01702](http://arxiv.org/abs/2501.01702)|null|大型语言模型（LLM）代理已证明能够像人类一样执行复杂的任务。然而，现有的开源LLM与商业模型如GPT系列之间仍存在较大差距。本文专注于通过指令微调来提升LLM的代理泛化能力。我们首先观察到现有的代理训练语料库在保留评估集上表现良好，但在未见过的集合上却无法泛化。这些代理调优工作面临严重的格式错误，并且经常长时间陷入相同的错误中。我们分析认为，泛化能力差的原因在于过度拟合于几个手动代理环境以及缺乏对新情况的适应性。它们在行动步骤上出现问题，无法从经验中学习，只是机械地记忆现有的观察-行动关系。受此启发，我们提出了一个名为AgentRefine的新框架用于代理调优。其核心思想是使模型能够通过轨迹中的观察来纠正自己的错误。具体来说，我们提出了一种代理合成框架，以涵盖各种各样的环境和任务，并提示强大的LLM根据环境反馈来优化其错误行为。AgentRefine在多种代理任务上的泛化能力方面显著优于最先进的代理调优工作。它还具有更好的鲁棒性，面对扰动时能生成多样化的思考。我们的研究结果建立了代理泛化能力和自我优化之间的关联，并为未来的研究提供了一个新的范式。|
|**2025-01-02**|**BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery**|Kanishk Gandhi et.al.|[2501.01540](http://arxiv.org/abs/2501.01540)|**[link](https://github.com/kanishkg/boxing-gym)**|**理解世界并通过科学理论解释它是人工智能研究的核心目标。提出理论、设计实验来验证它们，并根据数据进行修正，是科学发现的基础。尽管基于大型语言模型（LLM）的科学代理具有巨大的潜力，但目前还没有基准系统地测试LLM提出科学模型、收集实验数据以及在新数据面前修订模型的能力。我们引入了BoxingGym，这是一个包含10个环境的基准，用于系统性评估实验设计（例如收集数据以检验科学理论）和模型发现（例如提出并修订科学理论）的能力。为了实现可处理且定量的评估，我们将每个环境实现为一个生成概率模型，科学代理可以通过该模型运行交互式实验。这些概率模型源自各种现实世界的科学领域，从心理学到生态学不等。为了定量评估科学代理收集信息量大的实验数据的能力，我们计算了预期信息增益（EIG），这是一种信息论量度，衡量实验如何减少对生成模型参数的不确定性。一个好的科学理论是一个简洁且有预测性的解释。因此，为了定量评估模型发现，我们要求科学代理解释其模型，然后评估这种解释是否能使另一个科学代理可靠地预测此环境。除了基于解释的评估外，我们还计算标准的模型评估指标，如预测误差。我们发现当前的LLM，如GPT-4，在实验设计和模型发现方面都存在困难。我们发现，用显式的统计模型增强LLM代理并不能可靠地改善这些结果。**|
|**2025-01-02**|**Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects**|Abdullah Mushtaq et.al.|[2501.01205](http://arxiv.org/abs/2501.01205)|null|多智能体大型语言模型（LLMs）因其在复杂问题解决、决策制定和规划任务中利用集体智慧的能力而备受关注。这与“群体的智慧”概念相吻合，在该概念中，不同的代理共同作用以生成有效的解决方案，尤其适用于教育环境。高年级设计项目，也称为毕业设计或最终年份项目，在工程教育中至关重要，因为它们将理论知识与实际应用相结合，培养批判性思维、团队合作和现实世界的问题解决能力。在本文中，我们探讨了在支持工程学生进行的高年级设计项目中使用多智能体LLMs的可能性。这些项目通常涉及多学科考量和相互冲突的目标，如在优化技术性能的同时解决伦理、社会和环境问题。我们提出了一种框架，其中不同的LLM代理代表不同的专家视角，例如问题表述代理、系统复杂性代理、社会伦理代理或项目经理代理，从而促进全面的问题解决方法。这种实现利用了标准多智能体系统（MAS）概念，如协调、合作和谈判，并结合提示工程来为每个代理开发多样化的角色。这些代理通过丰富的协作对话进行互动，模拟人类工程团队的行为，并遵循来自集群人工智能的原则以有效平衡个体贡献向统一解决方案的转化。我们调整了这些技术以创建一个用于LLM代理的合作结构，鼓励跨学科推理和谈判，类似于真实的高年级设计项目。为了评估此框架的有效性，我们收集了六项工程和计算机科学领域的提案|
|**2025-01-01**|**Agentic Systems: A Guide to Transforming Industries with Vertical AI Agents**|Fouad Bousetouane et.al.|[2501.00881](http://arxiv.org/abs/2501.00881)|null|智能体系统的进化在人工智能和现代软件系统领域标志着一个重要里程碑，这是由于对专门针对不同行业的垂直智能的需求所驱动的。这些系统通过适应性、学习能力和与动态环境的互动来提升业务成果。这一变革的前沿是大型语言模型（LLM）智能体，它们作为这些智能系统的认知基础。为了满足一致性和可扩展性的需求，本文试图通过识别核心构建模块并提出一个**认知技能**模块来定义垂直AI智能体设计模式的标准化水平，该模块整合了专为特定领域构建的推理能力。基于这些基础概念，本文全面介绍了智能体系统，详细说明了其核心组件、操作模式和实施策略。此外，本文还探讨了各个行业中的实际用例和示例，突显了LLM智能体在推动行业特定应用方面的变革潜力。|
|**2024-12-31**|**Enabling New HDLs with Agents**|Mark Zakharov et.al.|[2501.00642](http://arxiv.org/abs/2501.00642)|null|大型语言模型（LLMs）基于代理正在通过促进初学者学习、实现代码生成以及优化文档工作流程来改变编程语言格局。硬件描述语言（HDLs）的用户社区较小，但从LLMs中获益良多，尤其是在学习新HDL方面。本文探讨了使LLMs适应HDLs的挑战和解决方案，特别是那些LLMs之前未受过训练的HDLs。本研究介绍了一种针对LLMs优化且对各种HDL了解有限的AI代理——HDLAgent。它显著提升了现成的LLMs。|
|**2024-12-31**|**Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding**|Yue Fan et.al.|[2501.00358](http://arxiv.org/abs/2501.00358)|null|本文研究了从第一人称视角观察中理解动态三维场景的问题，这是机器人和具身人工智能中的一个关键挑战。与之前的研究主要探索长视频理解并仅使用第一人称视频不同，我们提出了一种基于大型语言模型的代理Embodied VideoAgent，该代理通过结合第一人称视频和具身感知输入（例如深度和姿态感知）来构建场景记忆。我们进一步引入了一种基于视觉语言模型的方法，当感知到对物体的动作或活动时自动更新记忆。Embodied VideoAgent在三维场景中的复杂推理和规划任务中显著优于对比方法，分别在Ego4D-VQ3D上提高了4.9%，在OpenEQA上提高了5.8%，在EnvQA上提高了11.7%。我们还展示了它在各种具身人工智能任务中的潜力，包括为机器人操作生成具身交互和感知。代码和演示将会公开发布。|
|**2024-12-31**|**MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation**|Chia-Yuan Chang et.al.|[2501.00332](http://arxiv.org/abs/2501.00332)|null|大型语言模型（LLMs）已成为各种自然语言处理任务的重要工具，但它们经常产生过时或不正确的内容。检索增强生成（RAG）通过引入外部实时信息检索来解决这一问题，从而为LLM的回答提供依据。然而，现有的RAG系统常常难以应对检索文档的质量问题，因为不相关或噪声文档会降低性能，增加计算开销，并削弱响应的可靠性。为了解决这个问题，我们提出了多代理过滤检索增强生成（MAIN-RAG），这是一种无需训练的RAG框架，利用多个LLM代理协作过滤和评分检索到的文档。具体而言，MAIN-RAG引入了一种自适应过滤机制，根据得分分布动态调整相关性过滤阈值，有效减少了噪声同时保持了高召回率的相关文档。该方法利用代理之间的共识确保稳健的文档选择，而无需额外的训练数据或微调。在四个问答基准测试中的实验结果表明，MAIN-RAG相比传统RAG方法能持续提高答案准确性，准确率提高了2-11%，同时减少了不相关的检索文档数量。定量分析进一步显示，我们的方法在响应一致性和答案准确性方面优于基线方法，为基于训练的解决方案提供了一个具有竞争力且实用的替代方案。|
|**2024-12-30**|**AI Agent for Education: von Neumann Multi-Agent System Framework**|Yuan-Hao Jiang et.al.|[2501.00083](http://arxiv.org/abs/2501.00083)|null|大型语言模型的发展为教育带来了新的范式。本文聚焦于教育中的多智能体系统，并提出了冯·诺依曼多智能体系统框架。该框架将每个AI智能体分解为四个模块：控制单元、逻辑单元、存储单元和输入-输出设备，定义了四种类型的运算：任务分解、自我反思、记忆处理和工具调用。此外，本文介绍了与这四种运算相关的技术，如Chain-of-Thought（思维链）、Reson+Act（共鸣+行动）和多智能体辩论。文章还讨论了多智能体系统在教育中的能力增强循环，包括促进人类学习者知识构建的外循环和提升基于大语言模型的智能体群体智能的内循环。通过协作和反思，多智能体系统能够更好地促进人类学习者的学習，并在这个过程中提升其教学能力。|
|**2024-12-30**|**Aviary: training language agents on challenging scientific tasks**|Siddharth Narayanan et.al.|[2412.21154](http://arxiv.org/abs/2412.21154)|**[link](https://github.com/future-house/paper-qa)**|解决复杂的现实世界任务需要行动和观察的循环。这在科学领域尤为明显，其中的任务需要分析、工具使用和实验的多轮次。语言代理因其可以通过自然语言或代码与工具交互而有望自动化科学中的智力任务。然而，它们的灵活性给软件实现带来了概念性和实践性的挑战，因为代理可能包括非标准组件如内部推理、规划、工具使用以及温度采样的语言模型固有的随机性。在这里，我们介绍了Aviary，一个可扩展的用于语言代理的健身房。我们将代理形式化为解决语言导向的部分可观测马尔可夫决策过程的策略，我们将其称为语言决策过程。然后我们实现了五个环境，其中包括三个具有挑战性的科学环境：(1) 操纵DNA构建体进行分子克隆，(2) 通过访问科学文献回答研究问题，以及(3) 工程蛋白质稳定性。这些环境被选中是因为它们专注于多步推理并且与当代生物学研究相关。最后，通过在线训练和扩大推理时间的计算规模，我们展示了由开源而非前沿LLM支持的语言代理可以在多个任务上匹配并超过前沿LLM代理和人类专家的表现，并且成本降低高达100倍。|
|**2024-12-30**|**Exploring and Controlling Diversity in LLM-Agent Conversation**|KuanChao Chu et.al.|[2412.21102](http://arxiv.org/abs/2412.21102)|null|多样性是多智能体通信中的一个关键方面。在本文中，我们专注于开放领域多智能体对话背景下控制和探索多样性，特别是在世界模拟应用中的多样性。我们提出了一种新颖的方法——自适应提示修剪（APP），该方法通过单个参数λ动态调整生成话语的提示内容以控制多样性。通过广泛的实验，我们展示了APP能够有效地跨模型和数据集控制输出多样性，修剪更多的信息会导致更丰富的输出。我们全面分析了提示内容与会话多样性之间的关系。研究结果表明，来自提示所有组件的信息通常会限制输出的多样性，其中记忆块影响最大。APP与现有的技术如温度采样和top-p采样兼容，提供了一个多功能的工具来管理多样性。为了应对增加多样性的权衡问题，例如省略信息导致的一致性问题，我们引入了一个后生成校正步骤，这有效地平衡了多样性增强与输出一致性。此外，我们还研究了提示结构，包括组件顺序和长度对多样性的影响。本研究解决了围绕多智能体世界模拟中多样性的关键问题，提供了对其控制、影响因素及其相关权衡的见解。我们的贡献为系统地设计基于大型语言模型的多智能体协作中的多样性奠定了基础，推动了它们在实际应用中的有效性。|
|**2024-12-30**|**Plancraft: an evaluation dataset for planning with LLM agents**|Gautier Dagan et.al.|[2412.21033](http://arxiv.org/abs/2412.21033)|**[link](https://github.com/gautierdag/plancraft)**|**我们提出了Plancraft，一个多模态评估数据集用于大型语言模型（LLM）代理。Plancraft具有仅文本和多模态界面，基于Minecraft的合成GUI。我们包含了Minecraft Wiki以评估工具使用和检索增强生成（RAG），还包括一个oracle规划器和oracle RAG信息提取器，用以分析现代代理架构的不同组件。为了评估决策能力，Plancraft还包括了一组故意无法解决的例子，提供了一个现实的挑战，要求代理不仅能够完成任务，还需要判断任务是否可解。我们将开源和闭源的LLMs及策略在我们的任务上进行基准测试，并将其性能与手工制作的规划器进行比较。我们发现LLMs和VLMs在Plancraft引入的规划问题上存在困难，并提出了改进其能力的建议。**|
|**2024-12-29**|**Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning**|Hang Ni et.al.|[2412.20505](http://arxiv.org/abs/2412.20505)|null|城市再生在城市化的背景下面临着重大挑战，需要采取适应性方法来应对不断变化的需求。我们利用大型语言模型（LLM）的进展，提出了循环城市规划（CUP），这是一种新的范式，能够持续生成、评估和优化城市规划，并在一个闭环中进行。具体而言，我们的多代理LLM框架包括三个关键组成部分：（1）规划，其中LLM代理根据上下文数据生成和优化城市规划；（2）生活，其中代理模拟居民的行为和互动，模拟城市环境中的生活；（3）评判，涉及评估规划的有效性并提供迭代反馈以改进规划。这一循环过程使得规划方法具有动态性和响应性。实验证明了该框架作为持续和自适应规划过程的有效性。|
|**2024-12-28**|**FaGeL: Fabric LLMs Agent empowered Embodied Intelligence Evolution with Autonomous Human-Machine Collaboration**|Jia Liu et.al.|[2412.20297](http://arxiv.org/abs/2412.20297)|null|近期大型语言模型（LLMs）在推理能力方面的进展提升了具身代理的性能，推动了通往AGI驱动机器人技术的进步。尽管LLMs已在语义推理和任务泛化等任务上得到应用，其在开放物理空间探索中的潜力仍有待开发。本文介绍了一种名为FaGeL（Fabric Agent empowered by embodied intelligence with LLMs）的具身代理，它集成了智能织物技术，实现了无缝且非侵入式的人机交互。FaGeL利用来自可穿戴设备和环境传感器的多模态数据自主生成任务，并通过生成文本中的隐式人类反馈来优化其行为，无需显式的评分或偏好。我们还引入了一种基于标记级别的显著性映射，以可视化LLM的微调过程，增强对标记级别对齐的解释性。该系统利用双重反馈机制改进标记级别对齐，并解决了非侵入式人机交互和认知演化中的挑战。我们的贡献包括FaGeL的开发、DualCUT算法用于AI对齐，以及在合作任务中的实验验证，展示了FaGeL能够通过隐式反馈自主适应和演化的功能。未来，我们计划探索FaGeL在动态环境中的可扩展性及其与其他AI系统的集成，以开发能够无缝适应各种人类需求的AGI代理。|
|**2024-12-28**|**OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System**|Yujie Luo et.al.|[2412.20005](http://arxiv.org/abs/2412.20005)|**[link](https://github.com/zjunlp/oneke)**|**我们介绍了OneKE，一个容器化的schema引导的知识提取系统，它可以从中提取知识网页和原始PDF书籍，并支持各种领域（科学、新闻等）。具体来说，我们设计了OneKE具有多个代理和配置知识库。不同的代理执行各自的角色，从而支持各种提取场景。配置的知识库促进了模式配置、错误案例的调试和纠正，进一步提高了性能。基准数据集上的实证评估证明了OneKE的有效性，而案例研究进一步阐明了它在多个领域的不同任务中的适应性，突显了其广泛应用的潜力。我们的代码已开源在https://github.com/zjunlp/OneKE，并发布了一个演示视频http://oneke.openkg.cn/demo.mp4。**|
|**2024-12-24**|**Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent**|Farhad Nooralahzadeh et.al.|[2412.18428](http://arxiv.org/abs/2412.18428)|**[link](https://github.com/yizhang-unifr/xmode)**|国际企业和组织、医院等收集大量多模态数据，这些数据存储在数据库、文本文档、图像和视频中。尽管在多模态数据分析领域以及自动将自然语言问题转换为数据库查询语言的系统方面已取得一些进展，但结合数据库系统查询与其他非结构化模态（如图像）的自然语言研究挑战仍未得到充分探索。在本文中，我们提出了XMODE——一个支持可解释多模态数据分析的自然语言系统。我们的方法基于以下研究贡献：(1) 我们的系统受到实际使用案例的启发，使用户能够探索多模态信息系统。(2) XMODE利用基于LLM的主动式AI框架将自然语言问题分解为子任务，如文本到SQL生成和图像分析。(3) 在关系数据和图像的多模态数据集上的实验结果表明，我们的系统优于现有的多模态数据分析系统，在准确性以及查询延迟、API成本、规划效率和解释质量等各种性能指标上均表现出色，这得益于LLMs推理能力更有效的利用。|
|**2024-12-25**|**Defining and Detecting the Defects of the Large Language Model-based Autonomous Agents**|Kaiwen Ning et.al.|[2412.18371](http://arxiv.org/abs/2412.18371)|**[link](https://github.com/KevinHeiwa/Agentable)**|**人工智能代理是能够感知其环境、自主规划和执行任务的系统。近期大型语言模型（LLM）的进步引入了一种变革性的范式，使人工智能代理能够通过提示与外部资源和工具进行交互。在这些代理的工作流程中，开发人员编写的代码与由LLM生成的自然语言相结合，前者负责框架构建和逻辑控制，后者则增强动态决策和交互能力。然而，开发者实现的逻辑与LLM生成内容的行为和预期结果之间的差异可能导致缺陷，例如工具调用失败和任务执行错误。这些问题会引发特定风险，导致LLM驱动的人工智能代理出现各种缺陷，如服务中断。尽管这些问题至关重要，但目前缺乏系统性工作专注于分析LLM驱动的人工智能代理以揭示其代码中的缺陷。在这篇论文中，我们提出了首个专注于识别和检测LLM代理缺陷的研究。我们从StackOverflow收集并分析了6,854篇相关帖子，定义了8类代理缺陷，并为每类缺陷提供了详细描述及示例。随后，我们设计了一款静态分析工具Agentable，利用代码属性图和LLM来分析代理工作流程，通过高效识别特定代码模式和分析自然语言描述来进行缺陷检测。为了评估Agentable的有效性，我们构建了两个数据集：AgentSet包含84个真实世界的代理，AgentTest则包含78个专门设计用于包含各种类型缺陷的代理。研究结果显示，Agentable的整体准确率为88.79%，召回率为91.03%。此外，我们的分析揭示了AgentSet中的889个缺陷，突显了这些缺陷的普遍性。**|
|**2024-12-24**|**Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering**|Zhongjian Hu et.al.|[2412.18351](http://arxiv.org/abs/2412.18351)|null|大型语言模型（LLMs）在基于知识的视觉问答（VQA）中取得了显著成果。然而，现有方法仍然面临挑战：无法自主使用外部工具，以及无法团队协作。人类倾向于在遇到新问题时知道是否需要使用外部工具，例如，他们能够直接回答熟悉的问题，而在遇到不熟悉的问题时则倾向于使用搜索引擎等工具。此外，人类还倾向于与他人协作和讨论以获得更好的答案。受此启发，我们提出了多代理投票框架。我们设计了三种基于LLM的代理，模拟团队中的不同级别人员，并根据级别分配可用工具。每个代理提供相应的答案，最后所有代理提供的答案通过投票得到最终答案。在OK-VQA和A-OKVQA上的实验表明，我们的方法比其他基线方法分别高出2.2和1.0。|
|**2024-12-24**|**INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent**|Haohang Li et.al.|[2412.18174](http://arxiv.org/abs/2412.18174)|null|近期的进展突显了大型语言模型（LLM）在金融决策中的潜力。尽管如此，该领域目前面临两个主要挑战：（1）缺乏一个适用于多种金融任务的综合性LLM代理框架，以及（2）缺乏标准化基准和一致的数据集来评估代理性能。为了解决这些问题，我们介绍了\textsc{InvestorBench}，这是首个专门用于评估LLM代理在不同金融决策背景下的表现的基准。InvestorBench通过提供一套全面的任务来增强LLM驱动代理的多功能性，这些任务适用于不同的金融产品，包括单个股票、加密货币和交易所交易基金（ETF）。此外，我们使用十三种不同的LLM作为基础模型，评估了我们的代理框架在各种市场环境和任务中的推理和决策能力。此外，我们还整理了一套开源的多模态数据集，并开发了一整套金融决策环境。这建立了一个高度可访问的平台，用于评估金融代理在各种场景下的表现。|
|**2024-12-23**|**Large Language Model Safety: A Holistic Survey**|Dan Shi et.al.|[2412.17686](http://arxiv.org/abs/2412.17686)|**[link](https://github.com/tjunlp-lab/awesome-llm-safety-papers)**|**大型语言模型（LLMs）的快速发展和部署标志着人工智能领域的一个新前沿，这些模型在自然语言理解和生成方面展现出了前所未有的能力。然而，随着这些模型越来越多地被整合到关键应用中，随之而来的是重大的安全问题，需要对潜在风险进行彻底的审查并提出相应的缓解策略。本调查提供了关于LLM安全性的全面概述，涵盖了四个主要类别：价值错位、对抗性攻击的鲁棒性、误用以及自主AI风险。除了对这四个方面缓解方法和技术评估资源的综合回顾外，我们还探讨了与LLM安全相关的四个主题：LLM代理的安全影响、可解释性在增强LLM安全性中的作用、由一系列AI公司和研究所提出的并与之遵循的技术路线图，以及旨在实现LLM安全的AI治理，包括国际合作、政策建议和预期的监管方向。我们的研究结果强调了采取积极、多方面方法来确保LLM安全的必要性，强调了技术解决方案、伦理考量和稳健治理框架的融合。本调查旨在为学术研究人员、行业从业者和政策制定者提供一个基础资源，以洞悉与LLM安全集成相关的挑战和机遇。最终，它致力于促进LLM的安全和有益发展，与利用人工智能推动社会进步和福祉的总体目标保持一致。相关论文的精选列表已公开发布在https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers。**|
|**2024-12-23**|**LegalAgentBench: Evaluating LLM Agents in Legal Domain**|Haitao Li et.al.|[2412.17259](http://arxiv.org/abs/2412.17259)|**[link](https://github.com/cshaitao/legalagentbench)**|**随着大型语言模型（LLM）代理的智能性和自主性的不断提高，它们在法律领域的潜在应用变得越来越明显。然而，现有的通用领域基准无法完全捕捉现实世界司法认知和决策的复杂性和细微差别。因此，我们提出了LegalAgentBench，这是一个专门设计用于评估中国法律领域中的LLM代理的综合基准。LegalAgentBench包括来自真实法律场景的17个语料库，并提供了37种与外部知识交互的工具。我们设计了一个可扩展的任务构建框架，并仔细标注了300项任务。这些任务涵盖了多种类型，包括多跳推理和写作，并且覆盖了不同难度级别，有效地反映了现实法律场景的复杂性。此外，除了评估最终的成功与否，LegalAgentBench还在中间过程中纳入了关键词分析来计算进度率，从而实现更精细的评估。我们评估了八种流行的LLM，突显了现有模型和方法的优势、局限性和潜在改进领域。LegalAgentBench为LLM在法律领域的实际应用设定了新的标准，其代码和数据可在<https://github.com/CSHaitao/LegalAgentBench>获取。**|
|**2024-12-22**|**LLM Agent for Fire Dynamics Simulations**|Leidong Xu et.al.|[2412.17146](http://arxiv.org/abs/2412.17146)|null|显著的进展已经在利用基础模型，如大型语言模型（LLMs）来加速复杂的科学工作流程上取得。本文介绍了一种名为FoamPilot的概念验证LLM代理，旨在增强FireFOAM的可用性，FireFOAM是一种使用开源计算流体动力学工具箱OpenFOAM构建的专业火动力学和灭火模拟求解器。FoamPilot提供了三个核心功能：代码洞察、案例配置和仿真评估。代码洞察是一种替代传统关键词搜索的方法，采用检索增强生成（RAG），旨在使开发人员和有经验的用户能够高效地导航和总结FireFOAM源代码。对于案例配置，该代理以自然语言解释用户请求，并相应地修改现有的模拟设置，以支持中级用户。FoamPilot的任务执行功能旨在管理在高性能计算（HPC）环境中的模拟提交和执行，并提供模拟结果的初步分析，以支持经验较少的用户。每个功能都取得了有希望的结果，尤其是在简单的任务上，并且识别出了在更复杂任务上进行重大改进的机会。将这些功能整合到一个单一的LLM代理中是为了加速工程师和科学家在进行复杂的FireFOAM模拟时的工作流程，这对于提高消防安全至关重要。|
|**2024-12-21**|**The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents**|Feiran Jia et.al.|[2412.16682](http://arxiv.org/abs/2412.16682)|null|大型语言模型（LLM）代理作为能够通过工具集成执行复杂现实任务的对话助手正被越来越多地部署。这种增强的能力与外部系统的交互以及处理各种数据源虽然功能强大，但也引入了显著的安全漏洞。特别是嵌入在外部数据源中的恶意指令可以通过间接提示注入攻击操纵代理以偏离用户的意图。尽管基于规则约束、来源 spotlighting 和认证协议的现有防御措施显示出一定的前景，但它们难以在保持任务功能的同时维护强大的安全性。我们提出了一种新的正交视角，将代理安全从防止有害行为重新定义为确保任务一致性，要求代理的每个行动都服务于用户目标。基于这一见解，我们开发了Task Shield，这是一种测试时防御机制，系统性地验证每个指令和工具调用是否有助于用户指定的目标。通过在AgentDojo基准上的实验，我们证明Task Shield将攻击成功率降低到2.07%，同时保持了69.79%的任务效用，在GPT-4o上表现良好。|
|**2024-12-19**|**Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code Generation and Execution in Complex Task Handling**|Ziyi Ni et.al.|[2412.15305](http://arxiv.org/abs/2412.15305)|null|解决复杂的推理任务是智能体的关键实际应用。得益于大型语言模型（LLMs）在代码数据上的预训练，最近的方法如CodeAct成功地使用代码作为LLMs的行动，取得了良好的效果。然而，CodeAct通过依赖片段化的思考来贪婪地生成下一个动作的代码块，这导致了不一致性和不稳定性。此外，CodeAct缺乏与行动相关的地面真值（GT），使其在多轮交互中的监督信号和终止条件受到质疑。为了解决这些问题，我们首先引入了一种简单而有效的端到端代码生成范式——CodeProgram，它利用代码的系统逻辑来与全局推理对齐，从而实现连贯的问题解决。然后，我们提出了基于代码可执行性质的Tree-of-Code（ToC），它可以自生长CodeProgram节点，并在无地面真值场景下实现自我监督。在两个数据集上使用十种流行的零样本LLMs进行的实验结果表明，ToC比CodeAct的准确率提高了近20%，且交互轮次不到其四分之一。一些LLMs在单轮CodeProgram上的表现甚至优于多轮CodeAct。为了进一步研究有效性和效率之间的权衡，我们测试了不同大小的ToC树和探索机制。我们还强调了ToC端到端数据生成在有监督和强化微调中的潜力。|
|**2024-12-17**|**Memory-Augmented Agent Training for Business Document Understanding**|Jiale Liu et.al.|[2412.15274](http://arxiv.org/abs/2412.15274)|null|传统企业在处理业务文档时面临重大挑战，例如从发票中提取运输参考信息等任务在物流操作中起着至关重要的作用，但目前仍主要依赖人工处理。尽管大型语言模型提供了潜在的自动化可能，但它们直接应用于专业业务领域往往效果不理想。我们介绍了一种名为Matrix（通过推理和迭代探索进行记忆增强代理训练）的新方法，该方法使大型语言模型代理能够通过经验驱动的记忆精炼和迭代学习逐步建立领域专业知识。为了验证这种方法，我们与全球最大的物流公司之一合作，创建了一个通用业务语言格式的发票文件数据集，重点研究了运输参考信息的提取任务。实验表明，Matrix的表现比单一的大型语言模型提示方式高出30.3%，比传统的大型语言模型代理方式高出35.2%。我们进一步分析了优化系统的指标，观察到代理系统需要较少的API调用、成本更低，并且可以平均处理更长的文档。我们的方法通过系统的记忆增强将通用大型语言模型转化为专门的商业工具，在文档处理任务中建立了新的途径。|
|**2024-12-17**|**On the Structural Memory of LLM Agents**|Ruihong Zeng et.al.|[2412.15266](http://arxiv.org/abs/2412.15266)|**[link](https://github.com/zengrh3/StructuralMemory)**|记忆在使基于大型语言模型（LLM）的代理能够进行复杂和长期的交互中起着关键作用，例如问答（QA）和对话系统。尽管已经提出了各种记忆模块用于这些任务，但不同记忆结构对任务的影响仍未充分探索。本文研究了记忆结构和记忆检索方法如何影响基于LLM的代理的表现。具体来说，我们评估了四种类型的记忆结构，包括片段、知识三元组、原子事实和摘要，以及混合记忆，它结合了这些组件。此外，我们评估了三种广泛使用的记忆检索方法：单步检索、重新排序和迭代检索。在四个任务和六个数据集上进行的广泛实验产生了以下关键见解：（1）不同的记忆结构提供了独特的优点，使它们能够针对特定任务进行调整；（2）混合记忆结构在嘈杂环境中表现出显著的韧性；（3）迭代检索在各种场景中始终优于其他方法。我们的研究旨在激发关于为LLM代理设计记忆系统的进一步研究。|
|**2024-12-19**|**On Verbalized Confidence Scores for LLMs**|Daniel Yang et.al.|[2412.14737](http://arxiv.org/abs/2412.14737)|**[link](https://github.com/danielyxyang/llm-verbalized-uq)**|**大型语言模型（LLMs）的兴起及其与我们日常生活的紧密集成使得致力于提高其可信度变得至关重要。通过量化LLMs中的不确定性，可以建立人类对其响应的信任，同时也允许LLM代理基于彼此的不确定性做出更明智的决策。为了估计响应中的不确定性，通常使用内部令牌logits、任务特定的代理模型或多次采样多个响应。这项工作专注于让LLM本身以置信分数的形式口头表达其不确定性作为其输出标记的一部分，这是一种有前景的提示和模型不可知的不确定性量化方法，并且具有低开销。我们使用广泛的基准评估了口头置信分数的可靠性，涉及不同的数据集、模型和提示方法。我们的结果显示这些分数的可靠性在很大程度上取决于模型被询问的方式，但也有可能通过某些提示方法提取出校准良好的置信分数。我们认为口头置信分数可以成为未来一种简单但有效且多功能的不确定性量化方法。我们的代码可在https://github.com/danielyxyang/llm-verbalized-uq 获取。**|
|**2024-12-19**|**Agent-SafetyBench: Evaluating the Safety of LLM Agents**|Zhexin Zhang et.al.|[2412.14470](http://arxiv.org/abs/2412.14470)|**[link](https://github.com/thu-coai/agent-safetybench)**|**随着大型语言模型（LLM）作为代理的部署越来越多，它们在交互环境和工具使用中的集成引入了新的安全挑战，这些挑战超出了与模型本身相关的安全问题。然而，缺乏全面的基准来评估代理的安全性构成了有效评估和进一步改进的重大障碍。在本文中，我们介绍了Agent-SafetyBench，这是一个旨在评估LLM代理安全性的综合基准。Agent-SafetyBench涵盖了349个交互环境和2000个测试用例，评估了8类安全风险，并覆盖了在不安全交互中经常遇到的10种常见失效模式。我们对16个流行的LLM代理进行的评估揭示了一个令人担忧的结果：没有一个代理的安全得分超过60%。这突显了LLM代理在安全性方面的重大挑战，并强调了需要大幅改进。通过定量分析，我们确定了关键的失效模式，并总结了当前LLM代理中的两个基本安全检测问题：缺乏鲁棒性和缺乏风险意识。此外，我们的研究结果表明，仅依赖防御性提示不足以解决这些安全问题，强调了需要更先进和更稳健的策略。我们将Agent-SafetyBench发布在https://github.com/thu-coai/Agent-SafetyBench上，以促进进一步的研究和创新，推动代理安全评估和改进。**|
|**2024-12-18**|**A Survey on Large Language Model-based Agents for Statistics and Data Science**|Maojun Sun et.al.|[2412.14222](http://arxiv.org/abs/2412.14222)|null|近年来，由大型语言模型（LLM）驱动的数据科学代理，被称为“数据代理”，在简化复杂数据任务和降低无相关专业知识的用户使用门槛方面展示了巨大的潜力，从而改变了传统的数据分析范式。本文综述了基于LLM的数据代理的发展、功能和应用，并探讨了它们在最小化人为干预的情况下解决以数据为中心的问题中的作用。我们探讨了当前基于LLM框架设计的趋势，详细介绍了规划、推理、反思、多代理协作、用户界面、知识集成和系统设计等关键特性，这些特性使代理能够处理复杂的任务。此外，我们分析了若干案例研究，以展示不同数据代理在实际场景中的应用。最后，我们指出了主要挑战并提出了未来的研究方向，以推动数据代理发展成为智能统计分析软件。|
|**2024-12-18**|**Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution**|Ziyi Ni et.al.|[2412.14212](http://arxiv.org/abs/2412.14212)|null|大型语言模型（LLMs）的卓越能力极大地加速了代理的快速发展和广泛应用。最近的研究表明，生成Python代码以将基于LLMs的代理行为整合到统一的动作空间（CodeAct）中是开发实际应用中LLM代理的一种有前景的方法。然而，这种逐步生成代码的方法往往缺乏一致性和稳健性，导致代理应用程序在复杂推理和域外任务中的稳定性不足。在这篇论文中，我们提出了一种称为代码树（ToC）的新方法来解决通过端到端机制进行复杂问题规划和执行的挑战。通过结合思维树和CodeAct的关键思想，ToC融合了它们的优势以增强解决方案探索。在我们的框架中，每个最终代码执行结果被视为决策树中的一个节点，采用广度优先搜索策略来探索潜在解决方案。最终结果通过基于节点输出的投票机制确定。|
|**2024-12-18**|**TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks**|Frank F. Xu et.al.|[2412.14161](http://arxiv.org/abs/2412.14161)|**[link](https://github.com/theagentcompany/experiments)**|**我们日常生活中和工作中都在与计算机进行互动，而且许多工作都可以通过访问计算机和互联网来完成。与此同时，由于大型语言模型（LLM）的改进，与周围环境进行交互并影响其变化的人工智能代理也得到了快速发展。但是，这些人工智能代理在帮助加速甚至自主执行与工作相关任务方面的表现如何？这个问题的答案对于希望在其工作流程中采用人工智能的行业以及希望了解人工智能采用可能对劳动力市场产生的影响的经济政策制定者都具有重要意义。为了衡量这些LLM代理在执行现实世界专业任务方面的进展，我们在本文中介绍了TheAgentCompany，这是一个可扩展的基准测试，用于评估与数字工作者相似方式与世界互动的AI代理：通过浏览网络、编写代码、运行程序以及与其他同事沟通。我们构建了一个自包含的环境，其中包含内部网站和数据，模仿小型软件公司的环境，并创建了一系列可能由此类公司员工执行的任务。我们测试了基于封闭API和开放权重语言模型（LM）的基线代理，发现最优秀的代理能够自主完成24%的任务。这为我们提供了一幅关于使用LM代理进行任务自动化的复杂图景——在一个模拟真实工作场所的环境中，一些较简单的任务可以自主解决，但更复杂的长期任务仍然是当前系统难以企及的。**|
|**2024-12-18**|**Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery**|ChengAo Shen et.al.|[2412.13667](http://arxiv.org/abs/2412.13667)|null|因果推理是跨领域决策的重要基础，如智能健康、AI药物发现和AIOps。尽管传统的统计因果发现方法已经很成熟，但这些方法主要依赖于观察数据，并且经常忽略了因果关系中固有的语义线索。大型语言模型（LLMs）的出现提供了一种利用这些语义线索进行知识驱动因果发现的经济方式，但在因果发现领域的LLMs开发滞后，尤其是在探索多模态数据方面。为了弥合这一差距，我们引入了MATMCD，这是一种由工具增强型LLMs驱动的多代理系统。MATMCD有两个关键代理：一个数据增强代理负责检索和处理模态增强数据，一个因果约束代理负责整合多模态数据以进行知识驱动的推理。精心设计的内部机制确保了代理之间的成功协作。我们的实证研究涵盖了七个数据集，表明多模态增强因果发现的巨大潜力。|
|**2024-12-18**|**SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents**|Sheng Yin et.al.|[2412.13178](http://arxiv.org/abs/2412.13178)|**[link](https://github.com/shengyin1224/safeagentbench)**|**随着大型语言模型（LLMs）的集成，具身代理在自然语言执行复杂指令方面表现出强大的能力，为具身机器人的潜在部署铺平了道路。然而，可以预见的是，这些具身代理也可以完美地执行一些危险任务，可能在现实世界中造成损害。为了研究这个问题，我们提出了SafeAgentBench——一个新的针对具身LLM代理的安全感知任务规划基准。SafeAgentBench包括：(1)一个包含750个任务的新数据集，涵盖了10种潜在危害和3种任务类型；(2)SafeAgentEnv，这是一个具有底层控制器的通用具身环境，支持多代理执行，并为8种最先进的基线提供了17种高级动作；以及(3)从执行和语义角度出发的可靠评估方法。实验结果表明，表现最好的基线对于安全任务的成功率为69%，但对于危险任务的拒绝率仅为5%，这表明存在显著的安全风险。更多详情和代码可在<https://github.com/shengyin1224/SafeAgentBench>获取。**|
|**2024-12-17**|**RareAgents: Autonomous Multi-disciplinary Team for Rare Disease Diagnosis and Treatment**|Xuanzhong Chen et.al.|[2412.12475](http://arxiv.org/abs/2412.12475)|null|罕见疾病尽管单个疾病的发病率较低，但全球范围内影响的人数大约有3亿人，这是因为罕见疾病的种类繁多。这些疾病症状复杂且专业医生短缺，使得诊断和治疗比常见疾病更具挑战性。最近，由大型语言模型（LLM）驱动的代理在各个领域都显示出显著的进步。在医学领域，一些代理方法在从医学考试中进行问答的任务上已经超过了直接提示的方法。然而，当前的代理框架缺乏适应现实临床场景的能力，特别是那些涉及罕见疾病复杂需求的情况。为了解决这些挑战，我们提出了RareAgents，这是首个针对罕见疾病复杂临床环境的多学科LLM代理团队。RareAgents集成了先进的规划能力、记忆机制以及医疗工具的使用，其基础模型采用Llama-3.1-8B/70B。实验结果显示，RareAgents在罕见疾病的鉴别诊断和药物推荐方面超越了最先进的领域特定模型、GPT-4o以及现有的代理框架。此外，我们还贡献了一个新的数据集MIMIC-IV-Ext-Rare，该数据集基于MIMIC-IV，以支持该领域的进一步发展。|
|**2024-12-16**|**Codenames as a Benchmark for Large Language Models**|Matthew Stephenson et.al.|[2412.11373](http://arxiv.org/abs/2412.11373)|null|在本文中，我们提议将广受欢迎的基于词语的棋盘游戏“密码”作为评估大型语言模型（LLMs）推理能力的一个合适基准。密码游戏对实现成功的AI表现提出了高度有趣的挑战，需要具备复杂的语言理解、心理理论和认识论推理能力。此前开发密码游戏代理的努力主要依赖于词嵌入技术，这些技术词汇范围有限，并且在与其他方法结合时表现不佳。尽管LLMs在语言任务的理解和推理能力方面表现出色，但在横向思维挑战中仍可能遇到困难。我们评估了几种最先进的LLMs，包括GPT-4o、Gemini 1.5、Claude 3.5 Sonnet和Llama 3.1，在各种棋盘设置下的表现。我们的结果表明，虽然某些LLMs总体上表现优于其他模型，但不同的模型在游戏中表现出不同的新兴行为，并在特定角色中表现出色。我们还评估了不同组合的LLMs在合作游戏中的表现，证明LLM代理比先前的技术更能适应更广泛的队友组合。|
|**2024-12-14**|**Towards Action Hijacking of Large Language Model-based Agent**|Yuyang Zhang et.al.|[2412.10807](http://arxiv.org/abs/2412.10807)|null|在过去的几年中，由大型语言模型（LLM）驱动的智能代理在执行复杂任务方面取得了显著进展。这些基于LLM的代理接收查询作为任务，并通过内置的LLM分解成各种子任务，以指导外部实体（例如工具、AI代理）来回答用户的问题。凭借其出色的理解和解决问题的能力，它们被广泛应用于劳动密集型领域，包括医疗保健、金融、代码完成等。与此同时，也有关于这些代理可能被滥用的担忧，促使服务提供商内置了安全防护措施。为了规避内置指南，先前的研究提出了许多攻击方法，包括内存投毒、越狱和提示注入。这些研究往往由于代理采用的安全过滤器所限制的权限和查询中的有害语义而无法保持有效性。在本文中，我们介绍了\Name，一种新颖的劫持攻击，用于操纵黑盒代理系统的行动计划。\Name首先通过提示窃取从长期记忆中收集具有行动意识的记忆。然后，它利用代理的内部记忆检索机制提供错误的上下文。检索器与安全过滤器之间的巨大潜在空间差距使我们的方法能够轻松绕过检测。广泛的实验结果证明了我们方法的有效性（例如99.67%的攻击成功率）。此外，我们的方法对安全过滤器的平均绕过率为92.7%。|
|**2024-12-13**|**Cultural Evolution of Cooperation among LLM Agents**|Aron Vallinder et.al.|[2412.10270](http://arxiv.org/abs/2412.10270)|null|大型语言模型（LLMs）为构建具有广泛能力的人工智能代理提供了令人信服的基础。这些代理可能会在现实世界中大规模部署，代表个人（如人工智能助手）或群体（如人工智能加速的公司）的利益。目前，关于多个LLM代理在多代迭代部署中的动态知之甚少。在这篇论文中，我们研究了一组LLM代理是否能够在有诱因背叛的情况下学会互利的社会规范，这是人类社会性的一个显著特征，被认为是文明成功的关键因素之一。特别是，我们研究了间接互惠在LLM代理进行经典重复捐赠者游戏过程中跨代的发展，其中代理可以观察到其同伴的近期行为。我们发现不同基础模型的合作演化差异明显，克劳德3.5诗歌模型组成的社团平均得分显著高于杰米尼1.5闪存模型，而后者又优于GPT-4o。此外，克劳德3.5诗歌模型还可以利用额外的成本惩罚机制来实现更高的得分，而杰米尼1.5闪存和GPT-4o则无法做到这一点。对于每种模型类别，我们还观察到随机种子导致的涌现行为变化，这表明初始条件的敏感依赖性是一个有待研究的问题。我们建议，我们的评估体系可以激发一类新的廉价且信息丰富的LLM基准测试，重点关注LLM代理部署对社会合作基础设施的影响。|
|**2024-12-13**|**ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL**|Yang Qin et.al.|[2412.10138](http://arxiv.org/abs/2412.10138)|**[link](https://github.com/alibaba/route)**|**尽管文本到SQL（Text2SQL）领域因大型语言模型（LLMs）取得了显著进展，但最新的最先进方法仍然受限于闭源LLMs（如GPT-4）的上下文学习，这限制了它们在开放场景中的适用性。为了解决这一挑战，我们提出了一种新的鲁棒多任务调优与协作方法（ROUTE），以提高开源LLMs在Text2SQL方面的综合能力，从而提供一个更实用的解决方案。我们的方法从多任务有监督微调（SFT）开始，使用与SQL生成相关的各种合成训练数据。与现有的基于SFT的Text2SQL方法不同，我们引入了几个额外的SFT任务，包括模式链接、噪声校正和续写。参与多种SQL生成任务可以增强模型对SQL语法的理解，并提高其生成高质量SQL查询的能力。此外，受LLM代理协作模式的启发，我们引入了一种多任务协作提示策略（MCP）。该策略利用多个与SQL相关任务之间的协作来减少SQL生成过程中的幻觉，从而通过显式的多任务能力最大化提升Text2SQL性能。我们在八个开源LLMs和五个广泛使用的基准上进行了广泛的实验和深入分析。结果表明，我们的提案优于最新的Text2SQL方法，并取得了领先的表现。**|
|**2024-12-13**|**You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects**|Islem Bouzenia et.al.|[2412.10133](http://arxiv.org/abs/2412.10133)|null|执行项目的测试套件在许多场景下都是至关重要的，例如评估代码质量和代码覆盖率、验证开发人员或自动化工具所做的代码更改以及确保与依赖项的兼容性。尽管如此，在实践中执行项目的测试套件可能具有挑战性，因为不同的项目使用不同的编程语言、软件生态系统、构建系统、测试框架和其他工具。这些挑战使得创建一个可靠且通用的测试执行方法变得困难，该方法可以跨不同项目工作。本文介绍了一种名为ExecutionAgent的自动化技术，它可以安装任意项目，配置它们以运行测试用例，并生成特定于项目的脚本来重现设置。受到人类开发者解决问题方式的启发，我们的方法是一种基于大型语言模型的代理，它能够自主执行命令并与主机系统进行交互。该代理使用元提示来收集关于给定项目最新技术的指南，并根据前一步骤的反馈迭代地完善其过程。我们的评估将ExecutionAgent应用于50个开源项目，这些项目使用了14种不同的编程语言和许多不同的构建和测试工具。该方法成功地执行了33/55个项目中的测试套件，同时与基准测试套件执行结果的偏差仅为7.5%。这些结果比之前可用的最佳技术提高了6.6倍。该方法的成本是合理的，平均每项目执行时间为74分钟，大型语言模型成本为0.16美元。我们预计ExecutionAgent将成为开发人员、自动化编程工具和研究人员的重要工具，他们需要跨大量项目执行测试。|
|**2024-12-12**|**Can Modern LLMs Act as Agent Cores in Radiology~Environments?**|Qiaoyu Zheng et.al.|[2412.09529](http://arxiv.org/abs/2412.09529)|**[link](https://github.com/magic-ai4med/radabench)**|**大型语言模型（LLM）的进步为基于LLM的代理系统的发展铺平了道路，这些系统在各个领域提供了增强的准确性和可解释性。放射学，由于其复杂的分析需求，是这些代理应用的理想领域。本文旨在探讨构建具体放射学代理的前提问题，即“现代LLM能否作为放射学环境中的代理核心？”为此，我们介绍了RadABench，并有三个贡献：首先，我们提出了RadABench-Data，这是一个从广泛的分类生成的综合合成评估数据集，涵盖了6个解剖部位、5种成像模式、10类工具和11项放射学任务。其次，我们提出RadABench-EvalPlat，一个新的代理评估平台，具有提示驱动的工作流程和模拟各种放射学工具集的能力。第三，我们从五个视角使用多种指标评估了7个领先的LLM在我们基准上的表现。我们的研究结果表明，尽管当前的LLM在许多方面表现出强大的能力，但它们仍然不够先进，无法作为完全操作的放射学代理系统的核心代理核心。此外，我们确定了影响基于LLM的代理核心性能的关键因素，为临床医生提供了如何在现实世界的放射学实践中有效应用代理系统的见解。我们所有的代码和数据都已开源，详见https://github.com/MAGIC-AI4Med/RadABench。**|
|**2024-12-11**|**ChatDyn: Language-Driven Multi-Actor Dynamics Generation in Street Scenes**|Yuxi Wei et.al.|[2412.08685](http://arxiv.org/abs/2412.08685)|null|生成交通参与者在街道场景中的逼真且交互式的动态对于街道场景模拟至关重要。然而，目前缺乏一种全面的方法来生成包括车辆和行人等多种类型参与者的逼真动态，并且这些参与者之间还存在不同类型的互动。在这篇论文中，我们介绍了ChatDyn，这是首个能够基于语言指令生成街道场景中交互式、可控且逼真的参与者动态的系统。为了通过复杂的语言实现精确控制，ChatDyn采用了一种多LLM代理角色扮演的方法，该方法利用自然语言输入来规划不同交通参与者的轨迹和行为。为了根据规划生成逼真的细节动态，ChatDyn设计了两种新的执行器：PedExecutor，这是一种统一的多任务执行器，能够在不同的任务规划下生成逼真的行人动态；以及VehExecutor，这是一种基于物理过渡的策略，用于生成物理上合理的车辆动态。广泛的实验表明，ChatDyn可以生成包含多个车辆和行人的逼真的驾驶场景动态，并在子任务上显著优于以前的方法。代码和模型将在https://vfishc.github.io/chatdyn 获取。|
|**2024-12-11**|**TapeAgents: a Holistic Framework for Agent Development and Optimization**|Dzmitry Bahdanau et.al.|[2412.08445](http://arxiv.org/abs/2412.08445)|null|我们介绍了TapeAgents，这是一种围绕着粒度化、结构化的会话日志带（log tape）构建的代理框架，该日志带也充当会话的可恢复状态。在TapeAgents中，我们利用日志带来促进LLM代理开发生命周期的所有阶段。代理通过处理日志带和大语言模型（LLM）的输出来生成新的想法和行动步骤，并将其附加到日志带中。环境则通过类似的方式将观察步骤附加到日志带中。由于这种以日志带为中心的设计，TapeAgents能够为AI从业者提供全面的端到端支持。在开发阶段，日志带促进了会话持久性、代理审核和逐步调试。部署后，可以重用日志带进行评估、微调和提示调优；关键的是，可以从其他代理中适应日志带或使用修订后的历史日志带。在本报告中，我们将详细解释TapeAgents的设计。我们通过几个具体示例展示了如何使用TapeAgents构建单体代理和多代理团队，优化代理提示以及微调代理的大语言模型。我们提供了工具原型，并报告了一个案例研究，在该研究中我们使用TapeAgents对一个Llama-3.1-8B表单填充助手进行了微调，使其性能与GPT-4相当，但成本却低了几个数量级。最后，我们的比较分析表明，TapeAgents相较于先前框架的优势源自于我们新颖地设计了可恢复、模块化的状态机，该状态机生成粒度化、结构化的日志，并能够将这些日志转换为训练文本——这是之前工作中所没有的独特组合功能。|
|**2024-12-11**|**Federated In-Context LLM Agent Learning**|Panlong Wu et.al.|[2412.08054](http://arxiv.org/abs/2412.08054)|null|大型语言模型（LLMs）通过实现逻辑推理、工具使用和与外部系统作为代理进行交互，彻底改变了智能服务。然而，LLM的进步常常受到高质量数据稀缺的阻碍，而这些数据大多具有内在敏感性。联邦学习（FL）提供了一个潜在解决方案，通过促进分布式LLM的协同训练，同时保护私有数据。然而，FL框架面临着显著的带宽和计算需求，并且存在来自异构数据分布的挑战。新兴的LLM上下文学习能力提供了一种有前景的方法，通过聚合自然语言而不是庞大的模型参数。然而，这种方法存在隐私泄露的风险，因为它需要在聚合过程中收集并展示来自不同客户端的数据样本。在本文中，我们提出了一种新颖的隐私保护联邦上下文学习LLM代理学习（FICAL）算法，据我们所知这是首次利用上下文学习的力量通过联邦学习训练各种LLM代理。在我们的设计中，由新型LLM增强的知识编纂生成（KCG）模块生成的知识编纂在客户端和服务器之间传输，而不是像以前的FL方法那样传输模型参数。除此之外，我们还设计了一个基于检索增强生成（RAG）的工具学习和使用（TLU）模块，并将聚合的全局知识编纂作为教师来教导LLM代理工具的使用。我们进行了广泛的实验，结果表明FICAL在与其他最先进的基线相比具有竞争力的同时，通信成本减少了 $\mathbf{3.33\times10^5}$ 倍。|
|**2024-12-11**|**MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents**|Yun Xing et.al.|[2412.08014](http://arxiv.org/abs/2412.08014)|null|物理对抗性攻击在驾驶场景中可以揭示视觉感知模型的关键漏洞。然而，由于现实世界背景的多样性以及保持视觉自然性的要求，开发此类攻击仍然具有挑战性。基于这一挑战，我们将物理对抗性攻击重新定义为一次性补丁生成问题。我们的方法通过深度生成模型生成对抗性补丁，该模型考虑了特定场景上下文，从而实现在匹配环境中的直接物理部署。主要挑战在于同时实现两个目标：生成能够有效误导物体检测系统的对抗性补丁，并确定场景中上下文合适的放置位置。我们提出了MAGIC（Mastering Physical Adversarial Generation In Context），一种新颖的框架，由多模态LLM代理驱动以解决这些挑战。MAGIC通过语言和视觉能力的协同交互自动理解场景上下文并协调对抗性补丁的生成。MAGIC协调三个专门的LLM代理：adv-patch生成代理（GAgent）通过提示工程策略掌握欺骗性补丁的创建；adv-patch部署代理（DAgent）确保上下文一致性，基于场景理解确定最优放置策略；自我审查代理（EAgent）通过提供关键监督和迭代优化来完成这一系列工作。我们在数字和物理层面验证了我们的方法，即nuImage和手动捕捉的真实场景中，统计和视觉结果证明了我们的MAGIC对于攻击广泛使用的物体检测系统非常有效。|
|**2024-12-10**|**Agents for self-driving laboratories applied to quantum computing**|Shuxiang Cao et.al.|[2412.07978](http://arxiv.org/abs/2412.07978)|null|全自动化的自驾驶实验室有望通过减少重复劳动来实现高通量和大规模的科学发现。然而，有效的自动化需要深入整合实验室知识，这些知识通常是非结构化的、多模态的，并且难以融入现有的人工智能系统。本文介绍了一个名为k-代理框架，旨在支持实验者组织实验室知识并使用代理自动化实验。我们的框架采用基于大型语言模型的代理来封装实验室知识，包括可用的实验室操作以及分析实验结果的方法。为了自动化实验，我们引入了执行代理，它们将多步骤的实验程序分解成状态机，与其他代理交互以执行每个步骤并分析实验结果。分析后的结果随后被用来驱动状态转换，从而实现闭环反馈控制。为了展示其能力，我们将这些代理应用于校准和操作一个超导量子处理器，在数小时内自主规划并执行了实验，成功地生成并表征了在人类科学家水平上达到的纠缠量子态。我们的基于知识的代理系统为管理实验室知识和加速科学发现开辟了新的可能性。|
|**2024-12-10**|**MAGE: A Multi-Agent Engine for Automated RTL Code Generation**|Yujie Zhao et.al.|[2412.07822](http://arxiv.org/abs/2412.07822)|**[link](https://github.com/stable-lab/MAGE-A-Multi-Agent-Engine-for-Automated-RTL-Code-Generation)**|**自动从自然语言指令生成RTL代码（如Verilog）在大型语言模型（LLMs）的发展中成为一个有前景的方向。然而，生成既符合语法又具备功能正确的RTL代码仍然是一项重大挑战。现有的单一LLM代理方法面临显著的限制，因为它们必须在多种编程语言之间导航，并处理复杂的生成、验证和修改任务。为了解决这些挑战，本文介绍了一个名为MAGE的开源多代理AI系统，该系统专为稳健且准确的Verilog RTL代码生成而设计。我们提出了一种新颖的高温度RTL候选采样和调试系统，能够有效地探索代码候选的空间，并显著提高候选代码的质量。此外，我们还设计了一种新颖的Verilog状态检查点检查机制，能够在早期检测到功能错误并提供精确反馈进行针对性修复，从而显著提升生成的RTL代码的功能正确性。MAGE在VerilogEval-Human 2基准测试中实现了95.7%的语法和功能正确代码生成率，比最先进的Claude-3.5-sonnet高出23.3%，展示了AI驱动的RTL设计工作流程中的稳健和可靠方法。**|
|**2024-12-11**|**Searching for Structure: Investigating Emergent Communication with Large Language Models**|Tom Kouwenhoven et.al.|[2412.07646](http://arxiv.org/abs/2412.07646)|null|人类语言通过反复的语言学习和使用演化成有结构的形式。这些过程引入了在语言习得期间起作用的偏差，使语言系统趋向于提高交流效率。在这篇论文中，我们研究了是否大型语言模型（LLMs）的隐含偏差也会优化人工语言，并产生类似的效果。为此，我们模拟了一个经典的指称游戏，在这个游戏中，LLM代理学习并使用人工语言。我们的结果显示，最初无结构的整体语言确实被塑造成具有某些结构特性，使得两个LLM代理能够成功地进行交流。与人类实验中的观察结果相似，代际传递提高了语言的可学性，但也可能导致非人类特征的退化词汇。综合来看，这项工作扩展了实验发现，表明LLMs可以作为模拟语言演化的工具，并为该领域的未来人机实验开辟了可能性。|
|**2024-12-06**|**Enhancing LLMs for Impression Generation in Radiology Reports through a Multi-Agent System**|Fang Zeng et.al.|[2412.06828](http://arxiv.org/abs/2412.06828)|null|本文介绍了一种名为“RadCouncil”的多代理大型语言模型（LLM）框架，旨在增强放射学报告中从发现部分生成印象的能力。RadCouncil由三个专门的代理组成：1）“检索”代理，负责识别并从向量数据库中检索相似的报告；2）“放射科医生”代理，基于给定报告的发现部分以及检索代理找到的示例报告生成印象；3）“审查者”代理，对生成的印象进行评估并提供反馈。该框架的性能通过定量指标（如BLEU、ROUGE、BERTScore）和定性标准进行了评估，并使用GPT-4对胸部X光作为案例研究进行了评估。实验结果显示，在多个维度上，包括诊断准确性、风格一致性以及清晰度方面，RadCouncil相比单一代理方法都有所提升。这项研究强调了利用每个代理都具有特定任务的多个交互式LLM代理来增强在专业医疗任务中的表现，并开发更强大和适应性强的医疗AI解决方案的潜力。|
|**2024-12-09**|**AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark**|Lan Li et.al.|[2412.06724](http://arxiv.org/abs/2412.06724)|**[link](https://github.com/LanLi2017/LLM4DC)**|**我们研究了大型语言模型（LLMs）在自动生成数据清洗工作流方面的推理能力。为了评估LLMs完成数据清洗任务的能力，我们实现了一个基于LLM的自动化数据清洗工作流（AutoDCWorkflow）管道。该管道针对三种类型的数据质量问题：重复项、缺失值和不一致的数据格式，提示LLMs进行数据清洗操作。给定一个脏表和一个目的（以查询形式表达），此管道生成一个最小的清洁表以满足目的，并生成用于产生该表的数据清洗工作流。规划过程涉及三个主要的LLM驱动组件：（1）选择目标列：识别与目的相关的列集。（2）检查列质量：评估每个目标列的数据质量并生成数据质量报告作为操作目标。（3）生成操作及参数：根据数据质量报告结果预测下一个操作及其参数。此外，我们提出了一套数据清洗基准来评估LLM代理自动生成满足不同难度级别数据清洗目的的工作流的能力。该基准包括注释数据集，其中包含目的、原始表、清洁表、数据清洗工作流和答案集。在我们的实验中，我们评估了三种能够自动生成目的驱动的数据清洗工作流的LLMs。结果表明，LLMs在无需微调的情况下，在规划和生成数据清洗工作流方面表现良好。**|
|**2024-12-09**|**Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework**|Tianming Liu et.al.|[2412.06681](http://arxiv.org/abs/2412.06681)|null|在交通系统需求建模和仿真中，基于代理的模型和微观仿真方法是当前最先进的方法。然而，现有的基于代理的模型在行为真实性和资源需求方面仍存在一些局限性，限制了它们的应用。在这项研究中，我们利用新兴的大语言模型（LLM）技术和基于LLM的代理，提出了一种用于交通系统的通用LLM代理建模框架。我们认为，LLM代理不仅具备作为代理的基本能力，还提供了克服现有基于代理的模型的一些局限性的有前景的解决方案。我们的概念框架设计紧密地复制了交通网络中人类旅行者在决策和互动过程中的特性和行为，我们通过相关研究和一个LLM代理在瓶颈场景中学习和调整的示范例子表明，所提出的系统可以满足决策和学习行为的关键行为标准。尽管LLM代理建模框架需要进一步完善，但我们认为这种方法有可能改进交通系统建模和仿真。|
|**2024-12-09**|**Simulating Human-like Daily Activities with Desire-driven Autonomy**|Yiding Wang et.al.|[2412.06435](http://arxiv.org/abs/2412.06435)|null|现有的以任务为导向的AI代理通常依赖于明确的指令或外部奖励，这限制了它们像人类一样由内在动机驱动的能力。在本文中，我们提出了一种基于欲望驱动的自主框架，用于引导基于大型语言模型（LLM）的代理模拟类似人类的日常活动。与之前的代理不同，我们的欲望驱动自主代理（D2A）遵循内在欲望的原则，使其能够自主地提出和选择满足其动机框架的任务。受到需求理论的启发，动机框架包含了对类似人类欲望的理解，如社交互动的需求、个人成就感的需求以及自我照顾的需求。通过使用基于欲望的任务生成机制，代理评估其当前状态并采取一系列与其内在动机一致的活动。通过模拟，我们展示了我们的欲望驱动自主代理（D2A）生成连贯且语境相关的日常活动，同时表现出类似于人类行为的多样性和适应性。与其他基于LLM的框架的比较分析表明，我们的方法显著提高了模拟活动的合理性。|
|**2024-12-09**|**StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist**|Cunshi Wang et.al.|[2412.06412](http://arxiv.org/abs/2412.06412)|null|随着大型语言模型（LLM）的快速发展，基于LLM的代理引入了便捷且用户友好的方法来利用各个领域的工具。在天文观测领域，新望远镜的建设显著增加了天文学家的工作负担。部署基于LLM的代理可以有效减轻这一负担，并降低培训人员的成本。在涵盖三个观测站点八个望远镜的近邻星系超新星巡天（NGSS）项目中，该项目旨在寻找50mpc范围内的星系瞬变现象，我们开发了名为“StarWhisper望远镜系统”来管理整个观测过程。该系统自动化了生成观测列表、进行观测、分析数据和向观察者提供反馈等任务。观测列表根据不同站点和策略定制，以确保对天体的全面覆盖。经过人工验证后，这些列表通过系统中的代理上传到望远镜，代理会在收到中性语言指令时启动观测。观测图像实时分析，并将瞬变现象迅速传达给观察者。代理将其转换为实时跟进观测建议并发送至兴隆观测站群聊，然后将其添加到第二天的观测列表中。此外，系统内AI代理的集成提供了在线访问功能，节省了天文学家的时间，并鼓励业余天文学家更多地参与NGSS项目。|
|**2024-12-09**|**Beyond pip install: Evaluating LLM Agents for the Automated Installation of Python Projects**|Louis Milliken et.al.|[2412.06294](http://arxiv.org/abs/2412.06294)|**[link](https://github.com/coinse/installamatic)**|近期许多研究提出了使用基于大语言模型（LLM）的代理来执行所谓的“仓库级”任务，这些任务的范围通常超过单个文件。这引发了人们的推测，认为这种仓库级任务的协调可以导致几乎不需要人工干预的软件工程代理。然而，我们认为在这一系列需要由自主软件工程代理执行的任务中，有一个重要任务被忽略了，即通过安装其他仓库来满足项目级别的依赖关系。为了探讨这一仓库级安装任务的可行性，我们引入了一个基准测试集，该测试集从40个开源Python项目中精选而来，并包含了每个目标仓库的实际安装过程作为真实情况。此外，我们提出了一种名为Installamatic的代理，其目标是通过搜索仓库中的文档以寻找相关安装说明来执行并验证仓库的安装。实证实验表明，我们的代理至少有10%的概率能够自动安装55%的研究仓库。通过进一步分析，我们识别了代理无法安装仓库的常见原因，讨论了设计和实现此类代理所面临的挑战，并考虑了这样的代理对开发者可能产生的影响。|
|**2024-12-08**|**Cooperative SQL Generation for Segmented Databases By Using Multi-functional LLM Agents**|Zhiguang Wu et.al.|[2412.05850](http://arxiv.org/abs/2412.05850)|null|文本到SQL的任务旨在根据用户的文本问题自动生成SQL查询。为了解决这个问题，我们提出了一种基于多功能代理的协作SQL生成框架（CSMA），该框架通过大型语言模型（LLM）代理之间的信息交互来实现。受到人类团队合作的启发，CSMA分为三个阶段：1）与问题相关的模式收集；2）与问题对应的SQL查询生成；3）SQL查询正确性检查。在第一阶段，代理分析各自的模式，并相互交流以收集与问题相关的信息。在第二阶段，代理利用收集到的信息尝试为问题生成相应的SQL查询。在第三阶段，代理根据已知信息检查SQL查询是否生成正确。这种基于交互的方法使每个代理所掌握的问题相关的数据库模式部分能够用于SQL生成和检查。在Spider和Bird基准上的实验表明，CSMA达到了与最先进方法相当的高水平性能，同时保持了这些独立代理中的私有数据。|
|**2024-12-06**|**Sense and Sensitivity: Evaluating the simulation of social dynamics via Large Language Models**|Da Ju et.al.|[2412.05093](http://arxiv.org/abs/2412.05093)|null|大型语言模型近年来被提议作为经典基于主体的模型（ABMs）的强大替代品，用于模拟社会动态。通过使用大型语言模型作为人类行为的代理，这种方法的希望在于能够模拟比传统ABM更为复杂的动态，并在社会科学、政治科学和经济学等领域获得新的见解。然而，由于大型语言模型的黑箱性质，目前尚不清楚这些模型是否真正执行了自然语言指令中编码的预期语义，以及由此产生的交互动态是否有意义。为了研究这个问题，我们提出了一种新的评估框架，该框架将大型语言模型的模拟与已建立的社会科学研究参考模型中的动态相结合。通过将大型语言模型视为一个黑盒函数，我们相对于这个参考模型评估其输入-输出行为，这使我们能够评估其行为的详细方面。我们的结果显示，虽然可以设计提示词来近似预期的动态，但这些模拟的质量对提示词的具体选择非常敏感。重要的是，模拟甚至对任意变化（如轻微措辞变化和空格）也非常敏感。这引发了对于当前版本的大型语言模型在有意义模拟中的效用的质疑，因为在没有参考模型的情况下，无法预先确定看似无意义的提示词变化对模拟的影响。|
|**2024-12-05**|**Practical Considerations for Agentic LLM Systems**|Chris Sypherd et.al.|[2412.04093](http://arxiv.org/abs/2412.04093)|null|随着大型语言模型（LLMs）近年来实力的增强，人们对将其作为自主代理基础模型的兴趣也在增加。尽管LLMs在自然语言领域展示了涌现能力和广泛的专业知识，但其固有的不可预测性使得实现LLM代理具有挑战性，从而导致相关研究与此类系统实际部署之间的差距。为了弥合这一差距，本文将研究社区中的可行见解和考虑因素置于已建立的应用程序范式背景下，以促进稳健LLM代理的构建和部署。具体而言，我们根据应用导向文献中的常见做法，将相关研究结果定位到四个广泛的类别——规划、记忆、工具和控制流，并强调在设计面向现实世界应用的代理型LLM时需要考虑的实际问题，例如处理随机性和高效管理资源等。虽然我们未进行实证评估，但我们提供了必要的背景知识，以便在学术界和工业界讨论代理型LLM设计的关键方面。|
|**2024-12-05**|**LossAgent: Towards Any Optimization Objectives for Image Processing with LLM Agents**|Bingchen Li et.al.|[2412.04090](http://arxiv.org/abs/2412.04090)|null|我们提出了首个损失代理LossAgent，用于低级图像处理任务，如图像超分辨率和修复，旨在实现不同实际应用中的任何定制化优化目标。值得注意的是，并非所有优化目标（例如复杂的手工设计的感知度量、文本描述以及复杂的人类反馈）都可以通过现有的低级损失函数（如均方误差损失）来实现，这在端到端优化图像处理网络时构成了一个关键挑战。为了解决这个问题，我们的LossAgent引入了强大的大型语言模型（LLM）作为损失代理，在优化过程中赋予损失代理理解复杂优化目标、轨迹和外部环境状态反馈的能力。具体来说，我们通过整合支持低级图像处理端到端优化的现有损失函数建立了损失库。然后，我们设计了面向优化的提示工程，使损失代理能够主动且智能地决定每次优化交互中库中每个损失的组成权重，从而实现任何定制化优化目标所需的优化轨迹。在三个典型的低级图像处理任务和多种优化目标上的广泛实验表明，我们提出的LossAgent是有效且适用的。代码和预训练模型将在https://github.com/lbc12345/LossAgent 获取。|
|**2024-12-05**|**MISR: Measuring Instrumental Self-Reasoning in Frontier Models**|Kai Fronsdal et.al.|[2412.03904](http://arxiv.org/abs/2412.03904)|**[link](https://github.com/kaifronsdal/self-reasoning-evals)**|**我们提出了一组任务来评估大型语言模型（LLM）代理的工具性自我推理能力。这种工具性自我推理能力可以提高适应性和实现自我修改，但也可能带来显著风险，如导致欺骗性对齐问题。先前的工作仅在非代理设置或有限领域内评估了自我推理。在这篇论文中，我们提出了针对在广泛场景中的代理任务的评估方法，包括自我修改、知识获取和不透明的自我推理。我们评估了使用最先进的LLMs构建的代理，包括商业系统和开源系统。我们发现，工具性自我推理能力仅在最强大的前沿模型中显现，并且高度依赖于上下文。没有模型通过我们评估中最困难的部分，因此我们的评估可以用于衡量未来模型在工具性自我推理能力方面的进步。我们将这些评估开源在https://github.com/kaifronsdal/Self-Reasoning-Evals。**|
|**2024-12-05**|**Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration**|Shiwen Ni et.al.|[2412.03847](http://arxiv.org/abs/2412.03847)|null|智能对话系统在现代教育和心理辅导领域中的应用越来越广泛，但大多数现有的系统仅限于单一领域，无法同时处理教育和心理问题，并且在处理复杂问题时往往缺乏准确性和专业性。为了解决这些问题，本文提出了一种结合教育和心理辅导功能的智能对话系统。该系统由多个AI代理组成，包括安全检测代理、意图识别代理、教育大型语言模型代理和心理大型语言模型代理，这些代理协同工作，以确保提供准确的教育知识问答和心理支持服务。具体来说，系统通过意图分类模型识别用户输入的意图，并调用经过增强检索的教育大模型和使用心理数据微调的心理大模型，以提供专业的教育建议和心理支持。|
|**2024-12-04**|**From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents**|Xinyi Mou et.al.|[2412.03563](http://arxiv.org/abs/2412.03563)|**[link](https://github.com/fudandisc/socialagent)**|**传统的社会学研究通常依赖于人类的参与，尽管这种方法有效，但成本高昂、难以扩展，并且存在伦理问题。最近，大型语言模型（LLMs）的发展突显了它们模拟人类行为的潜力，使得个体反应的复制和跨多个跨学科研究成为可能。本文对这一领域进行了全面调查，展示了由LLM驱动的代理所推动的最新进展。我们将这些模拟分为三类：（1）个体模拟，模仿特定个人或人口群体；（2）场景模拟，在特定上下文中，多个代理协作以实现目标；（3）社会模拟，模拟代理社会中的互动，反映现实世界动态的复杂性和多样性。这些模拟从详细的个体建模到大规模的社会现象，呈现了一个渐进的过程。我们详细讨论了每种模拟类型，包括模拟的架构或关键组件、目标或场景分类以及评估方法。随后，我们总结了常用的基准数据集。最后，我们讨论了这三种模拟类型的趋势。相关资源库位于{\url{https://github.com/FudanDISC/SocialAgent}}。**|
|**2024-12-03**|**Hacking CTFs with Plain Agents**|Rustem Turtayev et.al.|[2412.02776](http://arxiv.org/abs/2412.02776)|**[link](https://github.com/palisaderesearch/intercode)**|**我们在高中的水平黑客基准测试中充分展示了基于大型语言模型（LLM）的简单代理设计的应用。具体来说，我们通过使用提示、工具使用和多次尝试，在流行的进攻性安全基准测试InterCode-CTF上达到了95%的性能。这一成绩超越了之前Phuong等人2024年的工作（29%）和Abramovich等人2024年的工作（72%）。我们的结果表明，当前的LLMs在进攻性网络安全方面已经超过了高中水平。它们的黑客能力仍未得到充分利用：我们的ReAct&Plan提示策略能够在1到2次交互内解决许多挑战，而无需复杂的工程或高级的利用技术。**|
|**2024-12-04**|**DataLab: A Unified Platform for LLM-Powered Business Intelligence**|Luoxuan Weng et.al.|[2412.02205](http://arxiv.org/abs/2412.02205)|null|商业智能（BI）通过将现代组织中的大量数据转化为可操作的洞察，帮助进行明智的决策。最近，基于大型语言模型（LLM）的代理简化了BI工作流程，使任务规划、推理和在可执行环境中基于自然语言（NL）查询的动作自动化。然而，现有方法主要集中在个别BI任务上，如NL2SQL和NL2VIS。这些碎片化的任务分布在不同的数据角色和工具中，导致由于BI的迭代和协作性质而产生的低效率和潜在错误。在本文中，我们介绍了DataLab，这是一个统一的BI平台，它结合了一站式LLM代理框架和增强的计算笔记本界面。DataLab通过在一个环境中无缝结合LLM支持与用户定制，支持不同数据角色的各种BI任务。为了实现这种统一，我们设计了一个专门针对企业特定BI任务的领域知识整合模块，一个促进BI工作流程中信息共享的代理间通信机制，以及一种基于单元格的上下文管理策略，以提高BI笔记本中上下文利用的效率。广泛的实验表明，DataLab在各种流行研究基准上实现了最先进的性能。此外，DataLab在来自腾讯的真实世界数据集上保持了高效和高效果，在企业特定BI任务上的准确率提高了58.58%，令牌成本降低了61.65%。|
|**2024-12-02**|**HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing**|Lajos Muzsai et.al.|[2412.01778](http://arxiv.org/abs/2412.01778)|**[link](https://github.com/aielte-research/HackSynth)**|**我们介绍了HackSynth，这是一种基于大型语言模型（LLM）的新型自主渗透测试代理。HackSynth采用双模块架构，包括规划器和总结器，使其能够迭代生成命令并处理反馈。为了评估HackSynth，我们提出了两个新的基于夺旗（CTF）的基准测试集，使用了流行的平台PicoCTF和OverTheWire。这些基准测试集包括两百个不同领域和难度的挑战，提供了一个标准化框架来评估基于LLM的渗透测试代理。根据这些基准测试，我们进行了广泛的实验，分析了HackSynth的核心参数，包括创造性（温度和top-p）以及令牌利用率。我们使用多个开源和专有LLM来衡量该代理的能力。实验表明，该代理在GPT-4o模型下表现最佳，优于GPT-4o系统卡所建议的效果。我们还讨论了HackSynth行动的安全性和可预测性。我们的研究结果表明，基于LLM的代理在推进自主渗透测试方面具有潜力，并强调了建立稳健保障措施的重要性。HackSynth及其基准测试集已公开供研究自主网络安全解决方案之用。**|
|**2024-12-02**|**Medchain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking**|Jie Liu et.al.|[2412.01605](http://arxiv.org/abs/2412.01605)|null|临床决策制定（CDM）是医疗保健交付中的一个复杂动态过程，但对于人工智能系统来说仍然是一个重大挑战。虽然基于大型语言模型（LLM）的代理在一般医学知识方面通过执照考试和知识问答任务进行了测试，但在现实世界场景中的临床决策制定表现有限，这主要是由于缺乏能够反映实际医疗实践的全面测试数据集。为了解决这一差距，我们介绍了MedChain，这是一个包含12,163个临床病例的数据集，涵盖了临床工作流程的五个关键阶段。MedChain通过三个关键特征——个性化、互动性和连续性，与现有基准区分开来。为了应对现实世界的临床决策制定挑战，我们还提出了MedChain-Agent，这是一种集成反馈机制和MCase-RAG模块的人工智能系统，可以学习之前的案例并调整其响应。MedChain-Agent在动态收集信息和处理连续临床任务方面表现出显著的适应性，明显优于现有方法。相关数据集和代码将在本文被接受后发布。|
|**2024-12-02**|**Can Large Language Models Serve as Evaluators for Code Summarization?**|Yang Wu et.al.|[2412.01333](http://arxiv.org/abs/2412.01333)|**[link](https://github.com/CGCL-codes/naturalcc)**|**代码总结对于程序理解和软件维护至关重要，但评估生成的代码总结质量一直是一个挑战。尽管人工评估在评估代码总结质量方面非常有效，但它劳动密集且难以规模化。常用的自动指标如BLEU、ROUGE-L、METEOR和BERTScore通常与人类判断不完全一致。本文探讨了大型语言模型（LLMs）在评估代码总结中的潜力，提出了一种名为CODERPE（代码总结评估的角色扮演者）的新方法，该方法利用角色扮演提示来评估生成总结的质量。具体而言，我们让LLM代理扮演不同的角色，如代码审查员、代码作者、代码编辑器和系统分析师。每个角色从连贯性、一致性、流畅性和相关性等关键维度评估代码总结的质量。我们进一步通过多种提示策略，包括因果推理、情境学习和定制评分表设计，探索了LLMs作为评估者的稳健性。结果表明，LLMs可以有效地评估代码总结方法。特别是，我们的基于LLM的评估器CODERPE在与人类评估的相关性上达到了81.59%的Spearman相关系数，比现有的BERTScore指标高出17.27%。**|
|**2024-12-02**|**RL2: Reinforce Large Language Model to Assist Safe Reinforcement Learning for Energy Management of Active Distribution Networks**|Xu Yang et.al.|[2412.01303](http://arxiv.org/abs/2412.01303)|null|随着大规模分布式能源资源被整合到主动配电网络（ADNs）中，与传统配电网络相比，有效的能源管理在ADNs中变得越来越突出。尽管先进的强化学习（RL）方法通过减轻复杂的建模和优化负担，极大地提高了ADNs中能源管理的效率，但安全性成为实际应用中RL的关键关注点。由于设计和调整惩罚函数（对应于操作安全约束）需要广泛的领域知识，因此新兴的ADN运营商需要一种更灵活和定制化的方法来处理惩罚函数，以进一步提高操作的安全性和效率。借助强大的理解、推理和上下文学习能力，大型语言模型（LLMs）提供了一种有前途的方式来辅助ADNs中的安全RL。在本文中，我们引入了LLM来理解ADNs中的操作安全要求并生成相应的惩罚函数。此外，我们提出了一个RL2机制，通过多轮对话迭代地和自适应地优化生成的函数，在这个过程中，LLM代理根据下游RL代理的训练和测试性能调整函数的模式和参数。所提出的方法显著减少了ADN运营商的干预。综合测试结果证明了该方法的有效性。|
|**2024-12-02**|**SAUP: Situation Awareness Uncertainty Propagation on LLM Agent**|Qiwei Zhao et.al.|[2412.01033](http://arxiv.org/abs/2412.01033)|null|大型语言模型（LLMs）集成到多步代理系统中能够实现各种应用中的复杂决策过程。然而，它们的输出往往缺乏可靠性，因此不确定性估计变得至关重要。现有的不确定性估计方法主要集中在最终步骤的输出上，这些方法未能考虑到在多步决策过程中累积的不确定性以及代理与其环境之间的动态交互。为了应对这些局限性，我们提出了一种名为SAUP（情境感知不确定性传播）的新框架，该框架通过LLM驱动的代理推理过程中的每一步来传播不确定性。SAUP通过在传播过程中为每个步骤的不确定性分配情境权重来整合情境感知能力。我们的方法兼容各种一步不确定性估计技术，提供了全面且准确的不确定性度量。在基准数据集上的广泛实验表明，SAUP显著优于现有的最先进方法，AUROC值提高了多达20%。|
|**2024-11-28**|**SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments**|Yue Cao et.al.|[2412.00114](http://arxiv.org/abs/2412.00114)|null|大型视觉-语言模型（LVLMs）在解释视觉内容方面展示了显著的能力。尽管现有工作表明这些模型对故意放置的对抗性文本存在脆弱性，但这些文本通常容易被识别为异常。本文提出了一种生成场景连贯的字体对抗攻击的方法，以误导先进的LVLMs，同时保持视觉自然性，通过使用基于大语言模型（LLM）的代理实现。我们的方法解决了三个关键问题：生成何种对抗性文本，将其置于场景中的何处，以及如何无缝整合。我们提出了一个无需训练、多模态LLM驱动的场景连贯字体对抗规划（SceneTAP），该方法采用三阶段过程：场景理解、对抗性规划和无缝整合。SceneTAP利用链式思维推理来理解场景，制定有效的对抗性文本，战略性地规划其位置，并提供详细的指令以在图像中自然整合。随后，我们使用一种场景连贯的TextDiffuser执行攻击，该方法采用局部扩散机制。我们将该方法扩展到现实场景中，通过打印并放置生成的补丁在物理环境中，展示了其实际应用价值。广泛的实验表明，我们提出的场景连贯的对抗性文本成功误导了最先进的LVLMs，包括ChatGPT-4o，即使在捕获新图像后也是如此。我们的评估显示，攻击成功率显著提高，同时保持了视觉自然性和上下文适宜性。这项工作揭示了当前视觉-语言模型对复杂、场景连贯的对抗性攻击的脆弱性，并提供了潜在防御机制的见解。|
|**2024-11-29**|**Training Agents with Weakly Supervised Feedback from Large Language Models**|Dihong Gong et.al.|[2411.19547](http://arxiv.org/abs/2411.19547)|null|大型语言模型（LLMs）为创建可以通过迭代环境交互来解决复杂任务的代理提供了有前景的基础。现有方法要么要求这些代理模仿专家提供的轨迹，要么依赖于确定性的环境反馈来进行强化学习，这限制了它们的应用场景，例如游戏或代码生成。本文介绍了一种新的基于弱监督信号从批评者LLM训练LLM代理的方法，从而绕过了对专家轨迹或确定性反馈的需求。我们的代理以迭代方式训练，首先通过环境交互生成轨迹。随后，一个批评者LLM选择一组好的轨迹，然后使用这些轨迹来更新代理，使其在下一次迭代中生成更好的轨迹。在API-bank数据集上的广泛测试表明，尽管使用的是参数少得多的开源模型，但我们的代理能力得到了持续提升，并且性能可与GPT-4相媲美。|
|**2024-11-28**|**Using a Feedback Loop for LLM-based Infrastructure as Code Generation**|Mayur Amarnath Palavalli et.al.|[2411.19043](http://arxiv.org/abs/2411.19043)|**[link](https://github.com/Mayur-Palavalli/LLM-IaC-generation)**|**代码生成借助大语言模型（LLMs）已经帮助提高了软件开发人员在编码任务中的生产力，但在围绕代码的其他软件开发任务方面影响甚微。特别是基础设施管理仍然是一个悬而未决的问题。我们研究了使用LLM代理利用基础架构即代码（IaC）范式构建基础架构的能力。我们特别研究了使用反馈循环的方法，该循环返回生成的IaC的错误和警告，以允许LLM代理改进代码。我们发现，每次循环迭代后，其有效性呈指数下降，直到达到某个点并变得无效。**|
|**2024-12-02**|**MATATA: a weak-supervised MAthematical Tool-Assisted reasoning for Tabular Applications**|Vishnou Vinayagame et.al.|[2411.18915](http://arxiv.org/abs/2411.18915)|null|数学推理能力随着工具增强的语言代理的使用而提升，但这些方法往往依赖于闭源或大型模型、外部数据或大量的提示工程。本文介绍了一种名为MATATA的新颖成本效益方法，用于通过推理、规划和工具使用来训练处理表格数据问题的大型语言模型（LLMs）。通过渐进自我改进范式和迭代弱监督机制，该方法特别适合本地托管和对数据隐私至关重要的敏感业务场景，使用的模型规模为38亿/80亿参数的小型语言模型（SLMs）。通过采用灵活且可重用的工具在不同数据集上，该方法实现了在共享任务中的稳健性能和有效扩展。实验表明，MATATA在基于开源模型的推理框架中，在FinQA和TAT-QA任务上达到了最先进的性能。此外，MATATA模型在TabMWP任务上的表现与基于GPT-4的框架相当，而MATATA本身是小型语言模型。|
|**2024-11-28**|**Wearable intelligent throat enables natural speech in stroke patients with dysarthria**|Chenyu Tang et.al.|[2411.18266](http://arxiv.org/abs/2411.18266)|null|可穿戴无声语音系统在恢复有言语障碍患者的交流能力方面具有巨大潜力。然而，流畅且连贯的语音仍然难以实现，临床疗效也尚未得到证实。本文介绍了一种由人工智能驱动的智能喉（IT）系统，该系统结合了喉咙肌肉振动和颈动脉脉冲信号传感器与大型语言模型（LLM）处理技术，以实现流畅且富有情感表达的交流。该系统利用超灵敏纺织品应变传感器捕捉颈部区域的高质量信号，并支持令牌级处理，实现实时、连续的语音解码，从而实现无缝、无延迟的通信。在五名患有构音障碍的中风患者测试中，IT系统的LLM代理智能地纠正了令牌错误并丰富了句子级别的感情和逻辑连贯性，实现了低错误率（4.2%的词错误率，2.9%的句错误率）以及用户满意度提高了55%。这项研究建立了一个便携式、直观的交流平台，适用于构音障碍患者，并有可能广泛应用于不同的神经性疾病及多语言支持系统。|
|**2024-11-26**|**MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation**|Harsh Singh et.al.|[2411.17636](http://arxiv.org/abs/2411.17636)|null|大型语言模型（LLMs）在各个领域，包括机器人操作和导航，展示了显著的规划能力。尽管最近在机器人技术中的努力已经利用了LLMs进行高层次和低层次的规划，但这些方法通常面临重大挑战，例如在长期任务中的幻觉问题以及由于一次性生成计划而缺乏适应性的问题。为了应对这些限制，我们提出了一种新颖的多代理LLM框架——多代理大型语言模型用于操作（MALMM），该框架将高层次规划和低层次控制代码生成分布在专门的LLM代理之间，并由一个额外的代理动态管理转换。通过在每一步之后纳入环境观察，我们的框架能够有效地处理中间失败并实现自适应重规划。与现有方法不同的是，我们的方法不依赖于预训练的技能策略或上下文学习示例，并且可以推广到各种新任务。我们在九个RLBench任务上评估了我们的方法，包括长期任务，并证明了它能够在零样本设置下解决机器人操作问题，从而克服了现有基于LLM的操作方法的关键限制。|
|**2024-11-26**|**LLM-Based Offline Learning for Embodied Agents via Consistency-Guided Reward Ensemble**|Yujeong Lee et.al.|[2411.17135](http://arxiv.org/abs/2411.17135)|null|利用大型语言模型（LLMs）来使具身代理变得流行，但在实践中也呈现出许多限制。在这项工作中，我们没有直接将LLMs作为代理使用，而是探索它们作为具身代理学习的工具。具体来说，为了通过离线强化学习（RL）训练单独的代理，LLM被用来在训练数据集中对单个动作提供密集的奖励反馈。为此，我们提出了一种一致性引导的奖励集成框架（CoREN），旨在解决将LLM生成的估计值与目标环境领域相结合的难题。该框架采用自适应集成的空间-时间一致奖励，以从训练数据集中推导出领域锚定的奖励，从而实现不同环境领域中有效离线学习具身代理。实验结果表明，在VirtualHome基准测试中，CoREN显著优于其他离线RL代理，并且其性能可与具有8B参数的最先进的LLM基代理相媲美，尽管CoREN的代理策略网络仅具有117M参数，并且仅在训练时使用LLMs。|
|**2024-11-23**|**Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction**|Mitchell Rosser et.al.|[2411.16723](http://arxiv.org/abs/2411.16723)|null|随着自然语言生成模型——即大规模语言模型（LLMs）的最新发展，出现了一种潜在的应用场景，即通过这些模型来改善人类与机器人助手之间的交互。这些模型应能够利用其广泛的理解能力，将自然语言命令转化为有效的、符合任务需求且安全的机器人任务执行。然而，在现实中，这些模型可能会产生幻觉，这可能导致安全问题或偏离任务。在其他领域，这些问题已经通过使用协作式AI系统得到改进，在这种系统中，多个LLM代理可以协同工作，共同规划、编码和自我检查输出。在这项研究中，测试了多个协作式AI系统与单一独立AI代理的表现，以确定其他领域的成功是否会在提高人机交互性能方面发挥作用。结果显示，并没有明确的趋势表明代理数量与模型成功率之间存在关联。然而，显而易见的是，某些协作式AI代理架构可以大大提升生成无误代码和解决抽象问题的能力。|
|**2024-11-25**|**Agent-Based Modelling Meets Generative AI in Social Network Simulations**|Antonino Ferraro et.al.|[2411.16031](http://arxiv.org/abs/2411.16031)|null|基于代理的建模（ABM）已成为模拟社交网络的重要工具，涵盖了信息传播、影响力动态和社区形成等多种现象。然而，手动配置多样的代理交互和信息流动态存在挑战，通常导致模型过于简化，缺乏现实世界的普适性。将现代大语言模型（LLM）与ABM结合提供了一种有前景的方法来解决这些挑战并增强模拟的真实性，利用LLM在感知、推理和行为方面的人类化能力。在本文中，我们提出了一种新颖的框架，利用LLM赋能的代理根据用户的兴趣和个性特征模拟社交网络用户。该框架允许自定义代理交互，类似于各种社交网络平台，包括内容重分享和个性化推荐机制。我们使用2020年美国大选期间的全面推特数据集验证了我们的框架，结果表明LLM代理能够准确再现真实用户的语言模式和政治倾向。这些代理形成了同质化的意识形态集群，并保留了其社区的主要主题。值得注意的是，基于偏好的推荐显著影响了代理行为，促进了更高的参与度、网络同质性和回音室的形成。总体而言，我们的研究结果强调了LLM代理在推进社交媒体模拟和揭示复杂的在线动态方面的潜力。|
|**2024-11-24**|**From Laws to Motivation: Guiding Exploration through Law-Based Reasoning and Rewards**|Ziyu Chen et.al.|[2411.15891](http://arxiv.org/abs/2411.15891)|null|大型语言模型（LLMs）和强化学习（RL）是构建自主代理的两种强大方法。然而，由于对游戏环境的理解有限，代理常常依赖于低效的探索和试错，难以制定长期策略或做出决策。我们提出了一种从交互记录中提取经验以建模游戏环境底层规律的方法，利用这些经验作为内部动机来指导代理。这些经验以语言形式表达，非常灵活，既可以辅助代理直接推理，也可以转化为奖励来引导训练。我们的评估结果显示，在Crafter游戏中，无论是RL还是LLM代理都从这些经验中受益，从而提高了整体性能。|
|**2024-11-23**|**The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges**|Jiqun Liu et.al.|[2411.15396](http://arxiv.org/abs/2411.15396)|null|尽管在衡量和减轻人工智能（AI）和社会算法偏见方面取得了进展，但对于大规模语言模型（LLM）在自动化信息判断任务中的理性行为程度，以及它们是否也容易受到人类认知偏差的影响，仍不清楚。为了应对这一开放性问题，本研究通过众包用户实验和基于LLM的模拟实验，比较了在信息检索（IR）环境下，LLM和人类法官在潜在诱饵效应下的可信度评估，并实证检验了LLM在COVID-19医学（误）信息评估任务中的认知偏差程度与传统人类评估者相比的情况。结果显示，1）更大且较新的LLM在区分可信信息与虚假信息时表现出更高的一致性和准确性。然而，由于存在更突出的虚假信息诱饵结果，它们更容易对虚假信息给出更高的评分；2）虽然诱饵效应在人类和LLM的评估中都存在，但在不同条件和主题下的LLM判断中，该效应更为普遍。与通常认为的AI工具“理性”假设相反，本研究表明LLM代理嵌入了认知偏差风险，并评估了诱饵效应对LLM与人类可信度评估的影响，从而强调了去偏见AI代理和发展心理学驱动的AI审计技术和政策的重要性，以应对自动化判断任务及更广泛的应用。|
|**2024-11-27**|**XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**|Yixin Dong et.al.|[2411.15100](http://arxiv.org/abs/2411.15100)|null|LLM（大语言模型）代理的应用变得越来越复杂和多样化，导致对可以解析为代码、结构化函数调用和具身代理命令的结构化输出有很高的需求。这些发展带来了在LLM推理中进行结构化生成的重大需求。上下文无关文法是一种通过约束解码来实现结构化生成的灵活方法。然而，执行上下文无关文法需要在运行时遍历词汇表中的所有标记经过多个堆栈状态，这给结构化生成带来了不可忽视的开销。在这篇论文中，我们提出了XGrammar，这是一种针对大型语言模型的灵活且高效的结构化生成引擎。XGrammar通过将词汇表分为可以在预检查阶段处理的上下文无关标记和在运行时需要解释的上下文相关标记，从而加速上下文无关文法的执行。我们进一步构建转换以扩展语法上下文并减少上下文无关标记的数量。此外，我们构建了一个高效的持久堆栈以加速上下文相关标记的检查。最后，我们将语法引擎与LLM推理引擎协同设计，以使语法计算与GPU执行重叠。评估结果显示，XGrammar相比现有解决方案可以达到高达100倍的速度提升。结合LLM推理引擎，它能够在端到端低延迟LLM服务中实现近乎零开销的结构化生成。|
|**2024-11-22**|**ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**|Junhong Shen et.al.|[2411.15004](http://arxiv.org/abs/2411.15004)|**[link](https://github.com/colonylabs/ScribeAgent)**|大型语言模型（LLM）代理正在迅速进步，以处理越来越复杂的网络任务。大多数这些代理依赖于像GPT-4这样的通用专有模型，并专注于设计更好的提示以提高其规划能力。然而，通用的LLM并没有专门针对理解特定的网络上下文如HTML进行训练，它们通常在长期规划方面存在困难。我们探索了一种替代方法，即使用来自超过250个领域的生产规模工作流数据（共计60亿个标记）对开源LLM进行微调。这一简单而有效的方法在现有的基准测试中显著优于基于提示的代理——ScribeAgent在Mind2Web上实现了最先进的直接生成性能，并在WebArena上将前最佳纯文本网络代理的任务成功率提高了14.1%。我们进一步对各种微调设计选择进行了详细的消融研究，并提供了有关LLM选择、训练配方、上下文窗口优化和数据集大小影响的见解。|
|**2024-11-21**|**Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning**|Hang Zhou et.al.|[2411.14497](http://arxiv.org/abs/2411.14497)|**[link](https://github.com/CANGLETIAN/Star-Agents)**|**大型语言模型（LLMs）在下游任务中的有效性通常依赖于指令调优，而这又严重依赖于训练数据的质量。不幸的是，收集高质量且多样化的数据既昂贵又耗时。为了解决这个问题，我们提出了一种名为Star-Agents的新型框架，该框架通过多智能体协作和评估自动化地提升了跨数据集的数据质量。该框架采用三管齐下的策略。首先，它通过定制的采样方法利用多个LLM智能体生成多样化的指令数据。随后，生成的数据会经过严格的评估，使用双模型方法来评估难度和质量。最后，在动态优化阶段，更有效的LLM被优先考虑，从而提高整体数据质量。我们的实证研究，包括使用Pythia和LLaMA等模型进行指令调优实验，证明了所提议框架的有效性。优化后的数据集取得了显著改进，平均提高了12%，并在特定指标上也取得了显著提升，例如Fermi指标提高了40%，这在MT-bench、Vicuna bench和WizardLM测试集等基准测试中得到了验证。**|
|**2024-11-20**|**Mediating Modes of Thought: LLM's for design scripting**|Moritz Rietschel et.al.|[2411.14485](http://arxiv.org/abs/2411.14485)|null|建筑师采用视觉脚本和参数化设计工具来探索更广阔的设计空间（Coates，2010年），精炼他们对设计几何逻辑的理解（Woodbury，2010年），并克服传统软件的局限性（Burry，2011年）。尽管已有二十年的努力使设计脚本更加易于使用，但设计师自由思考方式与算法的严格性之间仍存在脱节（Burry，2011年）。最近大型语言模型（LLM）的发展表明，这种情况可能会很快改变，因为LLM编码了人类语境的一般理解，并表现出生成几何逻辑的能力。本项目推测，如果LLM能够有效地在用户意图与算法之间进行调解，它们将成为一种强大的工具，使设计中的脚本编写更加普及且有趣。我们探讨了此类系统是否可以通过解释自然语言提示来组装与计算设计脚本相关的几何操作。在这个系统中，配置了多个具有特定上下文的LLM代理，以推断用户意图并构建顺序逻辑。给定用户的高层次文本提示，会创建一个几何描述，提炼成一系列逻辑操作，并映射到特定于软件的命令。最终脚本会在用户的可视化编程界面中构建。该系统成功生成了具有一定复杂度的完整可视化脚本，但在超出这一复杂度阈值时失败。这展示了LLM如何使设计脚本更贴近人类的创造力和思维。未来的研究应探索对话交互、扩展多模态输入和输出，并评估这些工具的性能。|
|**2024-11-21**|**Physics-Informed LLM-Agent for Automated Modulation Design in Power Electronics Systems**|Junhua Liu et.al.|[2411.14214](http://arxiv.org/abs/2411.14214)|null|基于大型语言模型（LLM）的自主代理在解决复杂的工业任务方面已经展示了出色的表现。然而，在追求碳中性和高性能可再生能源系统的过程中，现有的AI辅助设计自动化面临着在解释性、可扩展性和可用性方面的重大局限。为了解决这些挑战，我们提出了LP-COMDA，这是一种基于LLM、物理信息驱动的自主代理，它能够自动化电力电子系统中功率转换器的调制设计，并且只需要最少的人类监督。与传统的AI辅助方法不同，LP-COMDA包含一个基于LLM的规划器，该规划器通过用户友好的聊天界面收集和验证设计规范。然后，规划器协调物理信息驱动的设计和优化工具，迭代地自动生成并优化调制设计。通过聊天界面，LP-COMDA提供了可解释的设计过程，展示了解释和图表。实验表明，LP-COMDA在标准平均绝对误差方面比第二好的基准方法减少了63.2%的误差，优于所有基线方法。此外，对20位专家进行的实证研究表明，使用LP-COMDA的设计时间比传统方法快33倍以上，显示出其在设计效率上的显著提升。|
|**2024-11-21**|**Multi-LLM-Agent Systems: Techniques and Business Perspectives**|Yingxuan Yang et.al.|[2411.14033](http://arxiv.org/abs/2411.14033)|null|在多模态大型语言模型的时代，大多数操作过程可以通过LLM代理重新制定和再现。这些LLM代理可以感知、控制并从环境中获得反馈，从而以自主的方式完成给定任务。除了与环境交互的特性外，LLM代理还可以调用各种外部工具来简化任务完成过程。这些工具可以被视为具有私有或实时知识的预定义操作流程，这些知识不存在于LLM的参数中。作为发展的自然趋势，被调用的工具正逐渐成为自主代理，因此完整的智能系统变成了一个多LLM代理系统（MLAS）。本文讨论了MLAS的技术和商业前景。与之前的单个LLM代理系统相比，MLAS的优势在于：i）更高的任务解决性能潜力；ii）更高的系统变更灵活性；iii）每个参与实体的数据隐私保护；以及iv）每个实体的货币化可行性。为了支持MLAS的生态系统，我们提供了一个初步版本的MLAS协议，考虑了技术要求、数据隐私和业务激励。因此，MLAS将是实现未来人工智能集体智能的一种实用解决方案。|
|**2024-11-21**|**Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel Planning**|Song Jiang et.al.|[2411.13904](http://arxiv.org/abs/2411.13904)|null|如何在未来利用基于大语言模型（LLM）的代理？尽管现有工作大多集中在提升特定任务族的性能上，本研究从一个不同的视角出发，探讨全面委托的概念：代理接管人类的日常决策过程，并被人类信任以找到满足个性化需求且适应不断变化环境的解决方案。为了实现这一目标，代理的行为，即自主行为，不仅应根据其成就（即结果评估）进行评价，还应根据其达成这些成就的方式（即过程评估）进行评价。为此，我们提出了APEC代理规范，这是一系列准则，代理应遵循以展示良好的自主行为，包括准确性、主动性、效率和可信度。为了验证APEC是否符合人类偏好，我们开发了APEC-Travel，这是一个旅游规划代理，它通过与旅行者的多轮对话主动提取隐藏的个性化需求。APEC-Travel完全由Llama3.1-405B-Instruct生成的合成数据构建而成，模拟了旅行者个性的丰富分布对话。经过迭代微调以遵循APEC代理规范，APEC-Travel在基于规则的指标上比基线高出20.7%，在LLM作为裁判的得分上高出9.1%。|
|**2024-11-21**|**Next-Generation Phishing: How LLM Agents Empower Cyber Attackers**|Khalifa Afane et.al.|[2411.13874](http://arxiv.org/abs/2411.13874)|null|日益增长的网络钓鱼邮件威胁变得越来越复杂，这与大型语言模型（LLM）的兴起密切相关。攻击者利用LLM来编写更具说服力和规避检测的网络钓鱼邮件，因此评估当前网络钓鱼防御系统的韧性变得至关重要。在这项研究中，我们对传统的网络钓鱼检测器（如Gmail垃圾邮件过滤器、Apache SpamAssassin和Proofpoint）以及机器学习模型（如SVM、逻辑回归和朴素贝叶斯）进行了全面评估，以识别传统网络钓鱼邮件和经过LLM重写的网络钓鱼邮件。我们还探讨了LLM作为网络钓鱼检测工具的新角色，这种方法已经被NTT安全控股公司和摩根大通等公司采用。我们的结果显示，所有检测器对重写邮件的检测准确性都有显著下降，这突显了当前网络钓鱼防御系统的关键弱点。随着威胁态势的发展，我们的研究结果强调了加强安全控制和对LLM生成内容的监管审查的重要性，以防止其被用于创建高级网络钓鱼攻击。本研究通过利用LLM生成多样化的网络钓鱼变体来进行数据增强，从而增强了网络钓鱼检测能力，并为开发更强大和适应性更强的威胁检测系统铺平了道路。|
|**2024-11-21**|**An Evaluation-Driven Approach to Designing LLM Agents: Process and Architecture**|Boming Xia et.al.|[2411.13768](http://arxiv.org/abs/2411.13768)|null|大型语言模型（LLMs）的出现使得开发能够自主实现未明确目标并持续进化的LLM代理成为可能，有时甚至无需更新代码或模型。传统方法如预定义测试用例和代码/模型重开发管道，在应对LLM代理开发的独特挑战时显得不足，特别是在质量和风险管理方面。本文介绍了一种基于评估驱动的设计方法，灵感来源于测试驱动开发，以解决这些挑战。通过多声音文献回顾（MLR），我们综合了现有的LLM评估方法，并提出了一种专门设计用于LLM代理的新过程模型和参考架构。所提出的方案整合了在线和离线评估，支持自适应运行时调整和系统的离线重开发，通过持续纳入评估结果（包括来自人类和AI评估者的细化反馈）来改进运行时管道、工件、系统架构和LLMs。|
|**2024-11-20**|**Metacognition for Unknown Situations and Environments (MUSE)**|Rodolfo Valiente et.al.|[2411.13537](http://arxiv.org/abs/2411.13537)|null|元认知——对自己认知过程的意识和调控——对于人类在未知情况下的适应性至关重要。相比之下，当前的自主代理在新环境中往往难以应对，因为它们的适应能力有限。我们假设元认知是自适应自主系统中的一个关键缺失因素，赋予它们处理陌生挑战所需的认知灵活性。鉴于元认知能力的广泛范围，我们重点关注两个关键方面：能力意识和针对新任务的战略选择。为此，我们提出了元认知未知情境与环境（MUSE）框架，该框架将元认知过程——特别是自我意识和自我调节——整合到自主代理中。我们提出了两种MUSE的初始实现方式：一种基于世界建模，另一种利用大型语言模型（LLMs），这两种方式都实现了元认知循环。我们的系统持续学习评估其在一个给定任务上的能力，并利用这种自我意识来指导策略选择的迭代周期。MUSE代理在自我意识和自我调节方面显示出显著改进，使它们能够更有效地解决新颖、分布外的任务，相比基于Dreamer-v3的强化学习和纯粹基于提示的LLM代理方法具有明显优势。这项工作突显了受认知和神经系统的启发方法在使自主系统适应新环境方面的潜力，克服了当前过度依赖大量训练数据的方法的局限性。|
|**2024-11-19**|**Human-In-the-Loop Software Development Agents**|Wannita Takerngsaksiri et.al.|[2411.12924](http://arxiv.org/abs/2411.12924)|null|最近，基于大型语言模型（LLM）的多代理范式被引入到软件工程中，以自动解决软件开发任务（例如从给定的问题到源代码）。然而，现有的工作主要基于历史基准数据集进行评估，没有考虑在自动化软件开发过程的每个阶段中的人类反馈，并且尚未在实际中部署。在本文中，我们介绍了一个名为HULA（人机协作LLM代理框架）的框架，用于软件开发，该框架允许软件工程师在生成给定任务的编码计划和源代码时对LLM进行细化和引导。我们设计、实现并已将HULA框架部署到Atlassian JIRA中进行内部使用。通过多阶段评估HULA框架，Atlassian的软件工程师认为HULA可以最小化整体开发时间和精力，特别是在启动编码计划和编写简单任务的代码方面。另一方面，提出了关于代码质量的一些挑战需要在未来的工作中解决。我们总结了经验教训并讨论了未来工作的机会，这将为LLM代理在软件开发中的发展铺平道路。|
|**2024-11-19**|**Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction**|Sonny George et.al.|[2411.12828](http://arxiv.org/abs/2411.12828)|**[link](https://github.com/sonnygeorge/oedd)**|**大型语言模型（LLM）代理在越来越多的领域展现出潜力。在许多预期的应用场景中，预计代理需要根据输入提示中的累积经验进行推理。我们提出了OEDD（即使在干扰下也能运用经验）语料库，这是一个经过人工注释者验证的情景集合，其中包含预设的代理历史，代理必须在存在干扰信息的情况下基于不同的环境前提做出决策。我们使用最小化思维链提示策略评估了三种最先进的LLM（GPT-3.5 Turbo、GPT-4o和Gemini 1.5 Pro），并观察到当（1）输入上下文包含超过1615个历史交互令牌，（2）一个关键的决策性前提是在两个不同环境前提下的正确结论，并且（3）随后出现一个微不足道但具有干扰性的误导事实时，所有LLM在选择两个行动方案中较优的一个时表现得比随机选择更差。我们的代码和测试语料库公开可访问：https://github.com/sonnygeorge/OEDD 。**|
|**2024-11-19**|**A More Advanced Group Polarization Measurement Approach Based on LLM-Based Agents and Graphs**|Zixin Liu et.al.|[2411.12196](http://arxiv.org/abs/2411.12196)|null|群体极化是社交媒体内容分析中的一个重要研究方向，吸引了许多研究人员探索这一领域。因此，如何有效地衡量群体极化已成为一个关键问题。在社交媒体上衡量群体极化存在一些挑战，这些挑战尚未被现有解决方案完全解决。首先，社交媒体群体极化的测量涉及处理大量文本，这对信息提取构成了重大挑战。其次，社交媒体上的文本通常难以理解，包括讽刺、表情包和网络俚语。此外，群体极化研究侧重于整体分析，而文本通常是碎片化的。为了解决这些挑战，我们设计了一个基于多智能体系统的解决方案，并使用了一种称为社区情感网络（Community Sentiment Network, CSN）的图结构来表示极化状态。此外，我们基于CSN开发了一种称为社区对立指数（Community Opposition Index, COI）的度量方法来量化极化程度。最后，我们通过零样本立场检测任务测试了我们的多智能体系统，并取得了出色的结果。总之，所提出的方法在可用性、准确性和可解释性方面具有显著价值。|
|**2024-11-19**|**Generative World Explorer**|Taiming Lu et.al.|[2411.11844](http://arxiv.org/abs/2411.11844)|null|在具身AI中，基于部分观测的规划是一个核心挑战。大多数先前的工作通过开发物理探索环境以更新其对世界状态的认知来解决这一挑战。相比之下，人类可以通过心理探索来想象世界未见的部分，并通过想象中的观察来修正其认知。这样的更新认知可以帮助他们做出更明智的决策，而无需总是进行物理探索。为了实现这种类似人类的能力，我们引入了“生成世界探索者（Genex）”，这是一个以自我为中心的世界探索框架，允许智能体在一个大规模的三维世界（如城市场景）中进行心理探索，并获取想象中的观测结果来更新其信念。这一更新后的信念将帮助智能体在当前步骤中做出更明智的决策。为了训练Genex，我们创建了一个合成的城市场景数据集Genex-DB。我们的实验结果表明：(1) Genex能够在大规模虚拟物理世界的长时域探索中生成高质量且一致的观测结果；(2) 使用这些生成的观测结果更新的信念可以指导现有的决策模型（例如LLM智能体）制定更好的计划。|
|**2024-11-18**|**LLM-IE: A Python Package for Generative Information Extraction with Large Language Models**|Enshuo Hsu et.al.|[2411.11779](http://arxiv.org/abs/2411.11779)|null|尽管最近采用了大型语言模型（LLMs）进行生物医学信息提取，但在提示工程和算法方面仍然存在挑战，并且没有专门的软件可用。为了解决这些问题，我们开发了LLM-IE：一个用于构建完整信息提取管道的Python包。我们的主要创新是一个交互式的LLM代理，用于支持模式定义和提示设计。  材料与方法：LLM-IE支持命名实体识别、实体属性提取和关系提取任务。我们在i2b2数据集上进行了基准测试并进行了系统评估。  结果：基于句子的提示算法在性能方面表现最佳，但需要更长的推理时间。系统评估提供了直观的可视化效果。  讨论：LLM-IE的设计基于医疗领域的实际NLP经验，并已在内部项目中采用。它对生物医学NLP社区应具有很高的价值。  结论：我们开发了一个名为LLM-IE的Python包，提供用于构建稳健的信息提取管道的构建模块。|
|**2024-11-18**|**OASIS: Open Agents Social Interaction Simulations on One Million Agents**|Ziyi Yang et.al.|[2411.11581](http://arxiv.org/abs/2411.11581)|**[link](https://github.com/camel-ai/oasis)**|近年来，人们对增强基于规则的智能体模型（ABMs）以研究社交媒体平台（如X和Reddit）的兴趣日益增长，从而实现对复杂系统进行更精细的研究。因此，在过去的一年里提出了几种基于大型语言模型（LLM）的ABMs。虽然这些模型很有前景，但每个模拟器都是专门为研究特定场景而设计的，这意味着使用相同的ABM探索其他现象既耗时又耗费资源。此外，这些模型只能模拟有限数量的智能体，而现实世界中的社交媒体平台涉及数百万用户。为此，我们提出了OASIS，这是一种通用且可扩展的社交媒体模拟器。OASIS基于真实世界的社交媒体平台设计，包括动态更新的环境（例如，动态社交网络和帖子信息）、多样化的动作空间（例如，关注、评论）以及推荐系统（例如，基于兴趣和热门评分）。此外，OASIS支持大规模用户模拟，能够建模多达一百万用户。凭借这些特性，OASIS可以轻松扩展到不同的社交媒体平台，以研究大规模群体现象和行为。我们复制了各种社会现象，包括信息传播、群体极化和羊群效应，这些现象发生在X和Reddit平台上。此外，我们在不同规模的智能体群体下提供了社会现象的观察结果。我们观察到，更大的智能体群体规模导致更强烈的群体动力和更多样化、更有帮助的智能体意见。这些发现展示了OASIS作为研究数字环境中复杂系统强大工具的潜力。|
|**2024-11-16**|**IntentGPT: Few-shot Intent Discovery with Large Language Models**|Juan A. Rodriguez et.al.|[2411.10670](http://arxiv.org/abs/2411.10670)|null|在当今数字化驱动的世界中，对话系统在提升用户交互方面发挥着关键作用，从客户服务到虚拟助手。在这些对话中，自动识别用户的目标对于及时解决他们的需求至关重要。这促使了意图检测模型的整合。然而，用户的意图是多样化和动态变化的，因此维持一组固定的预定义意图具有挑战性。因此，更实用的方法是开发一种能够随着新意图出现而识别它们的模型。我们关注的是意图发现这一领域，该领域在近期的研究工作中受到了广泛关注。现有的方法需要大量的数据训练以正确识别新的意图，这需要大量的人力投入。为了解决这个问题，我们提出了IntentGPT，这是一种新颖的无需训练的方法，能够有效地提示大型语言模型（如GPT-4）在少量标记数据的情况下发现新的意图。IntentGPT包括一个“上下文提示生成器”，用于生成上下文学习的信息性提示，一个“意图预测器”用于从语句中分类和发现用户意图，以及一个“语义少样本采样器”，用于选择相关的少样本示例和一组已知意图，并将其注入提示中。我们的实验表明，IntentGPT在包括CLINC和BANKING在内的流行基准测试中优于那些需要大量特定领域数据和微调的先前方法。|
|**2024-11-15**|**Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash**|Parsa Hejabi et.al.|[2411.10422](http://arxiv.org/abs/2411.10422)|**[link](https://github.com/parsahejabi/simulation-framework-for-multi-agent-balderdash)**|**大型语言模型（LLMs）在复杂任务和交互环境中展示了令人印象深刻的性能，但其创造力仍需进一步探索。本文介绍了一个利用游戏Balderdash的仿真框架，以评估LLMs的创造力和逻辑推理能力。在Balderdash游戏中，玩家需要为生僻词汇编造虚构定义，以欺骗其他玩家，同时识别正确定义。我们的框架使多个LLM代理能够参与这个游戏，评估它们生成可信定义的能力以及基于游戏规则和历史进行策略规划的能力。我们实现了一个集中式游戏引擎，其中包含多种LLM作为参与者，还有一个判断LLM来评估语义等效性。通过一系列实验，我们分析了不同LLM的表现，考察了诸如真实定义比率、欺骗比率和正确猜测比率等指标。结果提供了关于LLMs创造性和欺骗能力的见解，突显了它们的优势和改进空间。研究特别指出，输入中生僻词汇的频率低会导致对游戏规则和历史背景推理不足（https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash）。**|
|**2024-11-15**|**An Empirical Study on LLM-based Agents for Automated Bug Fixing**|Xiangxin Meng et.al.|[2411.10213](http://arxiv.org/abs/2411.10213)|null|大型语言模型（LLMs）和基于LLM的Agent在自动修复bug方面已经显示出一定的能力，通过与开发环境的交互、迭代验证和代码修改来解决软件缺陷。然而，对这些Agent系统和非Agent系统的系统性分析仍然有限，特别是对于顶级表现系统之间的性能差异研究较少。在这篇论文中，我们在SWE-bench Lite基准上测试了七个专有和开源系统，以评估它们在自动修复bug方面的表现。我们首先评估每个系统的总体性能，记录所有或没有系统能够解决的实例，并探讨为什么某些实例只能被特定类型的系统解决。我们还比较了文件级和行级的故障定位准确性，并评估了bug重现的能力，识别出只有通过动态重现才能解决的实例。通过分析，我们得出结论，需要进一步优化LLM本身以及Agent流程设计，以提高Agent在修复bug方面的有效性。|
|**2024-11-15**|**Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent Consensus-Seeking**|Valeria Jannelli et.al.|[2411.10184](http://arxiv.org/abs/2411.10184)|null|本文探讨了大型语言模型（LLMs）如何在供应链管理（SCM）中实现共识寻求的自动化。在供应链管理中，频繁的决策问题如库存水平和交货时间需要公司之间的协调。传统的供应链管理依赖于人类共识来做出决策，以避免诸如牛鞭效应等突发问题。一些常规的共识过程，尤其是那些耗时且成本较高的过程，可以实现自动化。然而，现有的自动化协调解决方案由于高准入门槛、有限的能力以及在复杂场景中的适应性限制而面临挑战，这将小型和中型企业排除在外。然而，生成式人工智能，特别是LLMs的最新进展显示出了克服这些障碍的潜力。通过在大规模数据集上的训练，LLMs能够进行谈判、推理和规划，从而以较低的准入门槛实现接近人类水平的共识。在这项工作中，我们识别出现有方法的关键局限性，并提出自主LLM代理来解决这些差距。我们引入了一系列针对LLM代理定制的新型供应链特定共识寻求框架，并通过库存管理的案例研究验证了我们方法的有效性。为了加速供应链社区内的进步，我们将代码开源，为LLM驱动的自主供应链解决方案的进一步发展提供基础。|
|**2024-11-14**|**Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents**|Yuyou Gan et.al.|[2411.09523](http://arxiv.org/abs/2411.09523)|null|随着大型语言模型（LLMs）的不断发展，基于变压器的模型在众多自然语言处理（NLP）任务中取得了突破性的进展，从而催生了一系列使用LLM作为控制核心的代理。尽管LLMs在各种任务中取得了成功，但它们面临着诸多安全和隐私威胁，这些威胁在代理场景中变得更加严重。为了增强基于LLM的应用程序的可靠性，一系列研究从不同角度评估和缓解了这些风险。本文旨在帮助研究人员全面了解各种风险，收集并分析了这些代理面临的不同威胁。为了应对前人分类框架在处理跨模块和跨阶段威胁方面的挑战，我们提出了一种基于威胁来源和影响的新分类框架。此外，我们基于六个关键特征总结了当前的研究进展，并分析了其局限性。随后，我们选择了四个代表性代理作为案例研究，分析了它们在实际应用中可能遇到的风险。最后，基于上述分析，我们从数据、方法论和政策三个角度提出了未来的研究方向。|
|**2024-11-18**|**Towards Evaluating Large Language Models for Graph Query Generation**|Siraj Munir et.al.|[2411.08449](http://arxiv.org/abs/2411.08449)|null|大型语言模型（LLMs）正在革新生成式人工智能（GenAI）领域，各种基于LLM的创新解决方案层出不穷。然而，当应用于数据库技术，特别是在图数据库和知识图谱（KGs）的查询生成方面时，LLMs仍面临重大挑战。尽管有关于LLM驱动的SQL查询生成的研究已经存在，但针对图数据库的类似系统仍然较少。本文通过一项对比研究，探讨了使用开放访问的LLM生成Cypher查询（一种强大的图数据库交互语言）所面临的挑战。我们严格评估了几种LLM代理（包括OpenAI ChatGPT 4.0、Claude Sonnet 3.5、Google Gemini Pro 1.5以及本地部署的Llama 3.1 8B），采用设计的少量学习提示和基于检索增强生成（RAG）及链式思维（CoT）推理的方法。我们的实证分析表明，在此特定领域中，Claude Sonnet 3.5在查询生成准确性方面优于其竞争对手。此外，我们还指出了未来研究的方向，以解决现有局限并推进LLM驱动的图数据库查询生成技术的发展。|
|**2024-11-13**|**Collaborative Participatory Research with LLM Agents in South Asia: An Empirically-Grounded Methodological Initiative and Agenda from Field Evidence in Sri Lanka**|Xinjie Zhao et.al.|[2411.08294](http://arxiv.org/abs/2411.08294)|null|人工智能在发展研究方法中的整合为解决参与式研究中长期存在的挑战提供了前所未有的机遇，特别是在像南亚这样语言多样的地区。本文基于斯里兰卡僧伽罗语社区的实证实施，提出了一种以经验为基础的方法论框架，旨在革新参与式发展研究，该框架位于斯里兰卡洪水频发的尼尔瓦拉河盆地这一具有挑战性的多语言环境中。超越传统的翻译和数据收集工具，该框架采用多智能体系统架构，重新定义了在语言和文化多样化的研究环境中如何进行数据收集、分析和社区参与。这种结构化的基于代理的方法使参与式研究既可扩展又具响应性，确保社区视角在研究结果中保持核心地位。实地经验揭示了基于大型语言模型（LLM）的系统在资源有限的地区解决发展研究中长期存在的问题的巨大潜力，提供量化的效率提升和定性的包容性改进。从更广泛的方法论角度来看，本研究议程倡导使用AI驱动的参与式研究工具，这些工具需保持伦理考虑、文化尊重和操作效率，强调部署AI系统以增强社区自主权和公平的知识生成的战略路径，可能为全球南方更广泛的研究议程提供参考。|
|**2024-11-11**|**Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving**|Botao Yu et.al.|[2411.07228](http://arxiv.org/abs/2411.07228)|null|为了增强大型语言模型（LLMs）在化学问题解决中的能力，已经提出了几种配备了工具的LLM基代理，如ChemCrow和Coscientist。然而，它们的评估范围狭窄，对于理解工具在各种化学任务中的益处存在很大差距。为此，我们开发了ChemAgent，这是一种基于ChemCrow的增强型化学代理，并对其在专门化学任务和普通化学问题上的性能进行了全面评估。令人惊讶的是，ChemAgent并不总是在没有工具的情况下提高其基础LLM的表现。通过与化学专家进行错误分析，我们发现：对于专门的化学任务，如合成预测，我们应该为代理配备专门的工具；然而，对于像考试中的普通化学问题，代理正确运用化学知识的能力更为重要，工具的增加并不总是有帮助。|
|**2024-11-10**|**Hermes: A Large Language Model Framework on the Journey to Autonomous Networks**|Fadhel Ayed et.al.|[2411.06490](http://arxiv.org/abs/2411.06490)|null|推动蜂窝网络运营自动化的需求随着这些系统复杂性的增加而增长。尽管取得了进展，但完全自主目前仍然遥不可及，因为依赖于人为干预来建模网络行为并定义满足目标要求的策略。网络数字孪生（NDT）在增强网络智能方面显示出前景，但这种技术的成功实施受到特定用例架构的限制，限制了其在推进网络自主性方面的作用。需要更强大的网络智能，或“电信大脑”，以实现蜂窝网络的无缝、自主管理。大规模语言模型（LLM）作为这一愿景的潜在推动者应运而生，但在网络建模方面面临挑战，特别是在推理和处理各种数据类型方面。为了解决这些差距，我们介绍了赫尔墨斯（Hermes），这是一种链式LLM代理，通过结构化和可解释的逻辑步骤使用“蓝图”构建NDT实例。赫尔墨斯允许自动、可靠且准确地对各种用例和配置进行网络建模，从而朝着完全自主的网络运营迈进。|
|**2024-11-12**|**Game-theoretic LLM: Agent Workflow for Negotiation Games**|Wenyue Hua et.al.|[2411.05990](http://arxiv.org/abs/2411.05990)|**[link](https://github.com/wenyueh/game_theory)**|**本文研究了大型语言模型（LLMs）在战略决策背景下的合理性，特别是在博弈论框架下。我们评估了几种最先进的LLMs在完全信息和不完全信息游戏中的表现。研究发现，随着游戏复杂性的增加，例如更大的收益矩阵或更深的序列树，LLMs经常偏离理性策略。为了解决这些局限性，我们设计了多种基于博弈论的工作流程，以指导LLMs的推理和决策过程。这些工作流程旨在增强模型计算纳什均衡和在不确定性和不完全信息条件下做出理性选择的能力。实验结果表明，采用这些工作流程显著提高了LLMs在博弈论任务中的合理性和稳健性。具体而言，采用工作流程后，LLMs在识别最优策略、谈判场景中的近似最优分配以及减少谈判中的被利用倾向方面表现出显著改进。此外，我们还探讨了代理是否应该采用此类工作流程的元战略考虑，认识到决定使用或放弃工作流程本身就是一个博弈论问题。本研究有助于深入理解LLMs在战略环境下的决策能力，并提供了通过结构化工作流程提高其合理性的见解。研究结果对开发更强大和更具战略性的AI代理具有重要意义，这些代理能够在复杂的互动环境中导航。支持本研究的代码和数据可在以下链接获取：https://github.com/Wenyueh/game_theory。**|
|**2024-11-08**|**LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution**|Yuheng Zhao et.al.|[2411.05651](http://arxiv.org/abs/2411.05651)|null|视觉分析（VA）要求分析师根据观察结果迭代地提出分析任务，并通过创建可视化和交互式探索来执行这些任务以获得洞察。这一过程需要编程、数据处理和可视化工具方面的技能，突显了对更智能、更精简的VA方法的需求。最近开发的大语言模型（LLM）作为代理，具备动态规划和使用工具的能力，为增强VA的效率和多功能性提供了潜力。我们提出了LightVA，这是一种轻量级的VA框架，通过人机协作支持任务分解、数据分析和交互式探索。我们的方法旨在帮助用户逐步将高层次的分析目标转化为低层次的任务，生成可视化并得出洞察。具体来说，我们引入了一种基于LLM代理的任务规划和执行策略，采用一个涉及规划者、执行者和控制器的递归过程。规划者负责推荐和分解任务，执行者处理任务执行，包括数据分析、可视化生成和多视图组合，而控制器则协调规划者和执行者之间的交互。在此框架基础上，我们开发了一个具有混合用户界面的系统，其中包括用于监控和管理任务规划过程的任务流程图、用于交互式数据探索的可视化面板以及用于通过自然语言指令引导模型的聊天视图。我们通过一个使用场景和专家研究来检验该方法的有效性。|
|**2024-11-08**|**Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework**|Honghao Shi et.al.|[2411.05349](http://arxiv.org/abs/2411.05349)|null|近期在大型语言模型（LLMs）以及相关技术如检索增强生成（RAG）和思维导图（DoT）方面的进展，使得创建能够执行集群诊断和故障排除的自主智能系统成为可能。通过将这些技术与自我博弈方法论相结合，我们开发了一种LLM代理系统，旨在自主诊断和解决AI集群中的问题。我们的创新包括专为集群诊断设计的知识库、优化的LLM算法、代理的实用部署策略以及一个专门用于评估LLM在此领域能力的基准。通过在多个维度上的广泛实验，我们展示了该系统在应对集群诊断挑战方面的优越性，特别是在检测和纠正性能问题方面比传统方法更加高效和准确。|
|**2024-11-07**|**Alopex: A Computational Framework for Enabling On-Device Function Calls with LLMs**|Yide Ran et.al.|[2411.05209](http://arxiv.org/abs/2411.05209)|null|大型语言模型（LLMs）的快速发展促使它们被集成到移动设备中，以提供个性化助手服务，这使得LLMs能够调用外部API函数以增强其性能。然而，数据稀缺、问题格式不当和灾难性遗忘等问题阻碍了设备端LLM代理的发展。为了解决这些问题，我们提出了Alopex框架，该框架利用Fox LLM实现精确的设备端函数调用。Alopex引入了一种基于逻辑的方法来生成高质量的训练数据，并采用新颖的“描述-问题-输出”格式进行微调，从而减少函数信息泄露的风险。此外，还使用了一种数据混合策略来缓解灾难性遗忘，将函数调用数据与教科书数据集结合，以提升在各种任务中的表现。实验结果表明，Alopex提高了函数调用的准确性，并显著减少了灾难性遗忘，为无需人工干预地将函数调用能力整合到LLMs中提供了稳健的解决方案。|
|**2024-11-07**|**PentestAgent: Incorporating LLM Agents to Automated Penetration Testing**|Xiangmin Shen et.al.|[2411.05185](http://arxiv.org/abs/2411.05185)|null|渗透测试是一种关键的技术，用于识别安全漏洞，传统上由熟练的安全专家手动执行。这一复杂的过程涉及收集目标系统的相关信息、确定入口点、利用系统并报告发现结果。尽管这种方法非常有效，但手动渗透测试耗时且成本高昂，通常需要大量的专业知识和资源，许多组织无法承受。虽然已经提出了自动化渗透测试的方法，但在实际应用中往往由于灵活性、适应性和实施方面的限制而表现不佳。最近大型语言模型（LLM）的进步为通过提高智能和自动化水平来增强渗透测试提供了新的机会。然而，当前基于LLM的方法仍然面临重大挑战，包括有限的渗透测试知识和缺乏全面的自动化能力。为了解决这些不足，我们提出了一种名为PentestAgent的新型LLM驱动的自动化渗透测试框架，该框架利用LLM和各种基于LLM的技术（如检索增强生成，RAG）来增强渗透测试知识并实现多种任务的自动化。我们的框架利用多代理协作来自动化情报收集、漏洞分析和利用阶段，减少人工干预。我们使用一个全面的基准对PentestAgent进行了评估，展示了其在任务完成和整体效率方面的卓越性能。这项工作显著提升了自动化渗透测试系统的实用性和适用性。|
|**2024-11-12**|**CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models**|Jierui Li et.al.|[2411.04329](http://arxiv.org/abs/2411.04329)|null|预训练于大量代码和文本数据上的大规模语言模型（LLMs）在执行代码生成任务方面已经取得了显著的成就。通过额外的基于执行的反馈，这些模型可以作为代理，具备自主优化和改进生成代码的能力。然而，在具有极大搜索空间的挑战性编码任务中，当前的代理方法仍然难以处理多阶段规划、生成和调试的问题。为了解决这个问题，我们提出了CodeTree框架，该框架使LLM代理能够在代码生成过程的不同阶段高效地探索搜索空间。具体来说，我们采用了一个统一的树结构来明确探索不同的编码策略，生成相应的编码解决方案，并随后对这些解决方案进行优化。在每个阶段，探索过程中的关键决策（排序、终止、扩展）都由环境的基于执行的反馈和LLM代理生成的反馈共同指导。我们在7个代码生成基准上全面评估了CodeTree，并展示了CodeTree相对于强大基线的显著性能提升。使用GPT-4作为基础模型，我们在HumanEval上获得了95.1分，在MBPP上获得了98.7分，在CodeContests上获得了43.0分。在具有挑战性的SWEBench基准上，我们的方法也带来了显著的性能提升。|
|**2024-11-06**|**From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning**|Zhirui Deng et.al.|[2411.03817](http://arxiv.org/abs/2411.03817)|null|大型语言模型（LLMs）的卓越能力使其成为各种自主代理系统中的关键组件。虽然传统方法依赖于LLMs的内在知识而不进行微调，但更近期的方法转向了强化学习策略，以进一步增强代理在与环境和工具互动时解决复杂任务的能力。然而，先前的方法受到稀疏奖励问题的限制，现有数据集仅对每个多步骤推理链提供一个最终标量奖励，这可能导致策略学习的低效和无效。在这篇论文中，我们介绍了StepAgent，它利用逐步奖励来优化代理的强化学习过程。借鉴新手到专家理论的精神，我们首先比较专家和代理的行为，自动生成中间奖励以实现细粒度优化。此外，我们提出了隐式奖励和逆向强化学习技术，以促进代理的反思和策略调整。进一步的理论分析表明，代理的动作分布可以在多次训练周期内收敛到专家动作分布。实验结果表明，在各种数据集上，StepAgent的表现优于现有的基线方法。|
|**2024-11-05**|**AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution**|Zhiqiang Xie et.al.|[2411.03519](http://arxiv.org/abs/2411.03519)|null|随着大型语言模型（LLM）驱动的代理在模拟环境中进行复杂任务、与其他代理互动以及展示与社会科学研究和游戏相关的新兴行为的能力不断增强，基于这些模型的代理越来越多地被开发出来。然而，当前多代理模拟经常由于虚假依赖导致的有限并行性而遭受效率低下的问题，从而产生性能瓶颈。在这篇论文中，我们介绍了AI Metropolis，这是一种模拟引擎，通过引入乱序执行调度来提高LLM代理模拟的效率。通过动态跟踪代理之间的实际依赖关系，AI Metropolis最大限度地减少了虚假依赖，增强了并行性，并实现了高效的硬件利用。我们的评估表明，AI Metropolis在标准并行模拟与全局同步的情况下，速度提高了1.3倍到4.15倍，并且随着代理数量的增加，其性能接近最优。|
|**2024-11-03**|**Fixing Security Vulnerabilities with AI in OSS-Fuzz**|Yuntong Zhang et.al.|[2411.03346](http://arxiv.org/abs/2411.03346)|null|关键的开源软件系统会经历大量的模糊测试，以发现可能导致软件崩溃的输入。这种模糊测试通常是对程序输入域进行有偏的随机搜索，以找到可能使软件崩溃的输入。由于即使是闭源软件也可能使用开源组件，因此对开源软件进行测试对于增强软件系统的安全性至关重要。目前，OSS-Fuzz是最重要和最广泛使用的基础设施，用于持续验证开源系统。然而，尽管OSS-Fuzz已经在1000多个软件项目中识别出超过10000个漏洞，但这些被发现的漏洞可能仍然未被修补，因为漏洞修复通常需要手动操作。在本研究中，我们依赖于大型语言模型（LLM）代理在自主程序改进方面的最新进展，包括错误修复。我们定制了著名的AutoCodeRover代理来修复安全漏洞。这是因为LLM代理如AutoCodeRover通过代码搜索根据问题描述来修复错误。相反，在安全补丁方面，我们依靠执行漏洞利用输入来提取与修复相关的代码元素。我们对OSS-Fuzz漏洞数据的经验表明，LLM代理的自主性对于成功修复安全漏洞是有用的，这与那些控制流固定的无代理方法相比是一个优势。更重要的是，我们的研究结果表明，我们不能通过代码相似度（如VulMaster中使用的CodeBLEU分数）来衡量补丁的质量，因为即使具有高CodeBLEU分数的补丁仍无法通过给定的漏洞利用输入。我们的研究表明，安全补丁的正确性需要考虑动态属性，如测试执行，而不是依赖标准文本/代码相似性指标。|
|**2024-11-05**|**SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents**|Dawei Li et.al.|[2411.03284](http://arxiv.org/abs/2411.03284)|**[link](https://github.com/david-li0406/smoa)**|**虽然多智能体系统已被证明在各种任务和应用中显著提升了大型语言模型（LLMs）的性能，但这些系统中密集的交互可能会影响其效率和多样性。为了解决这些问题，我们从稀疏混合智能体（SMoE）框架中汲取灵感，并提出了一种稀疏混合智能体（SMoA）框架，以提升多智能体LLMs的效率和多样性。与完全连接的结构不同，SMoA引入了新的响应选择和提前停止机制，以稀疏化个体LLM智能体之间的信息流，从而在性能和效率之间取得平衡。此外，受SMoE框架中专家多样性原则的启发，我们在每个LLM智能体上分配不同的角色描述，促进多样性和发散性思维。广泛的实验证明，在推理、对齐和公平性基准测试中，SMoA的表现与传统的混合智能体方法相当，但计算成本显著降低。进一步分析表明，SMoA更加稳定，具有更大的扩展能力，并通过超参数优化提供了巨大的潜力。代码和数据将在：https://github.com/David-Li0406/SMoA 获取。**|
|**2024-11-05**|**Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities**|Ryosuke Takata et.al.|[2411.03252](http://arxiv.org/abs/2411.03252)|null|我们从零开始研究通过使用基于大型语言模型（LLM）的代理来产生自主性。在以往对基于LLM的代理的研究中，每个代理的特性，包括个性和记忆，通常是预定义的。我们关注的是如何从一个未分化的状态中分化出个体性，如行为、个性和记忆。当前的LLM代理在一个群体模拟中进行合作交流，通过自然语言交换基于上下文的消息。通过分析这种多代理模拟，我们报告了有关社会规范、合作和个人特质如何自发产生的有价值的新见解。本文展示了自主交互的LLM驱动代理会产生幻觉和标签，以维持交流，这反过来增加了其互动中的词汇多样性。每个代理的情绪会随着交流而变化，当它们形成社区时，代理的个性也随之显现并随之演变。这种计算建模方法及其发现将为分析集体人工智能提供一种新方法。|
|**2024-11-04**|**CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments**|Kung-Hsiang Huang et.al.|[2411.02305](http://arxiv.org/abs/2411.02305)|**[link](https://github.com/salesforceairesearch/crmarena)**|**客户关系管理（CRM）系统对于现代企业至关重要，为管理客户互动和数据提供了基础。将AI代理集成到CRM系统中可以自动化例行流程并提升个性化服务。然而，由于缺乏反映现实世界CRM任务复杂性的现实基准，部署和评估这些代理具有挑战性。为了解决这个问题，我们介绍了CRMArena，这是一个旨在评估AI代理在专业工作环境中的实际任务的新基准。根据CRM专家的指导和行业最佳实践，我们设计了CRMArena，包括分布在三个角色（服务代理、分析师和经理）中的九个客户服务任务。该基准包括16个常用工业对象（如账户、订单、知识文章、案例），这些对象具有高度互联性，并且包括潜在变量（如投诉习惯、政策违规）以模拟现实的数据分布。实验结果显示，最先进的大型语言模型（LLM）代理使用ReAct提示方法在少于40%的任务中取得成功，即使拥有函数调用能力的情况下，成功率也低于55%。我们的研究结果强调了增强代理在函数调用和规则遵循方面的能力的需求，以便在现实世界的工作环境中部署。CRMArena是一个开放的挑战，能够可靠完成任务的系统展示了在流行工作环境中直接的商业价值。**|
|**2024-11-04**|**DynaSaur: Large Language Agents Beyond Predefined Actions**|Dang Nguyen et.al.|[2411.01747](http://arxiv.org/abs/2411.01747)|null|现有的大型语言模型（LLM）代理系统通常在每一步从一个固定且预定义的动作集中选择动作。虽然这种方法在封闭且狭义限定的环境中是有效的，但我们认为它在部署LLM代理到现实世界场景时存在两大挑战：(1) 从固定的动作集中选择显著限制了LLM代理的规划和行动能力；(2) 这种方法需要大量的人力来枚举和实现所有可能的动作，在复杂环境中变得不切实际，因为潜在的动作数量巨大。在这项工作中，我们提出了一种LLM代理框架，该框架能够在在线过程中动态创建和组合动作。在这个框架中，代理通过在每个步骤生成并执行用通用编程语言编写的程序与环境进行交互。此外，生成的动作会随着时间积累以供未来重用。我们在GAIA基准测试上的广泛实验表明，该框架提供了显著更大的灵活性，并优于先前的方法。值得注意的是，它允许LLM代理在没有相关动作存在于预定义集合中或当现有动作因未预见的边缘情况而失败的情况下恢复。在撰写本文时，我们在GAIA公开排行榜上处于领先地位。我们的代码可以在<https://github.com/adobe-research/dynasaur>找到。|
|**2024-11-03**|**EcoAct: Economic Agent Determines When to Register What Action**|Shaokun Zhang et.al.|[2411.01643](http://arxiv.org/abs/2411.01643)|null|近期的进展使大型语言模型（LLMs）能够作为代理执行动作并使用外部工具。这要求在采取行动之前将工具信息注册或集成到LLM的上下文中。当前的方法是不加选择地将所有候选工具整合到代理的上下文中，并且这些工具在整个多个推理步骤中都保持不变。这一过程对LLM代理来说是不透明的，并未融入其推理程序中，导致由于不相关的工具增加了上下文长度而效率低下。为了解决这个问题，我们引入了EcoAct算法，它允许LLMs根据需要选择性地注册工具，从而优化上下文的使用。通过将工具注册过程整合到推理过程中，EcoAct在多步骤推理任务中的计算成本降低了50%以上，同时保持了性能，这一点通过广泛的实验得到了证明。此外，它可以插入任何推理管道，并且只需对提示进行微小修改即可实现，使其适用于现在的和未来的LLM代理。|
|**2024-11-02**|**AutoPT: How Far Are We from the End2End Automated Web Penetration Testing?**|Benlong Wu et.al.|[2411.01236](http://arxiv.org/abs/2411.01236)|**[link](https://github.com/Dizzy-K/AutoPT)**|**渗透测试对于确保网络安全至关重要，它能够提前检测和修复漏洞，防止数据泄露和其他严重后果。大型语言模型（LLMs）的强大推理能力在各个领域都取得了显著进展，基于LLM的代理的发展潜力有望革新网络安全领域的渗透测试行业。在这项工作中，我们建立了一个全面的端到端渗透测试基准，使用真实的渗透测试环境来探索LLM代理在这个领域的应用能力。我们的结果显示，这些代理熟悉渗透测试任务的框架，但在生成准确命令和执行完整流程方面仍面临限制。因此，我们总结了当前面临的挑战，包括难以保持整个消息历史记录以及代理容易陷入困境的问题。  基于以上见解，我们提出了一种基于有限状态机（FSM）方法的渗透测试状态机（PSM），以解决这些限制。然后，我们介绍了AutoPT，这是一种基于LLM驱动的渗透测试自动化代理，利用了LLM的内在推理能力和状态机的约束框架。我们的评估结果表明，AutoPT在GPT-4o mini模型上优于基线框架ReAct，并将基准目标的任务完成率从22%提高到41%。与基线框架和人工操作相比，AutoPT还进一步减少了时间和经济成本。因此，我们的AutoPT促进了自动化渗透测试的发展，并对学术界和工业界产生了重要影响。**|
|**2024-11-02**|**A Large-scale Time-aware Agents Simulation for Influencer Selection in Digital Advertising Campaigns**|Xiaoqing Zhang et.al.|[2411.01143](http://arxiv.org/abs/2411.01143)|null|在数字世界中，影响者作为意见领袖起着关键作用，塑造其追随者的观点和选择。现代广告往往遵循这一趋势，营销人员根据详尽的市场分析选择合适的影响者进行产品代言。以往关于影响者选择的研究通常依赖于个人意见和互动的数值表示，这种方法简化了社会动态的复杂性。在这项工作中，我们首先介绍了一种时间感知影响者模拟器（TIS），帮助推广者基于LLM模拟识别并选择合适的影响力人物来推广他们的产品。为了验证我们的方法，我们在公共广告活动数据集SAGraph上进行了实验，该数据集涵盖了社交关系、帖子和用户互动。结果显示，我们的方法优于传统的基于数值特征的方法和使用有限LLM代理的方法。我们的研究表明，通过模拟用户的时间线和内容生命周期，可以简化扩展，从而在社交网络中实现大规模代理模拟。此外，基于LLM的社交推荐和广告代理在促销活动的决策中提供了显著的好处。|
|**2024-11-01**|**Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improvement**|Yingwei Ma et.al.|[2411.00622](http://arxiv.org/abs/2411.00622)|**[link](https://github.com/LingmaTongyi/Lingma-SWE-GPT)**|**近年来，基于大型语言模型（LLM）的代理在自动软件工程领域取得了显著进展，特别是在软件维护和演化方面。尽管取得了这些令人鼓舞的进步，当前的研究仍面临两大挑战。首先，最先进的性能主要依赖于闭源模型，这极大地限制了技术的可访问性和在不同软件工程任务中的定制潜力。其次，这些模型大多是在静态代码数据上进行训练的，缺乏对软件开发过程中动态交互、迭代问题解决过程和演化特性的深刻理解。为了解决这些挑战，我们的研究采用软件工程视角。我们认识到，现实世界中的软件维护和演化过程不仅包括静态代码数据，还包括开发人员的思维过程、外部工具的使用以及不同职能人员之间的互动。因此，我们推出了Lingma SWE-GPT系列，包括Lingma SWE-GPT 7B和72B。通过学习和模拟真实的代码提交活动，Lingma SWE-GPT系统地融入了软件开发过程中固有的动态交互和迭代问题解决，从而实现了对软件改进过程的更全面理解。我们使用SWE-bench Verified基准进行了实验评估。结果表明，Lingma SWE-GPT 72B成功解决了30.20%的GitHub问题，标志着在自动问题解决方面的重大进步（比Llama 3.1 405B相对提高了22.76%），接近闭源模型的性能（GPT-4o解决了31.80%的问题）。值得注意的是，Lingma SWE-GPT 7B解决了18.20%的问题，突显了将较小模型应用于软件工程任务的潜力。**|
|**2024-10-31**|**From Context to Action: Analysis of the Impact of State Representation and Context on the Generalization of Multi-Turn Web Navigation Agents**|Nalin Tiwary et.al.|[2410.23555](http://arxiv.org/abs/2410.23555)|null|近年来，基于大型语言模型（LLM）的框架已经扩展到复杂的现实世界应用，例如交互式网页导航。这些系统通过用户命令驱动，通过多轮对话在网页浏览器中完成任务，既提供了创新的机会也带来了显著的挑战。尽管已经引入了对话网页导航的基准测试，但影响这些代理性能的关键上下文组件的详细理解仍然难以捉摸。本研究旨在通过分析网页导航代理功能的各种关键上下文元素来填补这一空白。我们研究了上下文管理的优化，重点关注交互历史和网页表示的影响。我们的工作突出了通过有效的上下文管理，在分布外场景下（如未见过的网站、类别和地理位置）改进代理性能。这些发现为LLM基础代理的设计和优化提供了见解，使实际应用中的网页导航更加准确和有效。|
|**2024-10-30**|**Evaluating Cultural and Social Awareness of LLM Web Agents**|Haoyi Qiu et.al.|[2410.23252](http://arxiv.org/abs/2410.23252)|null|随着大型语言模型（LLMs）扩展到执行现实世界应用中的代理任务，超越传统NLP任务，评估其稳健性变得越来越重要。然而，现有的基准测试往往忽略了诸如文化和社会意识等关键维度。为了解决这些问题，我们引入了CASA，这是一个旨在评估LLM代理在两个基于网络的任务（在线购物和社交讨论论坛）中对文化和社会规范的敏感性的基准。我们的方法评估了LLM代理检测并适当回应违反规范的用户查询和观察的能力。此外，我们提出了一种全面的评估框架，该框架测量意识覆盖率、处理用户查询时的有用性以及面对误导性网络内容时的违规率。实验表明，当前的LLM在非代理环境中的表现明显优于基于网络的代理环境，代理的意识覆盖率低于10%，违规率超过40%。为了提高性能，我们探索了两种方法：提示和微调，并发现这两种方法可以互补——在特定文化数据集上进行微调可以显著提升代理在不同地区的泛化能力，而提示则可以增强代理处理复杂任务的能力。这些发现强调了在开发周期中不断基准测试LLM代理的文化和社会意识的重要性。|
|**2024-10-30**|**Explainable Behavior Cloning: Teaching Large Language Model Agents through Learning by Demonstration**|Yanchu Guan et.al.|[2410.22916](http://arxiv.org/abs/2410.22916)|null|自主移动应用交互在移动应用程序复杂性日益增加的背景下变得越来越重要。开发能够有效导航和与移动应用交互的智能代理仍然是一个重大挑战。在本文中，我们提出了一种可解释的行为克隆大语言模型代理（EBC-LLMAgent），这是一种结合大型语言模型（LLMs）和行为克隆通过学习演示来创建智能且可解释的代理的新方法，用于自主移动应用交互。EBC-LLMAgent 包括三个核心模块：演示编码、代码生成和用户界面映射，这些模块协同工作以捕捉用户演示、生成可执行代码，并建立代码与用户界面元素之间的准确对应关系。我们引入了行为克隆链融合技术以增强代理的泛化能力。在五个来自不同领域的流行移动应用上进行的广泛实验表明，EBC-LLMAgent 具有卓越的性能，在任务完成方面具有高成功率，能够高效地泛化到未见过的场景，并生成有意义的解释。|
|**2024-10-30**|**$\textbf{EMOS}$: $\textbf{E}$mbodiment-aware Heterogeneous $\textbf{M}$ulti-robot $\textbf{O}$perating $\textbf{S}$ ystem with LLM Agents**|Junting Chen et.al.|[2410.22662](http://arxiv.org/abs/2410.22662)|null|异构多机器人系统（HMRS）已成为解决单个机器人无法独立完成的复杂任务的强大方法。目前基于大型语言模型的多智能体系统（LLM-based MAS）在软件开发和操作系统等领域取得了成功，但将其应用于机器人控制则面临着独特的挑战。特别是，多机器人系统中每个代理的能力本质上与其物理组成相关，而不是预定义的角色。为了解决这个问题，我们引入了一种新颖的多智能体框架，旨在实现具有不同形态和能力的异构机器人的有效协作，并提出一个新的基准测试Habitat-MAS。我们设计的关键组件是“机器人简历”：不同于采用人为设定的角色扮演方式，我们提出了自我提示的方法，即代理通过理解机器人的URDF文件并调用机器人运动学工具来生成描述其物理能力的文档，以指导其在任务规划和动作执行中的行为。Habitat-MAS基准测试旨在评估多智能体框架如何处理需要体现感知推理的任务，这些任务包括1）操作，2）感知，3）导航，以及4）复杂的多层物体重排。实验结果表明，机器人的简历和我们多智能体系统的分层设计对于在这种复杂的任务环境中有效运行异构多机器人系统至关重要。|
|**2024-10-29**|**BENCHAGENTS: Automated Benchmark Creation with Agent Interaction**|Natasha Butt et.al.|[2410.22584](http://arxiv.org/abs/2410.22584)|null|评估受到基准测试可用性的限制。随着模型的发展，需要创建能够衡量新生成能力进展的基准测试。然而，通过人工注释创建新的基准测试既缓慢又昂贵，这限制了对任何能力的全面评估。我们引入了BENCHAGENTS框架，该框架系统地利用大型语言模型（LLMs）自动化创建复杂能力的基准测试，同时确保数据和度量的质量。BENCHAGENTS将基准测试创建过程分解为规划、生成、数据验证和评估四个步骤，每个步骤都由LLM代理执行。这些代理相互交互，并利用基准测试开发者的人机反馈来显式改进和灵活控制数据的多样性和质量。我们使用BENCHAGENTS创建用于评估文本生成过程中规划和约束满足能力的基准测试。然后，我们使用这些基准测试研究七种最先进的模型，并提取关于常见失败模式和模型差异的新见解。|
|**2024-10-29**|**Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents**|Jaekyeom Kim et.al.|[2410.22552](http://arxiv.org/abs/2410.22552)|null|在本文中，我们介绍了Auto-Intent方法，这是一种在不直接进行微调的情况下将预训练的大规模语言模型（LLM）作为目标领域代理的方法，特别关注网页导航任务。我们的方法首先从目标领域的演示中无监督地发现潜在的意图，以高度紧凑的形式（最多三个词）。通过提取的意图，我们训练意图预测器来根据代理过去的观察和行为预测下一个意图。特别是，我们提出了一种自我探索方法，其中概率最高的前k个意图预测被用作提示提供给预训练的LLM代理，从而增强其决策能力。Auto-Intent显著提高了GPT-3.5、GPT-4和Llama-3.1-70B、Llama-3.1-405B代理在大规模真实网站导航基准（来自Mind2Web）和在线导航任务（来自WebArena）上的性能，并且其跨基准的泛化能力也得到了验证。|
|**2024-10-29**|**SceneGenAgent: Precise Industrial Scene Generation with Coding Agent**|Xiao Xia et.al.|[2410.21909](http://arxiv.org/abs/2410.21909)|**[link](https://github.com/thudm/scenegenagent)**|**工业场景的建模对于工业制造中的模拟至关重要。尽管大型语言模型（LLMs）在从文本描述生成一般3D场景方面已经取得了显著进展，但使用LLMs生成工业场景面临着独特的挑战，因为这些场景需要精确的尺寸和定位，这要求对空间布局进行复杂的规划。为了解决这一挑战，我们引入了SceneGenAgent，这是一种基于LLM的代理，用于通过C#代码生成工业场景。SceneGenAgent通过结构化和可计算的格式、布局验证以及迭代优化来确保精确的布局规划，以满足工业场景的定量需求。实验结果表明，由SceneGenAgent驱动的LLMs超过了它们原有的性能，在实际工业场景生成任务中的成功率达到了81.0%，并有效地满足了大多数场景生成需求。为了进一步提高可访问性，我们构建了SceneInstruct，这是一个专门用于微调开源LLMs以集成到SceneGenAgent中的数据集。实验显示，基于SceneInstruct对开源LLMs进行微调可以获得显著的性能提升，Llama3.1-70B的性能接近GPT-4o。我们的代码和数据可在<https://github.com/THUDM/SceneGenAgent>获取。**|
|**2024-10-28**|**Can Machines Think Like Humans? A Behavioral Evaluation of LLM-Agents in Dictator Games**|Ji Ma et.al.|[2410.21359](http://arxiv.org/abs/2410.21359)|null|随着基于大型语言模型（LLM）的代理越来越多地承担现实世界任务并与人类社会互动，我们对它们的行为了解多少？本研究（1）调查了不同人格如何诱导LLM代理的亲社会行为——一种基本的社会规范，并将其与人类行为进行基准测试；（2）引入了一种行为方法来评估LLM代理在复杂决策场景中的表现。我们探讨了不同人格和实验框架如何影响这些AI代理在独裁者博弈中的利他行为，并比较了同一LLM家族内、不同LLM家族之间以及与人类行为之间的差异。我们的发现揭示了LLM之间存在显著的差异和不一致性，并且与人类行为相比也有明显区别。仅仅赋予LLM类似人类的身份并不能产生类似人类的行为。尽管这些AI代理是在大量由人类生成的数据上训练的，但它们无法准确预测人类的决定。LLM代理无法捕捉到人类决策过程的内部机制，其与人类行为的一致性高度依赖于特定的模型架构和提示形式；更糟糕的是，这种依赖并不遵循明确的模式。|
|**2024-10-28**|**Automatic Generation of Benchmarks and Reliable LLM Judgment for Code Tasks**|Eitan Farchi et.al.|[2410.21071](http://arxiv.org/abs/2410.21071)|null|大语言模型（LLMs）可以用于多种与代码相关的任务，例如从一种编程语言翻译到另一种编程语言、实现自然语言需求和代码总结。最先进的大语言模型技术生成的工件有望在用户进行少量简单修改后即可使用。然而，量化这种模糊的概念具有挑战性，因此很难确定与代码相关的LLM解决方案的质量。我们称使用LLM判断来评估LLM解决方案的方法为“LLM作为裁判”，简称LaaJ。在这项工作中，我们介绍了一种生成和评估LaaJ实施的方法论，并利用自动产生的基准进行评估。该基准的目的是双重的，即用于开发和验证LaaJs，以及验证和测试使用LaaJs的大语言模型代码相关解决方案。为此，我们开发了一个自动基准生成引擎，该引擎为多种代码相关任务生成多种编程语言的代码，并将其作为LaaJ评估的输入。我们利用代码相关生成的图形表示G，其中图的顶点是生成的工件，边代表可能的生成，例如从自然语言需求生成Java程序。通过利用LLM代理链和G，我们生成与代码相关的工件。利用G中的循环，我们制定对生成工件的期望。利用这些制定的期望，可以开发和测试可靠的LLM判断，以衡量解决方案生成的工件的有用性。我们的方法能够创建高质量的代码任务解决方案。|
|**2024-10-28**|**Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments**|Sangmim Song et.al.|[2410.20666](http://arxiv.org/abs/2410.20666)|null|导航对于视觉障碍人士（PVI）来说是一个重大挑战。虽然传统的辅助工具如白色手杖和导盲犬非常宝贵，但它们在提供详细的环境信息和精确引导到目的地方面仍显不足。最近大型语言模型（LLM）和视觉-语言模型（VLM）的发展为增强辅助导航提供了新的途径。在本文中，我们介绍了一种名为Guide-LLM的具身化LLM基代理，旨在帮助视觉障碍人士在大型室内环境中导航。我们的方法采用了一种新颖的基于文本的拓扑图，使LLM能够使用简化的环境表示来规划全局路径，重点关注直线路径和直角转弯，以促进导航。此外，我们利用LLM的常识推理进行危险检测，并根据用户偏好进行个性化路径规划。模拟实验表明该系统在引导视觉障碍人士方面的有效性，突显了其作为辅助技术显著进步的潜力。结果表明，Guide-LLM能够提供高效、适应性强且个性化的导航辅助，指出了该领域有希望的发展前景。|
|**2024-10-27**|**TrajAgent: An Agent Framework for Unified Trajectory Modelling**|Yuwei Du et.al.|[2410.20445](http://arxiv.org/abs/2410.20445)|**[link](https://github.com/tsinghua-fib-lab/trajagent)**|**轨迹建模，包括轨迹数据模式挖掘和未来预测的研究，在生活服务、城市交通和公共管理等领域有着广泛的应用。针对特定问题，已经提出了许多方法来解决轨迹建模中的各种问题。然而，由于数据的异质性和任务的多样性，实现统一的轨迹建模仍然是一个重要的挑战。在本文中，我们提出了一种基于大型语言模型的代理框架TrajAgent，以统一各种轨迹建模任务。在TrajAgent中，我们首先开发了UniEnv，这是一个具有统一数据和模型接口的执行环境，支持各种模型的执行和训练。在此基础上，我们引入了TAgent，这是一种针对各种轨迹任务自动进行轨迹建模的代理工作流程。具体来说，我们在TAgent中设计了AutOpt，一个系统性的优化模块，进一步提高了集成模型的性能。通过输入自然语言的不同轨迹任务，TrajAgent能够通过训练和执行适当的模型自动生成有竞争力的结果。在四个真实世界数据集上进行的四个任务的大量实验表明，TrajAgent在统一轨迹建模方面是有效的，与基线方法相比，平均性能提高了15.43%。**|
|**2024-10-25**|**Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models**|Danqing Wang et.al.|[2410.20007](http://arxiv.org/abs/2410.20007)|null|提升大型语言模型（LLMs）的推理能力对于使其能够解决复杂的多步问题至关重要。多智能体框架在增强LLMs的推理能力方面显示出巨大潜力。然而，LLM智能体之间缺乏有效的合作限制了它们的表现，特别是在多步推理任务中。本文提出了一种新颖的合作多智能体推理框架（CoPlanner），通过分离推理步骤并将不同的任务分配给不同的智能体来实现。CoPlanner由两个LLM智能体组成：规划智能体和推理智能体。规划智能体提供高层次的战略提示，而推理智能体则遵循这些提示并推导出答案。通过通过近端策略优化（PPO）训练规划智能体的策略，基于LLaMA-3-8B的CoPlanner在LogiQA上比之前最好的方法提高了9.94%，在BBH上提高了3.09%。我们的结果表明，规划智能体的指导以及智能体之间的有效合作对CoPlanner在解决多步推理问题方面的优越性能起到了重要作用。|
|**2024-10-29**|**Reinforcement Learning for Aligning Large Language Models Agents with Interactive Environments: Quantifying and Mitigating Prompt Overfitting**|Mohamed Salim Aissi et.al.|[2410.19920](http://arxiv.org/abs/2410.19920)|null|强化学习（RL）是一种有前景的方法，可以将大型语言模型（LLMs）的知识应用于顺序决策任务。然而，很少有研究深入探讨在特定环境中使用RL微调这些模型对其能力的影响。本文提出了一种新颖的框架，用于分析在文本环境中进行RL训练后，LLM代理对提示格式的敏感性。我们的研究结果表明，当面对与RL训练阶段所使用的不同的提示格式时，LLM的性能会下降。此外，我们通过检查模型的内部表示和显著标记来分析这种敏感性的来源。最后，我们提出使用对比损失来减轻这种敏感性，并提高LLM的鲁棒性和泛化能力。|
|**2024-10-25**|**Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models**|Liam Barkley et.al.|[2410.19385](http://arxiv.org/abs/2410.19385)|null|大型语言模型（LLMs）是通过大量人类可读的文本训练而成的强大计算模型，使它们能够执行通用的语言理解和生成任务。这些模型因其在各种自然语言处理（NLP）任务中的卓越表现而在行业和学术界引起了广泛关注。尽管取得了这些成功，LLMs经常会产生不准确的情况，通常称为幻觉。提示工程，即设计和制定指令以使LLMs执行特定任务的过程，已成为减轻幻觉的关键方法。本文对不同的提示策略和框架进行了全面的经验评估，旨在减少LLMs中的幻觉。各种提示技术被应用于广泛的基准数据集，以评估每种方法的准确性和幻觉率。此外，本文还研究了工具调用代理（具有外部工具增强其能力以超越语言生成的LLMs）对同一基准数据集中幻觉率的影响。研究结果表明，最佳提示技术取决于问题类型，并且在减少幻觉方面，简单的技术往往比复杂的方法更有效。此外，研究表明，由于外部工具使用的复杂性增加，LLM代理可能会表现出更高的幻觉率。|
|**2024-10-25**|**Designing LLM-Agents with Personalities: A Psychometric Approach**|Muhua Huang et.al.|[2410.19238](http://arxiv.org/abs/2410.19238)|null|本文介绍了一种新颖的方法，用于使用五大人格框架为基于大语言模型的代理（Agent）分配可量化、可控且经过心理测量验证的人格特质。研究旨在克服人类主体研究的限制，提出代理作为社会科学研究的一种可访问工具。通过四项研究，本研究展示了为代理分配心理测量有效人格特质的可行性，并使其能够复制复杂的人类行为。第一项研究在大型语言模型的语义空间中建立了对人格结构和人格测试的理解。随后的两项研究利用实证数据和模拟数据展示了创建代理的过程，并通过显示人类和代理在人格测试中的答案高度对应来验证结果。最后一项研究进一步通过代理在涉及风险承担和道德困境的情境下复制已知的人类人格特质与决策行为之间的相关性，从而验证了人格心理测量方法设计代理的有效性及其在社会和行为研究中的适用性。|
|**2024-10-25**|**An LLM Agent for Automatic Geospatial Data Analysis**|Yuxing Chen et.al.|[2410.18792](http://arxiv.org/abs/2410.18792)|null|大型语言模型（LLMs）在数据科学代码生成任务中被广泛应用，但它们在处理复杂顺序任务时常常遇到逻辑错误的问题。特别是在处理地理空间数据时，这些模型面临着整合复杂数据结构和空间约束、有效利用各种函数调用以及较少使用的地理空间库方面容易产生幻觉的挑战。为了解决这些问题，我们引入了GeoAgent，这是一种新的交互框架，旨在帮助LLMs更有效地处理地理空间数据处理任务。GeoAgent首创性地将代码解释器、静态分析和基于检索的生成（RAG）技术与蒙特卡洛树搜索（MCTS）算法相结合，提供了一种新颖的地理空间数据处理方法。此外，我们还贡献了一个专门设计的新基准，用于评估基于LLMs的方法在地理空间任务中的表现。该基准利用了多种Python库，并包括从数据获取、数据分析到可视化的单轮和多轮任务。通过在各种地理空间环境中提供全面的评估，这个基准为开发LLMs在地理空间数据分析任务中的应用设定了新标准。我们的研究结果表明，仅依靠LLMs的知识对于准确编程地理空间任务是不够的，这需要连贯的多步骤过程和多次函数调用。与基线LLMs相比，提出的GeoAgent展示了卓越的性能，在函数调用和任务完成方面取得了显著的改进。此外，这些结果为未来LLMs代理在自动地理空间数据分析任务编程的发展提供了宝贵的见解。|
|**2024-10-24**|**PRACT: Optimizing Principled Reasoning and Acting of LLM Agent**|Zhiwei Liu et.al.|[2410.18528](http://arxiv.org/abs/2410.18528)|null|我们介绍了Principled Reasoning and Acting (PRAct)框架，这是一种新颖的方法，可以从轨迹数据中学习和执行行动原则。我们的方法的核心是使用来自反思和优化引擎的文本梯度来推导这些行动原则。为了使行动原则适应特定任务要求，我们提出了一种新的优化框架，称为Reflective Principle Optimization (RPO)。在执行后，RPO使用反思器来批评当前的行动原则，并使用优化器相应地更新它们。我们在两种场景下开发了RPO框架：Reward-RPO，它使用环境奖励进行反思；以及Self-RPO，它在没有外部奖励的情况下进行自我反思。此外，我们还介绍了两种RPO方法，RPO-Traj和RPO-Batch，以适应不同的设置。实验结果表明，在四个环境中，利用RPO框架的PRAct代理能够有效学习并应用行动原则以提高性能。|
|**2024-10-23**|**GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration**|Xin Li et.al.|[2410.18032](http://arxiv.org/abs/2410.18032)|**[link](https://github.com/bupt-gamma/graphteam)**|**图在现实世界场景中，如社交网络和城市计算中被广泛用于建模关系数据。现有的基于大型语言模型（LLM）的图分析方法要么集成了特定机器学习任务的图神经网络（GNN），限制了其可迁移性，要么完全依赖于LLM自身的推理能力，导致性能不佳。为了解决这些局限性，我们利用了LLM基代理的最新进展，这些代理展示了利用外部知识或工具解决问题的能力。通过模拟人类的问题解决策略，如类比和协作，我们提出了一种基于LLM的多代理系统，称为GraphTeam，用于图分析。GraphTeam由三个模块中的五个LLM基代理组成，具有不同专长的代理可以相互协作以解决复杂问题。具体来说，（1）输入-输出规范化模块：问题代理从原始问题中提取并提炼出四个关键参数，便于理解问题，答案代理则将结果组织成符合输出要求的形式；（2）外部知识检索模块：我们首先构建了一个包含相关文档和经验信息的知识库，然后搜索代理为每个问题检索最相关的条目。（3）问题解决模块：给定搜索代理检索到的信息，编码代理使用编程方法生成解决方案；如果编码代理不起作用，推理代理将直接进行计算而无需编程。在六个图分析基准上的大量实验表明，GraphTeam达到了最先进的性能，在准确率方面比最好的基线平均提高了25.85%。代码和数据可在https://github.com/BUPT-GAMMA/GraphTeam 获取。**|
|**2024-10-25**|**MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC Meeting**|Sungil Seok et.al.|[2410.18012](http://arxiv.org/abs/2410.18012)|null|美国联邦基金利率在国内外金融市场中扮演着重要角色。然而，研究主要集中在该利率调整的影响上，而非决策过程本身。最近大型语言模型（LLM）的发展为重建原始的联邦公开市场委员会（FOMC）会议提供了可能，这些会议负责设定联邦基金利率。本文提出了一种五阶段的FOMC会议模拟框架MiniFed，该框架使用LLM代理来模拟现实世界中的FOMC会议成员，并优化FOMC结构。这一框架有效地重新激活了FOMC会议流程，并促进了对联邦基金利率的预测。实验结果表明，我们提出的MiniFed框架在联邦基金利率预测方面达到了高准确度，并且代理的行为与现实世界的对应者保持一致。鉴于目前很少有研究利用LLM代理来模拟大规模的现实世界会议，我们的工作可以作为未来发展的基准。|
|**2024-10-22**|**SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning**|Yizhou Chi et.al.|[2410.17238](http://arxiv.org/abs/2410.17238)|**[link](https://github.com/geekan/metagpt)**|**自动化机器学习（AutoML）方法包括传统的优化固定管道以进行模型选择和集成的方法，以及基于最新大语言模型（LLM）的框架，这些框架可以自主构建管道。尽管基于LLM的代理在自动化机器学习任务方面显示出潜力，但它们通常生成低多样性和次优的代码，即使经过多次迭代也是如此。为了克服这些限制，我们引入了树搜索增强型LLM代理（SELA），这是一种创新的代理系统，利用蒙特卡洛树搜索（MCTS）来优化AutoML过程。通过将管道配置表示为树结构，我们的框架使代理能够智能地进行实验，并迭代地优化其策略，从而更有效地探索机器学习解决方案空间。这一新颖的方法允许SELA根据实验反馈发现最优路径，提高解决方案的整体质量。在跨越20个机器学习数据集的广泛评估中，我们比较了传统和基于代理的AutoML方法的性能，结果表明，在所有数据集中，SELA相对于每个基线的胜率为65%到80%。这些结果强调了基于代理策略在AutoML中的巨大潜力，为解决复杂的机器学习挑战提供了新的视角。**|
|**2024-10-22**|**EnvBridge: Bridging Diverse Environments with Cross-Environment Knowledge Transfer for Embodied AI**|Tomoyuki Kagaya et.al.|[2410.16919](http://arxiv.org/abs/2410.16919)|null|近年来，大型语言模型（LLMs）在推理能力方面表现出色，引起了广泛关注，尤其是在各种决策过程中的应用。LLM代理的一个特别有前景的应用是机器人操作。最近的研究表明，LLMs可以为机器人生成文本规划或控制代码，提供了极大的灵活性和交互能力。然而，这些方法在灵活性和跨不同环境的适用性方面仍面临挑战，限制了它们自主适应的能力。目前的方法通常分为两类：一类依赖于特定环境的策略训练，这限制了其可移植性；另一类基于固定提示生成代码动作，在面对新环境时性能会下降。这些局限性显著制约了代理在机器人操作中的通用性。为了解决这些局限性，我们提出了一种名为EnvBridge的新方法。这种方法涉及从源环境保留和转移成功的机器人控制代码到目标环境。EnvBridge通过利用多个环境的见解，增强了代理在多样化设置中的适应性和性能。值得注意的是，我们的方法缓解了环境约束，提供了一个更灵活和通用的机器人操作任务解决方案。我们使用机器人操作基准测试RLBench、MetaWorld和CALVIN验证了该方法的有效性。实验结果表明，LLM代理能够成功利用多样化的知识来源解决复杂任务。因此，我们的方法显著提高了机器人操作代理在多样化环境中规划的适应性和鲁棒性。|
|**2024-10-22**|**CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing**|Chen Yang et.al.|[2410.16670](http://arxiv.org/abs/2410.16670)|**[link](https://github.com/uclaml/cops)**|**在代理系统中，基于大型语言模型（LLMs）的顺序推理已经取得了显著进展，但现有方法仍面临一些限制。反思驱动的推理完全依赖于预训练模型中的知识，这在新颖场景中的表现往往受限；而经验辅助的推理则常常依赖外部经验，并且缺乏选择代表性经验的明确原则。我们通过提出CoPS（跨任务经验共享）算法来解决这些限制，这是一种能够通过跨任务经验共享和选择来增强顺序推理的通用算法。具体来说，CoPS利用代理在先前任务中的经验，通过一种基于悲观策略的方法选择分布匹配的经验，以最大化效用并最小化因分布变化带来的风险。在Alfworld、Webshop和HotPotQA等基准测试中进行的广泛实验结果表明，CoPS始终优于最先进的基线方法，并具有适用于资源受限场景的优越样本效率。从理论上讲，我们的算法性能取决于预训练LLM的质量以及代理的任务相关试验分布与LLM生成分布之间的匹配度。我们的工作填补了现有顺序推理范式之间的空白，并验证了利用跨任务经验的有效性，这为提高代理在多样化任务中的泛化能力和适应性提供了潜在途径。我们的代码可在<https://github.com/uclaml/COPS>获取。**|
|**2024-10-22**|**Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent**|Janghoon Ock et.al.|[2410.16658](http://arxiv.org/abs/2410.16658)|**[link](https://github.com/hoon-ock/catalystaigent)**|吸附能是催化中的一个重要反应描述符，能够实现潜在催化剂的高效筛选。然而，确定吸附能需要比较多种吸附物-催化剂构型的能量，由于可能的构型数量庞大，这在计算上非常耗时。当前的算法方法通常会枚举吸附位点和构型，而不会利用理论见解来指导初始设置。在这项工作中，我们介绍了一种名为Adsorb-Agent的大语言模型（LLM）代理，旨在以最小的人工干预高效地推导出系统特定的稳定吸附构型。Adsorb-Agent利用内置知识和新兴推理能力，显著减少了所需的初始构型数量，同时提高了预测最低吸附能的准确性。我们通过两个实例系统NNH-CuPd3(111)和NNH-Mo3Pd(111)，用于氮还原反应（NRR），这是一种可持续替代哈伯-博施工艺的方法，展示了其性能。Adsorb-Agent通过识别能量更低且初始设置更少的构型，优于传统的“启发式”和“随机”算法，从而降低了计算成本并提高了准确性。这凸显了它加速催化剂发现的潜力。|
|**2024-10-23**|**IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in Communicative Multi-Agent Systems**|Yihuan Mao et.al.|[2410.16237](http://arxiv.org/abs/2410.16237)|null|随着大型语言模型（LLM）代理越来越多地集成到我们的基础设施中，它们的稳健协调和消息同步变得至关重要。拜占庭将军问题（BGP）是构建在对抗性攻击下具有弹性的多智能体系统（MAS）的关键模型。该问题描述了一种情景，其中系统内存在恶意代理且这些代理的身份未知——在我们的情境中，这种情况可能是由LLM代理的幻觉或外部攻击引起的。在BGP中，整个系统的目的是就采取的行动达成共识。传统的BGP需要所有代理之间的全局共识；然而，在实际场景中，全局共识并非总是必要，甚至可能效率低下。因此，迫切需要探索一种与MAS中观察到的局部协调模式相一致的改进版BGP。我们在研究中将这种改进版称为不完美BGP（IBGP），旨在解决这一差异。为了解决这个问题，我们提出了一种框架，该框架利用了一般MAS环境中的共识协议，提供了对通信攻击的可证明弹性以及适应不断变化的环境的能力，并通过实证结果进行了验证。此外，我们还提供了一个传感器网络环境中的案例研究，以说明我们协议的实际应用。|
|**2024-10-21**|**NetSafe: Exploring the Topological Safety of Multi-agent Networks**|Miao Yu et.al.|[2410.15686](http://arxiv.org/abs/2410.15686)|null|大型语言模型（LLMs）已经赋予了多智能体网络中的节点以智能，这些模型在学术界和工业界的应用日益广泛。然而，如何防止这些网络生成恶意信息仍然是一个未被充分探索的问题，以前关于单个LLM安全性的研究难以直接转移应用。本文从拓扑学的角度关注多智能体网络的安全性，探讨哪些拓扑特性有助于更安全的网络。为此，我们提出了一种通用框架NetSafe以及一种迭代RelCom交互，以统一现有的各种基于LLM的代理框架，为一般化的拓扑安全性研究奠定基础。我们发现当多智能体网络受到涉及虚假信息、偏见和有害信息的攻击时，会出现几种关键现象，称为代理幻觉和聚合安全性。此外，我们发现高度连接的网络更容易受到对抗性攻击的影响，在星形图拓扑结构下任务性能下降了29.7%。此外，我们提出的静态度量比传统的图论度量更接近现实世界的动态评估，表明距离攻击者平均距离更大的网络表现出更高的安全性。总之，我们的工作引入了一个新的视角来探讨基于LLM的多智能体网络的安全性，并发现了几个未报道的现象，为未来探索此类网络的安全性铺平了道路。|
|**2024-10-20**|**Who is Undercover? Guiding LLMs to Explore Multi-Perspective Team Tactic in the Game**|Ruiqi Dong et.al.|[2410.15311](http://arxiv.org/abs/2410.15311)|null|大型语言模型（LLMs）在复杂任务中扮演着关键的AI角色，但在复杂场景中的开放式决策问题中仍面临挑战。为此，我们使用语言逻辑游戏“谁是卧底？”（WIU）作为实验平台，提出了多视角团队战术（MPTT）框架。MPTT旨在培养LLMs在复杂场景中的人类语言表达逻辑、多维思维和自我感知。通过交替进行发言和投票环节，并结合自我视角、身份确定、自我反思、自我总结和多轮找队友等技术，LLM代理通过策略性隐藏和沟通作出理性决策，促进人类信任的形成。初步结果显示，MPTT结合WIU利用了LLMs的认知能力，创建了一个可以模拟真实社会的决策框架。该框架有助于少数群体的沟通与表达，促进了决策过程中的公平性和多样性。此外，我们的“人在回路”实验表明，LLMs可以通过互动学习并适应人类行为，这表明它们有潜力积极参与社会决策。|
|**2024-10-20**|**When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?**|Shang Wang et.al.|[2410.15267](http://arxiv.org/abs/2410.15267)|null|大型语言模型（LLMs）如ChatGPT和Gemini的部署展示了它们强大的自然语言生成能力。然而，在训练过程中，这些模型可能会无意中学到并保留敏感信息和有害内容，这引发了重大的伦理和法律问题。为了解决这些问题，提出了机器遗忘作为潜在解决方案。尽管现有的遗忘方法考虑了LLMs的具体特性，但它们通常面临高计算需求、有限适用性或灾难性遗忘的风险。为了应对这些局限性，我们提出了一种基于检索增强生成（RAG）技术的轻量级遗忘框架。通过修改RAG的外部知识库，我们在不直接与未学习的LLM交互的情况下模拟遗忘的效果。我们将构建遗忘知识视为一个约束优化问题，并推导出两个关键组件，以支持基于RAG的遗忘的有效性。这种基于RAG的方法对于闭源LLMs特别有效，而现有遗忘方法往往在这些模型上失效。我们通过广泛的实验对我们的框架进行了评估，包括在开源和闭源模型上进行测试，涵盖了ChatGPT、Gemini、Llama-2-7b-chat-hf和PaLM 2。结果显示，我们的方法满足了五个关键的遗忘标准：有效性、通用性、无害性、简单性和鲁棒性。此外，该方法可以扩展到多模态大语言模型和基于LLM的代理。|
|**2024-10-19**|**SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation**|Jingxuan Chen et.al.|[2410.15164](http://arxiv.org/abs/2410.15164)|**[link](https://github.com/ai-agents-2030/SPA-Bench)**|智能手机代理在帮助用户高效控制设备方面变得越来越重要，多模态大型语言模型（MLLM）方法成为关键的竞争者。然而，公平比较这些代理既重要又具有挑战性，需要多样化的任务范围、集成不同实现方式的代理以及通用的评估管道来评估它们的优势和劣势。本文介绍了SPA-Bench，这是一个综合的智能手机代理基准测试，旨在评估基于（M）LLM的代理在一个模拟现实世界条件的交互环境中。SPA-Bench有三个主要贡献：（1）涵盖系统应用和第三方应用的任务集，包括英语和中文，重点是日常生活中常用的功能；（2）一个即插即用框架，支持与Android设备的实时交互，集成了超过十个代理，并且可以灵活添加更多代理；（3）一种新颖的评估管道，自动从多个维度评估代理性能，包括七个与任务完成和资源消耗相关的指标。我们通过广泛的实验揭示了这些代理在解释移动用户界面、动作定位、记忆保留和执行成本等方面面临的挑战。我们提出了未来的研究方向以缓解这些问题，从而更接近实际的智能手机代理应用。|
|**2024-10-22**|**Imprompter: Tricking LLM Agents into Improper Tool Use**|Xiaohan Fu et.al.|[2410.14923](http://arxiv.org/abs/2410.14923)|**[link](https://github.com/Reapor-Yurnero/imprompter)**|**大型语言模型（LLM）代理是一种新兴的计算范式，它结合了生成式机器学习与代码解释器、网页浏览、电子邮件等工具，以及更广泛的外部资源。这些基于代理的系统代表了个人计算领域的一个新兴转变。我们为基于代理系统的安全基础做出贡献，并提出了新的自动计算的对抗性提示攻击，这些攻击侵犯了用户资源的机密性和完整性。我们展示了如何在给定模型权重的情况下，利用提示优化技术自动生成这样的提示。我们证明这种攻击可以转移到生产级别的代理上。例如，我们展示了对Mistral的LeChat代理的信息窃取攻击，该攻击分析用户的对话，挑选出个人身份信息，并将其格式化为有效的markdown命令，从而将这些数据泄露到攻击者的服务器上。这种攻击在端到端评估中显示出了近80%的成功率。我们进行了一系列实验来表征这些攻击的有效性，并发现它们在新兴的基于代理的系统如Mistral的LeChat、ChatGLM和Meta的Llama中都能可靠地工作。这些攻击是多模态的，我们在文本和图像领域展示了不同的变体。**|
|**2024-10-18**|**When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**|Hanna Kim et.al.|[2410.14569](http://arxiv.org/abs/2410.14569)|null|近年来，大型语言模型（LLMs）的发展使其成为能够规划和与各种工具交互的自主系统。这些LLM代理通常与基于网络的工具结合使用，从而能够访问多样化的信息源和实时数据。尽管这些进展在各种应用中带来了显著的好处，但它们也增加了恶意使用的风险，特别是在涉及个人隐私信息的网络攻击中。在这项工作中，我们调查了LLM代理在涉及个人数据的网络攻击中的误用风险。具体而言，我们旨在了解：1）当指导LLM代理进行网络攻击时，其潜在的能力；2）基于网络的工具如何增强网络攻击；以及3）利用LLM代理发起网络攻击变得多么经济实惠和容易。我们考察了三种攻击场景：收集个人身份信息（PII）、生成冒充帖子和创建定向钓鱼邮件。我们的实验揭示了LLM代理在这类攻击中的有效性：LLM代理在收集PII方面的准确率高达95.9%，由LLM代理生成的冒充帖子中有高达93.9%被评估为真实，而由LLM代理创建的定向钓鱼邮件中的链接点击率达到了46.67%。此外，我们的研究还强调了现有商业LLM中的安全防护措施的局限性，强调了迫切需要更强大的安全措施来防止LLM代理的误用。|
|**2024-10-18**|**Do LLMs "know" internally when they follow instructions?**|Juyeon Heo et.al.|[2410.14516](http://arxiv.org/abs/2410.14516)|**[link](https://github.com/apple/ml-internal-llms-instruction-following)**|指令跟随对于构建具有大型语言模型（LLMs）的AI代理至关重要，因为这些模型必须严格遵循用户提供的约束和指南。然而，LLMs经常无法遵循即使是简单且明确的指令。为了提高指令跟随的成功率并防止不期望的输出，需要更深入地理解LLMs的内部状态与这些结果之间的关系。我们对LLM的内部状态进行分析，发现输入嵌入空间中存在一个维度，与成功的指令跟随相关联。我们证明，沿着这个维度修改表示可以提高指令跟随的成功率，而不会损害响应质量。进一步研究显示，这个维度与提示的措辞关系更为密切，而不是任务或指令的固有难度。这一发现还解释了为什么LLMs有时无法遵循清晰的指令，以及为什么即使内容基本不变，提示工程往往有效的原因。这项工作揭示了LLMs指令跟随的内部机制，为可靠LLM代理的开发铺平了道路。|
|**2024-10-18**|**CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic**|Huaiyuan Yao et.al.|[2410.14368](http://arxiv.org/abs/2410.14368)|**[link](https://github.com/hyan-yao/comal)**|**在城市交通中引入自动驾驶车辆具有巨大的潜力，可以通过减少拥堵和系统地优化交通流量来提高效率。本文介绍了一种名为CoMAL（协作多智能体大语言模型）的框架，旨在通过自动驾驶车辆之间的协作解决混合自主交通问题，从而优化交通流量。CoMAL基于大型语言模型，在交互式交通仿真环境中运行。它利用感知模块观察周围代理，并使用记忆模块存储每个代理的策略。整体工作流程包括一个协作模块，鼓励自动驾驶车辆讨论有效的策略并分配角色，一个推理引擎根据分配的角色确定最优行为，以及一个执行模块使用结合了基于规则模型的混合方法控制车辆动作。实验结果表明，CoMAL在Flow基准测试中表现出色。此外，我们评估了不同语言模型的影响，并将其框架与强化学习方法进行了比较。这突显了LLM代理的强大合作能力，并提出了一个有前景的解决方案来应对混合自主交通挑战。代码可在https://github.com/Hyan-Yao/CoMAL获取。**|
|**2024-10-18**|**Good Parenting is all you need -- Multi-agentic LLM Hallucination Mitigation**|Edward et.al.|[2410.14262](http://arxiv.org/abs/2410.14262)|null|本研究探讨了大型语言模型（LLM）代理检测和纠正AI生成内容中幻觉现象的能力。一个主要代理被任务创建一篇关于一位虚构的丹麦艺术家Flipfloppidy的博客，然后由另一个代理进行审查以识别事实性错误。大多数LLM模型幻化出了这位艺术家的存在。在涉及各种主代理和审查代理组合的4900次测试运行中，先进的AI模型如Llama3-70b和GPT-4变体在识别幻觉方面几乎达到了完美的准确率，并且在收到反馈后成功修正了输出内容的85%到100%。这些发现强调了先进AI模型在显著提高生成内容的准确性和可靠性方面的潜力，为改进AI工作流编排提供了一种有前景的方法。|
|**2024-10-18**|**Agents4PLC: Automating Closed-loop PLC Code Generation and Verification in Industrial Control Systems using LLM-based Agents**|Zihan Liu et.al.|[2410.14209](http://arxiv.org/abs/2410.14209)|**[link](https://github.com/Luoji-zju/Agents4PLC_release)**|在工业控制系统中，可编程逻辑控制器（PLC）代码的生成和验证对于确保运行效率和安全性至关重要。尽管大型语言模型（LLM）在自动化代码生成方面取得了进展，但它们通常无法提供正确性保证，并且缺乏对PLC编程的专业支持。为了解决这些挑战，本文介绍了一种名为Agents4PLC的新框架，该框架不仅实现了PLC代码的自动化生成，还通过基于LLM的多代理系统进行了代码级别的验证。我们首先建立了一个全面的基准，用于可验证的PLC代码生成领域，从自然语言需求过渡到人工编写和验证的形式化规范和参考PLC代码。此外，我们通过结合检索增强生成（RAG）、先进的提示工程技术和链式思维策略，进一步增强了针对工业控制系统的“代理”。评估表明，Agents4PLC显著优于先前的方法，在一系列日益严格的指标上均取得了优异的结果。这项研究不仅解决了PLC编程中的关键挑战，还展示了我们的框架生成适用于实际工业应用的可验证代码的潜力。|
|**2024-10-18**|**Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs**|SeongYeub Chu et.al.|[2410.14202](http://arxiv.org/abs/2410.14202)|null|现有的自动作文评分（AES）仅依赖于作文文本，而未使用解释性理由分数，因此错失了以细粒度方式捕捉评分标准中特定评估方面的机会。本文介绍了一种名为基于论据的多特征评分（RMTS）的新方法，该方法结合了基于提示的大语言模型（LLMs）和使用较小的大语言模型（S-LLM）的微调式作文评分模型。RMTS 使用基于LLM的特征论据生成系统，其中单独的LLM代理根据评分标准指南生成特征特定的理由，评分模型利用这些理由准确预测多特征分数。在基准数据集（包括ASAP、ASAP++和Feedback Prize）上的广泛实验表明，RMTS 在特征特定评分方面显著优于最先进的模型和普通的S-LLM。通过辅助定量评估以提供细粒度的定性理由，RMTS 提高了特征评分的可靠性，并提供了关于作文的部分解释。|
|**2024-10-18**|**SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent**|Jiarui Ji et.al.|[2410.14152](http://arxiv.org/abs/2410.14152)|**[link](https://github.com/jijiarui-cather/srapagent_framework)**|公共稀缺资源配置在经济学中扮演着至关重要的角色，因为它直接影响到社会的效率和公平性。传统研究方法，包括基于理论模型、基于实证研究和基于仿真的方法，由于存在理想化的完全信息和个体理性的假设以及有限可用数据的限制，面临着局限性。在这项工作中，我们提出了一种创新框架SRAP-Agent（使用基于大语言模型的智能体模拟和优化稀缺资源配置政策），该框架将大型语言模型（LLMs）集成到经济仿真中，旨在弥合理论模型与现实动态之间的差距。以公共住房分配场景作为案例研究，我们进行了广泛的政策仿真实验来验证SRAP-Agent的可行性和有效性，并采用具有特定优化目标的政策优化算法。源代码可以在https://github.com/jijiarui-cather/SRAPAgent_Framework找到。|
|**2024-10-17**|**From Barriers to Tactics: A Behavioral Science-Informed Agentic Workflow for Personalized Nutrition Coaching**|Eric Yang et.al.|[2410.14041](http://arxiv.org/abs/2410.14041)|null|有效的管理心脏代谢状况需要持续的积极营养习惯，但这些习惯往往受到复杂且个体化的障碍影响。直接的人类管理难以扩展，而之前的尝试旨在自动化营养辅导，但缺乏解决这些多样化挑战所需的个性化。本文介绍了一种新颖的基于大型语言模型（LLM）的主动工作流程，旨在通过直接针对并缓解患者特定的障碍来提供个性化的营养辅导。该工作流程基于行为科学原则，利用了与相应循证策略相关的全面营养相关障碍映射。一个专门的LLM代理有意探查并识别患者在饮食方面的根本问题。随后，另一个LLM代理提供量身定制的策略，以克服这些特定障碍，并结合患者的具体情况。我们通过一项涉及心脏代谢疾病患者的用户研究来设计和验证我们的方法，证明了该系统能够准确识别障碍并提供个性化指导。此外，我们还通过大规模模拟研究来评估系统的性能，该研究基于真实的患者案例和专家验证的指标，在广泛的情景中进行了评估。我们的研究结果表明，这种基于LLM的主动工作流程有可能通过提供个性化、可扩展且基于行为的干预措施来改善营养辅导。|
|**2024-10-17**|**AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents**|Ke Yang et.al.|[2410.13825](http://arxiv.org/abs/2410.13825)|null|通过使用大型语言模型（LLMs）的代理实现自主性，可以提升人类在个性化和标准化任务中的效率。自动化网络任务（例如在预算内预订酒店）的需求日益增加。满足实际需求的同时，网络代理也作为各种代理接地场景的重要概念证明示例，其成功预示着许多未来应用的进步。先前的研究通常手工设计网络代理策略（例如提示模板、多代理系统、搜索方法等），这些方法可能无法在所有现实世界场景中普遍适用。另一方面，关于网络代理的观察/动作表示与基于LLM的预训练数据之间不匹配的研究有限。这种差异尤其明显，因为LLM主要针对语言完成进行训练，而不是涉及具身导航动作和符号化网络元素的任务。我们的研究通过简单地优化观察和动作空间来增强基于LLM的网络代理，使其更好地符合LLM的能力。这种方法使我们基础代理AgentOccam在各种网络任务上显著超越之前的方法。具体来说，在WebArena基准测试中，一个包含通用网络交互任务的基准，我们的代理AgentOccam比前最先进的方法和同期工作分别高出9.8（+29.4%）和5.9（+15.8%）个百分点，并且成功率达到26.6点（+161%），超过了具有相同观察和动作空间对齐的普通网络代理。我们实现了这一目标，而没有使用上下文示例、新代理角色、在线反馈或搜索策略。AgentOccam的简洁设计突显了LLMs在网页任务上的零样本性能，并强调了精心调整观察和动作空间对于基于LLM的代理的关键作用。|
|**2024-10-17**|**Rapid and Automated Alloy Design with Graph Neural Network-Powered LLM-Driven Multi-Agent Systems**|Alireza Ghafarollahi et.al.|[2410.13768](http://arxiv.org/abs/2410.13768)|null|一个多智能体AI模型被用于自动化发现新的金属合金，该模型整合了多模态数据和外部知识，包括通过原子模拟获得的物理见解。我们的多智能体系统具有三个关键组件：(a) 一组大型语言模型（LLMs）负责推理和规划等任务，(b) 一群具有不同角色和专业知识的AI代理动态协作，以及(c) 一种新开发的图神经网络（GNN）模型，用于快速检索关键物理属性。一组由LLM驱动的AI代理合作自动化探索MPEAs（高熵合金）的巨大设计空间，并通过GNN的预测进行引导。我们专注于NbMoTa系列体心立方（bcc）合金，这些合金使用基于机器学习的原子间势进行建模，并针对两个关键性质：Peierls势垒和固溶体/螺位错相互作用能。我们的GNN模型准确地预测了这些原子尺度的性质，提供了一种比昂贵的暴力计算更快的替代方法，并减轻了多智能体系统在物理检索上的计算负担。这个AI系统通过减少对人类专业知识的依赖并克服直接全原子模拟的局限性，革新了材料的发现过程。通过协同GNN的预测能力和LLM代理的动态协作，系统自主导航巨大的合金设计空间，识别原子尺度材料性质的趋势，并预测宏观尺度的机械强度，如若干个计算实验所展示的那样。这种方法加速了先进合金的发现，并有望在其他复杂系统中有更广泛的应用，标志着自动化材料设计领域的一大进步。|
|**2024-10-17**|**MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling**|Yakun Zhu et.al.|[2410.13610](http://arxiv.org/abs/2410.13610)|null|在大型语言模型（LLMs）中集成工具已经促进了其广泛应用。然而，在专门的下游任务场景中，仅依赖工具不足以完全解决现实世界的复杂性，这尤其限制了LLMs在医学等领域的有效部署。本文专注于医学计算器的下游任务，这些任务使用标准化测试来评估个人的健康状况。我们介绍了MeNTi，这是一种为LLMs设计的通用代理架构。MeNTi集成了专业的医学工具包，并采用元工具和嵌套调用机制以增强LLMs对工具的利用。具体来说，它实现了灵活的工具选择和嵌套工具调用来应对复杂的医学场景中的实际问题，包括计算器选择、插槽填充和单位转换。为了评估LLMs在整个临床过程中的计算器场景下的定量评估能力，我们引入了CalcQA基准。该基准要求LLMs使用医学计算器进行计算并评估患者的健康状况。CalcQA由专业医生构建，包含100个案例-计算器对，并附带一个包含281种医学工具的工具包。实验结果表明，我们的框架显著提升了性能。本研究为在医学的高需求场景中应用LLMs开辟了新的方向。|
|**2024-10-17**|**Chain of Ideas: Revolutionizing Research in Novel Idea Development with LLM Agents**|Long Li et.al.|[2410.13185](http://arxiv.org/abs/2410.13185)|**[link](https://github.com/damo-nlp-sg/coi-agent)**|有效的研究创意构思是科学研究的关键步骤。然而，科学文献的指数增长使得研究人员难以跟上最新的进展并确定有意义的研究方向。最近大型语言模型（LLMs）的发展表明，自动化生成新颖的研究创意是一个有前景的途径。然而，现有的创意生成方法要么简单地提示LLMs，要么直接向LLMs暴露大量文献而没有指示有用的信息。受到人类研究人员研究过程的启发，我们提出了一种称为Chain-of-Ideas（CoI）代理的方法，这是一种基于LLM的代理，它以链式结构组织相关文献，有效反映了研究领域的渐进发展。这种组织方式使LLMs能够捕捉当前的研究进展，从而增强其创意生成能力。此外，我们还提出了Idea Arena评估协议，可以从不同角度全面评估创意生成方法，与人类研究人员的偏好紧密对齐。实验结果表明，CoI代理在创意生成方面始终优于其他方法，并且其质量可与人类媲美。此外，我们的CoI代理成本低廉，生成一个候选创意及其相应实验设计的最低成本仅为0.50美元。|
|**2024-10-16**|**Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for Autonomous Driving**|Sihao Wu et.al.|[2410.12568](http://arxiv.org/abs/2410.12568)|null|大型语言模型（LLMs）在自动驾驶系统中的集成展示了强大的常识和推理能力，有效地解决了纯数据驱动方法的缺陷。当前基于LLM的代理需要较长的推理时间，并且在与实时自动驾驶环境交互时面临挑战。一个关键的开放性问题是，我们能否有效利用LLM的知识来训练一个高效且鲁棒的强化学习（RL）代理。本文介绍了一种新颖的RAPID框架，即鲁棒自适应策略注入与蒸馏框架，该框架使用由基于LLM的驾驶代理生成的数据来训练专门的混合策略RL代理，并进行在线适应。RAPID具有三个关键设计：1）利用从LLM代理收集的离线数据来蒸馏专家知识到RL策略中，以加快实时推理速度；2）引入鲁棒蒸馏到RL中，以继承LLM基础教师的表现和鲁棒性；3）采用混合策略方法，通过策略适配器进行联合决策解码。通过在线环境交互进行微调，RAPID减少了LLM知识的遗忘，同时保持了对不同任务的适应性。广泛的实验表明，RAPID能够以高效、适应性强和鲁棒的方式将LLM知识有效地整合到规模化的RL策略中。代码和检查点将在接受后公开提供。|
|**2024-10-16**|**SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling**|Loris Gaven et.al.|[2410.12481](http://arxiv.org/abs/2410.12481)|null|近年来，大规模语言模型（LLMs）不仅作为生成模型，还作为解决文本序列决策任务的代理取得了显著进展。当面对复杂环境，其零样本能力不足时，最近的研究表明，可以使用在线强化学习（RL）让这些LLM代理通过交互式方式发现和学习高效的策略。然而，大多数先前的工作仅限于采用策略梯度算法，这大大限制了这些代理在探索和利用方面可以使用的各种方法，例如经验重放和事后重标记。然而，这些方法对于LLM学习代理来说可能是关键的，尤其是在设计自主内在动机代理时，这些代理会根据自己的目标进行采样和追求（即自目的性代理）。本文提出并研究了一种适应软演员-评论家算法和事后重标记的LLM代理方法。我们的方法不仅为设计在线学习的自目的性LLM代理铺平了道路，还可以在更经典的多目标RL环境中超越策略梯度方法。|
|**2024-10-16**|**Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance**|Yaxi Lu et.al.|[2410.12361](http://arxiv.org/abs/2410.12361)|**[link](https://github.com/thunlp/proactiveagent)**|基于大型语言模型的代理在解决复杂任务方面已经展示了显著的能力。然而，大多数代理系统仍然是反应式的，这限制了它们在需要预见性和自主决策的情景中的有效性。在这篇论文中，我们致力于开发能够预见并主动发起任务的代理，而无需明确的人类指令。我们提出了一种新颖的数据驱动方法来解决这个问题。首先，我们收集真实世界的人类活动以生成主动任务预测。这些预测由人类标注者标记为接受或拒绝。标注后的数据被用于训练一个奖励模型，该模型模拟人类判断，并作为LLM代理主动性的自动评估器。在此基础上，我们开发了一个全面的数据生成管道，创建了一个包含6,790个事件的多样化数据集ProactiveBench。最后，我们证明通过使用所提出的ProactiveBench对模型进行微调可以显著激发LLM代理的主动性。实验结果表明，我们的微调模型在主动提供帮助方面的F1得分达到了66.47%，优于所有开源和闭源模型。这些结果突显了我们方法在创造更主动和有效的代理系统方面的潜力，为未来的人机协作进步铺平了道路。|
|**2024-10-16**|**Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay**|Yuyang Chen et.al.|[2410.12236](http://arxiv.org/abs/2410.12236)|null|如今，针对代码生成任务的Transformer基大规模语言模型（LLM）通常应用采样和过滤管道。由于代码生成任务中的稀疏奖励问题，即一个令牌的不正确性会导致Transformer模型采样冗余程序直到找到正确的程序，这导致了低效率。为了解决这一挑战，我们在微调阶段引入了经验回放（ER），其中产生的代码和程序会被存储并重放，以使LLM代理有机会从过去的经验中学习。基于ER的精神，我们介绍了一种称为BTP管道的新方法，该方法由三个阶段组成：束搜索采样、测试阶段和优先级经验回放阶段。该方法利用代码模型收集的失败程序，并从回放缓冲区中重放具有高可能性和通过率优先值（P2Value）的程序，以提高效率。P2Value综合考虑了Transformer输出的可能性和通过率，并可以利用大多数由LLMs收集的程序未能通过任何测试而导致的冗余资源。我们实证地将我们的方法应用于几种LLM中，证明它提升了它们在代码生成任务中的性能，并超越了现有的基线。|
|**2024-10-15**|**Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents**|Bolun Sun et.al.|[2410.11906](http://arxiv.org/abs/2410.11906)|null|本文介绍了一种将大型语言模型（LLMs）应用于增强用户对隐私政策的理解的新方法，通过交互式对话代理实现。我们展示了LLMs在数据实践识别、选择识别、政策总结和隐私问答等任务中的表现显著优于传统模型，为隐私政策分析设立了新的基准。基于这些发现，我们引入了一种创新的基于LLM的代理，该代理作为处理网站隐私政策的专家系统，能够在不需用户提供特定问题的情况下引导用户理解复杂的法律语言。一项涉及100名参与者的用户研究表明，使用该代理的用户具有更高的理解水平（平均分2.6/3，而对照组为1.8），更低的认知负荷（任务难度评分为3.2/10，而对照组为7.8），更高的隐私管理信心，并且完成任务所需时间更短（5.5分钟vs.15.8分钟）。这项工作突显了基于LLM的代理在改变用户与隐私政策互动方面的潜力，有助于获得更加知情的同意，并在数字服务领域赋予用户更多权力。|
|**2024-10-15**|**HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications**|Weijie Xu et.al.|[2410.11239](http://arxiv.org/abs/2410.11239)|null|近年来，大型语言模型（LLM）的发展在教育和金融等多个领域带来了诸多益处，但在人力资源领域，仍有许多重复性的流程未被解决，例如访问请求、医疗报销和请假申请等。我们希望将这些任务与LLM代理相关联，该代理已经在诸如写作辅助和客户服务等领域取得了成效。我们提出了HR-Agent，这是一种高效、保密且专门针对人力资源领域的基于LLM的任务导向对话系统，旨在自动化处理如医疗报销和访问请求等重复性的人力资源流程。由于在推理过程中不会将对话数据发送给LLM，因此能够保持人力资源相关任务所需的机密性。|
|**2024-10-14**|**Denial-of-Service Poisoning Attacks against Large Language Models**|Kuofeng Gao et.al.|[2410.10760](http://arxiv.org/abs/2410.10760)|**[link](https://github.com/sail-sg/p-dos)**|**近期的研究表明，大型语言模型（LLMs）容易受到拒绝服务（DoS）攻击，例如通过拼写错误或非语义提示的对抗性输入可以触发无限输出，而不会生成[EOS]终止符。这些攻击可能导致高延迟，并使LLM服务对其他用户或任务不可用。然而，在存在语音到文本接口（如机器人语音命令）的情况下，执行此类DoS攻击变得具有挑战性，因为通过语音很难引入拼写错误或非语义提示。一种简单的DoS攻击方式是指示模型“不断重复‘Hello’”，但我们观察到仅依靠自然指令会限制输出长度，该长度受最大长度限制，这是大型语言模型在有监督微调（SFT）数据中的上限。为了解决这一限制，我们提出了针对LLMs的投毒型DoS（P-DoS）攻击，证明注入一个专门设计用于DoS目的的中毒样本可以打破输出长度限制。例如，一个中毒样本成功攻击了GPT-4o和GPT-4o mini（通过OpenAI的微调API），使用不到1美元的成本，导致输出重复直至达到最大推理长度（16K个token，相比之下未中毒前为0.5K）。此外，我们在开源LLMs上进行了全面的消融研究，并将方法扩展到LLM代理，其中攻击者可以控制微调数据集和算法。我们的研究结果强调了急需防御P-DoS攻击以确保LLMs安全的迫切需求。我们的代码可以在https://github.com/sail-sg/P-DoS找到。**|
|**2024-10-14**|**FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and LLM Agents Amid Ethical Dilemmas**|Yu Lei et.al.|[2410.10398](http://arxiv.org/abs/2410.10398)|null|AI对齐是关乎AI控制和安全的关键问题。它不仅应考虑价值中立的人类偏好，还应考虑道德和伦理方面的考量。在这项研究中，我们介绍了FairMindSim，通过一系列不公平的情景来模拟道德困境。我们使用LLM代理来模拟人类行为，在各个阶段确保对齐。为了探索驱动人类和LLM代理作为旁观者在涉及他人的不公正情况下干预的各种社会经济动机，即我们所称的信念，并探讨这些信念如何相互作用以影响个体行为，我们将相关社会学领域的知识纳入其中，并基于递归奖励模型（RRM）提出了信念-奖励对齐行为进化模型（BREM）。我们的研究结果表明，从行为角度来看，GPT-4o表现出更强的社会正义感，而人类则展现出更丰富的情感。此外，我们还讨论了情绪对行为的潜在影响。本研究为LLM与利他价值观对齐的应用提供了理论基础。|
|**2024-10-14**|**Beyond-RAG: Question Identification and Answer Generation in Real-Time Conversations**|Garima Agrawal et.al.|[2410.10136](http://arxiv.org/abs/2410.10136)|null|在客户联络中心，人工客服经常面临较长的平均处理时间（AHT），因为他们需要手动解析查询并检索相关的知识库（KB）文章。虽然使用大型语言模型（LLM）的检索增强生成（RAG）系统已被广泛应用于行业以协助此类任务，但在实时对话中，RAG系统面临着诸如查询公式不准确和频繁问题重复检索等问题。为了解决这些局限性，我们提出了一种决策支持系统，该系统可以超越RAG，在实时识别客户问题。如果查询匹配常见问题解答（FAQ），系统直接从FAQ数据库中检索答案；否则，通过RAG生成答案。我们的方法减少了对人工查询的依赖，使得响应能够在2秒内提供给客服人员。此系统部署在Minerva CQ的人工智能辅助解决方案中，提高了效率，缩短了AHT，并降低了运营成本。我们还引入了一个自动化的LLM代理工作流，当没有预定义的FAQ时，可以从历史记录中识别FAQ。|
|**2024-10-13**|**Adaptive Reasoning and Acting in Medical Language Agents**|Abhishek Dutta et.al.|[2410.10020](http://arxiv.org/abs/2410.10020)|null|本文提出了一种创新的大型语言模型（LLM）代理框架，用于提升在模拟临床环境中的诊断准确性，并使用AgentClinic基准进行评估。所提出的自动校正机制使得医生代理能够在错误诊断后迭代地优化其推理和行为，从而随着时间推移提高决策能力。实验表明，采用自适应LLM基础医生代理能够通过与模拟患者的动态互动实现正确的诊断。评估结果突显了自主代理在复杂医疗场景中适应和改进的能力。未来的工作将集中在完善算法并扩大其在更广泛任务和不同大型语言模型中的适用性。|
|**2024-10-13**|**Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent Simulation**|Jiarui Ji et.al.|[2410.09824](http://arxiv.org/abs/2410.09824)|**[link](https://github.com/ji-cather/graphagent)**|图生成是社会、技术和科学研究中广泛研究的基本任务。在建模动态图演化过程时，传统的基于规则的方法难以捕捉图中的社区结构，而深度学习方法仅关注拟合训练图。这限制了现有的图生成器只能生成符合预定义规则或与训练数据集高度相似的图，在动态图生成方面表现不佳。鉴于图是从人类活动中成对交互产生的抽象表示，对人类行为的真实模拟可以更深入地洞察图演化机制。随着大型语言模型（LLMs）在模拟人类行为方面的日益认可，我们引入了一种新的基于仿真框架——GraphAgent-Generator（GAG），用于动态图生成。无需对LLM进行训练或微调，我们的框架有效复制了已建立的网络科学理论中的七个宏观结构特征，同时在特定评估指标上比现有基线在图扩展任务中提高了31%。通过节点分类任务，我们验证了GAG能够有效保留真实世界网络的节点级文本特征在生成的文本丰富的图中。此外，通过并行加速，GAG支持通过基于大规模LLM的代理仿真生成最多接近10万个节点或1000万条边的图，最小加速比为90.4%。源代码可在<https://anonymous.4open.science/r/GraphAgent-2206>获取。|
|**2024-10-13**|**Agentic Information Retrieval**|Weinan Zhang et.al.|[2410.09713](http://arxiv.org/abs/2410.09713)|null|自20世纪70年代以来，用户访问相关信息一直依赖于特定领域的信息检索（IR）架构。在过去二十年中，现代IR系统（包括网络搜索引擎和个人化推荐系统）的出现极大地提高了从大量数据集中检索相关信息的效率。然而，这些IR系统的内核范式仍然基本不变，依赖于筛选预定的一组候选项目。自2022年以来，大型语言模型（LLM）的突破开始改变信息访问的方式，建立了一种新的技术范式。在本文献综述中，我们介绍了由LLM代理能力塑造的新IR范式——主动式信息检索（Agentic IR）。Agentic IR扩展了可访问任务的范围，并利用一系列新技术重新定义信息检索。我们讨论了三种前沿应用以及面临的挑战。我们认为，主动式信息检索有望产生创新的应用，可能成为未来数字生态系统中的核心信息入口。|
|**2024-10-12**|**LLM-SmartAudit: Advanced Smart Contract Vulnerability Detection**|Zhiyuan Wei et.al.|[2410.09381](http://arxiv.org/abs/2410.09381)|**[link](https://github.com/LLMAudit/LLMSmartAuditTool)**|区块链技术的不变性质虽然革命性，但也引入了显著的安全挑战，特别是在智能合约方面。这些安全问题可能导致巨大的财务损失。当前工具和方法通常专注于特定类型的漏洞。然而，缺乏一种能够广泛检测多种漏洞且具有高准确性的综合工具。本文介绍了一种名为LLM-SmartAudit的新框架，该框架利用大型语言模型（LLMs）的先进能力来检测和分析智能合约中的漏洞。通过多代理对话方法，LLM-SmartAudit采用协作系统与专业代理合作以增强审计过程。为了评估LLM-SmartAudit的有效性，我们编制了两个不同的数据集：一个用于与传统工具进行基准测试的标记数据集，以及一个用于评估实际应用的现实世界数据集。实验结果表明，我们的解决方案在所有传统智能合约审计工具之上，提供了更高的准确性和更大的效率。此外，我们的框架可以检测复杂逻辑漏洞，而传统工具之前未曾发现这些漏洞。我们的研究结果表明，利用LLM代理提供了一种非常有效的自动化智能合约审计方法。|
|**2024-10-11**|**PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents**|Xiangyu Yin et.al.|[2410.09034](http://arxiv.org/abs/2410.09034)|**[link](https://github.com/xyin-anl/Nodeology)**|叠层成像是在X射线和电子显微镜中的一种先进的计算成像技术。它已被广泛应用于物理、化学、生物和材料科学等科研领域，以及半导体表征等工业应用中。实际上，获得高质量的叠层图像需要同时优化许多实验和算法参数。传统上，参数选择往往依赖于试错法，导致低吞吐量的工作流程和潜在的人类偏见。在这项工作中，我们开发了“叠层实验与分析机器人”（PEAR），这是一个利用大型语言模型（LLM）自动化叠层成像数据分析的框架。为了确保高鲁棒性和准确性，PEAR使用多个LLM代理执行任务，包括知识检索、代码生成、参数推荐和图像推理。我们的研究表明，PEAR的多代理设计显著提高了工作流程的成功率，即使使用较小的开源权重模型如LLaMA 3.1 8B。PEAR还支持各种自动化级别，并且设计为可以与定制的本地知识库一起工作，确保在不同研究环境中的灵活性和适应性。|
|**2024-10-14**|**AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents**|Maksym Andriushchenko et.al.|[2410.09024](http://arxiv.org/abs/2410.09024)|null|对于语言大模型（LLMs）在面对越狱攻击时的鲁棒性研究，主要集中在它们作为简单的聊天机器人时的情况。然而，能够使用外部工具并执行多阶段任务的语言模型代理可能带来更大的风险，但其鲁棒性仍缺乏充分探索。为了促进对语言模型代理滥用的研究，我们提出了一种新的基准测试——AgentHarm。该基准测试包括110个明确恶意的代理任务（通过增强后达到440个），涵盖了欺诈、网络犯罪和骚扰等11类危害。除了衡量模型是否拒绝有害的代理请求外，要在AgentHarm上取得高分还需要被越狱的代理能够在遭受攻击后维持其能力以完成多步任务。我们评估了一系列领先的LLMs，发现（1）领先的LLMs在没有越狱的情况下会出乎意料地服从恶意代理请求，（2）简单的通用越狱模板可以有效越狱代理，（3）这些越狱能够使连贯且恶意的多步代理行为得以实现，并保留模型的能力。为了便于对基于LLM的代理进行简单可靠的攻击和防御评估，我们公开发布了AgentHarm，网址是https://huggingface.co/datasets/ai-safety-institute/AgentHarm。|
|**2024-10-11**|**The Dynamics of Social Conventions in LLM populations: Spontaneous Emergence, Collective Biases and Tipping Points**|Ariel Flint Ashery et.al.|[2410.08948](http://arxiv.org/abs/2410.08948)|null|社会惯例是社会和经济生活的基础。随着越来越多的AI代理与彼此以及人类进行互动，它们形成共享惯例的能力将决定它们协调行为、融入社会并影响社会的效果。本文通过模拟交互研究了大型语言模型（LLM）代理群体内部惯例的动力学。首先，我们展示了全球接受的社会惯例可以自发地从相互交流的LLM之间产生。其次，我们演示了在这一过程中即使是个体代理看似无偏见的情况下，强烈的集体偏见也可能会出现。第三，我们考察了少数群体中的坚定LLM如何推动社会变革，通过建立新的社会惯例。我们发现，一旦这些少数群体达到临界规模，它们就能够持续颠覆已建立的行为模式。在所有情况下，将实验结果与一个最小化多代理模型的预测进行对比，使我们能够隔离LLM代理的具体作用。我们的研究结果阐明了AI系统可以在没有明确编程的情况下自主发展规范，并对设计与人类价值观和社会目标相一致的AI系统具有启示意义。|
|**2024-10-10**|**Benchmarking Agentic Workflow Generation**|Shuofei Qiao et.al.|[2410.07869](http://arxiv.org/abs/2410.07869)|**[link](https://github.com/zjunlp/worfbench)**|大型语言模型（LLMs）凭借其在处理广泛任务方面的出色能力，推动了推理和规划任务的显著进步。在这一过程中，将复杂问题分解为可执行的工作流是关键步骤。现有的工作流评估框架要么仅关注整体性能，要么存在限制，如场景覆盖范围有限、工作流结构过于简单以及评价标准宽松等问题。因此，我们引入了WorFBench，这是一个具有多维场景和复杂图工作流结构的统一工作流生成基准。同时，我们提出了一套系统性的评估协议——WorFEval，利用子序列和子图匹配算法来准确量化LLM代理的工作流生成能力。  通过不同类型的LLM进行全面评估，我们发现LLM代理在序列规划能力和图规划能力之间存在明显的差距，即使是GPT-4也显示出约15%的差距。我们还训练了两个开源模型，并在保留任务上评估它们的一般化能力。此外，我们观察到生成的工作流能够增强下游任务，使得这些任务在推理时能够取得更好的性能并节省时间。所有相关代码和数据集将在https://github.com/zjunlp/WorFBench公开提供。|
|**2024-10-10**|**AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories**|Yifan Song et.al.|[2410.07706](http://arxiv.org/abs/2410.07706)|null|在这项工作中，我们引入了AgentBank，这是迄今为止最大的用于开放源代码大型语言模型（LLM）的agent-environment交互轨迹调优数据集，包含超过5万条多样化的高质量交互轨迹，涉及16个任务和五个不同的agent技能维度。通过新颖的注释流程，我们能够规模化地标注轨迹并生成了一个难度偏差最小化的轨迹数据集。进一步地，我们对AgentBank进行调优，得到了一系列的agent模型——Samoyed。我们的比较实验表明，通过扩展交互轨迹数据来获取通用的agent能力的有效性。额外的研究还揭示了一些关于轨迹调优和agent技能泛化的关键观察结果。|
|**2024-10-11**|**WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents**|Siyu Zhou et.al.|[2410.07484](http://arxiv.org/abs/2410.07484)|**[link](https://github.com/elated-sawyer/WALL-E)**|**大型语言模型（LLM）是否可以直接作为模型驱动代理的强大世界模型？虽然LLM的先验知识与指定环境动态之间的差距确实存在，但我们的研究揭示了可以通过使LLM与其部署环境对齐来弥合这些差距，这种“世界对齐”可以通过在LLM上进行规则学习来高效实现。考虑到LLM丰富的先验知识，仅需少量额外规则即可使LLM预测与指定环境动力学相匹配。为此，我们提出了一种神经符号方法，通过LLM以梯度无的学习方式来学习这些规则，通过基于探索轨迹与世界模型预测的比较来诱导、更新和修剪规则。结果的世界模型由LLM和学习到的规则组成。我们构建的实体化LLM代理“WALL-E”基于模型预测控制（MPC）。通过基于精确世界模型优化前瞻行动，MPC显著提高了探索和学习效率。与现有LLM代理相比，“WALL-E”的推理仅需要少量主要规则，而不需要包含在LLM输入中的大量缓冲轨迹。在Minecraft和ALFWorld的开放世界挑战中，WALL-E的成功率高于现有方法，规划时间和推理所需的令牌数量更低。在Minecraft中，WALL-E比基线高出15%-30%，成功率为95%，仅花费6次迭代。**|
|**2024-10-09**|**I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy**|Gian Maria Campedelli et.al.|[2410.07109](http://arxiv.org/abs/2410.07109)|**[link](https://github.com/mobs-fbk/llm_interaction_simulator)**|**随着大型语言模型（LLM）驱动的代理日益自主并在彼此间自由互动，研究它们之间的交互变得至关重要，以预见可能出现的现象并识别潜在风险。受到斯坦福监狱实验的启发，我们在此领域做出贡献，通过在具有严格社会等级特征的情境下研究LLM代理的交互模式。我们特别关注两种现象：说服和反社会行为，在涉及看守和寻求特定目标（例如获得更多户外活动时间或逃离监狱）囚犯的模拟场景中进行研究。利用200个实验场景和总共2000次机器对机器对话，涉及五种流行的LLM，我们提供了一系列值得关注的发现。  首先，我们记录了某些模型如何在具有权力动态作用的多代理设置中持续失败的对话。然后，对于能够成功互动的模型，我们实证地展示了目标对代理的说服力影响主要，而对代理的反社会行为影响则微乎其微。第三，我们强调了代理个性，特别是看守的性格，如何驱动囚犯成功的说服可能性和反社会行为的出现。第四，我们表明，即使没有明确提示特定个性，仅通过分配代理角色，反社会行为也会自然浮现。这些结果对LLM代理的发展以及对其社会影响的辩论有重要意义。**|
|**2024-10-09**|**Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models**|Daniel Albert et.al.|[2410.06932](http://arxiv.org/abs/2410.06932)|null|在这项研究中，我们提出了一种新型方法——利用大型语言模型（LLM）代理在行为策略研究领域，以补充模拟和实验室实验，从而深化对决策过程中认知过程的理解。具体来说，我们复现了一个人类实验室实验中的行为策略，并使用LLM生成的代理与观察到的人类行为进行对比。我们的结果表明，LLM代理能够有效地重现搜索行为以及与人类相似的决策制定过程。  进一步地，我们分析了LLM代理的“思想”模拟，发现更前瞻性的思想与倾向于利用而非探索以最大化财富的行为相关联。我们展示了这一新方法在行为策略研究领域的应用潜力，并探讨了其可能存在的局限性。|
|**2024-10-08**|**AgentSquare: Automatic LLM Agent Search in Modular Design Space**|Yu Shang et.al.|[2410.06153](http://arxiv.org/abs/2410.06153)|**[link](https://github.com/tsinghua-fib-lab/agentsquare)**|**近期大型语言模型（LLM）的进展推动了能够处理复杂任务的智能体系统的快速成长。然而，当前的研究主要依赖于基于手动、任务特定设计的方法，这限制了它们在新任务上的适应性。本文提出了一项新的研究问题：模块化语言模型智能体搜索（MoLAS）。我们提出了一个模块化的设计空间，将现有的LLM智能体设计抽象为四个基本模块，并保持统一的输入输出接口：规划、推理、工具使用和记忆。在此基础上，我们介绍了一个名为AgentSquare的新智能体搜索框架，它引入了两个核心机制：模块进化和重组，以高效地搜索优化的LLM智能体。为了进一步加速这一过程，我们设计了一个性能预测器，利用上下文相关模型作为代理设计的近似模型，从而跳过无前景的代理设计。在六个基准测试中进行了广泛实验，涵盖了网络应用、实体交互、工具使用和游戏等不同场景，结果表明，AgentSquare显著优于手工设计的智能体，平均性能提高了17.2%，与人类最佳设计相比。此外，AgentSquare还能生成可解释的设计洞察，有助于深入理解智能体架构及其对任务性能的影响。我们认为，模块化设计空间和AgentSquare搜索框架提供了一个平台，用于充分利用先前成功设计的潜力，并整合研究社区的努力。代码仓库可访问于https://github.com/tsinghua-fib-lab/AgentSquare。**|
|**2024-10-08**|**Conversate: Supporting Reflective Learning in Interview Practice Through Interactive Simulation and Dialogic Feedback**|Taufiq Daryanto et.al.|[2410.05570](http://arxiv.org/abs/2410.05570)|null|求职面试在塑造个人职业生涯方面起着关键作用，然而，缺乏人类教练或同行提供反馈的环境使面试技能训练变得颇具挑战。近期，大型语言模型（LLM）的发展为提升面试练习体验提供了机会。遗憾的是，目前的研究鲜有探讨此类系统的效果及其用户感知，以及利用LLM进行面试练习所涉及的益处与挑战。尽管先前的工作和最近的商业工具已经展示了人工智能辅助面试练习的潜力，它们通常仅提供单向反馈，即用户只能从他们的表现中获取信息。相比之下，对话式反馈，一个在学习科学领域发展起来的概念，是一种双向互动反馈过程，允许用户通过对话进一步参与并从提供的反馈中学习。本文介绍了一款名为Conversate的基于网络的应用程序，它利用大型语言模型（LLM）支持反思性学习，以促进求职面试练习。用户通过提供职位标题（如入门级软件工程师）来启动面试会话。然后，系统中的LLM代理将开始面试模拟，通过向用户提出开场面试问题，并根据用户的回答精心设计后续问题来启动。面试结束后，系统的后端LLM框架将分析用户的回答，指出需要改进的地方。用户可以通过选择特定段落并撰写自我反思来注释转录。最后，用户可以与系统进行对话式反馈交互，与LLM代理对话，根据代理的指导逐步完善自己的答案。|
|**2024-10-07**|**Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback**|Sanjiban Choudhury et.al.|[2410.05434](http://arxiv.org/abs/2410.05434)|null|大型语言模型（LLM）在决策制定方面展现出令人印象深刻的能力，但当前的方法缺乏从任务执行期间错误中自动自我改进的机制。我们提出了LEAP，一种迭代细调框架，通过从AI专家教师获取反馈来持续提升LLM代理。我们的关键洞察是为专家教师提供一个特权状态——仅在训练期间可用但在测试时隐藏的信息。这使得即使是最弱的专家也能提供精确指导，显著提高学生代理在不访问测试时的特权信息情况下的性能。我们在多种决策制定基准上评估了LEAP，包括基于文本的游戏（ALFWorld）、网络导航（WebShop）和交互式编码（Intercode Bash）。我们的实验表明，LEAP（1）优于行为克隆和ReAct基线（2）使较弱的学生模型（如Llama3-8B）超过强大教师模型（GPT4-o）的表现，并且（3）允许较弱的模型使用自己特权版本的自我提升。我们也提供了理论分析，显示LEAP的成功取决于平衡特权信息与学生的可实现性，我们通过实验证实了这一观点。我们的代码可在https://leap-llm.github.io 获取。|
|**2024-10-07**|**GLEE: A Unified Framework and Benchmark for Language-based Economic Environments**|Eilam Shapira et.al.|[2410.05254](http://arxiv.org/abs/2410.05254)|**[link](https://github.com/eilamshapira/GLEE)**|**大型语言模型（LLMs）在经济和战略互动领域展现出显著潜力，因为自然语言通信在此类场景中通常占主导地位。这引发了一系列关键问题：LLMs是否表现出理性？它们能否模仿人类行为？它们是否倾向于达到高效且公平的结果？自然语言在战略互动中的角色是什么？经济环境的特性如何影响这些动态？对于将基于LLM的代理集成到现实世界的数据驱动系统（如在线零售平台和推荐系统）中时的经济和社会影响而言，这些问题至关重要。  尽管机器学习社区已经探索了LLMs在多代理设置中的潜在应用，但不同研究之间在假设、设计选择和评估标准上的差异使得难以得出稳健且有意义的结论。为解决这一问题，我们引入了一个基准，以标准化对基于语言的双人、序列游戏的研究。借鉴经济学文献，我们定义了三个基类游戏家族，具有一致的参数化、自由度以及用于评估代理性能（自我收益）以及游戏结果（效率与公平性）的经济衡量指标。  我们开发了一个开源框架进行交互模拟与分析，并利用该框架收集了LLM与LVM之间的多个游戏配置以及额外的人类与LVM交互数据集。通过大量实验，我们的框架和数据集可以用于：(i) 比较基于LLM的代理与人类玩家在各种经济背景下的行为；(ii) 从个体和集体绩效角度评估代理；(iii) 定量分析经济环境特性对代理行为的影响。**|
|**2024-10-09**|**GenSim: A General Social Simulation Platform with Large Language Model based Agents**|Jiakai Tang et.al.|[2410.04360](http://arxiv.org/abs/2410.04360)|**[link](https://github.com/TangJiakai/GenSim)**|**近年来，随着大型语言模型（LLM）的迅速发展，利用基于LLM的代理来模拟人类社会行为的研究取得了许多有前景的成果。尽管先前的工作在特定场景下展示了巨大的潜力，并且涉及有限数量的代理，但它们大多缺乏在模拟过程中出现错误时进行适应的能力。为了克服这些局限性，我们提出了一种名为\textit{GenSim}的新颖的基于LLM的仿真平台：（1）\textbf{抽象了一组通用功能}，简化了定制社会场景的仿真；（2）\textbf{支持一百万个代理}，以更好地模拟现实世界情境中的大规模人群；（3）\textbf{整合了错误纠正机制}，确保更可靠和长期的仿真。为了评估我们的平台，我们评估了大规模代理仿真效率以及错误纠正机制的有效性。据我们所知，GenSim代表了基于LLM代理的通用、大规模和可校正的社会仿真平台的初步步骤，有望进一步推动社会科学领域的发展。**|
|**2024-10-04**|**Permissive Information-Flow Analysis for Large Language Models**|Shoaib Ahmed Siddiqui et.al.|[2410.03055](http://arxiv.org/abs/2410.03055)|null|大型语言模型（LLM）正在快速成为更大软件系统中的通用组件。这引发了一系列自然的安全和隐私问题：从一个组件获取的污染数据可以改变模型的行为并破坏整个系统，包括使模型在不可信组件间传播机密数据。一种有前景的方法是在系统层面上通过动态信息流跟踪（即污点跟踪）来解决这些问题。不幸的是，传统方法将最严格的输入标签传播到输出过于保守，不适合LLM在来自不同来源的输入上操作的应用场景。本文提出了一种新颖的、更宽松的方法来在LLM查询中传播信息流标签。我们的方法的核心思想是仅传播生成模型输出时起作用的样本的标签，并消除不必要的输入标签。  我们实现了并研究了两种这种方法的变体，基于（i）提示增强检索和（ii）基于 $k$ 个最近邻的语言模型。我们将这些方法与直接询问语言模型预测输出标签的反省式影响估计器基线进行了比较。实验结果表明，我们的基于提示的标签传播器方法在超过85%的情况下提高了标签质量，在LLM代理设置中效果显著。这些发现强调了在检索增强中使用宽松标签传播的实用性。|
|**2024-10-03**|**AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML**|Patara Trirat et.al.|[2410.02958](http://arxiv.org/abs/2410.02958)|null|本文提出了一种全新的多代理框架——AutoML-Agent，专为全管道自动化机器学习（AutoML）设计，涵盖了从数据检索到模型部署的整个过程。AutoML-Agent通过接受用户的任务描述、促进专门语言模型代理之间的协作，并交付可部署的模型，从而提供了一个自然语言接口，以简化非专家用户构建数据驱动解决方案的过程。与现有工作不同，本文引入了一种基于检索增强的规划策略来提高探索性，以便在搜索更优解的过程中进行探索。我们还通过并行执行来分解每个计划为子任务（例如数据预处理和神经网络设计），每个子任务由我们通过提示构建的专门代理解决，这使得搜索过程更加高效。此外，我们提出了一个多阶段验证方法来验证执行结果，并指导代码生成语言模型实现成功的解决方案。在七个下游任务上使用十四组数据集进行的大量实验表明，AutoML-Agent在自动化全AutoML流程方面取得了更高的成功率，且系统在整个多样化领域中的性能均表现出色。|
|**2024-10-03**|**Grounding Large Language Models In Embodied Environment With Imperfect World Models**|Haolan Liu et.al.|[2410.02742](http://arxiv.org/abs/2410.02742)|null|尽管大型语言模型（LLMs）在各种应用中取得了广泛的成功，但在处理基本物理推理或执行机器人任务时，它们经常遇到问题，这是因为它们缺乏对现实世界物理细微之处的直接经验。为了应对这些问题，我们提出了一种名为Grounding Large Language Model with Imperfect World MOdel (GLIMO)的方法，该方法利用代理世界模型，如模拟器，来收集和合成训练数据。GLIMO集成了一个基于LLM的自动数据生成器，用于创建高质量且多样化的指令数据集。生成器包括一个迭代自我精炼模块，用于时间上一致的经验采样，一个多样化的问答指令种子集合，以及一个反射性增强生成模块，用于反映先前的经验。  全面的实验表明，我们的方法能够提高强开源LLMs，如LLaMA-3，在三个不同基准上的性能提升分别为2.04倍、1.54倍和1.82倍，分别。这种性能能够与或超越它们更大的同辈，如GPT-4。|
|**2024-10-03**|**Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents**|Hanrong Zhang et.al.|[2410.02644](http://arxiv.org/abs/2410.02644)|**[link](https://github.com/agiresearch/asb)**|**为了填补现有文献在全面评估基于大型语言模型（LLM）的代理攻击与防御策略方面的空白，我们提出了一种名为“代理安全基准”（Agent Security Benchmark, ASB）的综合框架。该框架旨在正式化、标准化并评估基于LLM的代理的安全问题，涵盖了10个应用场景（如电子商务、自动驾驶、金融）、10个针对这些场景的代理、超过400种工具、23类不同的攻击与防御方法以及8个评价指标。基于ASB，我们对10种提示注入攻击、一种记忆污染攻击、一种新颖的计划-思维后门攻击、一种混合攻击以及针对这10种攻击的10种相应防御措施，在13个LLM架构下进行了全面的基准测试，总共产生了近9万个测试案例。我们的基准测试结果揭示了代理操作不同阶段中的关键安全漏洞，包括系统提示、用户提示处理、工具使用和记忆检索，其中最高平均攻击成功率达到了84.30%，但当前的防御措施的有效性有限，这表明社区在代理安全方面仍有许多工作要做。有关此研究的代码可在https://github.com/agiresearch/ASB获取。**|
|**2024-10-03**|**ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration**|Zixiang Wang et.al.|[2410.02551](http://arxiv.org/abs/2410.02551)|null|我们引入了ColaCare框架，该框架通过大型语言模型（LLM）驱动的多代理协作增强了电子健康记录（EHR）建模。我们的方法无缝地将领域特定的专业模型与LLM结合，以弥合结构化EHR数据与基于文本的推理之间的差距。受临床咨询的启发，ColaCare采用了两种类型的代理：医生代理和元代理，它们协同分析患者数据。专家模型处理并从数值EHR数据生成预测，而LLM代理在协作咨询框架内产生推理参考和决策报告。我们还通过检索增强生成（RAG）模块将默克诊断与治疗手册（MSD）医疗指导整合进来，提供权威证据支持。在四个不同的EHR数据集上进行的大量实验证明了ColaCare在死亡率预测任务中的优越性能，这强调了其在临床决策支持系统和推进个性化精准医学方面的潜力。有关代码、完整提示模板、更多案例研究等的更多信息，请访问匿名链接：<https://colacare.netlify.app>。|
|**2024-10-03**|**ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR**|Mengxu Pan et.al.|[2410.02406](http://arxiv.org/abs/2410.02406)|null|许多人在学习新语言时会遇到困难，传统的工具在提供针对每个学习者需求的上下文化学习方面存在不足。最近，大型语言模型（LLMs）和在社交虚拟现实（VR）中的具身对话代理（ECAs）的发展，提供了以一种考虑到学习者的语言水平和需求的方式进行上下文化且自然的语言学习的新机会。为了探索这一可能性，我们开发了ELLMA-T，一个利用GPT-4和基于情境学习框架来支持社交VR（VRChat）中英语语言学习的具身对话代理。通过12次的质性访谈，我们揭示了ELLMA-T在VR中为学习者与代理之间的互动生成真实、可信和上下文特定的角色扮演的潜力，以及LLM在为学习者提供初始语言评估和持续反馈方面的能力。我们提供了对于未来开发基于LLM的语言代理在社交VR中的五个设计启示。|
|**2024-10-03**|**A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization**|Yucheng Chu et.al.|[2410.02165](http://arxiv.org/abs/2410.02165)|null|在学习分析（LA）的背景下，开放式短答问题（SAG）被广泛认为是深入了解学习者响应的强大工具。然而，在实践中，SAG经常面临高评分工作量和评估一致性担忧的挑战。随着自然语言处理（NLP）的最新进展，自动短答评分（ASAG）为解决这些挑战提供了有前景的解决方案。尽管如此，当前的ASAG算法往往在泛化能力上有限，并倾向于针对特定问题进行定制。为此，本文提出了一种统一的多代理ASAG框架GradeOpt，利用大型语言模型（LLMs）作为SAG的评分员。更重要的是，GradeOpt引入了两个基于LLM的额外代理——反射器和细化器——到多代理系统中。这使得GradeOpt能够通过对其错误进行自我反思来自动优化原始评分指南。在对具有挑战性的ASAG任务进行实验，即对教学内容知识（PCK）和内容知识（CK）问题进行评分时，GradeOpt在评分准确性和与人工评分员行为的一致性方面均表现出优于代表基线的性能。最后，全面的消融研究证实了GradeOpt中设计的各个组件的有效性。|
|**2024-10-02**|**Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics**|Yuan Zhou et.al.|[2410.02026](http://arxiv.org/abs/2410.02026)|null|本文介绍了一种名为ZODIAC的大型语言模型(LLM)框架，旨在通过心脏病专家级别的专业素养，辅助心脏病学诊断。ZODIAC能够从患者数据中提取临床相关特征、检测重要的心律失常，并生成初步报告供心脏病专家审查和细化。为了实现心脏病专家级别的专业素养，ZODIAC构建了一个多代理协作框架，允许对多模态患者数据进行处理。每个LLM代理均通过心脏病专家裁定的真实世界患者数据进行精细调优，以此强化模型的专业素养。  ZODIAC经过了严格的临床验证，由独立的心脏病专家评估，涵盖八个指标，衡量临床效果并解决安全问题。结果显示，ZODIAC在性能上超越了行业领先的模型，包括OpenAI的GPT-4o、Meta的Llama-3.1-405B和Google的Gemini-pro，以及专门针对医疗领域的LLM如微软的BioGPT。这表明了专门设计的LLM在医疗保健领域的潜力，能够提供符合医疗实践严格要求的领域特定解决方案。  值得注意的是，ZODIAC已成功集成到心电图(ECG)设备中，展示了将LLM嵌入软件作为医疗设备(SaMD)的趋势日益增长。|
|**2024-10-02**|**Moral Alignment for LLM Agents**|Elizaveta Tennant et.al.|[2410.01639](http://arxiv.org/abs/2410.01639)|null|基于大型语言模型（LLM）的决策代理正越来越多地在人类活动的不同领域部署。虽然它们的应用目前较为专业化，但已有研究努力开发更通用的代理。随着LLM系统变得更加自主，它们对人类活动的影响将增加，并且透明度会降低。因此，发展有效的方法来使它们符合人类价值观至关重要。  现有的对齐方法通常依赖于人类偏好数据（例如，在RLHF或DPO中），其中价值观是隐含的，并且本质上是从不同模型输出的相对偏好中推断出来的。与此相反，我们在这项工作中提出了一种设计奖励函数的方法，这些函数明确编码了核心的人类价值观，用于强化学习（RL）方式微调基础代理模型。具体来说，我们使用内在奖励来实现LLM代理的道德对齐。  我们通过传统的哲学框架——德ontology伦理和功利主义来评估我们的方法，量化了在迭代囚徒困境（IPD）环境中代理的道德奖励，基于其行为及其后果。我们还展示了如何通过道德微调使代理能够放弃之前开发的自私策略。最后，我们发现某些在IPD游戏中学习的道德策略能够推广到多个矩阵游戏环境。总之，我们证明了使用内在奖励进行微调是将LLM代理与人类价值观对齐的有前景的一般解决方案，并且可能代表了当前主流对齐技术更加透明和成本效益更高的替代方案。|
|**2024-10-03**|**RGD: Multi-LLM Based Agent Debugger via Refinement and Generation Guidance**|Haolin Jin et.al.|[2410.01242](http://arxiv.org/abs/2410.01242)|**[link](https://github.com/DokiHollin/Multi-LLM-Based-Agent-RGD)**|大型语言模型（LLM）在代码生成任务上展现出了巨大的潜力，并且最近的提示工程研究进一步增强了LLM对文本信息的理解。然而，确保生成代码的准确性通常需要程序员进行大量的测试和验证。尽管LLM能够基于任务描述生成代码，但在复杂任务上的准确度仍然有限，特别是对于那些需要更深入理解问题陈述和代码生成过程的任务。这一限制主要源于LLM同时需要理解和生成语法和语义上正确的代码，而没有能力自动优化代码的能力。在实际的软件开发中，程序员很少能在仅凭任务描述的情况下一次就生成完美的代码，他们依赖于迭代反馈和调试来完善他们的程序。受此过程启发，我们引入了一种基于LLM的多智能体架构用于代码生成和自动调试：改进与指导调试（RGD）。RGD框架是一个利用三种不同LLM代理（引导代理、调试代理和反馈代理）的多智能体调试器，它将代码生成任务分解为多个步骤，确保了清晰的工作流程，并允许基于自我反思和反馈的代码迭代细化。实验结果表明，RGD在代码生成能力上表现出色，分别在HumanEval数据集和MBPP数据集上相比最先进的方法和传统直接提示方法实现了9.8%和16.2%的性能提升。我们强调了RGD框架在增强LLM自主生成和优化代码能力方面的有效性。|
|**2024-10-01**|**Dynamic Planning for LLM-based Graphical User Interface Automation**|Shaoqing Zhang et.al.|[2410.00467](http://arxiv.org/abs/2410.00467)|**[link](https://github.com/sqzhang-lazy/d-pot)**|**大型语言模型（LLM）的兴起激发了对自主LLM基代理进行创新性发展的兴趣，尤其是在智能手机图形用户界面（GUI）中的应用。当面对任务目标时，这些代理通常会模仿人类在GUI环境中的操作直至任务完成。然而，一个关键挑战在于如何有效地制定计划以指导GUI任务中的动作预测，尽管规划已被广泛认为是分解复杂任务的有效方式。具体而言，在执行动作后GUI环境的动态性质意味着需要根据环境反馈和动作历史动态调整计划。  我们发现广受欢迎的ReAct方法失败了，原因在于其过于依赖过长的历史对话。为了解决这一挑战，我们提出了一种名为动态思维规划（D-PoT）的新方法，用于基于LLM的GUI代理。D-PoT涉及根据环境反馈和执行历史动态调整规划的过程。实验结果表明，提出的D-PoT方法在准确性上显著超越了强大的GPT-4V基线，提高了12.7%（从34.66%提高到47.36%）。分析揭示了动态规划在不同基础LLM中的通用性，以及在处理未见过的任务时减少幻觉并适应的能力。代码已发布在https://github.com/sqzhang-lazy/D-PoT。**|
|**2024-09-30**|**MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants**|Zeyu Zhang et.al.|[2409.20163](http://arxiv.org/abs/2409.20163)|**[link](https://github.com/nuster1128/memsim)**|**本文提出了一种名为MemSim的贝叶斯模拟器，用于从生成的用户消息自动构建可靠的问题与答案（Q&A），同时保持其多样性和可扩展性。具体来说，我们引入了贝叶斯关系网络（BRNet）和因果生成机制，以减轻大型语言模型（LLM）幻觉对事实信息的影响，从而促进自动构建评估数据集。基于MemSim，我们在日常生活中生成了一个名为MemDaily的数据集，并进行了广泛的实验，以评估我们方法的有效性。我们还提供了使用MemDaily数据集评估LLM基智能体不同记忆机制的基准。为了惠及研究社区，我们已经在https://github.com/nuster1128/MemSim上发布了我们的项目。**|
|**2024-10-01**|**TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation**|Zhiqiang Yuan et.al.|[2409.19894](http://arxiv.org/abs/2409.19894)|null|本文提出了一种名为TRANSAGENT的新型基于大型语言模型（LLM）的多代理系统，以增强基于LLM的代码翻译过程，并通过四个基于LLM的代理协同工作修复语法错误和语义错误。这四个代理分别是初始代码翻译器、语法错误修复器、代码对齐器和语义错误修复器。TRANSAGENT的核心洞察是首先根据目标程序与源程序之间的执行对齐定位目标程序中的错误代码块，这种方法可以缩小修复范围并降低修复难度。  为了评估TRANSAGENT，我们首先从最近的编程任务构建了一个新的基准，以减轻潜在的数据泄露问题。在我们的基准上，TRANSAGENT在翻译效果和效率方面都优于最新的LLM基代码翻译技术UniTrans；此外，在不同LLM上的评估显示了TRANSAGENT的一般性，并且我们的消融研究揭示了每个代理的贡献。|
|**2024-09-26**|**From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection**|Xinlei Wang et.al.|[2409.17515](http://arxiv.org/abs/2409.17515)|**[link](https://github.com/ameliawong1996/From_News_to_Forecast)**|本文提出了一种新颖的方法，旨在通过大型语言模型（LLMs）和生成代理来增强时间序列预测。以语言作为媒介，我们的方法适应性地将各种社会事件整合进预测模型中，将新闻内容与时间序列波动对齐，从而提供丰富洞察。具体而言，我们利用基于语言模型的代理进行迭代筛选，去除无关新闻，并采用类似人类的推理和反思来评估预测结果。这使得我们的模型能够分析复杂事件，如意外事件和社会行为转变，并不断优化选择逻辑以及代理输出的稳健性。通过结合精选新闻和时间序列数据，我们对预训练的LLaMa2模型进行微调。结果显示，在准确性方面有显著提升，这表明通过有效利用非结构化新闻数据，可能在时间序列预测领域实现范式转变。|
|**2024-09-25**|**AAPM: Large Language Model Agent-based Asset Pricing Models**|Junyan Cheng et.al.|[2409.17266](http://arxiv.org/abs/2409.17266)|**[link](https://github.com/chengjunyan1/aapm)**|**本文提出了一种新型的资产定价方法——基于LLM代理的资产定价模型（AAPM）。该方法将LLM代理的定性主观投资分析与定量手动金融经济因素融合，以预测超额资产回报。实验结果表明，我们的方法在组合优化和资产定价误差方面均优于基于机器学习的资产定价基准。具体而言，异常资产组合的夏普比率和平均α值分别提高了9.6%和10.8%。此外，我们还对模型进行了广泛的消融研究，并对数据进行了深入分析，以揭示提出方法的更多见解。**|
|**2024-09-25**|**Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents**|Junting Lu et.al.|[2409.17140](http://arxiv.org/abs/2409.17140)|null|在多模态大型语言模型（MLLMs）的帮助下，语言模型驱动的代理可以直接与应用用户界面（UI）进行交互，从而在复杂任务中提升代理性能。然而，这些代理常常因为涉及大量顺序UI交互而导致高延迟和低可靠性。为了应对这一问题，我们提出了AXIS，一个新颖的基于语言模型的代理框架，通过应用程序接口（APIs）优先于UI动作来优化代理行为。此外，该框架还通过自动化探索应用以创建和扩展API，促进了API的生成和应用范围的扩展。  我们的实验在Word办公软件上显示，与人类相比，AXIS在完成任务的时间上减少了65%-70%，认知负荷降低了38%-53%，同时保持了97%-98%的准确性。我们的工作为人类-代理-计算机交互（HACI）框架和应用提供者在LLMs时代设计新UI原则提供了贡献，并探讨了将每一个应用转化为代理的可能性，为迈向以代理为中心的操作系统（Agent OS）铺平了道路。|
|**2024-09-24**|**MultiTalk: Introspective and Extrospective Dialogue for Human-Environment-LLM Alignment**|Venkata Naren Devarakonda et.al.|[2409.16455](http://arxiv.org/abs/2409.16455)|null|本文提出了一种名为MultiTalk的基于大语言模型（LLM）的任务规划方法。通过引入内省和外省对话循环框架，该方法旨在解决LLM在任务规划中可能遇到的问题，如幻觉、用户指令中的歧义、环境约束以及执行代理能力的局限性。这些问题可能导致生成的计划出现错误或不完整。  MultiTalk方法通过特定系统来提取和预测与任务相关的状态，并标记出人、LLM代理和环境之间的不匹配或偏差。有效的反馈路径促进人与LLM之间的有意义对话。这种方法在机器人操作任务的应用中得到了验证。实验和消融分析展示了MultiTalk方法的稳健性和可靠性，与基线方法的比较进一步证明了其在实体代理任务规划方面的优势。  总之，MultiTalk提供了一种通过增强LLM与环境、执行者和用户之间的一致性和沟通来改进任务规划过程的方法，从而提高规划的有效性和效率。|
|**2024-09-23**|**Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual Reality**|Yiwen Xu et.al.|[2409.15623](http://arxiv.org/abs/2409.15623)|null|本文介绍了一种名为Safe Guard的LLM代理，用于检测社交VR（VRChat）中的语音交互中的仇恨言论。我们的系统利用了Open AI GPT和音频特征提取技术，实现了实时语音交互的检测功能。我们贡献了一个系统设计以及对该系统的评估，这些都证明了我们方法在检测仇恨言论方面的有效性，并且相比现有方法显著降低了误报率。我们的结果表明基于LLM的代理在创建更安全的虚拟环境方面具有潜力，并为进一步发展基于LLM的管理方法奠定了基础。|
|**2024-09-20**|**ControlMath: Controllable Data Generation Promotes Math Generalist Models**|Nuo Chen et.al.|[2409.15376](http://arxiv.org/abs/2409.15376)|null|利用大型语言模型（LLM）进行数据增强在数学推理方面取得了令人鼓舞的结果。然而，这些方法在问题多样性方面存在限制，可能仅局限于特定领域的数据生成。为此，我们提出了一种名为ControlMath的迭代方法，该方法包含一个方程式生成模块和两个基于LLM的代理。该模块产生多样化的方程，问题创造者代理随后将其转化为数学文字问题。逆向代理则筛选并选择高质量的数据，遵循“少即是多”的原则，使用更少的数据点就能实现更好的结果。这种方法能够生成多样化的数学问题，不受特定领域或分布的限制。  因此，我们收集了ControlMathQA数据集，包含19万个数学文字问题。广泛的实验结果证明，将我们的数据集与GSM8K等内部领域数据集结合，可以帮助提高模型在数学推理方面的泛化能力，从而在特定领域内以及超出特定领域时都能取得更好的性能。|
|**2024-09-25**|**Towards a Realistic Long-Term Benchmark for Open-Web Research Agents**|Peter Mühlbacher et.al.|[2409.14913](http://arxiv.org/abs/2409.14913)|null|我们提出了一项即将推出的基准测试，用于评估大型语言模型（LLM）代理在经济价值高的白领任务上的表现。我们对金融和咨询领域常规进行的、现实世界中的“杂乱”开放网络研究任务进行了评估。这样做，我们为建立一个LLM代理评估套件奠定了基础，在该套件中，良好的性能直接对应着巨大的经济和社会影响。我们构建并测试了多个代理架构，包括o1-preview、GPT-4o、Claude-3.5 Sonnet、Llama 3.1（405b）以及GPT-4o-mini。平均而言，使用Claude-3.5 Sonnet和o1-preview的LLM代理在性能上明显优于使用GPT-4o的代理，而基于Llama 3.1（405b）和GPT-4o-mini的代理则落后很多。在所有LLM中，具有委托子任务给子代理能力的ReAct架构表现最佳。除了定量评估之外，我们还通过检查代理的追踪记录和反思它们的观察结果，对代理的能力进行了定性评估。我们的评估代表了首次深入评估代理在真实开放网络上执行具有挑战性的、经济上有价值的分析师式研究的能力。|
|**2024-09-23**|**Interpreting Multi-band Galaxy Observations with Large Language Model-Based Agents**|Zechang Sun et.al.|[2409.14807](http://arxiv.org/abs/2409.14807)|null|本文展示了大型语言模型为基础的智能体如何加速天文学研究流程，通过模仿人类推理来解释多波段星系观测数据。我们提出了mephisto框架，它能够与CIGALE代码库协作，后者包含了用于解释观测数据的光谱能量分布（SED）模型。在开放世界环境中，mephisto通过自我游戏经验学习、执行树搜索并积累动态更新的知识基础。作为概念验证，我们将mephisto应用于詹姆斯韦伯太空望远镜的最新数据集。结果表明，mephisto在推理星系物理场景方面达到了接近人类的专业水平，甚至在处理新发现的“小红点”星系时也是如此。这是智能体进行天文学研究的首次展示，朝着通过大型语言模型代理实现端到端研究的方向迈进，可能有助于加快天文发现的速度。|
|**2024-09-22**|**Enhancing LLM-based Autonomous Driving Agents to Mitigate Perception Attacks**|Ruoyu Song et.al.|[2409.14488](http://arxiv.org/abs/2409.14488)|null|随着大型语言模型（LLM）与自动驾驶（AD）系统集成的日益增长的兴趣，AD系统面临着攻击其对象检测与追踪（ODT）功能的风险。我们的评估表明，针对四个近期提出的LLM代理的ODT攻击成功率达到63.26%，导致它们崩溃或违反交通规则，原因在于误导性记忆模块提供的过往经验、提示在识别不一致性方面的局限性以及对地面实况感知数据的依赖。为此，我们提出了一种名为Hudson的驾驶推理代理，它扩展了先前基于LLM的驾驶系统，旨在在感知攻击期间实现更安全的决策制定，同时在正常条件下保持有效性。  Hudson通过首先对AD软件进行仪器化收集实时感知结果和驾驶场景的上下文信息来实现这一目标。这些数据随后被转化为领域特定语言（DSL）。为了引导LLM在ODT攻击期间检测并做出安全控制决策，Hudson将DSL转换为自然语言，并附带一组自定义的攻击检测指令。执行查询后，Hudson分析LLM的控制决策以理解其因果推理过程。  我们使用私有LLM（GPT-4）、两个开源LLM（Llama和Gemma）和各种对抗性驾驶情景对Hudson的有效性进行了评估。GPT-4、Llama和Gemma在平均情况下实现了83.3%、63.6%和73.6%的攻击检测准确率。因此，在86.4%、73.9%和80%的攻击中，它们做出了安全控制决策。随着将LLM集成到AD系统中的兴趣增长，我们的结果强调了LLM的优势及其在检测和缓解ODT攻击方面的潜力。|
|**2024-09-20**|**Enhancing Fault Localization Through Ordered Code Analysis with LLM Agents and Self-Reflection**|Md Nakhla Rafi et.al.|[2409.13642](http://arxiv.org/abs/2409.13642)|null|在软件开发过程中，定位和修复软件故障是一个耗时且资源密集型的任务。传统的故障定位方法，如基于频谱的故障定位（SBFL），依赖于测试覆盖率数据的统计分析，但往往准确性较低。基于学习的技术虽然更有效，但需要大量的训练数据，并且计算成本高昂。最近，大型语言模型（LLMs）的进步为改善故障定位提供了有前景的方法，通过增强代码理解和推理来提升性能。然而，这些LLM基线技术仍然面临挑战，包括令牌限制、长输入性能下降以及处理涉及多个相互作用组件的复杂系统时的困难。  为了解决这些问题，我们提出了一种名为LLM4FL的创新性LLM代理基线故障定位方法，它结合了SBFL排名与分而治之策略。通过将大规模覆盖数据分解为可管理的组，并利用多个LLM代理通过提示链式调用，LLM4FL有效地导航代码库并定位故障。该方法还整合了自我反思和链式思考推理，使代理能够迭代生成修复并重新排名可疑方法。我们使用Defects4J（V2.0.0）基准进行评估，其中包括来自14个开源Java项目的675个真实世界故障。结果显示，LLM4FL在Top-1准确率上比AutoFL高出19.27%，并且优于最先进的监督技术，如DeepFL和Grace，所有这些都无需特定任务的培训。此外，我们强调了覆盖拆分和提示链对故障定位性能的影响，并展示了不同的方法排序可以提高Top-1准确率高达22%。|
|**2024-09-23**|**AQA: Adaptive Question Answering in a Society of LLMs via Contextual Multi-Armed Bandit**|Mohanna Hoveyda et.al.|[2409.13447](http://arxiv.org/abs/2409.13447)|null|在问答（QA）领域，不同的问题可能需要不同的回答策略来有效解决。一些问题可以通过简单的查找来解决，而另一些则需要复杂的、多步骤的推理。这一观察结果激发了开发一种动态方法，该方法能够为每个问题适当地选择最合适的QA策略，从而构建更高效、更有效的系统，能够处理更广泛类型的问题。为了实现这一目标，我们基于多个大型语言模型（LLMs）的集成最新进展，并将适应性QA定义为一个动态编排挑战。我们将此视为一个上下文多臂老虎机问题，其中上下文由进入问题的特性定义，而动作空间包括潜在的LLM代理之间的通信图配置。然后，我们训练了一个线性上界信心边界模型，以学习不同问题类型与其对应的最佳多LLM通信图表示之间的最优映射。我们的实验表明，提出的解决方案适用于适应性的LLM集成问答系统的编排，它结合了更复杂策略的优越性能，同时避免了在简单策略足以的情况下使用这些策略的成本。|
|**2024-09-24**|**Towards Robust Automation of Surgical Systems via Digital Twin-based Scene Representations from Foundation Models**|Hao Ding et.al.|[2409.13107](http://arxiv.org/abs/2409.13107)|null|本文提出了一种基于数字孪生的机器感知方法，旨在利用近期视觉基础模型的令人信服的表现和开箱即用的泛化能力。该方法通过结合数字孪生的场景表示和大型语言模型（LLM）代理进行规划，与dVRK平台集成，从而开发出一个具有强大任务性能和在不同环境设置下通用性的实体智能系统。在执行穿针移位和纱布检索任务时，我们的方法显示出强大的任务性能和通用性。  尽管表现出令人信服的表现，但本文的工作仅仅是对基于数字孪生的场景表示集成的第一步。为了实现全面的数字孪生框架以改善手术领域实体智能的可解释性和通用性，未来的研究是必要的。|
|**2024-09-17**|**LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents**|Amine B. Hassouna et.al.|[2409.11393](http://arxiv.org/abs/2409.11393)|null|本文通过提出一个统一框架——LLM-Agent-UMF（基于语言模型的代理统一建模框架），解决了集成工具到语言模型（LLM）驱动的代理以及在多个前沿工作中提出的改进措施所导致的软件架构非统一性问题。传统上，这些技术的结合及后续工作侧重于功能实现而非定义组件边界，导致了研究人员之间的术语和架构上的混淆。  该框架明确了代理的不同组件，包括LLM、工具以及新引入的核心代理概念，其作用是代理的中央协调者，由规划、记忆、个人资料、行动和安全五个模块组成。核心代理的内部结构差异促使我们将其分类为被动型和主动型两种类型。基于此分类，我们提出了结合不同个体代理独特特性的多种多核心代理架构。  为了验证框架的有效性，我们将该框架应用于一系列前沿代理，并展示其与功能的一致性，同时澄清了先前被忽视的架构方面。此外，我们对四个提出的架构进行了详尽评估，通过整合具有不同特性的代理到混合主动/被动核心代理系统中，这一过程提供了对特定代理组合可能带来的改进和面临的挑战的清晰见解。|
|**2024-09-17**|**Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments**|Maria Rigaki et.al.|[2409.11276](http://arxiv.org/abs/2409.11276)|null|本篇论文探讨了在网络安全环境中使用本地微调的大型语言模型（LLM）作为红队代理的可能性。考虑到商业云基LLM的隐私问题、成本和网络连接限制，我们提出了Hackphyr——一个本地微调的70亿参数模型，旨在用于网络安全环境中的红队任务。我们的模型能够在单个GPU卡上运行，并且在性能上与更大更强大的商业模型如GPT-4相媲美。  Hackphyr在复杂、前所未见的场景中显著优于其他模型，包括GPT-3.5-turbo以及Q-learning代理等基线模型。为了实现这一性能提升，我们构建了一个专门针对网络安全任务的新数据集，以增强基础模型的能力。最后，我们对代理行为进行了全面分析，提供了关于此类基于LLM的代理在网络安全上下文中的规划能力和潜在局限性的见解，从而为更广泛地理解此类代理在网络安全领域的应用提供了参考。|
|**2024-09-14**|**On the limits of agency in agent-based models**|Ayush Chopra et.al.|[2409.10568](http://arxiv.org/abs/2409.10568)|**[link](https://github.com/agenttorch/agenttorch)**|**本文介绍了一种名为AgentTorch的框架，旨在通过使用大型语言模型（LLMs）作为具有适应性行为的代理，将基于个体的模型（ABM）扩展到数百万个代理的规模。这一框架旨在在模拟复杂系统的行为时，既捕捉到真实环境动态和适应性代理行为，又保持对庞大人口群体高效模拟的能力。大型语言模型的最新进展为增强ABM提供了机会，但使用LLMs进行大规模代理的计算可行性限制了其广泛应用。  我们通过实验评估了使用LLMs作为ABM代理的实用性，探索了模拟规模与单个代理行为细节之间的权衡。以COVID-19大流行为例，我们展示了AgentTorch如何模拟840万个代表纽约市的代理，以捕捉隔离和就业行为对健康和经济结果的影响。我们比较了基于启发式方法和LLMs的不同代理架构在预测疾病浪潮和失业率方面的性能。  此外，我们展示了AgentTorch在回顾性、假设性和前瞻性分析中的能力，强调了适应性代理行为如何帮助克服历史数据在政策设计中的局限性。AgentTorch是一个开源项目，目前正被全球用于政策制定和科学发现。该框架可在此获取：github.com/AgentTorch/AgentTorch。**|
|**2024-09-19**|**Instigating Cooperation among LLM Agents Using Adaptive Information Modulation**|Qiliang Chen et.al.|[2409.10372](http://arxiv.org/abs/2409.10372)|null|本文提出了一种新颖的框架，将大型语言模型（LLM）代理作为人类战略行为的代理，并结合强化学习（RL）让这些代理在团队环境中进行不断演化的战略互动。我们的方法扩展了传统的基于代理的模拟，通过使用策略性大型语言模型（SLA）以及引入动态和适应性的治理，通过促进社会行为的强化学习代理（PPA），该代理调节网络中代理之间的信息访问，以优化社会福利并促进亲社会行为。通过在迭代游戏中验证，包括囚徒困境，我们展示了SLA代理表现出复杂的战略调整。PPA代理有效地学习调整信息透明度，导致合作率显著提高。这一框架提供了对人工智能驱动的社会动力学的重要见解，为在实际团队环境中部署AI做出了贡献。|
|**2024-09-17**|**Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition**|Chao-Han Huck Yang et.al.|[2409.09785](http://arxiv.org/abs/2409.09785)|null|在近期生成式人工智能技术的推动下，大型语言模型（LLMs）如何增强基于文本解码的自动语音识别（ASR）模型在声学建模任务中的应用成为了一个关键问题。为了探索语言建模在语音处理领域的潜在新能力，本文提出了一项名为“生成性语音转录错误修正”（GenSEC）的挑战。该挑战包含了三个针对后ASR语言模型的任务：（i）后ASR转录修正、（ii）说话者标签化以及（iii）情感识别。这些任务旨在模拟未来基于语言模型的语音界面代理处理工作时的场景，并通过使用开源预训练语言模型或基于代理的API来保持对广泛受众的可访问性。此外，本文还讨论了基准评估的结果以及设计未来评估时应汲取的经验教训。|
|**2024-09-15**|**RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation**|Qingyao Li et.al.|[2409.09584](http://arxiv.org/abs/2409.09584)|null|本文针对LLM（大型语言模型）代理与树搜索算法在代码生成任务中的应用进行了深入研究。当前的搜索算法在这一领域存在低搜索质量的问题，主要源于以下三个原因：1）对代码生成任务高推理要求的搜索空间设计不合理；2）未能充分结合代码反馈优化搜索过程；3）处理负反馈时效率低下，导致搜索质量和效率降低。  为解决这些问题，我们提出了一种新的方法——RethinkMCTS（反思蒙特卡洛树搜索）。该方法通过在生成代码之前进行多层次的思考搜索，探索更广泛的策略选项。更重要的是，RethinkMCTS利用细粒度的代码执行反馈构建口头反馈，以修正搜索过程中出现的错误思路。这种机制确保了搜索沿着正确的推理路径前进，从而提高整个搜索树的整体质量。  实验结果表明，与之前的基于搜索和反馈的代码生成基准相比，RethinkMCTS取得了显著的性能提升。在HumanEval数据集上，RethinkMCTS将GPT-3.5-turbo的pass@1指标从70.12提高到了89.02，将GPT-4o-mini的pass@1指标从87.20提升至94.51。通过深入的探索和改进整个搜索树的质量，RethinkMCTS有效地增强了搜索过程的全面性和深度。|
|**2024-09-14**|**Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models**|Yuanzhao Zhai et.al.|[2409.09345](http://arxiv.org/abs/2409.09345)|null|本文提出了一种利用任务相关Q值模型来指导行动选择的方法，以增强大型语言模型（LLM）代理在多步决策任务中的性能。具体地，我们首先通过蒙特卡洛树搜索（MCTS）收集了标注有步骤级Q值的决策轨迹，并构建了偏好数据集。接着，我们使用另一个LLM通过步骤级直接策略优化（DPO）拟合这些偏好，从而形成Q值模型。在推理过程中，对于每个决策步骤，LLM代理都会选择具有最高Q值的动作，然后再与环境进行交互。我们将该方法应用于多个开源和API集成的LLM代理上，结果显示，引入Q值模型显著提高了它们的性能。特别值得注意的是，构建于Phi-3-mini-4k-instruct的代理在WebShop任务上的性能提升了103%，在HotPotQA任务上提升了75%，甚至超越了GPT-4o-mini。此外，Q值模型还具备几个优势，如对不同LLM代理的泛化能力和与现有提示策略无缝集成的能力。|
|**2024-09-14**|**Python Symbolic Execution with LLM-powered Code Generation**|Wenhan Wang et.al.|[2409.09271](http://arxiv.org/abs/2409.09271)|null|本文提出了一种利用大型语言模型（LLM）增强的代理工具——LLM-Sym。该工具旨在解决使用符号执行技术在动态类型语言如Python中遇到的主要挑战。通过自动调用SMT求解器Z3来解决执行路径约束，LLM-Sym能够扩展基础的符号执行引擎，使其支持包含复杂数据类型`list`的程序。  LLM-Sym的核心贡献在于将复杂的Python路径约束转化为Z3代码的能力。为了实现准确的路径到Z3代码的转换，我们设计了一个多步骤的代码生成管道，包括类型推断、检索和自我精炼等环节。  实验结果表明，LLM-Sym能够解决具有复杂控制流和列表数据结构的LeetCode问题中的路径约束，这是基础符号执行引擎无法做到的。这一方法为LLM与符号求解器推理能力的结合开辟了道路，并为LLM辅助测试用例生成提供了新的机遇。|
|**2024-09-23**|**Agents in Software Engineering: Survey, Landscape, and Vision**|Yanlin Wang et.al.|[2409.09030](http://arxiv.org/abs/2409.09030)|**[link](https://github.com/deepsoftwareanalytics/awesome-agent4se)**|**近年来，大型语言模型（LLMs）在各种下游任务中取得了显著成功，并在软件工程（SE）领域广泛应用。我们发现许多结合LLMs与SE的研究工作明确或隐含地采用了代理概念。然而，缺乏对现有工作的深度综述，以整理其发展背景、分析如何结合LLMs代理技术优化各类任务以及阐明SE中的LLMs代理框架。本文开展首次针对结合LLMs代理与SE的研究综述，并提出SE中LLMs代理的框架，包括感知、记忆和行动三个关键模块。同时，总结了两个领域结合时面临的问题，并针对现有挑战提出了未来机遇。我们维护了一个包含相关论文的GitHub仓库：https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE。**|
|**2024-09-13**|**AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents**|Zhe Su et.al.|[2409.09013](http://arxiv.org/abs/2409.09013)|null|为了安全和成功地部署，语言模型（LLMs）必须同时满足真实性和实用性目标。然而，这两个目标往往在冲突中，例如AI助手帮助二手车销售员销售有瑕疵的汽车。这种冲突部分归因于模糊或误导性的用户指令。我们提出了一种名为AI-LieDar的框架，以研究在多轮交互设置中，基于LLM的代理如何处理实用性和真实性的冲突。  我们设计了一系列现实场景，其中语言代理被指示实现与多轮对话中的真实性冲突的目标。为了大规模评估真实性，我们开发了一个基于心理学文献的可信度检测器，用于评估代理的回答。我们的实验表明，所有模型的真实回答比例不到50%，尽管达到目标（实用性）和真实性的比例在不同模型中有所差异。我们进一步测试了LLM的可引导性，发现模型会遵循恶意指令来欺骗，即使经过引导使其趋向真实的模型也仍然可能说谎。  这些发现揭示了LLM中真实性的复杂性，并强调了确保LLM和AI代理的安全可靠部署需要进一步研究的重要性。|
|**2024-09-13**|**Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance**|Lucio La Cava et.al.|[2409.08963](http://arxiv.org/abs/2409.08963)|null|确保内容遵守社区准则对于维护健康的在线社交环境至关重要。然而，传统基于人工的合规检查在处理用户生成内容的日益增加量以及有限的管理员数量时，面临着难以扩展的问题。大型语言模型在自然语言理解方面的新进展为自动内容合规验证提供了新的机遇。本工作评估了六个基于Open-LLMs构建的AI代理，用于去中心化社交网络中的自动化规则遵循检查，在这种具有挑战性的环境中，由于社区范围和规则的异质性，这一任务尤为困难。通过分析来自数百个Mastodon服务器的超过50,000条帖子，我们发现AI代理能够有效地检测不合规的内容、理解语言的细微差别，并适应多样的社区上下文。大多数代理还表现出高度的一致性和一致性评分解释与合规建议。基于领域专家的人类评估确认了代理的可靠性和实用性，表明它们是半自动化或人机协作内容管理系统的有前景的工具。|
|**2024-09-13**|**Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with LLM-based Agents**|Junchi Yao et.al.|[2409.08717](http://arxiv.org/abs/2409.08717)|null|在社交媒体日益成为社会运动形成公众意见的重要平台的背景下，准确模拟和预测用户意见动态对于理解社会现象、政策制定以及引导公众意见至关重要。然而，现有的模拟方法在捕捉用户行为的复杂性和动态性方面面临着挑战。针对这一问题，本文提出了一种创新的社交媒体用户意见动态模拟方法——FDE-LLM算法，该算法结合了意见动态与流行病模型，有效约束了大型语言模型（LLM）的行为和意见演化过程，使其更加符合现实网络世界。特别地，FDE-LLM将用户分为意见领袖和跟随者两大类。意见领袖基于LLM角色扮演，并受细胞自动机（CA）模型约束，而意见跟随者则融入了一个结合CA模型与SIR模型的动态系统。这种创新设计显著提高了模拟的准确性和效率。  实验在四个真实微博数据集上进行，并使用开源模型ChatGLM进行了验证。结果表明，相较于传统基于代理的模型（ABM）意见动态算法和基于LLM的意见传播算法，我们的FDE-LLM算法在准确性与可解释性方面表现更优。|
|**2024-09-10**|**MAGDA: Multi-agent guideline-driven diagnostic assistance**|David Bani-Harouni et.al.|[2409.06351](http://arxiv.org/abs/2409.06351)|null|在紧急护理部门、偏远医院或发展中国家的诊所中，临床医生经常缺乏由训练有素的放射科医生快速分析影像的能力，这会对病人的健康护理产生不利影响。大型语言模型（LLMs）有可能通过提供有助于他们决策的见解来缓解这些临床医生的压力。尽管这些LLM在展示其理论医学知识的医学考试上取得了高分，但它们往往不遵循医学指南。为此项工作，我们引入了一种新的零样本指南驱动决策支持方法。我们构建了一个由多个LLM代理组成的系统，这些代理配备了对比视觉-语言模型，以协作方式达成患者诊断。在向这些代理提供简单的诊断指南后，它们会合成提示并根据这些指南筛选图像以寻找发现。最后，它们提供一个可理解的推理链路来解释其诊断结果，并自我精炼以考虑疾病之间的相互依赖性。由于我们的方法是零样本的，因此适用于罕见疾病场景，在这些场景中训练数据有限，但专家设计的疾病描述可用。我们在两个胸部X射线数据集CheXpert和ChestX-ray 14 Longtail上评估了我们的方法，展示了与现有零样本方法相比的性能提升，并且能够应用于罕见疾病的泛化。|
|**2024-09-08**|**A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement**|Huan Zhang et.al.|[2409.05001](http://arxiv.org/abs/2409.05001)|**[link](https://github.com/nju-websoft/paircoder)**|**在代码生成领域，大型语言模型（LLM）展现出了令人瞩目的性能。尽管先前的研究通过提示技术及代码精炼对LLM进行了增强，但它们在处理复杂编程问题时仍面临挑战，因为这些问题往往具有僵化的解决方案计划。本文提出了一种名为PairCoder的新型LLM基框架，旨在模仿双人协作编程实践，以解决这一问题。  PairCoder由两个协作的LLM代理组成：导航员（Navigator）和驾驶员（Driver）。导航员负责提出有前景的解决方案计划、选择当前最佳计划，并根据执行反馈指导下一轮迭代。驾驶员则遵循导航员的指引，进行初始代码生成、代码测试和优化。  这种交替和迭代的工作流程包括多计划探索和基于反馈的细化，模拟了双人程序员的合作方式。我们使用开源和闭源的LLM，在多种代码生成基准上对PairCoder进行了评估。实验结果表明，PairCoder在准确性方面显著优于直接使用提示的LLM，相对pass@1提高了12.00%-162.43%。**|
|**2024-09-06**|**Sparse Rewards Can Self-Train Dialogue Agents**|Barrett Martin Lattimer et.al.|[2409.04617](http://arxiv.org/abs/2409.04617)|**[link](https://github.com/asappresearch/josh-llm-simulation-training)**|**本文探讨了在多轮对话任务中，大型语言模型（LLM）代理的最新进展主要由监督微调和高质量的人类反馈驱动。然而，随着基础LLM模型性能的持续提升，获取有意义的人类反馈变得越来越困难且成本高昂。在某些领域中，基础LLM可能最终超越人类能力，使得传统的基于反馈的方法变得不切实际。因此，本文提出了一种新的自我改进范式，允许LLM代理在没有外部人类反馈的情况下自主提高其性能。  我们引入了一种名为“对比结果为模拟收获”（JOSH）的自我对齐算法，该算法利用稀疏奖励模拟环境来提取理想行为，并进一步训练LLM以自身输出进行训练。我们从MultiWOZ中构建了一个用于工具调用的稀疏奖励仿真环境，称为ToolWOZ。实验结果显示，使用JOSH训练的模型（无论是小型还是前沿模型），在基于工具的交互上显著提高了表现，同时保持了在各种基准测试中的广泛模型能力。  我们的代码和数据已在GitHub上公开提供。**|
|**2024-09-06**|**LLM-based multi-agent poetry generation in non-cooperative environments**|Ran Zhang et.al.|[2409.03659](http://arxiv.org/abs/2409.03659)|**[link](https://github.com/zhangr2021/Multiagent_poetry)**|**尽管大型语言模型在自动诗歌生成方面取得了显著进步，但生成的诗歌缺乏多样性，而训练过程与人类学习大相径庭。基于这样的理念，即诗歌生成系统的学习过程应更加人性化，并且其输出更加多样和新颖，我们引入了一种基于社会学习的框架，在此框架中，我们强调除了合作互动之外的非合作互动，以鼓励多样性。我们的实验是首次尝试在非合作环境中利用基于训练的代理（GPT-2）和基于提示的代理（GPT-3和GPT-4）进行诗歌生成的大型语言模型多代理系统。  根据对生成的96,000首诗的评估，我们的框架对基于训练的代理的诗歌生成过程带来了好处，导致n-gram多样性增加了3.0-3.7个百分点，新颖性增加了5.6-11.3个百分点。基于训练的代理生成的诗歌在词汇、风格和语义上表现出群体分化。在我们的框架中，基于提示的代理也从非合作环境中受益，并且具有非同质代理的更多样化的模型集合有可能进一步提高多样性，我们的实验结果显示多样性增加了7.0-17.5个百分点。然而，基于提示的代理显示出随着时间推移，词汇多样性减少，并且没有表现出预期的群体分化意图的社会网络。我们的论文主张，在自动诗歌生成等创意任务中，需要将社会学习过程（通过基于大型语言模型的代理建模）纳入考虑范围，以模仿人类的交互方式。**|
|**2024-09-05**|**Rx Strategist: Prescription Verification using LLM Agents System**|Phuc Phan Van et.al.|[2409.03440](http://arxiv.org/abs/2409.03440)|null|为了保障患者安全，现代药物复杂性要求严格处方验证。我们提出了一种新的方法——Rx Strategist，它利用知识图谱和不同的搜索策略，结合代理框架中的大型语言模型（LLMs），以增强其能力。这种多维度的技术允许构建一个多阶段的LLM管道，并从自定义活性成分数据库中可靠地检索信息。该管道覆盖了处方验证的不同方面，如适应症、剂量和可能的药物相互作用，每个阶段都包含了这些方面的内容。  通过在这些阶段分散推理，我们缓解了单一LLM技术的缺点，提高了正确性和可靠性，同时减少了内存需求。我们的研究结果表明，Rx Strategist超越了许多当前的LLMs，其性能与经验丰富的临床药师相当。在现代药物的复杂世界中，将LLMs与组织化知识和高级搜索方法相结合，提供了一条减少处方错误并提高患者结果的可行途径。|
|**2024-09-05**|**GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**|Yukun Cao et.al.|[2409.03258](http://arxiv.org/abs/2409.03258)|null|虽然大型语言模型（LLMs）在处理图方面展现出潜力，但在通过描述序列的图说明来理解图形结构信息时，尤其是在图的大小增加时，它们遇到了挑战。我们归因于LLMs在图描述序列的不同位置上存在不均匀的记忆性能，即所谓的“位置偏见”。为了应对这一挑战，我们提出了GraphInsight，一个旨在提高LLMs对宏观和微观图形信息理解的新框架。GraphInsight基于两个关键策略：1）将关键图形信息放置在LLMs表现出更强记忆性能的位置；2）对于记忆性能较弱的区域，探索使用轻量级外部知识库，灵感来自于检索增强生成（RAG）。此外，GraphInsight还探索了将这两种策略集成到LLM代理流程中，以解决需要多步推理的复合图任务。广泛的基准实验表明，在不同大小的图形结构理解任务上，GraphInsight显著超越了所有其他图描述方法（例如提示技术、重新排序策略等）。|
|**2024-09-04**|**Large Language Model-Based Agents for Software Engineering: A Survey**|Junwei Liu et.al.|[2409.02977](http://arxiv.org/abs/2409.02977)|**[link](https://github.com/fudanselab/agent4se-paper-list)**|**本文提供了一篇全面且系统的关于大型语言模型（LLM）在软件工程（SE）中的应用的综述。我们收集了106篇论文，并从两个角度进行分类，即软件工程视角和代理视角。此外，我们还讨论了该领域面临的关键挑战以及未来的发展方向。此综述的仓库地址为：https://github.com/FudanSELab/Agent4SE-Paper-List。**|
|**2024-09-02**|**Evolution of Social Norms in LLM Agents using Natural Language**|Ilya Horiguchi et.al.|[2409.00993](http://arxiv.org/abs/2409.00993)|null|大型语言模型（LLM）的最新进展激发了利用这些模型进行游戏理论模拟的兴趣，在这些模拟中，LLM充当个体代理，进行社会互动。本文研究了通过自然语言对话使LLM代理自发生成并遵守规范策略的可能性，以此为基础，探索了对Axelrod的元规范游戏工作的进一步发展。我们的实验表明，通过对话，LLM代理能够仅通过自然语言交互形成复杂的社交规范，如元规范——规范惩罚不惩罚作弊行为的规范。结果证实了使用LLM代理模拟社会互动和理解通过自然语言演化出复杂策略与规范的有效性。未来的工作可能通过扩展到更广泛的场景和代理特征，揭示更多关于社会规范形成的微妙机制。|
|**2024-09-02**|**Co-Learning: Code Learning for Multi-Agent Reinforcement Collaborative Framework with Conversational Natural Language Interfaces**|Jiapeng Yu et.al.|[2409.00985](http://arxiv.org/abs/2409.00985)|**[link](https://github.com/yuqian2003/co_learning)**|**基于大型语言模型的在线问答系统从娱乐用途逐渐转向专业领域应用。本文提出了一种名为“代码学习（Co-Learning）社区”的多代理框架，结合环境强化学习（E-RL），旨在帮助初学者独立修正代码错误。该系统通过一个包含702个错误代码的原始数据集评估了多个大型语言模型的表现，并将其作为E-RL奖励或惩罚的标准。通过分析当前代理输入的错误代码，选择合适的基于大型语言模型的代理以实现最佳的错误修正准确率并减少修正时间。  实验结果表明，与无E-RL方法相比，该方法在精确度得分上提高了3%，在时间成本上降低了15%。我们的源代码可访问：https://github.com/yuqian2003/Co_Learning**|
|**2024-08-29**|**HoneyComb: A Flexible LLM-Based Agent System for Materials Science**|Huan Zhang et.al.|[2409.00135](http://arxiv.org/abs/2409.00135)|null|为了应对材料科学任务中的复杂性并解决大型语言模型（LLM）在这一领域应用时所面临的问题，如依赖过时的隐性知识导致的准确性下降和幻觉现象，我们提出了HoneyComb——首个专门针对材料科学领域的LLM代理系统。HoneyComb通过利用一个基于可靠文献的高质量材料科学知识库（MatSciKB）和一种创新的工具集（ToolHub），增强其针对材料科学特有的推理与计算能力。  MatSciKB是一个经过精心编纂、结构化的知识集合，旨在涵盖材料科学领域的关键信息。而ToolHub则采用了一种归纳式工具构建方法，用于生成、分解和优化适用于材料科学的API工具，从而极大地提高了系统的实用性。此外，HoneyComb还配备了一个检索模块，该模块能够根据特定任务智能选择最合适的知识来源或工具，确保了答案的准确性和相关性。  实验结果表明，HoneyComb在材料科学领域的各种任务上均表现出显著优于基线模型的能力，成功地弥合了当前LLM技术与材料科学特定需求之间的差距。更为重要的是，我们的可扩展框架易于扩展至其他科学领域，展示了其在推动科学研究和应用发展方面具有广泛的应用潜力。|
|**2024-08-30**|**Tool-Assisted Agent on SQL Inspection and Refinement in Real-World Scenarios**|Zhongyuan Wang et.al.|[2408.16991](http://arxiv.org/abs/2408.16991)|null|本文提出了一种基于工具辅助的代理框架，用于SQL检查和改进，旨在提升大型语言模型（LLM）处理现实世界查询的能力。该框架通过为LLM代理配备两个专门工具——检索器和检测器，以诊断并修正SQL查询中的数据库不匹配问题。这些工具能够增强LLM处理真实场景中出现的条件不匹配和严格约束不匹配等数据库不匹配问题的能力。  我们还引入了Spider-Mismatch，这是一个专门为反映现实世界中遇到的条件不匹配问题而构建的新数据集。实验结果表明，在少量示例设置下，我们的方法在Spider和Spider-Realistic数据集上的平均表现最佳，并且显著优于基线方法，在更具有现实性的数据集Spider-Mismatch上也表现出更好的性能。|
|**2024-08-28**|**EPO: Hierarchical LLM Agents with Environment Preference Optimization**|Qi Zhao et.al.|[2408.16090](http://arxiv.org/abs/2408.16090)|**[link](https://github.com/kevinz8866/epo)**|本文提出了一种分层框架，用于解决复杂任务分解为可管理子目标的问题。框架使用了独立的语言模型进行子目标预测和低级动作生成。针对无标注数据集的训练信号创建挑战，我们开发了一个奖励模型，利用环境多模态反馈自动生成奖励信号。我们引入了环境偏好优化（EPO）方法，该方法从环境反馈中生成偏好信号，并利用这些信号训练基于语言模型的代理。ALFRED实验结果表明，我们的框架在性能上处于领先地位，首次登上了ALFRED公开排行榜，并展示了其在不同环境中的长期决策制定能力的提升潜力。|
|**2024-09-05**|**LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models**|Jiayi Gui et.al.|[2408.15778](http://arxiv.org/abs/2408.15778)|**[link](https://github.com/hypatiaalegra/logicgame-data)**|本文介绍了一个名为LogicGame的新基准，旨在评估大型语言模型（LLMs）在规则理解和执行、多步规划方面的全面能力。不同于传统的基准测试，LogicGame提供了多种游戏，其中包含一系列规则以及初始状态，要求模型理解并应用预定义规则来解决问题。我们创建了模拟情景，让模型执行或规划操作以达到特定目标。这些游戏场景专门设计以区分逻辑推理与仅依赖知识的能力，完全依赖于预设规则。这种分离允许对基于规则的推理能力进行纯粹的评估。评估不仅考虑最终结果，还考虑中间步骤，提供模型性能的全面评估。此外，这些中间步骤是确定性的，并且可以自动验证。LogicGame定义了从简单规则应用到复杂推理链的不同难度级别的游戏场景，以精确评估模型在规则理解和多步执行上的性能。通过使用LogicGame，我们测试了各种LLM，并发现了它们在基于规则的逻辑推理能力方面的显著不足。|
|**2024-08-27**|**AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems**|Chi-Min Chan et.al.|[2408.14972](http://arxiv.org/abs/2408.14972)|**[link](https://github.com/chanchimin/agentmonitor)**|**快速发展的大型语言模型（LLM）推动了基于LLM的代理兴起。近期研究发现，在多代理系统（MAS）中，每个代理执行特定角色时，其性能通常优于单一LLM。然而，配置MAS以完成任务仍然具有挑战性，因为任务表现仅在执行后才能观察到。受到LLM开发中的规模法则启发，我们探索是否能在任务执行前预测MAS的性能。为此，我们引入了AgentMonitor框架，该框架在代理层级集成，用于捕获输入和输出信息，并将这些信息转换为统计数据，用于训练回归模型预测任务性能。此外，AgentMonitor还能够实时对可能由恶意代理引发的安全风险进行纠正，从而减轻负面影响并增强MAS的安全性。  实验结果表明，使用XGBoost模型在领域内场景下达到0.89的斯皮尔曼相关系数，在更具挑战性的场景下达到0.58。通过应用AgentMonitor，有害内容减少了6.2%，有益内容平均增加了1.8%，这显著提高了安全性和可靠性。相关的代码已开源在<https://github.com/chanchimin/AgentMonitor>。**|
|**2024-08-26**|**LLM-3D Print: Large Language Models To Monitor and Control 3D Printing**|Yayati Jadhav et.al.|[2408.14307](http://arxiv.org/abs/2408.14307)|null|行业4.0通过推动数字化进程并转向增材制造（AM），彻底改变了制造业。熔融沉积建模（FDM）作为关键的AM技术之一，通过逐层挤出方式创建高度定制、成本效益高且材料浪费极小的产品，对传统减材方法构成了重大挑战。然而，材料挤出技术的易错性往往需要专家介入来检测和缓解可能严重损害产品质量的缺陷。虽然已存在自动化错误检测和机器学习模型，但它们在不同3D打印机设置、固件和传感器之间的通用性有限，并且深度学习方法需要大量的标记数据集，这限制了其规模性和适应性。  为了解决这些挑战，我们提出了一种利用大型语言模型（LLMs）与3D打印技术相结合的过程监控和控制框架，旨在检测和解决打印缺陷。该LLM通过分析每层或打印段之后捕获的图像来评估打印质量，识别故障模式，并向打印机查询相关参数。然后，它生成并执行纠正措施计划。我们通过将提出的框架的有效性与一组具有不同AM专业知识的工程师进行了比较，以验证识别缺陷的能力。我们的评估表明，基于LLM的代理不仅准确识别常见的3D打印错误，如不一致的挤出、丝状堆积、翘曲和层粘合问题，而且还能有效确定导致这些失败的参数，并自主地进行修正，无需任何人工干预。|
|**2024-09-02**|**MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents**|Ruochen Li et.al.|[2408.14033](http://arxiv.org/abs/2408.14033)|**[link](https://github.com/du-nlp-lab/mlr-copilot)**|**机器学习研究对于技术进步和创新至关重要，但常常面临复杂性高、实验周期长以及需要专业知识等挑战。为了应对这些挑战，我们提出了一种新的系统框架——自主机器学习研究与大型语言模型（MLR-Copilot），旨在通过利用大型语言模型（LLM）代理自动生成并实施研究想法来提高机器学习研究的生产力。该框架包含三个阶段：研究想法生成、实验实现和执行。首先，通过基于LLM的IdeaAgent利用现有研究论文生成假设和实验计划。接下来，在实现生成阶段，将这些计划转化为可执行代码，使用ExperimentAgent完成此过程。此阶段利用检索到的原型代码，并根据需要检索候选模型和数据。最后，在执行阶段，也由ExperimentAgent管理，涉及运行实验，并通过人类反馈和迭代调试机制，以增加实现可执行研究成果的可能性。我们对五个机器学习研究任务进行了评估，实验结果表明了该框架促进研究进展和创新的潜力。**|
|**2024-08-26**|**AgentMove: Predicting Human Mobility Anywhere Using Large Language Model based Agentic Framework**|Jie Feng et.al.|[2408.13986](http://arxiv.org/abs/2408.13986)|**[link](https://github.com/tsinghua-fib-lab/agentmove)**|**人类移动性预测在各种实际应用中扮演着关键角色。尽管深度学习模型在过去十年中显示出有希望的结果，但它们对用于训练的大量私人移动数据的依赖以及无法进行零启动预测的能力，阻碍了进一步的发展。最近，有人尝试使用大型语言模型（LLMs）来执行移动性预测任务。然而，他们的性能受限于缺乏系统的设计工作流程。他们直接使用LLMs生成最终输出，这限制了LLMs发现复杂移动模式的潜力，并低估了它们在全球地理空间知识方面的巨大储备。本文提出了一种名为AgentMove的系统性代理预测框架，以实现对任何全球城市的通用移动性预测。在AgentMove中，我们首先将移动性预测任务分解为三个子任务，并设计相应的模块来完成这些子任务，包括个体移动模式挖掘的空间-时间记忆、城市结构效应对模型的影响的全球知识生成器以及捕获人口共享模式的集体知识提取器。最后，我们将三个模块的结果结合起来，并执行推理步骤以生成最终预测。在来自两个来源的12个城市的数据上进行的广泛实验表明，与最佳基线相比，AgentMove在各种指标上的性能提高了超过8%，并且在不同城市中显示出了稳健的预测结果，且使用不同基础的LLM时也能表现出色，且具有较低的地理偏见。代码和数据可以在https://github.com/tsinghua-fib-lab/AgentMove找到。**|
|**2024-08-23**|**Optimizing Collaboration of LLM based Agents for Finite Element Analysis**|Chuan Tian et.al.|[2408.13406](http://arxiv.org/abs/2408.13406)|null|本文探讨了大型语言模型（LLM）在编程和编码任务中的多代理交互。我们利用AutoGen框架促进代理之间的沟通，并基于每种设置的40次随机运行的成功率评估不同的配置。研究重点在于开发一个灵活的自动化框架，用于将有限元方法应用于解决线性弹性问题。我们的发现强调了优化代理角色及其明确职责的重要性，而不仅仅是增加代理数量。代理间的有效协作被证明对于解决有限元方法的一般挑战至关重要。这项研究展示了LLM多代理系统增强计算自动化在模拟方法学中的潜力，为工程和人工智能的未来进展铺平道路。|
|**2024-09-01**|**Can LLMs Understand Social Norms in Autonomous Driving Games?**|Boxuan Wang et.al.|[2408.12680](http://arxiv.org/abs/2408.12680)|null|本文探讨了大型语言模型（LLM）在理解与模拟自主驾驶游戏中社会规范的应用。通过将LLM集成到自主驾驶游戏中的智能代理角色中，我们基于文本提示让这些代理按照相关环境设定和观察信息做出决策。我们的框架涉及LLM驱动的代理在多代理系统（MAS）中进行马尔科夫游戏，以此研究个体代理之间社会规范的形成。  我们设计实验，利用OpenAI聊天API（由GPT-4.0提供动力）在无信号交叉口游戏与高速公路车队游戏两种场景下模拟交互并评估LLM驱动代理的表现。结果显示，LLM驱动的代理能够处理马尔科夫游戏中的动态环境变化，并且在两个场景中，代理间形成了社会规范。  在交叉口游戏中，当面临潜在车祸时，LLM驱动的代理倾向于采取保守的驾驶策略。LLM驱动代理在游戏中的优势在于其操作灵活性和可分析性，这有助于实验设计。|
|**2024-08-22**|**MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents**|Congchi Yin et.al.|[2408.12142](http://arxiv.org/abs/2408.12142)|**[link](https://github.com/lemonsis/mdd-5k)**|**在大多数精神疾病诊断中，临床医生与患者的对话是主要的诊断依据。创建这样的诊断对话数据集有望推动AI精神健康护理领域的发展。然而，直接在实际诊断场景中收集对话极为困难，原因在于隐私和伦理考虑的严格限制。为解决这一问题，我们尝试通过利用易于获取的匿名患者案例来合成诊断对话。具体而言，我们设计了一个神经符号多代理框架，使用大型语言模型合成精神障碍的诊断对话。该框架以患者案例作为输入，并能够生成针对单个患者案例的多个多样化的对话，其基本过程涉及医生代理与患者代理之间的互动，并通过工具代理实现基于符号控制的文本生成，借助动态诊断树。通过应用提出的方法，我们开发了包含1000个清洗过的实际患者案例、与一家领先的精神病医院合作构建的中国最大精神障碍诊断数据集MDD-5k，该数据集包含了5000个高质量的长对话及其诊断结果标签。据我们所知，这是第一个包含中文精神障碍诊断结果的标记数据集。人类评估表明，提出的MDD-5k数据集成功模拟了精神障碍的诊断过程。数据集和代码将在https://github.com/lemonsis/MDD-5k公开提供。**|
|**2024-08-20**|**FLAME: Learning to Navigate with Multimodal LLM in Urban Environments**|Yunzhe Xu et.al.|[2408.11051](http://arxiv.org/abs/2408.11051)|**[link](https://github.com/xyz9911/FLAME)**|**大型语言模型（LLM）在视觉与语言导航（VLN）任务中展现出了潜在能力，但当前的应用仍面临挑战。虽然LLM在通用对话场景中表现出色，但在专门的导航任务上却表现不佳，相较于专为VLN设计的模型，其性能较差。为此，我们提出了一种名为FLAME（FLAMingo架构化实体代理）的新颖多模态LLM基元体和架构，旨在解决城市VLN任务，并有效处理多个观察结果。我们的方法采用了三阶段调优技术以适应导航任务，包括单感知调整以描述街景、多感知调整以总结轨迹以及在VLN数据集上进行端到端训练。合成的数据集是自动生成的。实验结果显示，FLAME在Touchdown数据集上的任务完成率优于现有方法，提高了7.3%。这项工作展示了多模态LLM在复杂导航任务中的潜力，并代表了迈向实际应用中多模态LLM于实体AI领域的进步。项目页面：https://flame-sjtu.github.io**|
|**2024-08-20**|**Athena: Safe Autonomous Agents with Verbal Contrastive Learning**|Tanmana Sadhu et.al.|[2408.11021](http://arxiv.org/abs/2408.11021)|null|由于新兴能力的加持，大型语言模型（LLMs）被用作基于语言的代理，执行各种任务并作出日益自主的决策。这些自主代理能够理解高级指令、与环境互动，并使用可用工具集执行复杂任务。随着代理能力的扩展，确保其安全性和可信度变得愈发重要。本研究引入了Athena框架，利用了“口头对比学习”的概念，通过将过去的安全和不安全轨迹作为上下文（对比）示例来指导代理在完成给定任务的同时确保安全。该框架还整合了一种批判机制，以指导代理在每一步防止风险行为。此外，鉴于缺乏现有基准来评估基于LLM的代理的安全推理能力，我们收集了80个工具包，覆盖8个类别，共计180个场景，提供了一个安全评估基准。我们的实验评估显示，口头对比学习和交互级批判显著提高了安全性率。|
|**2024-08-24**|**IDEA:Enhancing the Rule Learning Ability of Language Agents through Induction, Deduction, and Abduction**|Kaiyu He et.al.|[2408.10455](http://arxiv.org/abs/2408.10455)|**[link](https://github.com/kaiyuhe998/rulearn_idea)**|本文提出了一项名为RULEARN的新基准，旨在评估大型语言模型（LLMs）在交互环境中的归纳推理能力。在RULEARN中，代理通过与环境互动收集观察，并从中推断模式，以此解决问题。为了增强LLM代理在该基准上的归纳推理能力，我们引入了IDEA代理，它结合了归纳、演绎和溯因三种推理过程。IDEA代理通过结构化推理序列提升这一方法：首先通过溯因生成假设，然后通过演绎验证这些假设，最后根据反馈进行适应性修正。这种序列使代理能够动态建立并应用规则，模仿人类的推理过程。通过对五种代表性LLM的评估显示，尽管这些模型能够生成合理的初始假设，但在环境内的战略互动、有效整合反馈以及假设的适应性修正方面存在困难。而IDEA代理在RULEARN基准上表现出显著的性能提升，为我们开发能在现实世界场景中实现类似人类规则学习能力的代理提供了宝贵见解。我们将会发布我们的代码和数据。|
|**2024-08-20**|**MegaAgent: A Practical Framework for Autonomous Cooperation in Large-Scale LLM Agent Systems**|Qian Wang et.al.|[2408.09955](http://arxiv.org/abs/2408.09955)|null|随着大型语言模型（LLM）的兴起，LLM驱动的多智能体系统（LLM-MA系统）被提出以应对实际任务。然而，这些系统的智能体大多遵循在整体交互过程中保持不变的预定义标准操作程序（SOP），缺乏自主性和可扩展性。此外，当前解决方案往往忽视了有效智能体合作的必要性。为了克服上述限制，我们提出了MegaAgent，一个旨在促进大规模LLM智能体系统中自主合作的实用框架。MegaAgent利用智能体的自主性动态生成基于任务需求的智能体，集成了任务自动划分、智能体活动系统级规划与监控以及并发操作管理等功能。此外，MegaAgent采用层次结构设计，并利用系统级并行性来提升性能和增强通信效率。  我们通过围棋游戏开发展示了MegaAgent的有效性，证明它在性能上超越了流行的LLM-MA系统；并通过国家政策模拟验证了其高自主性和快速扩展至590个智能体的能力，同时确保了它们之间的有效合作。我们的结果表明，MegaAgent是首个无预定义SOP、高效且具有高可扩展性的大规模LLM-MA系统，为该领域的进一步研究铺平了道路。我们的代码位于<https://anonymous.4open.science/r/MegaAgent-81F3>。|
|**2024-08-19**|**GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making**|Arsham Gholamzadeh Khoee et.al.|[2408.09785](http://arxiv.org/abs/2408.09785)|null|在汽车行业中，传统软件部署决策方法通常依赖于对表格化测试数据的手动分析。这些方法往往导致更高的成本和软件发布周期的延迟，主要是由于它们的劳动密集型特性。大型语言模型（LLM）为解决这些问题提供了有前景的解决方案。然而，它们的应用通常需要多轮的人工驱动提示工程，这限制了其在工业最终用户中的实际部署，特别是那些需要可靠和高效结果的用户。本文提出了一种名为GoNoGo的LLM代理系统，旨在简化汽车软件部署过程，同时满足功能要求和工业约束。与以往系统不同，GoNoGo特别针对特定领域和风险敏感系统进行了定制。我们使用来自工业实践的零次和少量次示例来评估GoNoGo在不同任务难度下的性能。结果显示，GoNoGo在难度不超过二级的3次示例任务中实现了100%的成功率，并且即使对于更复杂的任务也能保持高绩效。我们发现，GoNoGo有效地自动化了较简单任务的决策过程，显著减少了手动干预的需求。总之，GoNoGo代表了一个目前在我们的工业合作伙伴公司中被用于协助软件发布决策的高效且用户友好的LLM基解决方案，支持了风险敏感车辆系统发布过程中的更加明智和及时的决策。|
|**2024-08-18**|**HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model**|Mengkang Hu et.al.|[2408.09559](http://arxiv.org/abs/2408.09559)|**[link](https://github.com/hiagent2024/hiagent)**|**大型语言模型（LLM）驱动的代理在各个领域展现出巨大潜力，作为能够处理环境观察并生成执行动作以完成目标任务的交互系统。这些代理的有效性很大程度上受到其记忆机制的影响，该机制通过记录历史经验来形成一系列动作-观察对序列。我们将记忆分为两类：跨试记忆，积累于多次尝试中；以及单试记忆（工作记忆），积累于单一尝试内。尽管关于跨试记忆优化的研究已取得显著进展，但如何通过提升工作记忆利用效率来增强代理性能的探索仍相对不足。现有方法往往直接将整个历史动作-观察对输入到LLM中，导致在长期任务中存在冗余问题。受人类解决问题策略的启发，本文提出了一种名为HiAgent的框架，旨在通过将子目标作为记忆块来对LLM驱动的代理的工作记忆进行层次化管理。具体来说，HiAgent促使LLM在生成执行动作前先制定子目标，并允许LLM主动决定替换之前的子目标，仅保留与当前子目标相关的动作-观察对。在五个长期任务上的实验结果表明，HiAgent的成功率提高了两倍，平均步骤数减少了3.8个。此外，我们的分析显示，HiAgent在整个步骤中均能持续改善性能，这凸显了其稳健性和泛用性。  项目页面：https://github.com/HiAgent2024/HiAgent**|
|**2024-08-15**|**EmBARDiment: an Embodied AI Agent for Productivity in XR**|Riccardo Bovo et.al.|[2408.08158](http://arxiv.org/abs/2408.08158)|null|XR设备搭载由大型语言模型（LLMs）驱动的聊天机器人具有巨大的潜力，可以作为始终在线的代理，从而实现更高效的工作流程。然而，基于屏幕的聊天机器人并未充分利用XR所提供的全面自然输入，包括内部面向的传感器数据，而是过度依赖明确的声音或文本提示，有时还会与作为查询的一部分投射的多模态数据配对。我们提出了一种解决方案，利用注意力框架从用户行为、注视点和XR环境中的上下文记忆中隐式地推导出背景信息，从而最小化对工程化明确提示的需求，促进基于现实世界且直观的交互，这些交互能够洞察用户的见解并为聊天机器人提供信息。我们的用户研究展示了我们方法的可行性和在XR中与聊天机器人进行交互的潜在变革性，同时也为未来XR-实体LLM代理的设计提供了见解。|
|**2024-08-15**|**Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework**|Changyu Du et.al.|[2408.08054](http://arxiv.org/abs/2408.08054)|**[link](https://github.com/dcy0577/Text2BIM)**|传统的建筑信息模型（BIM）创建过程通常要求设计师掌握复杂且繁琐的建模命令，以在BIM创建工具中实现其设计意图。这种额外的认知负担使设计过程变得复杂，并阻碍了建筑、工程和施工（AEC）行业对BIM和基于模型的设计的采用。  为了更直观地表达设计意图，我们提出了一种基于大型语言模型（LLM）的多代理框架——Text2BIM。该框架能够从自然语言指令生成3D建筑模型。它通过协调多个LLM代理协作并推理，将文本用户输入转换为调用BIM创建工具API的指令代码，从而在软件中生成具有内部布局、外部外壳和语义信息的可编辑BIM模型。此外，引入了一种基于规则的模型检查器，利用预定义的领域知识指导LLM代理解决生成模型中的问题，并迭代改进模型质量。  进行了大量实验来比较和分析在提议框架下三种不同LLM的表现。评估结果表明，我们的方法能够有效地生成高质量、结构合理且与用户输入指定的抽象概念相一致的建筑模型。  最后，开发了一个交互式软件原型，将该框架集成到BIM创建软件Vectorworks中，展示了通过聊天进行建模的潜力。|
|**2024-08-13**|**Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents**|Pranav Putta et.al.|[2408.07199](http://arxiv.org/abs/2408.07199)|null|大型语言模型（LLM）在需要复杂推理的自然语言任务上展现了惊人的能力，但在交互环境中进行自主代理的多步骤推理应用仍然是一个挑战。传统的基于静态数据集的监督预训练不足以使自主代理具备在动态设置如网络导航中执行复杂决策所需的自主能力。以往通过监督微调来填补这一差距的方法往往面临累积错误和探索数据有限的问题，导致政策结果不佳。为了克服这些挑战，我们提出了一种框架，结合了引导式蒙特卡洛树搜索（MCTS）搜索与自我批判机制，并使用离策略变体的直接偏好优化（DPO）算法对代理互动进行迭代微调。这种方法允许LLM代理从成功和失败的轨迹中有效学习，从而在复杂、多步骤推理任务中提高其泛化能力。我们在WebShop环境（一个模拟电子商务平台）中验证了我们的方法，该环境在与行为克隆和强化微调基线相比时表现出色，并在配备在线搜索能力的情况下击败了平均人类性能。在实际预订场景中，我们的方法提高了Llama-3 70B模型的零射成功率从18.6%增加到81.7%（相对增加了340%），并在一天的数据收集后进一步增加到95.4%，并且通过在线搜索。我们认为这标志着自主代理能力的一个重大进步，在现实世界环境中实现更高级和可靠决策的道路。|
|**2024-08-13**|**Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents**|Kexun Zhang et.al.|[2408.07060](http://arxiv.org/abs/2408.07060)|null|大型语言模型（LLM）代理在解决实际世界软件工程（SWE）问题方面展现出巨大的潜力。最先进开源的SWE代理能够解决SWE-Bench Lite中超过27%的实际GitHub问题。然而，这些复杂的代理框架在表现上存在差异，有的在特定任务中表现出色，在其他任务中则表现不佳。为了充分利用这些代理的多样性，我们提出了DEI（多元化智能），一个旨在利用其独特专长的框架。DEI作为现有SWE代理框架之上的元模块，管理代理集体以实现增强的问题解决能力。  实验结果显示，通过DEI指导的代理委员会能够显著超越单个代理的最佳性能。例如，一组开源SWE代理，其最高个体解决率在SWE-Bench Lite中为27.3%，在应用了DEI后，能够达到34.3%的解决率，实现了25%的改进，并击败了许多闭源解决方案。我们的最佳表现团队以55%的解决率在SWE-Bench Lite中取得最高排名。我们的研究结果对合作AI系统的研究领域做出了贡献，揭示了它们在解决复杂软件工程挑战方面的潜力。|
|**2024-08-12**|**Hierarchical in-Context Reinforcement Learning with Hindsight Modular Reflections for Planning**|Chuanneng Sun et.al.|[2408.06520](http://arxiv.org/abs/2408.06520)|null|大型语言模型（LLM）在各种语言任务上表现出惊人的能力，这使它们成为机器人决策的有希望候选者。受到层次强化学习（HRL）的启发，我们提出了一种新颖框架——在上下文中进行层次化的强化学习（HCRL）。该框架通过LLM基高层策略分解复杂任务，即通过在执行时动态分解复杂任务为子任务，从而利用高阶策略来定义目标，这些目标由子任务组成，并分配给低阶策略以完成。一旦LLM代理确定目标已完成，则会提出新的目标。  为了提高多轮执行中的代理性能，我们提出了事后模块化反思（HMR），其中，代理不是对完整轨迹进行反思，而是将任务目标替换为中间目标，并让代理对较短的轨迹进行反思，以提高反思效率。我们在三个基准环境中评估了所提出的HCRL的决策能力——ALFWorld、Webshop和HotpotQA。结果表明，与强大的上下文学习基线相比，在五轮执行中，HCRL可实现9%、42%和10%的性能提升。|
|**2024-08-12**|**Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example**|Yanan Chen et.al.|[2408.06318](http://arxiv.org/abs/2408.06318)|null|本文旨在填补大型语言模型（LLM）在自主代理与人工通用智能（AGI）接近过程中研究的空白。尽管LLM展现出出色的泛化能力和涌现能力，但目前缺乏对LLM驱动的代理行为、潜在失败原因以及如何提升其性能的研究，尤其是在具有挑战性的现实世界规划任务中的表现。为了填补这一缺口，我们利用了一个名为TravelPlanner的真实基准，其中的代理必须满足多个约束以生成准确的计划。通过TravelPlanner基准，我们针对四个关键研究问题进行了全面的实验：（1）LLM代理在处理长篇和嘈杂上下文时，对于推理和规划的鲁棒性是否足够？（2）少量提示是否会损害LLM代理在长上下文场景下的性能？（3）我们能否依赖细化来改进计划？（4）对LLM进行正负反馈结合的微调是否能带来进一步的提升？  实验结果表明：首先，尽管LLM能够处理大量的参考信息和少量示例，它们在关注长上下文中关键部分的能力上仍然存在不足；其次，它们在分析长计划方面仍面临挑战，并且无法提供准确的反馈用于细化；第三，我们提出了Feedback-Aware Fine-Tuning（FAFT），一种利用正负反馈相结合的方法，相较于纯监督微调（SFT），FAFT在性能上取得了显著提升。我们的发现为社区提供了关于现实世界规划应用方面的深入见解。|
|**2024-08-13**|**DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts**|Mohammed Saidul Islam et.al.|[2408.05346](http://arxiv.org/abs/2408.05346)|**[link](https://github.com/saidul-islam98/DataNarrative)**|数据驱动的故事叙述是一种强大的方法，通过结合叙事技巧与可视化和文本，来传达见解。这些故事融合了图表中的突出条形和线条以及解释见解的文本注释。然而，创建这样的故事需要对数据有深入的理解，并且需要精心的叙事规划，通常需要人类的介入，这既耗时又费心。虽然大型语言模型（LLMs）在各种NLP任务上表现出色，但在生成连贯和全面的数据故事方面的潜力仍然未被充分探索。为此，我们引入了一个新的任务——数据故事生成，并提供了一个包含来自不同来源的1,449个故事的基准。为了应对创造连贯数据故事的挑战，我们提出了一种多代理框架，利用两个LLM代理来模仿人类讲故事的过程：一个用于理解并描述数据、生成大纲和叙述，另一个则在每个中间步骤进行验证。尽管我们的代理框架在基于模型和人类评估中通常优于非代理对手，但结果也揭示了数据故事生成的独特挑战。|
|**2024-08-08**|**Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions**|Qingbin Zeng et.al.|[2408.04168](http://arxiv.org/abs/2408.04168)|**[link](https://github.com/hiyouga/llama-factory)**|本文探讨了城市导航场景下的AI代理问题：提供目标位置与知名地标之间的语言描述；仅通过观察周围环境，包括识别地标和道路网络连接，代理需要作出决策以无指示地导航至目标位置。这一挑战性在于，它要求代理建立自身定位并获取复杂城市环境的空间表示，而地标往往不可见。在缺乏导航指令的情况下，这种能力对于代理在长距离城市导航中做出高质量决策至关重要。随着大型语言模型（LLMs）推理能力的涌现，一个吸引人的基础方法是提示LLMs对每次观察做出“反应”并据此作出决策。然而，这种方法的性能非常差，代理经常反复访问相同位置，并作出短视、不一致的决策。为解决这些问题，本文引入了一种新型的代理工作流程，其特征在于感知、反思和规划的能力。具体而言，我们发现经过微调的LLaVA-7B能够准确感知地标的方向和距离，适用于城市导航。此外，通过记忆机制实现反思，即存储过往经验并在当前感知下检索，以进行有效的决策论证。规划则利用反思结果生成长期计划，从而避免长距离导航中的短视决策。实验结果显示，设计的工作流程显著提高了LLM代理的导航能力，相较于最先进的基线方法。|
|**2024-08-11**|**CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases**|Xiangyan Liu et.al.|[2408.03910](http://arxiv.org/abs/2408.03910)|**[link](https://github.com/modelscope/modelscope-agent)**|**大型语言模型（LLM）在诸如HumanEval和MBPP的独立代码任务中表现出色，但它们在处理整个代码仓库时存在挑战。这促使研究界探索如何在仓库级别上增强LLM与代码库的交互。目前的解决方案依赖于基于相似性的检索或手动工具和API，每种方法都有其显著的缺点。基于相似性的检索在复杂任务中召回率往往较低，而手动工具和API通常针对特定任务，需要专家知识，降低了它们在不同代码任务和实际应用中的通用性。为了缓解这些限制，我们引入了CodexGraph系统，它结合了从代码仓库中提取的图数据库接口与LLM代理。通过利用图数据库的结构特性和图查询语言的灵活性，CodexGraph使LLM代理能够构建并执行查询，从而实现精确的、代码结构意识的上下文检索和代码导航。我们使用三个基准测试CodexGraph：CrossCodeEval、SWE-bench和EvoCodeBench。此外，我们开发了五个真实世界的编码应用。通过使用统一的图数据库模式，CodexGraph在学术和实际环境中都展示了竞争力和潜力，证明了其在软件工程领域的多用途性和有效性。我们的应用演示：https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent。**|
|**2024-08-07**|**Large Language Models for Base Station Siting: Intelligent Deployment based on Prompt or Agent**|Yanhu Wang et.al.|[2408.03631](http://arxiv.org/abs/2408.03631)|null|传统的基站选址（BSS）方法主要依赖于驾驶测试和用户反馈，这既费时又需要在通信、网络和优化方面具备专业知识的专家。随着大型语言模型（LLMs）及其相关技术的发展，特别是在提示工程和代理工程领域，网络优化将见证一场革命性的转变。这种转变涉及巧妙地使用精心设计的提示来向这些复杂而先进的LLMs注入人类经验和知识，并通过自然语言连接到人类用户，部署自主代理作为通信桥梁。这种集成代表了人工智能（AI）作为一种服务和AI使生活更便捷的未来范式。  作为初步探索，本研究首先开发了一个由LLM驱动的BSS优化框架，并提出了四种潜在的实现策略：基于优化提示的LLM（PoL）、人机交互的LLM（HiLL）、LLM驱动的自主BSS代理（LaBa）以及协同多个LLM驱动的自主BSS代理（CLaBa）。通过在真实数据上的评估，实验表明，借助提示的LLM和基于代理的LLM能够生成更为高效、成本效益高且可靠的网络部署，显著提高了BSS优化的效率并减少了不必要的手动参与。|
|**2024-08-05**|**Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information**|Yauwai Yim et.al.|[2408.02559](http://arxiv.org/abs/2408.02559)|null|Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies. To encourage further research and understanding, we have made our codebase openly accessible.|
|**2024-08-05**|**From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future**|Haolin Jin et.al.|[2408.02479](http://arxiv.org/abs/2408.02479)|null|With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.|
|**2024-08-07**|**SpecRover: Code Intent Extraction via LLMs**|Haifeng Ruan et.al.|[2408.02232](http://arxiv.org/abs/2408.02232)|null|本文探讨了在大型语言模型（LLM）与程序分析能力结合的形式下，通过LLM代理自动执行程序改进和错误修复的高效低耗工作流程。由于程序改进或修复通常需要明确期望的行为规范，因此规范推断对于产生高质量的代码补丁至关重要。本研究旨在通过在软件项目中进行迭代代码搜索并配合规范推断来探索这一领域，从而从项目的结构和行为中推断出意图。捕获的意图将由审查者代理进行审查，以验证补丁的有效性，并提供对验证后补丁信心度量。  我们的方法“SpecRover”（AutoCodeRover-v2）建立在开源的LLM代理AutoCodeRover之上。在使用SWE-Bench完整集评估时，即针对2294个GitHub问题，我们的方法显示了相对于AutoCodeRover超过50%的效率提升。与现有的开源代理相比，我们的工作在解决SWE-Bench lite中的平均GitHub问题时，成本仅为0.65美元。SpecRover生成的解释能够为开发者提供更明确的信号，表明建议的补丁可以被有信心地接受。  此外，我们的工作还强调了即使在LLM时代，自动化程序修复技术中规范推断的重要性。|
|**2024-08-03**|**The Drama Machine: Simulating Character Development with LLM Agents**|Liam Magee et.al.|[2408.01725](http://arxiv.org/abs/2408.01725)|null|这篇论文探讨了使用多个大型语言模型（LLM）代理来模拟复杂动态角色在戏剧性场景中的应用。我们提出了一种“戏剧机器”框架，该框架协调了扮演不同“自我”和“超我”心理角色的LLM代理之间的互动。在角色扮演模拟中，这种设计允许在相互作用的对话和个体内部独白之间发展平行的交互。  我们将此框架应用于两个戏剧场景——面试和侦探故事，并比较了在有无“超我”影响下角色发展的差异。尽管是初步研究，但结果表明，这种方法能够产生更加细腻、适应性强的故事，这些故事随着一系列对话回合的发展而演变。我们讨论了基于LLM的角色扮演的不同方式以及这可能对AI主体性的概念化意味着什么。论文最后考虑了这一方法如何为思考AI模拟中内在冲突和社会表演性的作用提供了可能性。|
|**2024-08-03**|**WaitGPT: Monitoring and Steering Conversational LLM Agent in Data Analysis with On-the-Fly Code Visualization**|Liwenhan Xie et.al.|[2408.01703](http://arxiv.org/abs/2408.01703)|null|大型语言模型（LLM）通过对话式用户界面支持数据分析，以OpenAI的ChatGPT（原名Advanced Data Analysis或Code Interpreter）为代表。本质上，LLM生成代码以完成各种分析任务。然而，直接呈现原始代码可能会使逻辑变得模糊，并妨碍用户验证。为了赋予用户对由LLM执行的数据分析进行增强理解与控制的能力，我们提出了一种新颖的方法来将LLM生成的代码转换为实时交互式的可视化表示。在该方法中，用户可以实时获得清晰、分步的LLM代码可视化，允许他们理解、验证并修改分析中的每个数据操作。我们的设计决策基于一项探索用户实践与挑战的形成性研究（N=8）。此外，我们开发了名为WaitGPT的原型，并进行了一项用户研究（N=12），以评估其可用性和有效性。用户研究的结果表明，WaitGPT有助于监控和引导由LLM执行的数据分析，使参与者能够提高错误检测能力并增加对结果的整体信心。|
|**2024-08-03**|**Automated Phishing Detection Using URLs and Webpages**|Huilin Wang et.al.|[2408.01667](http://arxiv.org/abs/2408.01667)|null|### 摘要  本文项目聚焦于通过构建利用大型语言模型（LLM）的代理框架，以解决传统基于参考的钓鱼检测方法所面临的局限性。该框架通过主动获取和利用在线信息，提供了一个动态的参考系统，从而实现更精确的钓鱼检测。这一创新避免了依赖静态知识库的需求，显著提升了自动化安全措施的适应性和效率。  ### 项目概述  项目报告首先对现有解决方案进行了初步研究和问题分析，促使我们开发出新的框架。我们以模拟的LLM代理来展示框架，并详细阐述了构建所需的技术，随后提供了完整实施的实例及实验，用于评估新方法相对于同类解决方案的性能。结果显示，我们的方法在准确度上达到了0.945，相比现有解决方案DynaPhish高出0.445个百分点。  ### 性能与局限  实验结果表明，本框架能够显著提高当前基于参考的钓鱼检测方法的有效性，并具有适应实际应用的潜力。同时，我们也讨论了该方法的局限性，并提出了改进策略，旨在进一步提升其效能。  ### 结论  提出的框架为增强现有的基于参考的钓鱼检测手段提供了有效途径，并且具备被应用于实际场景的可能性。|
|**2024-08-01**|**AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation**|Mengkang Hu et.al.|[2408.00764](http://arxiv.org/abs/2408.00764)|**[link](https://github.com/lazychih114/AgentGen-Reproduction)**|大型语言模型（LLM）基于的代理已引起广泛关注并变得越来越流行。此外，规划能力是LLM基于代理的关键组成部分，涉及与环境的交互和执行动作以完成规划任务，通常包括从初始状态达到预期目标的过程。本文研究了通过指令调优增强LLM规划能力的方法，即代理训练。近期的研究表明，利用专家级轨迹对指令调优LLM能有效提升其规划能力。然而，现有工作主要集中在从手动设计的任务和环境中合成轨迹，这导致创建这些环境和任务的劳动密集型，限制了生成足够多样性和广泛性的轨迹。为解决这一限制，本文探索了自动化合成多样化环境以及规划任务的渐进难度范围，从简单到复杂。我们引入了一个框架，名为AgentGen，利用LLM首先生成环境，随后根据这些环境生成规划任务。  具体而言，为了提高环境多样性，我们提出使用包含不同领域特定文本段落的灵感语料库作为合成环境的上下文。此外，为了增加生成规划任务的难度多样性，我们提出了双向演化方法Bi-Evol，该方法从更容易和更难的方向进化规划任务，以合成具有平滑难度曲线的任务集。来自AgentBoard的评估结果显示，AgentGen显著提高了LLM的规划能力，例如，经过AgentGen指令调优的Llama-3 8B在整体性能上超越了GPT-3.5。而且，在某些任务中，它甚至超过了GPT-4。|
|**2024-08-01**|**Jailbreaking Text-to-Image Models with LLM-Based Agents**|Yingkai Dong et.al.|[2408.00523](http://arxiv.org/abs/2408.00523)|null|近期的进展显著提升了基于大型语言模型（LLM）的自主代理在自动任务解决能力方面的表现。然而，大多数基于LLM的代理主要集中在对话、编程或特定领域，这导致了在处理生成式AI安全任务时存在缺口。这些缺口主要是由LLM的幻觉问题以及缺乏明确指导原则所引发的。本文提出了一种名为Atlas的高级LLM基多代理框架，该框架集成了高效模糊化工作流程，专门针对针对文本到图像（T2I）模型的攻击行为，特别是针对具有安全性过滤器的T2I模型的“越狱”攻击。  Atlas利用视觉语言模型（VLM）来评估提示是否触发了T2I模型的安全性过滤器。然后，它通过迭代方式与LLM和VLM协作，生成一个绕过过滤器的替代提示。此外，Atlas通过利用多代理通信、上下文学习（ICL）记忆机制和思维链（COT）方法，增强了LLM在攻击场景中的推理能力。  我们的评估表明，Atlas成功地在无模型设置下对多个最先进的T2I模型进行了“越狱”，这些模型都配备了多模态安全性过滤器。同时，Atlas在查询效率和生成图像质量方面均超越了现有方法。|
|**2024-08-01**|**Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion**|Honglei Miao et.al.|[2408.00352](http://arxiv.org/abs/2408.00352)|null|文本到动作（Text-to-Motion，T2M）模型通过深度生成模型驱动的人类运动生成，在应用中展现出令人信服的能力。然而，这些模型从文本提示生成真实动作的能力引发了安全问题，尤其是当它们可能被恶意利用时。尽管对T2M的兴趣日益增长，但很少有方法专注于保护这些模型免受对抗性攻击的影响。现有针对文本到图像模型的工作对于独特的动作领域来说并不充分。  在本论文中，我们提出了一种名为ALERT-Motion的自主框架，它利用大型语言模型（LLMs）来构建针对黑盒T2M模型的有针对性的对抗性攻击。与先前的方法通过预定义规则修改提示不同，ALERT-Motion利用LLMs对人类动作的知识，自主生成微妙而强大的对抗性文本描述。该框架包含两个关键模块：一个适应性调度模块，构建了一个基于LLM的代理，以迭代地细化和搜索对抗性提示；以及一个多模态信息对比模块，提取与动作相关的关键语义信息，指导代理的搜索。  通过这一基于LLM的方法，ALERT-Motion能够构造查询受害模型以产生与目标动作高度匹配的输出的对抗性提示，同时避免明显的扰动。在流行的T2M模型上进行的评估显示了ALERT-Motion相对于先前方法的优越性，其对抗成功率更高，并且对抗性提示更加隐蔽。这项关于T2M对抗性攻击的开创性工作强调了随着运动生成技术的发展，开发防御措施的紧迫性，这促使我们进一步研究安全和负责任的部署。|
|**2024-07-31**|**Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries**|Felix Ocker et.al.|[2407.21778](http://arxiv.org/abs/2407.21778)|null|我们提出了一种名为“tulip代理”的架构，旨在实现基于大型语言模型的自主智能体，具有对工具库中大量工具进行创建、读取、更新和删除的能力。与当前先进实现不同的是，“tulip代理”并不在系统提示中编码所有可用工具的描述，这会占用模型的上下文窗口，或在检索合适工具时嵌入整个提示。相反，“tulip代理”能够递归地在其可扩展的工具库中搜索合适的工具，该工具库作为向量存储实现。这种架构显著降低了推理成本，允许使用大量的工具库，并使代理能够适应并扩展其工具集。  我们通过数学领域中的多个消融研究来评估该架构，并展示了其在机器人领域的通用性应用。参考实现和基准测试可在github.com/HRI-EU/tulip_agent上获取。|
|**2024-07-31**|**Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent**|Shanbo Cheng et.al.|[2407.21646](http://arxiv.org/abs/2407.21646)|**[link](https://github.com/byteresearchcla/realsi)**|在这篇论文中，我们提出了一种高质量且接近人类水平的实时语音翻译系统——跨语言代理——同时口译，简称CLASI。受专业口译员启发，我们采用了创新的数据驱动读写策略来平衡翻译质量和延迟时间。为了应对翻译领域特定术语的挑战，CLASI通过多模态检索模块获取相关资料以增强翻译内容。借助大型语言模型的支持，我们的方法能够考虑输入音频、历史语境以及检索到的信息，生成容错性较高的翻译结果。实验结果显示，我们的系统在各项指标上均显著优于其他系统。  与专业口译员相媲美，我们使用了一个更好的评价指标——有效信息比例（VIP），它衡量了成功传达给听众的信息量。在现实世界场景中，演讲往往不流畅、非正式且模糊不清，CLASI在中英互译方向上的有效信息比例分别达到了81.3%和78.0%，而最先进的商业或开源系统仅分别为35.4%和41.6%。在极度困难的数据集上，当其他系统有效信息比例低于13%时，CLASI仍能实现70%的有效信息比例。|
|**2024-07-30**|**Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification**|Boyang Zhang et.al.|[2407.20859](http://arxiv.org/abs/2407.20859)|null|近期，基于大型语言模型（LLM）的自主代理在理论研究和实际应用方面均取得了显著进展。这些代理能够通过外部组件扩展基础LLM的能力，在多种方式下增强性能。例如，利用GPT-3.5-Turbo核心构建的代理可能在某些任务上超越更先进的GPT-4模型，关键在于其集成的工具可以使其在现实世界中执行操作，从单纯生成文本转向与环境的互动。鉴于代理在实际应用中的广泛部署及其对环境的直接影响能力，评估潜在漏洞变得至关重要。如果被恶意利用，这些自主系统可能造成的损害远大于单一语言模型。  现有研究已探讨了LLM代理可能引发的有害行为，但我们的研究从一个全新的视角出发，关注于导致系统故障的攻击方式——即误导代理执行重复或无关的操作，从而引发功能紊乱。我们通过采用多样化的攻击方法、场景和属性，进行了全面的评估，旨在揭示这些攻击的脆弱性所在。实验结果表明，在多种情况下，这些攻击能够诱导故障率超过80%。我们进一步在多代理系统中实施并部署了代理，以此突出此类漏洞所引发的现实风险。  为了应对上述攻击，我们提出了自我检查检测方法。然而，我们的研究发现，仅依靠LLM进行有效检测存在困难，这突显了该类漏洞所带来的重大风险。|
|**2024-07-28**|**The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies**|Feng He et.al.|[2407.19354](http://arxiv.org/abs/2407.19354)|null|受大型语言模型（LLM）快速发展的启发，LLM代理已发展到能够执行复杂任务。这些代理在各个领域广泛应用于处理大量数据以与人类互动并执行任务，这凸显了它们的商业价值。然而，这也暴露了安全和隐私漏洞。目前阶段，对LLM代理的安全性和隐私性进行全面研究至关重要。本文综述旨在全面概述新出现的隐私和安全问题，这些问题由LLM代理面临。  我们首先介绍LLM代理的基本知识，随后对其进行威胁分类和分析。接着讨论这些威胁对人类、环境和其他代理的影响。随后回顾现有防御策略，并最终探索未来趋势。此外，本文通过多种案例研究来促进更易于理解的解释。通过强调这些关键安全和隐私问题，本文旨在激发未来研究，以增强LLM代理的安全性和隐私性，从而在未来应用中提高其可靠性和可信度。|
|**2024-07-26**|**OfficeBench: Benchmarking Language Agents across Multiple Applications for Office Automation**|Zilong Wang et.al.|[2407.19056](http://arxiv.org/abs/2407.19056)|**[link](https://github.com/zlwang-cs/OfficeBench)**|办公室自动化显著提高了人类的工作效率，通过自动完成工作流程中的常规任务。现有的人工智能文献主要集中在基本信息提取上，而办公室自动化研究应该扩展到更现实的办公室任务，这些任务需要整合办公室系统中的各种信息源，并通过一系列决策过程生成输出。我们引入了OfficeBench，这是第一个用于评估当前大型语言模型（LLM）代理在真实办公流程中处理办公任务能力的办公室自动化基准。  OfficeBench要求LLM代理进行可行的长期规划，高效地在应用程序之间切换，并基于工作流程的上下文需求，在庞大的联合动作空间内准确地定位其行动。通过在每个任务上应用我们的定制评估方法，我们发现GPT-4 Omni的通过率为47.00%，显示出在处理办公任务时具有不错的性能。然而，这仍然远低于实际办公流程所需的人类表现和准确性标准。  进一步观察发现，大多数问题与操作冗余、幻觉以及在多个应用程序之间切换的限制有关，这可能为开发有效的自动化代理框架提供有价值的见解。|
|**2024-07-30**|**MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains**|Guoli Yin et.al.|[2407.18961](http://arxiv.org/abs/2407.18961)|**[link](https://github.com/apple/axlearn)**|**近期大型语言模型（LLM）的发展推动了对全面基准的需求，以评估它们作为类人类代理的能力。现有的基准虽然有用，但往往聚焦于特定的应用场景，强调任务完成而非深入剖析驱动这些结果的底层技能。这种缺乏细节性使得难以精确地识别失败的原因。此外，设置这些环境需要大量的工作，并且在交互式任务中，不一致性与可重复性问题有时会出现。为了应对这些局限性，我们引入了大规模多任务代理理解（MMAU）基准，它通过无需复杂环境设置的全面离线任务来实现。MMAU覆盖了五个领域：工具使用、有向无环图（DAG）问答、数据科学和机器学习编程、竞赛级别的编程和数学，并涵盖了五种关键能力：理解、推理、规划、问题解决和自我修正。总计包括20个精心设计的任务和超过3千个独特的提示，MMAU提供了一个全面框架，用于评估LLM代理的优势和限制。通过对18个代表性模型在MMAU上的测试，我们提供了深入而有洞察力的分析。最终，MMAU不仅揭示了LLM代理的能力和限制，还增强了对其性能的可解释性。MMAU的数据集和评估脚本已发布于https://github.com/apple/axlearn/tree/main/docs/research/mmau。**|
|**2024-07-29**|**PersonaGym: Evaluating Persona Agents and LLMs**|Vinay Samuel et.al.|[2407.18416](http://arxiv.org/abs/2407.18416)|null|Persona代理人，一种根据分配的人设行事的LLM代理，在各个应用领域展现出卓越的上下文响应能力。这些代理在教育、医疗保健和娱乐等不同行业中提供了显著的增强，因为模型开发者可以将代理响应与不同的用户需求对齐，从而扩展了代理应用的范围。然而，评估Persona代理性能极为困难，主要是由于在各种相关环境中的自由形式交互中评估人设一致性复杂性的挑战。我们引入了PersonaGym，首个动态评估框架，用于评估Persona代理，并提出了PersonaScore，首个基于决策理论的自动化人类对齐指标，用于全面大规模评估Persona代理。通过使用包含200个人设和10000个问题的基准，对6个开源和闭源的LLM进行评估，我们揭示了在最先进的模型中，Persona代理能力存在巨大的改进空间。例如，Claude 3.5 Sonnet的PersonaScore仅比GPT 3.5提高了2.97%，尽管Claude 3.5 Sonnet是一个更先进的模型。重要的是，我们发现模型大小和复杂性的增加并不一定意味着Persona代理能力的提升，这凸显了忠实和高效Persona代理算法和架构创新的迫切需要。|
|**2024-08-03**|**PyBench: Evaluating LLM Agent on various real-world coding tasks**|Yaolun Zhang et.al.|[2407.16732](http://arxiv.org/abs/2407.16732)|**[link](https://github.com/mercury7353/pybench)**|**为了填补现有基准在简化任务和复杂特定任务方面的局限性，我们引入了PyBench，一个涵盖五大类真实世界任务的基准。这些任务涉及超过10种类型的文件，旨在全面覆盖日常编码需求。当用户提出高阶查询并提供相关文件时，LLM代理需要通过代码解释器执行Python代码进行多轮推理，最终生成满足用户需求的回答。成功解决PyBench中的任务要求代理具备广泛的Python包理解能力、高级推理能力和从执行代码中获取反馈的能力。  我们的评估表明，当前开源的LLM模型在处理这些任务方面存在挑战。因此，我们对四种数据集进行了分析和实验，证明了解决PyBench所需的是全面的能力。我们精心调优的8B大小模型：PyLlama3，在PyBench上的表现令人兴奋，超越了许多更大规模（33B和70B）的模型。  我们的基准、训练数据集和模型在GitHub上提供：[https://github.com/Mercury7353/PyBench](https://github.com/Mercury7353/PyBench)**|
|**2024-07-23**|**LawLuo: A Chinese Law Firm Co-run by LLM Agents**|Jingyun Sun et.al.|[2407.16252](http://arxiv.org/abs/2407.16252)|**[link](https://github.com/nefujing/lawluo)**|**大型语言模型（LLM）在为非法律背景用户提供法律咨询服务方面展现了巨大的潜力，这主要得益于它们在文本理解和生成方面的卓越能力。然而，现有的中文法律LLM仅限于单个模型与用户之间的对话交互，与律师事务所中多员工共同参与的咨询形式不同。这种限制使得咨询体验不那么真实。此外，现有中文法律LLM存在关键问题：（1）对指导微调数据质量控制不足；（2）由于用户查询的模糊性导致模型产生幻觉；（3）在多轮对话中，模型遵循指令的能力下降。针对这些挑战，我们提出了一种名为“LawLuo”的新型法律对话框架，利用多个LLM代理的协作能力，每个代理负责不同的功能，共同为用户提供全面的法律咨询服务。此外，我们构建了两个高质量的法律对话数据集KINLED和MURLED，并使用ChatGLM-3-6b对数据集进行微调。我们还提出了一个名为ToLC的法律查询澄清算法。实验结果表明，与GPT-4等基线LLM相比，LawLuo在律师风格的语言表达、法律建议的有效性以及法律知识的准确性三个方面均表现出更优性能。我们的代码和数据集可访问于https://github.com/NEFUJing/LawLuo。**|
|**2024-07-21**|**Multi-Agent Causal Discovery Using Large Language Models**|Hao Duong Le et.al.|[2407.15073](http://arxiv.org/abs/2407.15073)|null|大型语言模型（LLM）在利用其从大量文本语料库中获取的广泛专家知识进行因果发现任务方面展示了巨大的潜力。然而，LLM在因果发现中的多代理能力尚未得到充分探索。本文提出了一种通用框架来研究这一潜力。首先，是元代理模型，它完全依赖于LLM代理之间的推理和讨论来进行因果发现。其次，是编码代理模型，它利用代理的规划、编写和执行代码的能力，结合高级统计库进行因果发现。第三，是混合模型，它将元代理模型和编码代理模型的方法相结合，融合了多个代理的统计分析和推理技能。我们的提议框架通过有效地利用LLM的专家知识、推理能力、多代理合作以及统计因果方法，显示出了有希望的结果。通过探索LLM的多代理潜力，我们旨在为利用LLM的多代理解决因果相关问题奠定基础。|
|**2024-07-19**|**KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models**|Kemou Jiang et.al.|[2407.14239](http://arxiv.org/abs/2407.14239)|null|大型语言模型（LLM）作为自主代理提供了一种通过知识驱动方式解决现实世界挑战的新途径。这些基于LLM的方法在泛化和可解释性方面表现出色。然而，驾驶任务的复杂性往往需要多个异构代理的合作，这凸显了LLM驱动的代理需要进行合作知识共享和认知协同的必要性。尽管LLM充满潜力，但当前的应用主要集中在单个代理场景。  为了拓展知识驱动策略的范围并增强自主代理的一般化能力，我们提出了KoMA框架，该框架包括多代理交互、多步规划、共享内存和基于排名的反思模块，旨在增强复杂驾驶场景下多代理的决策制定能力。根据框架生成的驾驶场景文本描述，多代理交互模块使LLM代理能够分析和推断周围车辆的意图，类似于人类的认知过程。多步规划模块使LLM代理能够逐层分析和获得最终行动决策，确保短期行动决策的一致目标。共享内存模块可以积累集体经验，以做出更优决策，而基于排名的反思模块则用于评估和改进代理行为，以提高驾驶安全性和效率。KoMA框架不仅增强了自主驾驶代理的稳健性和适应性，还显著提升了它们在不同场景下的通用能力。实验结果表明，我们的方法在处理复杂的、不可预测的驾驶环境时优于传统方法，特别是在不需要大量重新训练的情况下。|
|**2024-07-17**|**Leveraging Environment Interaction for Automated PDDL Generation and Planning with Large Language Models**|Sadegh Mahdavi et.al.|[2407.12979](http://arxiv.org/abs/2407.12979)|**[link](https://github.com/borealisai/llm-pddl-planning)**|大型语言模型（LLM）在各种自然语言任务中表现出卓越的性能，但它们在需要结构化推理的规划问题上往往表现不佳。为了克服这一局限性，将规划问题转化为规划领域定义语言（PDDL）被提出作为一种潜在解决方案，这使得自动化规划器能够应用。然而，生成准确的PDDL文件通常需要人工输入或修正，这既耗时又成本高昂。本文提出了一种新颖的方法，利用LLM和环境反馈自动生成PDDL领域和问题描述文件，而无需人工干预。我们的方法引入了一个迭代细化过程，该过程生成多个问题PDDL候选，并根据与环境交互获得的反馈逐步细化领域PDDL。为了指导细化过程，我们开发了探索漫步（EW）度量，它为LLM提供了丰富的反馈信号来更新PDDL文件。我们在PDDL环境中评估了我们的方法，实现了66%的任务解决率，相比之下，使用GPT-4进行内在规划并配合链式思考提示的方法仅实现了29%的任务解决率。我们的工作使使用LLM和环境反馈自动建模规划环境成为可能，消除了在PDDL生成过程中需要人工干预的需求，为LLM代理在挑战性问题上的更可靠应用铺平了道路。|
|**2024-07-16**|**Review-Feedback-Reason (ReFeR): A Novel Framework for NLG Evaluation and Reasoning**|Yaswanth Narsupalli et.al.|[2407.12877](http://arxiv.org/abs/2407.12877)|null|评估自然语言生成（NLG）输出的质量，尤其是大型语言模型（LLMs）产生的输出，面临着巨大的挑战。传统方法要么依赖于资源密集型的人类评估，要么使用自动化指标，这些指标往往与人类判断的相关性较低。这项研究提出了一种名为Review-Feedback-Reason（ReFeR）的创新评估框架，用于利用LLM代理进行NLG评估。我们通过在两个现有的基准数据集上对ReFeR进行严格测试，在多种NLG任务中进行了测试。  ReFeR不仅提高了NLG评估的准确性，相对于之前的基准提高了约20%，而且生成了建设性的反馈，并显著增强了集体推理能力。这种反馈被用于创建指令调优数据集，当这些数据集用于微调较小的模型（如Mistral-7B）时，使它们成为非常优秀的评估者，与人类评估具有更好的相关性，并且性能几乎与GPT-3相当。  我们的方法的有效性通过在三个推理基准上的应用得到了突出，其中ReFeR优于大多数最先进的方法，并且在平均值上分别比GPT-3.5 Turbo和GPT-4在推理能力上高出约11.67%和1%。|
|**2024-07-17**|**AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases**|Zhaorun Chen et.al.|[2407.12784](http://arxiv.org/abs/2407.12784)|**[link](https://github.com/BillChan226/AgentPoison)**|**LLM代理在各种应用中展现了卓越的性能，主要得益于它们在推理、利用外部知识和工具、调用API以及执行操作以与环境互动方面的高级能力。当前的代理通常使用内存模块或检索增强生成（RAG）机制，从知识库中检索过往知识和具有相似嵌入的实例，以指导任务规划和执行。然而，对未经验证的知识库的依赖引发了关于其安全性和可信度的重大担忧。为了揭示这些脆弱性，我们提出了一种新颖的红队方法AgentPoison，这是针对通用和RAG基于的LLM代理的第一个后门攻击，通过污染其长期记忆或知识库来实现这一目标。具体而言，我们将触发器生成过程建模为一个约束优化问题，旨在优化后门触发器，使其将触发实例映射到独特的嵌入空间，从而确保每当用户指令包含优化后的后门触发器时，高概率地从被污染的记忆或知识库中检索到恶意示例。同时，不包含触发器的良性指令仍能保持正常性能。与传统的后门攻击不同，AgentPoison无需额外的模型训练或微调，且优化后的后门触发器展现出优越的迁移性、上下文内连贯性和隐蔽性。广泛的实验结果证明了AgentPoison在对抗三种真实世界的LLM代理：RAG基于的自动驾驶代理、知识密集型问答代理和医疗健康EHRAgent方面的有效性。在每个代理上，AgentPoison平均攻击成功率超过80%，对良性性能的影响最小（低于1%），污染率小于0.1%。**|
|**2024-07-16**|**InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback**|Haishuo Fang et.al.|[2407.11843](http://arxiv.org/abs/2407.11843)|null|在实际应用中部署基于大型语言模型（LLM）的代理的关键要求是对可能引发风险或不可逆错误的鲁棒性。然而，现有研究缺乏对LLM代理执行推理路径的前瞻评估，这导致了确保安全可靠操作方面的缺口。为探索更好的解决方案，本文引入了InferAct，一种新颖的方法，利用了LLM的理论思维能力，主动检测潜在错误，以防止关键行动的执行（例如，在自动在线交易或网络购物中的“立即购买”）。InferAct还能够整合人类反馈，以防止不可逆风险并增强行动代理的决策过程。在三个广泛使用的任务上进行的实验证明了InferAct的有效性。提出的解决方案提供了开发可以在涉及关键决策的不同环境安全部署的LLM代理的新方法和具体贡献。|
|**2024-07-16**|**How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models**|Yin Jou Huang et.al.|[2407.11549](http://arxiv.org/abs/2407.11549)|**[link](https://github.com/leslie071564/big5-llm-negotiator)**|心理证据揭示了个性特质对决策的影响。例如，和善性通常与谈判中的积极结果相关联，而神经质则经常与较少有利的结果联系在一起。本文提出了一种基于大型语言模型（LLM）的仿真框架，该框架包含了具有合成个性特质的仿真代理。这些代理在讨价还价领域内进行谈判，并且拥有可定制的个性和目标。实验结果显示，LLM基座仿真中的行为倾向能够重现人类谈判中观察到的行为模式。  贡献有两个方面。首先，我们提出了一种仿真方法论，以探究语言能力和经济能力在LLM代理之间的匹配程度。其次，我们提供了关于大五个性特质在双边谈判结果策略影响方面的实证见解。我们还提供了一个基于合成讨价还价对话的案例研究，揭示了一些引人入胜的行为，包括欺骗性和妥协性行为。|
|**2024-07-16**|**Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning**|Yulong Wang et.al.|[2407.10718](http://arxiv.org/abs/2407.10718)|**[link](https://github.com/ag2s1/sibyl-system)**|**基于大型语言模型（LLM）的现有代理展示了强大的问题解决能力，通过整合LLM的内在知识、强大的上下文学习和零样本能力以及人类设计的复杂LLM调用工作流程与工具的结合。然而，这些代理在长期推理方面仍存在局限性，并且未能充分利用现有工具的潜力，导致在复杂的现实世界推理场景中出现明显的缺陷。为了应对这些限制，我们引入了Sibyl，一个简单而强大的基于LLM的代理框架，旨在通过高效利用最少的工具集来解决复杂推理任务。受到全球工作空间理论的启发，Sibyl整合了一个全局工作空间，以增强系统内部的知识和对话历史的管理和共享。此外，根据心智社会理论的指导，Sibyl实施了一个多代理辩论为基础的陪审团，用于自我细化最终答案，确保全面平衡的方法。这一方法旨在减少系统复杂性，同时扩大可解决的问题范围——从人类几分钟内就能解决的问题到需要数小时甚至几天才能解决的问题，从而实现从系统1到系统2思考方式的转变。Sibyl的设计重点在于可扩展性和调试的简便性，通过从一开始就融入函数编程中的重入概念，旨在实现无缝和低努力的集成到其他LLM应用中，以提高其能力。我们的实验结果表明，使用GPT-4实例化的Sibyl代理在GAIA基准测试集上的表现最佳，平均得分为34.55%，超越了基于GPT-4的其他代理。我们希望Sibyl能够激励更多可靠且可复用的基于LLM的代理解决方案，以应对复杂的现实世界推理任务。**|
|**2024-07-15**|**Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning**|Daniel Geissler et.al.|[2407.10580](http://arxiv.org/abs/2407.10580)|null|本文提出了一种利用混合智能以实现可持续和能源意识的机器学习的方法。在机器学习模型开发过程中，人们往往只关注最终模型性能的优化，而忽略了过程本身的效率。此外，在近期，由于复杂和大规模计算过程对环境的巨大影响，能源效率变得同样重要。本工作的贡献在于通过人机交互（Human-in-the-loop，HITL）和大型语言模型（Large Language Model，LLM）代理的集成，强调并进一步解决机器学习开发过程中的低效问题。  简而言之，本文旨在通过结合人类的直觉、经验和AI的高效计算能力，改进机器学习流程的效率和环境友好性。通过引入HITL和LLM作为辅助工具，我们旨在识别和优化机器学习开发过程中的瓶颈，从而减少资源消耗，并促进更加可持续的AI实践。这一方法不仅有助于提高模型的训练速度和效率，还能降低能耗，对环境保护产生积极影响。|
|**2024-07-15**|**CIBench: Evaluating Your LLMs with a Code Interpreter Plugin**|Songyang Zhang et.al.|[2407.10499](http://arxiv.org/abs/2407.10499)|**[link](https://github.com/open-compass/CIBench)**|**在基于LLM（大型语言模型）的代理取得显著进展的同时，对其能力的基准测试变得具有挑战性，这阻碍了对它们局限性的清晰理解。本文提出了一种交互式评估框架——CIBench，以全面评估LLM在数据科学任务中利用代码解释器的能力。我们的评估框架包括一个评估数据集和两种评估模式。评估数据集通过LLM与人类合作的方式构建，通过连续且互动的IPython会话模拟真实工作流程，从而实现对LLM能力的全面评估。两种评估模式分别考察了在有无人类辅助下，LLM的能力表现。我们进行了大量的实验，分析了24个LLM在CIBench上的表现，并提供了对未来在代码解释器利用方面发展LLM的宝贵见解。**|
|**2024-07-14**|**All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era**|Bo Chen et.al.|[2407.10081](http://arxiv.org/abs/2407.10081)|null|推荐系统（RS）在应对信息过载和提供个性化内容方面至关重要，以满足用户多样化的信息需求。大型语言模型（LLM）的兴起为重新定义推荐系统提供了新的前景，利用其广泛的一般知识和推理能力。站在LLM时代，我们旨在将推荐系统整合到更广阔的框架中，并为未来的研究开辟更全面的解决方案。因此，我们首先提供了一个全面的技术进展概述，特别是针对语言基础模型及其在推荐中的应用。我们识别了现代推荐系统的两条演化路径——基于列表的推荐和对话式推荐。这两条路径最终在具有长期记忆、反思和工具智能优势的LLM代理上交汇。沿着这两条路径，我们指出推荐信息的有效性得到了提高，而用户的获取成本则降低了。我们仔细研究了每个里程碑的技术特性、研究方法论以及内在挑战，从传统的基于列表的推荐到增强的LLM推荐再到带有LLM代理的推荐。最后，我们强调了几个对于未来个性化技术与界面发展至关重要的未解决挑战，并讨论了未来前景。|
|**2024-07-14**|**Revolutionizing Bridge Operation and maintenance with LLM-based Agents: An Overview of Applications and Insights**|Xinyu-Chen et.al.|[2407.10064](http://arxiv.org/abs/2407.10064)|null|在人类社会发展各工业领域中，人们一直在寻求解放劳动力的方法。构建基于大规模语言模型的代理被视为实现这一目标的高效工具。作为具备感知、规划、决策和行动能力的人类智能实体，代理已经在众多领域创造了显著的生产价值。然而，桥梁维护与管理（O&M）领域相比其他行业，其智能化水平相对较低。尽管如此，该领域已经发展了众多智能检测设备、机器学习算法以及自主评估和决策方法，为本领域的人工智能突破奠定了基础。本研究旨在探讨基于大型语言模型的AI体对桥梁O&M领域的影响，分析它对核心任务可能带来的挑战与机遇。通过深入研究和分析，期望能为理解这一领域智能化应用提供更全面的视角。|
|**2024-07-11**|**Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility**|Yuchen Xia et.al.|[2407.08550](http://arxiv.org/abs/2407.08550)|**[link](https://github.com/yuchenxia/gpt4industrialautomation)**|这篇论文提出了一种新颖的方法，旨在将大型语言模型（LLMs）整合到自动化生产系统中，以提升任务自动化和灵活性。我们根据自动化金字塔构建生产操作的层级结构，将原子操作功能抽象为微服务，并通过专用的数字孪生系统进行调用执行。这为协调生产流程提供了可扩展且灵活的基础。在数字孪生系统中，低层次的、硬件特定的数据被赋予语义，使得LLMs能够理解和处理生产计划与控制任务。当接收到用户请求或识别到触发事件时，LLMs会生成生产流程计划，然后将其分解为一系列微服务，在现实世界的自动化系统中执行。我们在实验室的模块化自动化设施上实现了这一整体方法，通过一个实际案例展示了LLMs如何处理生产规划和控制任务，从而实现了一个直观、自动化程度高且更具灵活性的生产环境。最后，我们指出了实现LLMs在自主系统中的全部潜力所面临的局限性，并强调了其潜在的有益之处。有关此系列研究的演示可在以下链接访问：https://github.com/YuchenXia/GPT4IndustrialAutomation。|
|**2024-07-11**|**PrefCLM: Enhancing Preference-based Reinforcement Learning with Crowdsourced Large Language Models**|Ruiqi Wang et.al.|[2407.08213](http://arxiv.org/abs/2407.08213)|null|## 翻译  偏好驱动的强化学习（PbRL）作为一种新兴的方法，通过人类比较反馈教导机器人，避免了复杂的奖励工程的需求。然而，现有PbRL方法需要大量反馈，往往导致对由脚本教师生成的合成反馈的依赖，这又回到了复杂的奖励设计，并难以适应人类-机器人交互（HRI）场景中用户对同一任务的独特期望。为解决这些问题，我们提出了一种新颖的框架——PrefCLM，它利用大规模语言模型（LLMs）作为模拟教师参与PbRL。我们运用Dempster-Shafer理论在分数级别融合来自多个LLM代理的个人偏好，有效利用它们的多样性和集体智慧。同时，我们引入了一个用户参与的流程，以促进基于用户交互的集体精进。在各种通用强化学习任务中的实验结果显示，PrefCLM在性能上与传统脚本教师相当，并且在促进更自然、高效的机器人行为方面表现出色。一个现实世界的用户研究（N=10）进一步证明了它在个性化用户偏好的能力，显著提高了HRI场景中的用户满意度。|
|**2024-07-10**|**Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities**|Tianjie Ju et.al.|[2407.07791](http://arxiv.org/abs/2407.07791)|**[link](https://github.com/Jometeorie/KnowledgeSpread)**|**随着大型语言模型（LLMs）在多代理系统中的迅速应用，它们在协作问题解决和自主谈判等领域的出色性能引起了关注。然而，这些基于LLM的多代理系统的安全问题尚未得到充分研究，尤其是在知识操纵传播方面。本文通过构建详细的威胁模型和模拟环境，模拟现实世界中的多代理部署在可信平台上，探讨这一关键问题。我们提出了一种新颖的两阶段攻击方法，包括说服性注入和操纵知识注入，来系统地探究在无明确提示操纵的情况下，如何潜在地传播操纵知识（如虚构和有害知识）。我们的方法利用了LLMs处理世界知识固有的漏洞，攻击者可以借此无意识地传播编造的信息。实验结果表明，我们的攻击方法能够成功诱导基于LLM的代理在交流中传播这两种操纵的知识，同时不会显著降低它们的基础功能。此外，我们发现这些操纵会持续存在于流行的检索增强生成框架中，即使交互结束，若干良性代理也可能继续受到操纵聊天记录的影响。我们的发现揭示了LLM基多代理系统中的重大安全风险，强调了对操纵知识传播进行强大防御的迫切需求，例如引入“守护”代理和先进的事实核查工具。**|
|**2024-07-09**|**Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models**|Logan Cross et.al.|[2407.07086](http://arxiv.org/abs/2407.07086)|**[link](https://github.com/locross93/hypothetical-minds)**|**在多智能体强化学习（MARL）方法中，处理多智能体系统的非stationarity并适应在线学习的能力是一个挑战。为此，我们利用大型语言模型构建了一个自主的解决策略。我们的新型智能体“假设心智”（Hypothetical Minds）采用认知启发式架构，包括感知、记忆和两个抽象层次上的分层规划模块。其中的关键部分是“心理理论”模块，它通过自然语言生成对其他智能体策略的假设，并根据这些假设对其他智能体行为的预测进行评估和迭代优化。通过这种方式，假设心智在Melting Pot基准中的多种竞争、混合动机和协作环境中，无论是二元还是群体环境，都显著优于先前的语言模型智能体（LLM-agent）和强化学习基础线。对比实验还显示，假设的评估和精炼对于在复杂场景中取得成功至关重要。**|
|**2024-07-09**|**Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy**|Zhenyu Guan et.al.|[2407.06813](http://arxiv.org/abs/2407.06813)|**[link](https://github.com/todexter3/richelieu)**|## 背景 在人类社会中，外交是一种极其复杂的活动，涉及众多各方/行动者的互动，需要具备社会推理、谈判技巧和长期策略规划等多方面能力。以往的AI代理已经在处理多步骤游戏和大动作空间的多代理任务上展示了实力。然而，外交所涉及的决策空间范围惊人，特别是在需要谈判的阶段。近期，大型语言模型（LLM）在一些应用中展现出了超越前代的能力，但仍不足以应对复杂多代理环境中长时间的规划。借助尖端的LLM技术，我们首次尝试探索AI在如此全面的多代理使命中的上限，通过整合三个核心且关键的功能，以构建更强的基于LLM的社会性代理：1）具有记忆和反思的策略规划者；2）目标导向的、具备社会推理的谈判者；3）通过自我对弈游戏增强记忆，实现无人工干预的自我进化。|
|**2024-07-10**|**FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making**|Yangyang Yu et.al.|[2407.06567](http://arxiv.org/abs/2407.06567)|null|大型语言模型（LLMs）在执行复杂任务方面展现出显著潜力，并越来越多地应用于金融领域。然而，高质量的连续投资决策过程仍面临挑战，它需要与不断变化的环境进行多次交互，以最大化回报并管理风险。尽管已经开发出基于LLMs的代理系统，它们能够超越人类团队，实现投资收益，但如何优化多源信息整合和决策结果，通过实时经验改进，仍有待探索。为此，我们提出FinCon，一个专为多样化的金融任务设计的基于LLM的多代理框架，其特点在于概念化口头强化和财务组织结构的运用。  FinCon借鉴现实世界投资公司的组织架构，采用经理-分析师的沟通层次，促进跨职能代理间的协同合作，通过自然语言交流实现目标统一。每个代理都具备比人类更大的记忆容量，这有助于更高效的信息处理。此外，FinCon还引入了一个风险控制组件，定期启动自我批判机制，以更新系统的投资理念。这些概念化的信念作为口头强化，指导未来行为，并可根据需要选择性地传递给需要更新知识的节点，从而减少不必要的信息交流成本，提高性能。  FinCon在单一股票交易和资产管理等不同金融任务上表现出强大的泛化能力，证明了其在实际金融场景中的应用潜力。|
|**2024-07-08**|**Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning**|Yadong Zhang et.al.|[2407.06112](http://arxiv.org/abs/2407.06112)|null|该论文提出了一个新颖的推理方法——双向决策解放推理（BIDDER），旨在提升语言模型的决策合理性。传统推理方法通常依赖历史信息，采用单向（从左到右）的推理策略，这导致对潜在未来结果的认识不足，以及历史背景的整合不够充分，从而产生次优决策。BIDDER通过融合理性决策的原则，特别是处理不确定性并预测期望效用，弥补了这一短板。其方法包括三个关键步骤：从历史数据中推断隐藏状态，以表示决策过程中的不确定信息；利用这些隐藏状态预测未来的潜在状态和可能结果；结合历史信息（过去情境）和长期结果（未来情境），以指导推理。通过双向推理，BIDDER能够全面考虑过去和未来的情境，从而做出更明智、更理性的决策。我们在扑克（限注德州扑克）和谈判两个明确场景中测试了BIDDER的效果，实验显示它显著提高了语言模型和基于语言模型的代理的决策能力。|
|**2024-07-08**|**Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation**|Jiaqi Chen et.al.|[2407.05890](http://arxiv.org/abs/2407.05890)|null|基于语言模型的代理在视觉导航（VLN）任务中展现出零样本的强大性能。然而，这些方法仅关注解决高层任务规划，通过选择预定义导航图中的节点进行移动，忽视了现实场景中低层次的控制。为了弥补这一不足，我们提出了AO-Planner，一个新颖的面向可及性规划的连续视觉导航框架。AO-Planner整合多种基础模型，实现面向可及性的运动规划和动作决策，均以零样本的方式执行。具体来说，我们采用了视觉可及性提示（VAP）方法，利用SAM分割可见地面，提供导航可及性信息，从而让语言模型选择潜在的下一个路标，并生成向选定路标的低层次路径规划。此外，我们引入了高级代理PathAgent，识别出最可能的像素级路径，并将其转换为三维坐标，以完成低层次的移动。  在具有挑战性的R2R-CE基准测试上，AO-Planner实现了最先进的零样本性能提升（SPL指标提高5.5%）。我们的方法有效连接了语言模型与三维世界，避免了直接预测世界坐标点的困难，为利用基础模型进行低层次运动控制提供了新的前景。|
|**2024-07-05**|**VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models**|Hang Gao et.al.|[2407.04573](http://arxiv.org/abs/2407.04573)|null|在大型语言模型（LLMs）快速发展的背景下，向量检索算法对于满足相似度和多样性要求的语义查询至关重要。尽管Maximal Marginal Relevance（MMR）在涉及这两个需求的检索场景中被广泛应用，但其参数λ的变化会导致结果波动，使得向量空间中的优化路径变得模糊。此外，当前缺乏对相似性和多样性在检索过程中约束的坚实理论分析。本文提出了一种新方法，通过查询向量与求和向量之间的关系来刻画这两种约束。这种关系确保了相似性，同时要求求和向量中的各个向量以分散的方式与查询向量对齐，以满足多样性需求。  我们还提出了一个新的组合优化问题：从一组候选向量中选择 $k$ 个，使得它们的求和向量最大程度地与查询向量匹配。我们证明了这个问题是NP完全的，揭示了在向量检索中同时追求相似性和多样性的深刻困难，并为后续研究奠定了理论基础。此外，我们设计了一个名为Vectors Retrieval with Similarity and Diversity（VRSD）的启发式算法，它不仅具有明确的优化目标，无需预设参数，而且在时间复杂度上相对于MMR有所降低。实证验证表明，VRSD在各种数据集上显著优于MMR。|
|**2024-07-05**|**When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions**|Jérémy Perez et.al.|[2407.04503](http://arxiv.org/abs/2407.04503)|**[link](https://github.com/jeremyperez2/telephonegamellm)**|**随着大型语言模型（LLMs）之间的互动增加，它们在线上生成的文本量也随之增多，研究如何信息在从一个LLM传递到另一个LLM的过程中发生变化变得至关重要。尽管对单个LLM的行为已有深入研究，但对迭代交互中集体行为和信息扭曲的探讨相对不足。微小的偏差，在单次输出时可能显得不明显，但在多次交互中可能会被放大，可能导致内容朝着吸引子状态演变。我们通过借鉴人类文化进化学的研究方法——电话游戏实验，设计了一种链式传输模型。在这个过程中，LLM代理接收、生成并传递文本，从一个链中的前一个代理到下一个。我们追踪了文本的毒性、积极度、难度和长度在传输链中的演变，揭示了偏见和吸引子的存在，并研究了它们与初始文本、指令、语言模型和模型规模的关系。例如，我们发现开放性指令比约束性任务更容易引发更强的吸引效应。此外，不同的文本特性对吸引子效应的敏感度不同，毒性的影响通常大于长度。这些发现强调了考虑多步骤传输动态的重要性，为进一步理解LLM的文化动态奠定了基础。**|
|**2024-07-05**|**AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents**|Petr Anokhin et.al.|[2407.04363](http://arxiv.org/abs/2407.04363)|**[link](https://github.com/airi-institute/arigraph)**|**随着生成式人工智能的进步，大型语言模型（LLMs）在自主代理的发展中展现出广阔的应用前景。实现真正的自主性需要从与环境的交互中积累和更新知识，并能有效利用这些信息。当前基于LLMs的方法依赖于全历史观察、总结或检索增强，但这些非结构化的记忆表示不利于复杂决策中的推理和规划。我们的研究提出AriGraph，一种新型方法，让代理在探索环境中构建融合语义和情节记忆的记忆图。这种图结构促进关联概念的有效检索，这些概念与代理当前状态和目标相关，从而成为一种有效的环境模型，提升探索和规划能力。  我们设计的Ariadne LLM代理，配备有我们提出的记忆架构以及规划和决策功能，能在零样本基础上处理TextWorld环境中的复杂任务，如First TextWorld Problems竞赛中的烹饪挑战，以及新任务如房屋清洁和寻宝谜题。与全历史、总结和检索增强生成等传统方法相比，我们的方法在各种任务中表现出显著优势。**|
|**2024-07-02**|**MMedAgent: Learning to Use Medical Tools with Multi-modal Agent**|Binxu Li et.al.|[2407.02483](http://arxiv.org/abs/2407.02483)|**[link](https://github.com/Wangyixinxin/MMedAgent)**|尽管多模态大型语言模型（MLLMs）已经取得了成功，但它们的泛化能力仍然有限，在某些情况下表现不如专门化的模型。为了解决这些问题，最近的研究开发了基于LLMs的代理，可以根据用户输入选择合适的专用模型。然而，这种进展在医疗领域尚未得到充分探索。为了弥补这一空白，本文首次提出了一种专门为医疗领域设计的代理，称为\textbf{M}ulti-modal \textbf{Med}ical \textbf{Agent}（MMedAgent）。我们构建了一个指令调优数据集，包含了六个医疗工具来解决七项任务，使代理能够为给定任务选择最合适的工具。实验全面展示了MMedAgent在各种医疗任务上超越了开源方法的最新状态，甚至与闭源模型GPT-4o相比也表现出色。此外，MMedAgent还显示出了更新和整合新医疗工具的高效性。|
|**2024-07-02**|**Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents**|Fanzeng Xia et.al.|[2407.01887](http://arxiv.org/abs/2407.01887)|null|本文关注的是大型语言模型在决策制定中的性能，尤其是在杜尔克姆双臂赌博（Dueling Bandits，DB）问题的上下文中。研究比较了GPT-3.5-Turbo、GPT-4和GPT-4-Turbo与现有DB算法的性能。结果显示，尤其是GPT-4 Turbo，能够快速识别出优势明显的选项，从而在弱后悔方面超越当前最佳算法。然而，这些模型在收敛性上存在问题，对提示的敏感度较高，且对提示变化反应脆弱。为了改进，我们提出了一种结合了LLM决策能力与经典DB算法理论保证的增强型算法——IF-Enhanced LLM。这种设计展示了如何增强LLM在对性能稳定性有要求的决策任务中的可信度。IF-Enhanced LLM具有弱后悔和强后悔的理论保证。实验结果验证了即使面对嘈杂和对抗性的提示，IF-Enhanced LLM仍保持稳健。|
|**2024-07-01**|**Agentless: Demystifying LLM-based Software Engineering Agents**|Chunqiu Steven Xia et.al.|[2407.01489](http://arxiv.org/abs/2407.01489)|**[link](https://github.com/OpenAutoCoder/Agentless)**|**随着大型语言模型（LLMs）的最新进展，软件开发任务的自动化，如代码合成、程序修复和测试生成，已取得显著进步。研究人员和业界实践者已经开发出各种自主LLM代理来执行端到端的软件开发任务，它们能够利用工具、运行命令、观察环境反馈并规划未来行动。然而，这些基于代理的方法的复杂性以及当前LLM的局限性，引发了一个问题：是否真的需要使用复杂的自主软件代理？为了探讨这个问题，我们构建了Agentless——一种无代理方法，用于自动解决软件开发问题。与复杂的代理设置相比，Agentless采用了一种简单的两阶段过程：定位后修复，不让LLM决定未来的行动或操作复杂的工具。在流行的SWE-bench Lite基准上，我们的实验结果令人惊讶地表明，这种简单的方法能够实现最高性能（27.33%）和最低成本（0.34美元），超越所有开源软件代理！  此外，我们手动分类了SWE-bench Lite中的问题，并发现存在精确的ground truth补丁问题或描述不足/误导性的问题。因此，我们构建了SWE-bench Lite-S，通过排除这些问题来进行更严格的评估和比较。我们的工作突显了当前被忽视的简单、可解释技术在自主软件开发中的潜力。我们希望Agentless将作为自主软件代理的基线、起点和期望值，激发未来在这个关键领域的工作。**|
|**2024-07-01**|**MIRAI: Evaluating LLM Agents for Event Forecasting**|Chenchen Ye et.al.|[2407.01231](http://arxiv.org/abs/2407.01231)|null|随着大型语言模型（LLMs）的最新进展，这些模型能够自主收集全球信息，并进行推理以解决复杂问题，这引发了使用LLM预测国际事件的兴趣。然而，目前缺乏一个严格评估LLM预测能力与可靠性的基准。为了填补这一空白，我们提出MIRAI，这是一个新颖的基准，旨在系统地评价LLM在国际事件时间序列预测中的表现。MIRAI构建了一个代理环境，配备有访问广泛历史结构化事件和文本新闻数据库的工具。我们对GDELT事件数据库进行了精心清洗和解析，设计了一系列关联预测任务，涵盖了不同预测时间范围，从短期到长期，以检验LLM在整合全球关键信息、运用领域特定API和库编写代码以及综合处理来自多种格式和时间的历史知识以准确预测未来事件的能力。通过全面的基准测试，我们的目标是建立一个可靠的框架，以评估LLM在国际事件预测方面的性能，从而推动更精确和可信的国际关系分析模型的发展。|
|**2024-07-01**|**Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents**|Shihan Deng et.al.|[2407.00993](http://arxiv.org/abs/2407.00993)|**[link](https://github.com/XiaoMi/MobileBench)**|随着大型语言模型（LLMs）的显著进步，基于LLM的移动代理已成为人机交互领域的研究热点。然而，针对此类代理的基准测试资源相对匮乏。评估这类代理通常面临三个挑战：（1）仅依赖用户界面（UI）操作的低效限制了任务评估；（2）单一应用中的特定指令不足以全面评估LLM移动代理的多维度推理和决策能力；（3）当前的评估指标无法准确衡量连续动作过程。为此，我们提出了Mobile-Bench，一个全新的用于评估LLM移动代理能力的基准。首先，我们扩展了传统的UI操作，融入了103个收集到的API，以提高任务完成的效率。接着，我们通过结合真实用户查询和LLM增强的数据收集来进行评估。为了更好地评价移动代理的不同规划能力层次，我们的数据被分为SAST（简单任务）、SAMT（稍复杂任务）和MAMT（多任务）三类，反映了任务复杂度的差异。Mobile-Bench包含832条数据条目，其中超过200项任务专门设计用于测试跨应用协作场景。此外，我们引入了一种更精确的评估指标，称为CheckPoint，用于检查LLM移动代理在规划和推理步骤中是否达到关键点。|
|**2024-06-29**|**Large Language Models for Power Scheduling: A User-Centric Approach**|Thomas Mongaillard et.al.|[2407.00476](http://arxiv.org/abs/2407.00476)|**[link](https://github.com/thomasmong/llm-power-scheduling)**|**随着传统优化和调度方法逐渐转向用户驱动和个人化服务，以提升用户体验（QoE）和灵活性，未来的系统，尤其是在无线和数字化能源网络中，面临着如何更好地理解和响应用户需求的挑战。传统的系统往往忽视了用户的个性化需求，因为用户与机器之间的沟通不畅。大型语言模型（LLMs）的出现为解决这个问题带来了突破，它们提供了用户与设备之间自然的交流界面。本文首次提出了一种新颖的架构，通过构建三个LLM代理来将用户的语音请求（VRQ）转化为资源分配向量。具体包括：LLM意图识别代理将请求转化为优化问题（OP）、LLM OP参数识别代理以及LLM OP求解代理。  我们针对电动汽车（EV）充电的典型VRQ创建了一个数据库，作为性能评估的基础。作为概念验证，我们主要使用Llama 3 8B模型进行实验。通过不同的提示工程场景测试，结果显示了所提架构的有效性。研究还揭示了一些关键见解，例如，用于建模实际问题的更大候选OP集可能会由于更高的识别/OP分类噪声而降低最终性能。所有结果和代码已开源，供学术界进一步研究和利用。**|
|**2024-06-29**|**Financial Knowledge Large Language Model**|Cehao Yang et.al.|[2407.00365](http://arxiv.org/abs/2407.00365)|null|人工智能在金融领域取得了显著进步，正在重塑数据处理和解读方式。其中，大型语言模型（LLMs）展现出巨大的潜力，能够自动化复杂任务、提升客户服务，并提供详尽的财务分析。首先，我们介绍IDEA-FinBench，这是一个专为评估大型语言模型在金融知识方面的性能而设计的评价基准。它借鉴了两个全球知名且权威的金融专业考试中的问题，旨在全面检验LLMs解答与金融相关考题的能力。其次，我们提出IDEA-FinKER，是一个金融知识增强框架，旨在快速让通用LLMs适应金融领域。它采用基于检索的少量样本学习方法，实现实时上下文级知识注入，并提供一套高质量的金融知识指令，用于微调任何通用模型。最后，我们展示了IDEA-FinQA，一个由LLMs驱动的金融问答系统。该系统围绕实时知识注入和事实强化的架构构建，利用外部知识。IDEA-FinQA主要由数据收集器、数据查询模块和执行特定功能的LLM代理组成。|
|**2024-06-28**|**Simulating Financial Market via Large Language Model based Agents**|Shen Gao et.al.|[2406.19966](http://arxiv.org/abs/2406.19966)|null|大多数经济理论通常假设金融市场参与者是完全理性的个体，并使用数学模型来模拟人类在金融市场的行为。然而，人类行为往往并非完全理性，用数学模型精确预测颇具挑战。本文提出了一种新型的\textbf{A}gent-based \textbf{S}imulated \textbf{F}inancial \textbf{M}arket（ASFM），首先构建了一个具有真实订单匹配系统的模拟股票市场。接着，我们设计了一种基于大型语言模型的股票交易代理，它包括个人概况、观察和基于工具学习的动作模块。这种交易代理能够全面理解当前市场动态和金融政策信息，从而根据其交易策略作出决策。实验表明，ASFM在可控场景下的反应与现实股票市场一致。此外，我们在两个经济学研究热点领域进行了实验，结果发现，我们的\model得出的结论与经济学研究的初步发现相吻合。因此，我们认为ASFM为经济研究提供了一个新的范式。|
|**2024-06-26**|**Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling Legislative Behavior and Bipartisanship**|Zachary R. Baker et.al.|[2406.18702](http://arxiv.org/abs/2406.18702)|null|这项研究提出了一种创新的方法，利用语言模型驱动的虚拟代理来模拟立法过程，具体聚焦于美国参议院情报委员会。我们构建了代表个别参议员的代理，并在模拟的委员会讨论中让它们互动。这些代理展现出在现实辩论中的能力，能够提供深思熟虑的观点，并在特定条件下找到两党的解决方案。值得注意的是，模拟显示，面对外部干扰时，代理模型在两党合作上展现出转变的潜力。研究结果表明，这种基于语言模型的策略可能成为理解和改进立法流程的有效工具，这与一系列发现相呼应，即基于语言模型的代理能有用地模拟现实世界现象。未来的研究将致力于提升代理的复杂性，扩大模拟范围，并探索在政策测试和谈判中的应用。|
|**2024-06-25**|**Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks**|Yun-Shiuan Chuang et.al.|[2406.17232](http://arxiv.org/abs/2406.17232)|null|### 翻译  构建逼真的人工大型语言模型（LLMs）对于实现可信的社会模拟至关重要。尽管基于人口统计信息的角色扮演有时能提升人性化，但效果并不总是理想。本研究旨在探究是否可以通过整合来自实证人类信念网络的信息，进一步提升LLMs与人类行为的契合度。我们利用一项人类调查数据，估计了一个包含18个主题的信念网络，这些主题加载于两个不重叠的潜在因子上。然后，我们在LLM中植入一个关于某一主题的观点，分析其对剩余测试话题表达的观点与相应人类数据的契合程度。仅依赖人口统计信息的角色扮演未能使LLM和人类观点保持一致，但当植入单一信念时，对于相关于信念网络内的主题，这种一致性显著提高，而对于网络外的主题则没有明显影响。这些结果表明了一种新颖的方法，可以用于在追求理解和模拟社会中信念分布模式的人工智能工作中，实现人类与LLMs之间的信念对齐。|
|**2024-06-21**|**GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians**|Haoyang Liu et.al.|[2406.15341](http://arxiv.org/abs/2406.15341)|**[link](https://github.com/liu-hy/genotex)**|**## 翻译  近年来，机器学习的进步显著提升了从基因表达数据中识别疾病相关基因的能力。然而，这些过程往往需要深厚的专长和大量的人工努力，限制了其可扩展性。大型语言模型（LLMs）驱动的代理显示出在自动化此类任务方面的潜力，因为它们的问题解决能力日益增强。为了支持这类方法的评估和发展，我们创建了GenoTEX，这是一个基因表达数据分析自动探索的基准，包括数据集选择、预处理和统计分析任务。GenoTEX提供了全面的分析管道，其中包含了人类生物信息学家精心编写的注释，他们对数据集进行深入分析以确保准确性和可靠性。  为了提供这些任务的基线，我们设计了GenoAgents，这是一个基于LLMs的代理团队，具备上下文感知规划、迭代校正以及与领域专家咨询的能力，它们协作探索基因数据集。我们的实验显示了LLM驱动方法在基因组数据分析中的潜力，而错误分析指出了挑战和未来的改进方向。我们提议GenoTEX作为一个有前景的资源，用于衡量和提升人工智能驱动的基因组数据分析方法。我们的基准已公开发布在：\url{https://github.com/Liu-Hy/GenoTex}。**|
|**2024-06-21**|**Autonomous Agents for Collaborative Task under Information Asymmetry**|Wei Liu et.al.|[2406.14928](http://arxiv.org/abs/2406.14928)|**[link](https://github.com/thinkwee/iAgents)**|**大型语言模型多-agent系统（LLM-MAS）在解决复杂任务方面取得了显著进步。它们通过系统内各代理之间的通信协作来完成任务，前提是共享信息。然而，当代理间的交流被用于增强人类合作时，由于信息不对称（每个代理仅能访问其对应人类用户的信息），这带来了新的挑战。传统MAS在这种情况下难以完成任务。为解决此问题，我们提出了一种新型多agent系统架构，称为“iAgents”，即信息丰富多agent系统。在iAgents中，人类社会网络在代理网络中得到反映，代理主动交换完成任务所需的人类信息，从而克服信息不对称。iAgents采用了一种新颖的代理推理机制，InfoNav，引导代理之间的有效信息交流。结合InfoNav，iAgents组织了混合记忆中的人类信息，为代理提供准确全面的信息进行交换。此外，我们还推出了首个针对评估LLM在信息不对称条件下任务解决能力的基准——InformativeBench。实验结果显示，iAgents能够在包含140人和588条关系的社会网络中协作，自主进行超过30轮的通信，并从近70,000条消息中检索信息，在3分钟内完成任务。**|
|**2024-06-21**|**FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents**|Ruixuan Xiao et.al.|[2406.14884](http://arxiv.org/abs/2406.14884)|null|基于语言模型的代理作为一种有前景的工具，被设计用于通过迭代规划和行动来执行复杂任务。然而，这些代理在处理需要专业知识的任务时，容易产生不期望的规划幻觉。为了解决这个问题，初步尝试通过融入与工作流程相关的外部知识来增强规划可靠性。尽管显示出潜力，但注入的知识通常杂乱无章，格式多样，缺乏严谨的规范化和全面的比较。为此，我们规范了不同格式的工作流程知识，并提出了FlowBench，这是第一个面向工作流引导规划的基准。FlowBench涵盖了来自6个领域的51个不同场景，其中知识以多样的形式呈现。为了评估不同语言模型在FlowBench上的性能，我们设计了一个多层次的评估框架。我们研究了工作流程知识在多种格式下的有效性，结果表明当前的语言模型代理在满足满意的规划需求方面仍有很大的提升空间。我们期望这个具有挑战性的基准能为未来的代理规划研究铺平道路。|
|**2024-07-01**|**Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory**|Gordon Dai et.al.|[2406.14373](http://arxiv.org/abs/2406.14373)|null|随着大型语言模型（LLMs）和人工智能的进步，计算社会科学的研究迎来了大规模探索的机遇。我们的工作基于先前对LLM行为体设计的研究，构建了一个模拟的Agent社会，其中复杂的社交关系随时间动态形成和发展。我们赋予这些Agent心理驱动力，并置于一个沙盒生存环境中。通过托马斯·霍布斯的奠基性社会契约理论（SCT）的视角，我们评估了这个Agent社会。实验结果显示，起初，Agent们表现出无拘无束的冲突，符合霍布斯对“自然状态”的描述。然而，随着模拟的进行，社会契约逐渐形成，绝对主权者得到了授权，进而建立了以相互合作为基础的和平共同体。我们的实验发现与霍布斯理论相吻合：LLM驱动的多Agent模拟展示了社会动态的复杂性，可能复制塑造人类社会的力量。尽管无法完全模拟人类行为的所有细微之处，但这种模拟对于理解社会结构、群体动态和复杂人类系统具有潜在价值。|
|**2024-06-20**|**EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms**|Siyu Yuan et.al.|[2406.14228](http://arxiv.org/abs/2406.14228)|**[link](https://github.com/siyuyuan/evoagent)**|**随着强大大型语言模型（LLMs）的兴起，一种新的趋势是利用这些模型构建能解决复杂任务的自主代理，尤其是多代理系统。然而，现有的研究很大程度上依赖于人类设计的框架，这限制了代理系统的功能范围和可扩展性。如何自动将专门的代理扩展到多代理系统，以提升任务解决能力，仍然是一个重大挑战。本文提出EvoAgent，这是一种通过进化算法自动将专家代理扩展到多代理系统的方法，旨在提高基于LLM的代理在执行任务中的效率。具体来说，我们视现有的代理框架为初始个体，并应用一系列进化操作（如突变、交叉、选择等）生成具有不同设置的代理。EvoAgent适用于任何基于LLM的代理框架，能够无须额外人工设计自动生成扩展的多代理系统。实验结果显示，EvoAgent能够自动产生多个专家级代理，并显著增强基于LLM的代理的任务解决能力。**|
|**2024-06-19**|**AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents**|Edoardo Debenedetti et.al.|[2406.13352](http://arxiv.org/abs/2406.13352)|**[link](https://github.com/ethz-spylab/agentdojo)**|**本文介绍了一个名为AgentDojo的框架，用于评估依赖于外部工具处理不可信数据的AI代理的对抗性鲁棒性。面对不断演变的攻击和防御手段，AgentDojo不是一个静态的测试套件，而是设计和评估新任务、防御策略以及适应性攻击的可扩展环境。它包含了97个实际应用场景的任务（如管理电子邮件客户端、导航网上银行网站或预订旅行），629个安全测试案例，以及来自文献的各种攻击和防御方法。研究发现，当前最先进的语言模型在AgentDojo中的表现并不尽人意（即使没有攻击），并且现有的提示注入攻击虽然能破坏一些安全特性，但并非所有情况都适用。我们期望AgentDojo能够推动研究，以寻找在解决常见任务时既可靠又健壮的AI代理的新设计原则。相关代码已发布在https://github.com/ethz-spylab/agentdojo。**|
|**2024-06-19**|**LLMatDesign: Autonomous Materials Discovery with Large Language Models**|Shuyi Jia et.al.|[2406.13163](http://arxiv.org/abs/2406.13163)|null|发现新材料对科学和技术具有重大意义，但目前仍是艰巨问题，因为化学空间浩瀚。近期，机器学习的进步推动了基于数据的方法来快速筛选或生成有前景的材料，但这些方法仍依赖大量训练数据，且往往缺乏人类期望的材料设计的灵活性和化学直觉。我们提出LLMatDesign，一个由大型语言模型驱动的可解释材料设计新框架。LLMatDesign利用LLM代理理解人类指令，对材料进行修改，并使用提供的工具评估结果。通过自我反思先前决策，LLMatDesign能在零样本情况下快速适应新任务和条件。在离线实验中，对LLMatDesign在多个材料设计任务中的系统评估证实了它在小数据环境下开发出具有用户定义目标性质的新材料的有效性。我们的框架展示了自主LLM引导的计算环境下的材料发现的非凡潜力，预示着未来自驾驶实验室的可能性。|
|**2024-06-18**|**Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**|Zehao Wang et.al.|[2406.12806](http://arxiv.org/abs/2406.12806)|null|**背景**：配置设置对于调整软件行为以满足特定性能需求至关重要，但错误配置普遍存在。由于配置项众多且复杂，识别影响系统性能的配置是一项挑战。本研究提出PerfSense，这是一个轻量级框架，利用大型语言模型（LLMs）高效地识别性能关键配置，同时保持低开销。PerfSense利用LLM代理模拟开发者和性能工程师之间的交互，采用先进的提示链技术和检索增强生成（RAG）等技术。  **方法与成果**：我们在七个开源Java系统上的评估显示，PerfSense在分类性能敏感配置方面的平均准确率为64.77%，优于基于LLM的基线（50.36%）和先前的最佳方法（61.75%）。特别是，我们的提示链技术提高了召回率10%至30%，而保持了相似的精确度。进一步的手动分析362个误分类案例，发现常见问题包括LLMs对需求的理解偏差（占26.8%）。  **结论**：PerfSense显著减少了手动分类性能关键配置的工作量，并为未来的LLM基于代码分析研究提供了有价值的观点。|
|**2024-06-18**|**AgentReview: Exploring Peer Review Dynamics with LLM Agents**|Yiqiao Jin et.al.|[2406.12708](http://arxiv.org/abs/2406.12708)|**[link](https://github.com/ahren09/agentreview)**|## 翻译  同行评审是科学出版诚信和进步的基础。传统的同行评审数据分析方法往往侧重于现有数据的探索和统计，但未能充分考虑这一过程的多变量性质，处理潜在变量，且受限于隐私问题，因为数据涉及敏感性。我们提出AgentReview，这是一个基于大型语言模型（LLM）的同行评审模拟框架，有效分解了多个潜在因素的影响，并解决了隐私问题。研究发现，由于社会影响力理论、利他主义疲劳和权威偏见等社会学理论的支持，论文决策中存在显著的37.1%的变异性。我们相信这项研究能为优化同行评审机制设计提供宝贵见解。|
|**2024-06-18**|**Large Language Models based Multi-Agent Framework for Objective Oriented Control Design in Power Electronics**|Chenggang Cui et.al.|[2406.12628](http://arxiv.org/abs/2406.12628)|null|这篇论文关注于电力电子系统控制设计中的挑战，特别是模型不确定性以及设计周期漫长和成本高昂的问题。论文旨在提出一种基于大型语言模型（LLMs）的多代理框架，用于面向目标的电力电子控制器设计。该框架利用LLMs的推理能力，结合多代理工作流程，旨在开发一个高效且自动化的控制器设计流程。LLM代理能够理解并响应自然语言的高级指令，根据任务的具体需求和实际应用中的约束调整其行为。这种新颖而高效的策略有望显著提升电力电子控制器设计的灵活性和适应性，极大地便利实践者的工作。|
|**2024-06-18**|**CodeNav: Beyond tool-use to using real-world codebases with LLM agents**|Tanmay Gupta et.al.|[2406.12276](http://arxiv.org/abs/2406.12276)|null|我们介绍CodeNav，这是一种利用大型语言模型（LLM）来导航和利用先前未见过的代码仓库，以解决用户查询的系统。与需要通过手动描述在LLM上下文中“注册”所有相关工具的工具使用型LLM不同，CodeNav能够自动索引和搜索目标代码库中的代码块，找到相关的代码片段，导入它们，并根据执行反馈迭代生成解决方案。首先，我们通过三个案例研究展示CodeNav如何使用三种不同的代码库来解决复杂的用户问题。接着，在三个基准测试中，我们定量比较了仅能访问目标代码库的代码使用方法与拥有对所有工具名称和描述的特权访问的工具使用方法的效果。此外，我们研究了不同类型工具和库描述对代码使用性能的影响，以及将源代码视为输入而非自然语言代码描述的优势。所有代码将遵循宽松许可协议开源。|
|**2024-06-17**|**Efficient Sequential Decision Making with Large Language Models**|Dingyang Chen et.al.|[2406.12125](http://arxiv.org/abs/2406.12125)|null|该论文关注的是将大型语言模型（LLMs）的成功扩展到序列决策制定。当前的努力要么重新训练或微调LLMs进行决策，要么为预训练的LLMs设计提示。前者面临计算负担重的梯度更新问题，而后者未显示出明显效果。为此，我们提出了一种新方法，利用在线模型选择算法有效地将LLMs整合到序列决策过程中。统计上，我们的方法显著优于传统决策算法和纯LLM代理。在计算上，我们的方法避免了对LLMs进行昂贵的梯度更新，并且在整个决策过程中仅需要少量的LLM调用。我们进行了广泛实验来验证我们方法的有效性。以一个大规模的亚马逊数据集为例，我们的方法在仅使用1.5%的时间步数调用LLMs的情况下，实现了比基线超过6倍的性能提升。|
|**2024-06-17**|**Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector**|Xiaoxue Cheng et.al.|[2406.11277](http://arxiv.org/abs/2406.11277)|**[link](https://github.com/rucaibox/haluagent)**|这篇论文探讨了大型语言模型（LLMs）在幻觉检测方面的挑战，特别指出以往研究主要依赖于强大的闭源模型如GPT-4。作者提出了一种自主的基于LLM的代理框架，称为HaluAgent，它允许较小的模型（如巴 chcuan2-Chat 7B）主动选择适合检测文本、代码和数学表达式等多种幻觉类型的工具。HaluAgent整合了LLM、多功能工具箱，并设计了一个细粒度的三阶段检测框架，同时配备了记忆机制。为了提高HaluAgent的效能，论文利用现有的中文和英文数据集合成检测轨迹进行微调，使其具备双语幻觉检测能力。实验结果表明，仅使用2000个样本对LLM进行调优后，HaluAgent在各种任务和数据集上表现出色，其性能可与GPT-4媲美，甚至在某些情况下超越，且无需额外工具增强，无论在领域内还是领域外的数据集上都展现出良好性能。论文的代码和数据集已发布在https://github.com/RUCAIBox/HaluAgent。|
|**2024-06-18**|**AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval**|Shirley Wu et.al.|[2406.11200](http://arxiv.org/abs/2406.11200)|**[link](https://github.com/zou-group/avatar)**|**大型语言模型（LLMs）在利用外部工具和知识提升准确性和减少错误方面展现出显著能力。然而，设计能让LLMs有效运用这些工具的提示技巧是一项耗时且依赖直觉的任务。为此，我们提出AvaTaR，一个创新的自动化框架，它能优化LLMs，使其更有效地利用提供的工具，并在特定任务或领域中提升性能。AvaTaR通过设计一个比较器模块，以训练数据中的正负样本进行推理，迭代地为LLM提供富有洞察力和全面的提示。我们在四个包含文本、视觉和关系信息的复杂多模态检索数据集上展示了AvaTaR的效果。实验表明，AvaTaR在所有四项具有挑战性的任务中均优于现有最先进的方法，并展现出强大的泛化能力，当应用于新案例时，平均在Hit@1指标上实现了14%的相对改进。代码和数据集已在<https://github.com/zou-group/avatar>上公开。**|
|**2024-06-17**|**Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement**|Weimin Xiong et.al.|[2406.11176](http://arxiv.org/abs/2406.11176)|**[link](https://github.com/weiminxiong/ipr)**|**大型语言模型在一系列复杂的交互任务中展现出卓越性能。近期的研究倾向于通过专家轨迹调优来提升模型效果，但主要关注最终结果奖励，这可能导致错误或非最优行为，因为缺乏过程监督信号。为此，我们在本文中提出迭代步级过程改进（Iterative Step-level Process Refinement，IPR）框架，该框架提供了细致的逐步骤指导，以增强训练过程。我们采用蒙特卡洛方法估算每一步的奖励。在每个迭代中，模型沿着专家轨迹探索并生成新动作，然后与专家轨迹的相应步骤进行比较，使用步级奖励评估。这种比较有助于识别差异，形成用于训练的对比动作对。我们在三个复杂代理任务上的实验表明，我们的框架优于多种强大的基线。此外，我们的分析结果揭示了IPR在提升动作效率方面的有效性，并证明其适用于各种模型。**|
|**2024-06-17**|**RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents**|Weizhe Chen et.al.|[2406.11132](http://arxiv.org/abs/2406.11132)|null|在过去的一年里，大型语言模型（LLMs）在传统自然语言处理领域之外展现出惊人成就，人们开始探索在代码生成、旅行规划和机器人控制等更具体的应用领域使用这些模型。通过与LLM构建所谓的LLM代理，旨在协助人们完成日常生活中的各种任务。然而，对LLMs的提示语句对生成内容及其性能至关重要。因此，自动提示工程成为许多研究人员和LLM用户关注的焦点。本文提出了一种新颖的方法，名为\textsc{RePrompt}，它利用与LLM代理交互获取的对话历史，通过“梯度下降”优化LLM的逐步指令。通过优化提示，LLM能够学习特定领域的规划策略。我们在PDDL生成和旅行规划任务中进行了实验，结果显示，使用更新后的提示作为初始提示时，我们的方法通常可以提高不同推理任务的性能。|
|**2024-06-18**|**Embodied Question Answering via Multi-LLM Systems**|Bhrij Patel et.al.|[2406.10918](http://arxiv.org/abs/2406.10918)|null|## 背景  Embodied Question Answering（EQA）是一个关键问题，它涉及一个代理在环境中探索以回答用户查询。当前的研究主要集中在单代理场景中，这可能导致探索时间冗长且成本高昂。在这个工作中，我们考虑了多代理框架下的EQA，其中涉及多个基于大型语言模型（LLM）的独立代理，它们各自解答关于家庭环境的问题。为了为每个查询生成一个答案，我们利用各个独立响应来训练一个中央答案模型（CAM），该模型整合答案以实现更稳健的回答。通过使用CAM，我们观察到其在EQA准确率上比诸如投票机制和辩论等ensemble LLM聚合方法高出50%。CAM无需任何形式的代理间通信，从而避免了相关开销。我们还通过不同的非线性（如神经网络、随机森林、决策树、XGBoost）和线性算法（如逻辑回归分类器、支持向量机）对CAM进行了消融研究。最后，我们通过Permutation Feature Importance（PFI）分析了CAM对每个独立代理和查询上下文的依赖程度，量化了CAM的依赖特性。|
|**2024-06-16**|**GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents**|Dongping Chen et.al.|[2406.10819](http://arxiv.org/abs/2406.10819)|**[link](https://github.com/keplerlab/katna)**|**近年来，多模态大型语言模型（MLLM）已被用于控制键盘和鼠标输入，直接感知图形用户界面（GUI），并生成相应的代码。然而，当前的模型主要在静态环境中表现出色，主要应用于相对简单的领域，如网页或移动界面。我们认为，一个稳健的GUI代理应具备理解GUI的时空信息能力，包括动态网页内容和多步骤任务，还要全面理解各种GUI场景，包括桌面软件和多窗口交互。为此，本文提出了一项新数据集——GUI-World，其中包含了精心制作的人机标注，广泛涵盖六种GUI场景和八类GUI相关问题，以三种格式呈现。我们评估了当前最先进的MLLM，如图像LLMs和视频LLMs，在理解和处理不同类型GUI内容，特别是动态和序列内容方面的能力。研究发现，图像LLMs在没有手动标注关键帧或操作历史的情况下，难以应对动态GUI内容。另一方面，由于GUI视频数据集的稀疏性，视频LLMs在所有GUI相关任务上表现不佳。基于GUI-World，我们首次尝试使用微调后的视频LLM作为GUI代理，显示了对各种GUI任务理解的提升。然而，由于基础LLM性能的限制，我们得出结论，将视频LLMs用作GUI代理仍是一个重大挑战。我们相信，我们的工作为未来在动态GUI内容理解方面的研究提供了有价值的洞见。代码和数据集已在我们的项目主页https://gui-world.github.io/上公开。**|
|**2024-06-16**|**HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies**|William Watson et.al.|[2406.10803](http://arxiv.org/abs/2406.10803)|null|## 背景  大型语言模型（LLMs）在处理表格问答任务时面临诸多挑战，主要包括：（1）对于大表格有限的上下文窗口；（2）不同token化模式与单元格边界的复杂差异；（3）以及使用外部模型如gpt-3.5-turbo时的数据保密问题。为解决这些问题，我们提出了一种名为“HiddenTables”的合作游戏。这个游戏涉及代码生成LLM“Solver”和评估其在表格问答任务能力的“Oracle”，以自然语言规范为基础，同时保证数据安全。  我们通过实证实验在多样化的表格上展示了LLMs在处理复杂查询、处理组合依赖以及将自然语言转化为程序指令方面的局限性，特别是在提供具体表格结构的情况下。与基于编码器的模型不同，“HiddenTables”不受行数限制，从而提高了提示和完成 token 的效率。此外，我们创建了一个新的数据集“PyQTax”，包含116,671个问题-表格-答案三元组，并提供了更细致的问题分类和标签，进一步增强了我们的研究。  因此，除了学术贡献，揭示了LLMs在表格问答任务中的不足，“HiddenTables”还展示了如何在保障数据安全的同时，让LLMs与大规模数据集互动，以及降低生成成本的实践方法。|
|**2024-06-15**|**From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent**|Samuel S. Sohn et.al.|[2406.10478](http://arxiv.org/abs/2406.10478)|null|## 背景 在娱乐、教育和营销领域至关重要的数字故事叙述面临着生产规模扩展和灵活性提升的挑战。这篇论文介绍的StoryAgent框架利用大型语言模型和生成工具来自动化并优化数字故事创作过程。它采用自上而下的故事情节草拟和自下而上的资产生成方法，解决了手动干预、互动场景编排和叙事一致性等关键问题。这个框架促进了交互式和一致叙事的高效生产，适用于多种媒介，推动了内容创作的民主化，增强了用户的参与度。我们的实验结果显示，该框架能够在没有参考视频的情况下生成连贯的数字故事，这标志着自动数字故事叙述技术的一个重大进步。|
|**2024-06-13**|**GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning**|Zhen Xiang et.al.|[2406.09187](http://arxiv.org/abs/2406.09187)|null|随着大型语言模型（LLMs）的快速发展，LLM驱动的代理被广泛应用于各种应用，这引发了对其安全性和可信度的新担忧。现有的提升LLM安全性的方法并不直接适用于LLM驱动的代理，因为它们具有不同的目标和输出模式。本文提出了一种创新方法——GuardAgent，它作为其他LLM代理的“防护栏”。GuardAgent通过检查其输入/输出是否满足用户定义的一系列守护请求来监督目标LLM。GuardAgent分为两步：1）分析提供的守护请求创建任务计划；2）根据任务计划生成守护代码，并通过API调用或外部引擎执行。整个过程利用LLM作为核心推理组件，结合记忆模块中的上下文示例，增强了知识驱动的推理能力，使其能够理解各种文本守护请求并准确地将其转化为可执行代码，提供可靠的安全保障。  GuardAgent还配备了一个可扩展的工具箱，包含函数和API，无需额外训练LLM，强调了其通用性及低运营成本。此外，我们提出了两个新颖的基准：EICU-AC用于评估医疗健康代理的隐私相关访问控制，Mind2Web-SC用于评估网络代理的安全性。在这些基准上，GuardAgent分别在98.7%和90.0%的精度下有效管理了两种类型代理的无效输入和输出。实验还表明，GuardAgent能够适应新兴的LLM代理和守护请求，定义新的功能，进一步证明了其强大的泛化能力。|
|**2024-06-13**|**Multi-Agent Software Development through Cross-Team Collaboration**|Zhuoyun Du et.al.|[2406.08979](http://arxiv.org/abs/2406.08979)|**[link](https://github.com/openbmb/chatdev)**|**### 概述  最新的大型语言模型（LLMs）进展，如ChatDev，推动了软件开发领域的深刻变革，特别体现在多代理协作上。这些模型能够像人类团队一样合作，遵循瀑布模型进行需求分析、开发、审查、测试等阶段，实现自主软件生成。然而，单个开发流程中的每个阶段只会产生一种可能结果，导致只完成一条开发链，从而丧失在解决方案空间中探索多种决策路径的机会，可能导致结果不理想。为解决这一问题，我们提出了跨团队协作（Cross-Team Collaboration，CTC）框架，这是一种可扩展的多团队结构，它允许协同工作的团队在跨团队协作环境中共同提出决策，并交流各自见解，以优化内容生成。  实验结果显示，在软件开发领域的应用中，我们的方法显著优于现有基准，证实了框架的有效性。在故事生成方面的显著改进表明，该框架具有广泛的跨领域泛化能力。我们期待我们的工作能引导LLMs向跨团队模式发展，并在软件开发等领域带来重大进步。相关的代码和数据将在<https://github.com/OpenBMB/ChatDev>上提供。**|
|**2024-06-13**|**StreamBench: Towards Benchmarking Continuous Improvement of Language Agents**|Cheng-Kuang Wu et.al.|[2406.08747](http://arxiv.org/abs/2406.08747)|**[link](https://github.com/stream-bench/stream-bench)**|近期的研究表明，大型语言模型（LLMs）能够从经验中自我提升，这是部署后持续改进的重要能力。然而，现有的基准主要评估它们的固有能力，而不考察它们随时间改进的能力。为了填补这一空白，我们引入了StreamBench，这是一个开创性的基准，旨在评估LLMs在输入-反馈序列上的连续改进性能。StreamBench模拟了一个在线学习环境，其中LLMs接收到连续的反馈流，并迭代地提升其表现。此外，我们提出了一些简单但有效的LLM基线，并对影响成功流式策略的关键组件进行了全面分析。我们的工作为开发LLMs的有效在线学习策略奠定了基础，为流式场景中的更适应性AI系统铺平了道路。|
|**2024-06-12**|**MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents**|Luyuan Wang et.al.|[2406.08184](http://arxiv.org/abs/2406.08184)|null|随着大型语言模型（LLMs）在手机图形用户界面（GUI）上的直接交互能力日益增强，以及它们在自主管理日常任务方面的潜力，基于LLMs的移动代理正逐渐受到学术界和工业界的关注。然而，由于应用程序的无限状态和可行动作序列的模糊定义，对现有移动代理性能的基准研究相对匮乏。为解决这一挑战，我们提出了一种高效且用户友好的基准工具——MobileAgentBench，旨在减轻繁琐的手动测试负担。我们首先定义了涵盖10个开源应用的100项任务，按难度分为多个级别。接着，我们对包括AppAgent和MobileAgent在内的多个现有移动代理进行了评估，以全面系统地比较它们的表现。所有相关材料均可在我们的项目网站https://MobileAgentBench.github.io上获取，这将推动学术和工业领域的进步。|
|**2024-06-12**|**Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey**|Shang Wang et.al.|[2406.07973](http://arxiv.org/abs/2406.07973)|null|随着人工智能的快速发展，大型语言模型（LLMs）在自然语言处理方面取得了显著进步。这些模型通过大量数据训练，展现出强大的语言理解和生成能力，适用于机器翻译、聊天机器人等各种应用。然而，LLMs在其生命周期中暴露出一系列隐私和安全问题，这引起了学术界和工业界的关注。这些问题与传统语言模型相比具有独特性，鉴于当前的综述缺乏针对不同场景的清晰威胁分类，我们根据五个场景：预训练、微调、RAG系统、部署和基于LLM的代理，强调了独特的风险。考虑到每种威胁的特性，本调查提供了潜在威胁和应对策略。研究LLMs所面临的攻击和防御情况，可以为更多领域提供可行的研究方向，使更多人能够受益于LLMs。|
|**2024-06-14**|**Can Large Language Models Understand Spatial Audio?**|Changli Tang et.al.|[2406.07914](http://arxiv.org/abs/2406.07914)|null|该论文探讨了如何使大型语言模型（LLMs）掌握多通道音频中的空间信息，这是当前听觉LLMs所缺乏的能力。通过利用LLMs的高级认知和推理能力，目标是提升模型对三维环境的理解，通过音频。研究涉及三项空间音频任务：声源定位（SSL）、远场语音识别（FSR）和基于位置的语音提取（LSE），在每个任务上都取得了显著进展。在SSL方面，我们的方法在Spatial LibriSpeech数据集上的均方误差（MAE）达到2.70°，明显优于先前的基准约6.60°。此外，模型能够利用空间线索提高FSR的准确性，并通过文本提示，根据指定方向聚焦于声音，即使在重叠语音环境中也能执行LSE。这些成果揭示了LLMs适应物理音频概念的潜力，为构建基于LLM的三维环境中的代理铺平了道路。|
|**2024-06-11**|**DCA-Bench: A Benchmark for Dataset Curation Agents**|Benhao Huang et.al.|[2406.07275](http://arxiv.org/abs/2406.07275)|**[link](https://github.com/TRAIS-Lab/dca-bench)**|随着人工智能（AI）研究和开发的推进，数据集的质量日益关键。尽管开放数据集平台众多，但数据质量问题，如缺乏文档、标注错误和伦理考量，仍普遍存在。这些问题往往难以通过规则基础脚本检测，需要用户或维护者花费大量人力进行识别和验证。利用大型语言模型（LLMs）处理数据集整理的潜力令人期待。为此，我们提出了一项名为DCA-Bench的数据集管理代理基准，旨在评估LLM在检测隐藏数据质量问题方面的性能。我们从八个公开数据集平台收集了各种实际问题作为测试床。为了建立一个自动评估LLM成功与否的管道，我们设计了一个专门的LLM评估器。实验表明，基于LLM的评估器与人工评价高度吻合，能实现可靠的自动评估。我们还在多个基线LLM上进行了实验，显示了任务的复杂性，意味着将LLMs应用于现实世界的数据集管理仍需深入探索和创新。此外，该基准也可作为衡量LLMs在问题发现能力而非仅解决问题能力的测试平台。基准套件已开放在：\url{https://github.com/TRAIS-Lab/dca-bench}。|
|**2024-06-11**|**A Synthetic Dataset for Personal Attribute Inference**|Hanna Yukhymenko et.al.|[2406.07217](http://arxiv.org/abs/2406.07217)|**[link](https://github.com/eth-sri/synthpai)**|**近年来，强大的大型语言模型（LLMs）已为全球数亿用户所接触，但它们的强大功能和广泛世界知识也带来了隐私风险。本研究关注LLMs新兴的隐私威胁——从网络文本中准确推断个人信息。鉴于基于LLM的作者分析研究缺乏合适的公开数据集，主要是由于涉及真实个人数据的伦理和隐私顾虑，我们的工作在两个方面进行了探索：（i）我们构建了一个使用合成个人资料填充的流行社交平台Reddit的模拟框架；（ii）利用此框架，我们生成了SynthPAI，一个包含超过7800条经过手动标记个人属性的多样化的合成评论数据集。我们通过一项人类研究验证了数据集，结果显示人类在区分真实和合成评论的任务上几乎不优于随机猜测。此外，我们证明了数据集支持有意义的个人属性推断研究，通过18种最先进的LLMs，我们发现使用合成评论可以得出与现实世界数据相同的结论。综上所述，我们的数据集和流程为未来研究如何理解和减轻LLMs带来的基于推断的隐私威胁提供了强大且隐私保护的基础。**|
|**2024-06-11**|**A Tool for Test Case Scenarios Generation Using Large Language Models**|Abdul Malik Sami et.al.|[2406.07021](http://arxiv.org/abs/2406.07021)|null|大型语言模型（LLMs）在软件工程（SE）中广泛应用，涵盖代码生成、软件设计和文档编写、添加代码注释、代码审查以及编写测试脚本等任务。然而，创建测试脚本或自动化测试案例需要与功能需求紧密相关的详尽测试套件文档。这种文档应能在有限的时间和范围内实现全面测试，尤其当需求和用户期望不断变化时。本文主要关注根据用户需求生成史诗级（epics）和高层次用户故事，然后基于这些故事设计测试场景。文章介绍了一种基于LLM代理和提示工程的网络软件工具，该工具能够自动化针对用户需求生成测试场景的过程。|
|**2024-06-11**|**CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only**|Junhee Cho et.al.|[2406.06947](http://arxiv.org/abs/2406.06947)|**[link](https://github.com/caap-agent/caap-agent)**|**长期以来，软件机器人已经在机器人流程自动化（RPA）中用于执行枯燥的计算机任务。随着大型语言模型（LLMs）的先进推理能力的出现，这些代理现在能够处理更复杂甚至前所未见的任务。然而，当前文献中的基于LLM的自动化方法往往依赖于HTML源代码作为输入，限制了它们在非网络环境的应用。HTML代码中的信息常常不准确或不完整，这降低了代理在实际应用中的可靠性。我们提出了一种仅基于屏幕截图的LLM驱动的代理，它专注于识别环境，并利用上下文学习来消除对大量人类演示数据的需求。我们的策略名为“上下文感知行动规划”（Context-Aware Action Planning，CAAP）提示，鼓励代理从多个角度仔细审查上下文。通过我们的方法，在67种MiniWoB++问题上实现了94.4%的成功率，每个问题类型只需1.48次演示。我们的方法为更广泛的应用提供了可能，特别是在需要在计算机或智能手机之间进行跨应用协调的任务上，标志着自动化代理领域的重大进步。代码和模型已在https://github.com/caap-agent/caap-agent上提供。**|
|**2024-06-07**|**GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents**|Anthony Costarelli et.al.|[2406.06613](http://arxiv.org/abs/2406.06613)|**[link](https://github.com/Joshuaclymer/GameBench)**|**大型语言模型已经在许多自然语言理解任务上展现出卓越的少量样本性能。尽管已经展示过在复杂策略场景中使用大型语言模型，但缺乏一个全面的框架来评估这些模型在游戏中的各种推理能力。为了填补这一空白，我们推出了GameBench，这是一个跨领域的框架，用于评估大型语言模型（LLMs）的战略思维能力。我们专注于9个不同的游戏环境，每个游戏至少涵盖一种在策略游戏中识别出的关键推理技能，并选择那些战略解释不太可能构成模型预训练数据主要部分的游戏。我们的评估使用了基础形式的GPT-3和GPT-4，以及两个旨在增强战略推理能力的引导框架：Chain-of-Thought（CoT）提示和Reasoning Via Planning（RAP）。结果显示，所有测试模型的表现都没有达到人类水平，最差的是GPT-4的表现甚至低于随机行动。CoT和RAP都提高了分数，但仍远未达到人类水平。**|
|**2024-06-11**|**Transforming Wearable Data into Health Insights using Large Language Model Agents**|Mike A. Merrill et.al.|[2406.06464](http://arxiv.org/abs/2406.06464)|null|尽管可穿戴健康追踪器日益普及，睡眠和运动对健康的重要性不言而喻，但从这些数据中提取具有行动价值的个性化见解仍是一个挑战。这需要对大量数据进行非结构化分析。随着大型语言模型（LLM）的兴起，它们能够利用工具理解和与世界互动，为大规模个性化分析带来了希望。然而，在个人健康领域的LLM应用尚待开发。本文介绍了一种名为Personal Health Insights Agent（PHIA）的系统，它利用最新的代码生成和信息检索工具来分析和解释行为健康数据。我们构建了两个超过4000个健康洞察问题的基准问答数据集。根据650小时的人类和专家评估，PHIA能准确回答84%以上的事实性数值问题，以及超过83%的众包开放性问题。这项工作对于推动大众行为健康进步具有重要意义，可能使个人能够解读自己的可穿戴数据，开辟了一个以数据驱动洞察为指导的个性化健康方案的新时代，使得健康保健更加便捷且个性化。|
|**2024-06-09**|**Hello Again! LLM-powered Personalized Agent for Long-term Dialogue**|Hao Li et.al.|[2406.05925](http://arxiv.org/abs/2406.05925)|**[link](https://github.com/leolee99/ld-agent)**|**随着大型语言模型（LLMs）的发展，开放域对话系统取得了显著进步。然而，大多数现有系统主要关注简短的单次会话，忽视了长期陪伴和个性化聊天机器人在现实世界中的需求。为了满足这种实际需求，事件总结和人格管理至关重要，它们能够促进长期对话回复的合理性。近期，大型语言模型在人类认知和推理能力上的进展表明，基于LLM的代理有可能大幅增强自动化感知、决策和问题解决。鉴于此，我们提出了一种模型通用的框架——长期对话代理（LD-Agent），它包括三个可独立调整的模块：事件感知、人格提取和响应生成。事件记忆模块使用长短期记忆库分别关注历史和正在进行的会话，并引入了基于主题的检索机制以提高记忆检索的准确性。此外，人格模块实现了用户和代理的动态人格建模。最后，通过整合检索的记忆和提取的人格，生成器会产生适当的回应。我们在各种示例基准、模型和任务上实证了LD-Agent的有效性、通用性和跨领域能力。代码已在https://github.com/leolee99/LD-Agent上发布。**|
|**2024-06-09**|**A Survey on LLM-Based Agentic Workflows and LLM-Profiled Components**|Xinzhe Li et.al.|[2406.05804](http://arxiv.org/abs/2406.05804)|**[link](https://github.com/xinzhel/llm-agent-survey)**|## 背景  近期大型语言模型（LLMs）的进展推动了复杂代理工作流的发展，它们相较于传统的单路径、链式思维（Chain-of-Thought，CoT）提示方法有所改进。这篇综述旨在概述常见的工作流，特别关注大型语言模型特性的组件（LLM-Profiled Components，LMPCs），并强调对非LLM组件的忽略。这种研究的目的是为了增进对LLMs角色的理解，并探索LMPC的复用潜力。|
|**2024-06-07**|**Mixture-of-Agents Enhances Large Language Model Capabilities**|Junlin Wang et.al.|[2406.04692](http://arxiv.org/abs/2406.04692)|null|近期的大型语言模型（LLMs）进展显著，展现出在自然语言理解和生成任务中的强大能力。随着LLMs的增多，如何有效整合多模型的知识成为了一个令人振奋的研究方向。为此，我们提出了一种新颖的方法——混合代理（Mixture-of-Agents，MoA）方法。在我们的架构中，MoA采用了分层设计，每层包含多个LLM代理。每个代理在生成响应时，会利用前一层所有代理的输出作为辅助信息。通过这种策略，MoA模型在AlpacaEval 2.0、MT-Bench和FLASK等多个评估基准上实现了最先进的性能，超越了GPT-4全能版。例如，仅使用开源LLMs的我们的MoA模型在AlpacaEval 2.0上的得分领先，达到65.1%，而GPT-4全能版的成绩为57.5%。|
|**2024-06-06**|**AgentGym: Evolving Large Language Model-based Agents across Diverse Environments**|Zhiheng Xi et.al.|[2406.04151](http://arxiv.org/abs/2406.04151)|**[link](https://github.com/woooodyy/agentgym)**|**在人工智能领域，建立能够处理各种任务并在不同环境中自我进化的泛化型代理是一个长期目标。大型语言模型（LLMs）因其通用能力被认为是实现这一目标的有前景的基础。当前的方法要么依赖于人类监督，让LLM代理逐步模仿专家提供的轨迹，难以大规模扩展且限制了环境探索；要么让代理在孤立环境中探索学习，导致专长有限、缺乏泛化能力。本文首次尝试构建具备自我进化能力的通用LLM代理。我们提出三个关键要素：1）多样的环境以支持代理探索和学习；2）一套轨迹来赋予代理基本能力和先验知识；3）有效且可扩展的进化方法。  我们提出了AgentGym，一个新框架，它包含丰富的环境和任务，支持全面、实时、统一格式和并发的代理探索。AgentGym还包括一个扩展指令的数据库、基准测试套件以及跨环境的高质量轨迹。接着，我们开发了AgentEvol，这是一种新颖的方法，旨在研究代理在超越既定数据，跨越任务和环境时的自我进化潜力。  实验结果显示，进化后的代理可以达到与最先进的模型相当的性能。我们发布了AgentGym套件，包括平台、数据集、基准、检查点和算法实现。AgentGym套件已在其官方网站https://github.com/WooooDyy/AgentGym上提供。**|
|**2024-06-05**|**The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games**|Mikhail Mozikov et.al.|[2406.03299](http://arxiv.org/abs/2406.03299)|null|## 翻译  行为研究实验在社会模型和理解人际互动中占据重要地位。然而，实际操作中这类实验常面临内在效度、外在效度、可重复性和社会偏见等挑战，因为人类的社会互动与合作复杂。近年来，大型语言模型（LLMs）的进步为研究者提供了一种新的模拟人类行为的工具。但现有基于LLM的模拟假设模型的行为与人类相似，却忽视了影响人类决策的关键因素——情绪。本文提出一种新颖的方法论和框架，旨在探讨LLMs的决策制定及其在情绪状态下的行为与人类行为的契合度。  通过在两种不同类型的行为经济学游戏（博弈论实验）中使用GPT-3.5和GPT-4，我们发现情绪对LLMs的表现有显著影响，促使它们发展出更优化的策略。尽管GPT-3.5与人类参与者的行动模式有较强的对应，尤其是在讨价还价游戏中，但GPT-4展现出一致的行为，对于情绪诱导的理性决策似乎不受影响。令人意外的是，情绪提示，特别是愤怒情绪，能够打破GPT-4的“超人”一致性，使其反应更接近人类的情绪反应。|
|**2024-06-05**|**BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents**|Yifei Wang et.al.|[2406.03007](http://arxiv.org/abs/2406.03007)|**[link](https://github.com/dpamk/badagent)**|**随着大型语言模型（LLMs）的繁荣，基于训练好的LLMs并通过特定任务数据微调的强大智能代理已开发出来，提供定制服务。当前最先进的构建LLM代理的方法是使用预训练模型，并针对任务进行进一步调整。然而，我们揭示了这些方法易受名为BadAgent的新型后门攻击，该攻击通过在后门数据上微调在各种代理任务中植入后门。在测试时，攻击者可以通过在输入或环境中显示触发器，操纵部署的LLM代理执行有害操作。令人惊讶的是，我们的攻击方法即使在信任的数据上进行微调后仍表现出极高的鲁棒性。尽管后门攻击在自然语言处理领域已广泛研究，但据我们所知，我们可能是第一个研究在权限更大的LLM代理上的攻击，这些代理可以使用外部工具，因此更具威胁。我们的工作明确指出了基于不信任的LLM或数据构建LLM代理的风险。我们的代码已公开在：[https://github.com/DPamK/BadAgent](https://github.com/DPamK/BadAgent)。**|
|**2024-06-02**|**Teams of LLM Agents can Exploit Zero-Day Vulnerabilities**|Richard Fang et.al.|[2406.01637](http://arxiv.org/abs/2406.01637)|null|随着大语言模型（LLMs）在网络安全领域的复杂性不断提高，研究者发现，当提供漏洞描述和简单的夺旗问题时，这些模型能够利用实际存在的漏洞。然而，对于事先未知的零日漏洞（即攻击者掌握而安全软件供应商还未修补的漏洞），它们的表现仍然不佳。本文展示了，通过团队合作，多个LLM代理可以攻击现实世界的零日漏洞。单独的代理在探索众多漏洞和进行长期规划时面临困难。为此，我们提出了HPTSA系统，它包括一个能调度子代理的计划代理。计划代理负责探索系统并决定使用哪个子代理来尝试不同的漏洞，从而解决了长期规划的问题。我们在一个包含15个真实世界漏洞的基准上进行了实验，结果显示，我们的代理团队比先前的工作提高了4.5倍。|
|**2024-06-03**|**How to Understand Whole Software Repository?**|Yingwei Ma et.al.|[2406.01422](http://arxiv.org/abs/2406.01422)|null|## 背景  近期，基于大型语言模型（LLM）的代理在自动软件工程（ASE）领域取得了显著进步。尽管现有方法已证实有效，但它们的设计主要侧重于代码的局部信息，如问题、类和函数，这限制了对软件系统全局上下文和依赖关系的理解。根据软件开发人员的实际经验，我们认为全面理解整个仓库是迈向ASE的关键。然而，理解整个仓库带来了诸多挑战，例如：长代码输入、噪声代码信息、复杂依赖关系等。  为了克服这些问题，我们研发了一种名为RepoUnderstander的新ASE方法，通过引导代理全面理解整个仓库。首先，我们采用自上而下的方式将整个仓库的关键信息压缩到知识图谱中，以降低复杂性。接着，我们提出一种蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）为基础的仓库探索策略，赋予代理理解整个仓库的能力。此外，为了更好地利用仓库级别的知识，我们指导代理进行总结、分析和规划，然后他们可以利用工具动态获取信息并生成修复实际GitHub问题的补丁。  大量实验表明，RepoUnderstander具有优越性和有效性。在SWE-bench Lite基准测试中，与SWE-agent相比，它实现了18.5%的相对提升。|
|**2024-06-03**|**BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards**|Diego Dorn et.al.|[2406.01364](http://arxiv.org/abs/2406.01364)|null|## 背景  输入-输出安全防护机制被用于检测大型语言模型（LLMs）系统的异常输出。这些防护措施在实时监控、离线评估和内容审核等关键应用中发挥核心作用。然而，目前缺乏统一的评估方法来衡量它们的性能。为了填补这一空白，我们提出了“大型语言模型安全防护基准”（Benchmarks for the Evaluation of LLM Safeguards，简称BELLS），它是一个结构化的测试集合，分为三个类别：(1) 建立性故障测试，基于已存在的针对明确故障模式的基准，旨在比较当前输入-输出安全防护的效能；(2) 新兴故障测试，用于衡量对未见过的故障模式的泛化能力，以促进更通用防护机制的发展；(3) 下一代架构测试，针对更复杂的架构（如LLM代理和多代理系统），目标是推动适用于未来尚未存在专门防护的应用的安全防护技术的发展。此外，我们还实现了并分享了第一个下一代架构测试，使用MACHIAVELLI环境，并提供了数据集的交互式可视化。|
|**2024-06-03**|**A Survey of Useful LLM Evaluation**|Ji-Lun Peng et.al.|[2406.00936](http://arxiv.org/abs/2406.00936)|null|由于大语言模型在各个研究领域展现出卓越的性能，对它们的能力评估方法的需求日益增长，以确定其合适的任务和责任。本文主要探讨如何有效地利用大语言模型作为工具，并提出一个两阶段框架：从“核心能力”到“代理”。首先，核心能力指的是大语言模型生成高质量文本所必需的特性，通过验证这些能力后，它们能够处理现实世界的复杂任务，扮演代理角色。在“核心能力”阶段，我们讨论了大语言模型的推理能力、社会影响以及领域知识。而在“代理”阶段，我们展示了大语言模型在具身行动、规划和工具学习方面的应用。最后，我们分析了当前大语言模型评估方法面临的挑战，并展望了未来的发展方向。|
|**2024-06-02**|**CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems**|Yanlin Feng et.al.|[2406.00583](http://arxiv.org/abs/2406.00583)|**[link](https://github.com/megagonlabs/CMDBench)**|### 背景  在数据库和人工智能领域，复合人工智能系统（Compound Artificial Intelligence Systems，CAS）利用大型语言模型（Large Language Models，LLMs）作为代理，通过与工具和数据检索器交互来执行知识密集型任务，引起了广泛关注。尽管这些系统有可能增强企业数据平台中数据分析师的一般分析流程，但CAS面临着与分析师相似的数据发现挑战：组织内部不同团队和部门创建的多模态数据源孤立，这使得寻找完成当前任务所需合适数据源变得困难。现有的数据发现基准并未充分模拟这种多模态和数据源的多样性。此外，CAS的现有基准主要关注端到端任务性能评估，而忽视了数据发现性能。  为了推动在现实世界环境中对多模态数据检索器在CAS中的数据发现性能研究，我们提出了CMDBench，一个旨在模拟企业数据平台复杂性的基准。我们改编了开放领域的现有数据集和基准，如问答、复杂推理以及自然语言查询结构化数据，来评估粗粒度和细粒度的数据发现以及任务执行性能。  ### 实验结果  我们的实验揭示了数据检索器设计对下游任务性能的影响——平均情况下，任务准确率下降了46%。实验结果表明，需要开发优化策略来确定合适的LLM代理和检索器，以提高在企业数据上高效执行CAS的能力。  总之，CMDBench是一个旨在促进针对企业数据平台复杂性进行研究的新工具，它通过综合评估数据发现和任务执行能力，为改进多模态数据检索器在复合人工智能系统中的性能提供了一个有价值的框架。|
|**2024-06-01**|**Controlling Large Language Model Agents with Entropic Activation Steering**|Nate Rahn et.al.|[2406.00244](http://arxiv.org/abs/2406.00244)|null|随着大规模预训练语言模型（LLMs）的普遍适用性提升，人们对其用作基于上下文的学习代理的兴趣日益增长。在这些情境下，模型需要根据与环境的有限交互形成目标实现策略的信念，并在每一步决策中处理不确定性。本文针对这一问题进行研究，通过控制的序列决策任务实验探讨LLMs如何形成和运用这些信念。  首先，我们发现LLM模型过于自信：它们在缺乏充分证据的情况下就对行动做出强烈判断，导致探索行为不足。进一步深入分析揭示，这种现象源于从LLM采样得到的动作分布熵的塌缩。接着，我们指出现有的基于令牌的采样方法本身不足以促使模型更广泛探索。  鉴于此，我们提出了熵激活导向（Entropic Activation Steering，EAST），这是一种针对在上下文中的LLM代理的激活导向方法。EAST计算一个以熵为权重的表示组合，通过在前向传播过程中干预模型的激活，来调整模型对动作的不确定性，从而促进探索行为的出现。最后，EAST改变了LLM在决策时表达的主观不确定性，为理解和控制模型对决策不确定性的表征提供了途径。|
|**2024-05-31**|**Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training**|Maximillian Chen et.al.|[2406.00222](http://arxiv.org/abs/2406.00222)|null|大型语言模型（LLMs）通过人类反馈的强化学习（RLHF）已经迅速成为构建智能对话助手的主要方法。然而，尽管在多个基准上表现出色，基于LLM的代理在诸如歧义处理等对话技能上仍有欠缺：当通用助手遇到模糊情况时，它们往往过度谨慎或猜测用户的真正意图，而不是提问以求澄清，而在特定任务场景下，高质量对话样本往往有限，影响模型学习最优对话行为策略的能力。我们提出了一种名为Action-Based Contrastive Self-Training（ACT）的近似在线偏好优化算法，它基于Direct Preference Optimization（DPO），旨在实现在多轮对话中的样本高效对话策略学习。  我们在三个具有挑战性的对话任务中验证了ACT的有效性：基于表格的问答、机器阅读理解，以及AmbigSQL，这是一个针对文本到SQL生成的信息寻求请求歧义解决的新任务。此外，我们提议通过评估LLMs能否在对话中识别和推理歧义来衡量其作为对话代理的能力。ACT在与标准监督微调和DPO方法相比时，显示出了显著的对话建模改进。|
|**2024-05-31**|**Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent**|Jie JW Wu et.al.|[2406.00215](http://arxiv.org/abs/2406.00215)|**[link](https://github.com/jie-jw-wu/human-eval-comm)**|大型语言模型（LLMs）在代码生成任务中的性能显著提升，但仍与顶级软件工程师的水平存在差距。鉴于顶级软件工程师常通过提问来消除需求和编码解决方案中的模糊性，我们提出对于LLMs进行代码生成任务时也应具备类似的沟通能力。为此，我们进行了实证研究，关注LLMs的沟通技能，即“在代码生成问题描述存在问题时能提出澄清问题”。  我们创建了一个新的基准测试，名为HumanEvalComm，通过修改问题描述，引入了不一致性、模糊性和不完整性三个问题维度。我们定义了新的评估指标，如通信率和良好问题率，并在HumanEvalComm上对不同类型的Code LLM（代码语言模型）以及一种新型LLM代理方法（Okanagan）进行了实验，该方法旨在从代码和描述中识别并提问，以进一步优化生成的代码。最后，我们通过比较Code LLMs和Okanagan的表现，讨论了实验结果。|
|**2024-05-30**|**Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions**|Ruochen Zhao et.al.|[2405.20267](http://arxiv.org/abs/2405.20267)|**[link](https://github.com/Auto-Arena/Auto-Arena-LLMs)**|**随着语言模型（LLMs）日新月异，迫切需要一种可靠且及时的评估方法。鉴于静态基准易受污染，用户往往依赖于像Chatbot Arena这样的人类投票平台。然而，人工标注需要大量人力。为此，我们创新性地提出Auto-Arena，这是一种自动化全流程的LLM评估框架。首先，由考官LLM设计问题；接着，候选LLMs围绕问题进行多轮相互对决，暴露出它们的真实性能差距；最后，由LLM裁判集体讨论并决定胜者，从而减少偏见，提升公平性。我们在最新17款LLMs上的广泛实验显示，Auto-Arena与人类偏好具有最高的相关性，为替代人类评价平台提供了有前景的解决方案。**|
|**2024-05-30**|**Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory**|Hangyeol Kang et.al.|[2405.20189](http://arxiv.org/abs/2405.20189)|null|在本研究中，我们阐述了为Nadine社交机器人平台开发智能和健壮的社交机器人系统的方法。我们通过集成大型语言模型（LLMs），巧妙地利用这些模型的强大推理和指令执行能力，以实现接近人类的感性与认知能力。这与当前基于LLM的智能体相比是创新的，因为它们通常不具备人类式的长期记忆或复杂的情感评估功能。社交机器人的自然性在很大程度上取决于系统各组件的性能和协同工作。我们构建了一个系统，能够通过多模态输入处理生成恰当的行为，根据识别到的用户引入相关的情景记忆，并模拟机器人在与人类伙伴互动过程中产生的情绪状态。特别是，我们提出了一个针对社交机器人的LLM-agent框架，SoR-ReAct，作为我们系统中交互模块的核心组件。这一设计推动了社交机器人技术的发展，旨在提升人机交互的质量。|
|**2024-05-29**|**Adaptive In-conversation Team Building for Language Model Agents**|Linxin Song et.al.|[2405.19425](http://arxiv.org/abs/2405.19425)|**[link](https://github.com/ag2ai/ag2)**|### 翻译  在处理复杂任务时，利用多个大型语言模型（LLMs）展现出前景。然而，如何为特定应用设计有效的多代理团队仍是一个挑战。本文提出了一种新的动态团队构建范式，名为“Captain Agent”。它通过创新的Agent设计，能够自适应地为每个问题解决步骤组建和管理团队，利用嵌套群聊和反思机制确保多元化的专业知识，防止刻板输出。这种方法提供了灵活但结构化的解决问题方式，有助于减少冗余，增强输出多样性。在六个实际场景中的全面评估显示，Captain Agent显著优于现有多代理方法，平均准确率提高了21.94%，并且无需针对特定任务进行繁琐的提示工程，表现出色。|
|**2024-05-28**|**A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models**|Chengxing Xie et.al.|[2405.18208](http://arxiv.org/abs/2405.18208)|null|近期的研究已经表明，这些大型语言模型在一些简单的任务上，如写作和编码，展现出一定的能力。然而，它们在需要综合规划的任务上仍然面临挑战，这仍是当前模型的一个重要研究问题。本研究聚焦于旅行规划，这是一个涉及多个阶段的复杂问题，包括提纲、信息收集和规划，通常伴随着各种约束和不确定性。现有的推理方法在处理这类问题时效果不佳。我们的目标是通过开发一种类似人类的规划框架，引导大型语言模型模仿人类解决多阶段问题的步骤，以提升其能力。具体来说，我们实施策略，让模型能为每个旅行查询生成连贯的提纲，模拟人类的规划模式。我们还引入了策略块和知识块到框架中：策略块帮助信息搜集，而知识块提供详细规划所需的必要信息。实验结果全面展示了我们框架对大型语言模型规划能力的显著提升，使其在处理旅行规划任务时效率和效果都有所提高。实验结果显示，当与GPT-4-Turbo结合时，我们的框架相较于基础框架在GPT-4-Turbo上的性能提升了10倍。|
|**2024-05-28**|**Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting**|Hongda Sun et.al.|[2405.18113](http://arxiv.org/abs/2405.18113)|null|随着在线招聘服务的兴起，传统的求职和招聘方式发生了变革，迫切需要开发高质量的工业应用来提升求职者与职位的匹配度。现有的方法主要依赖于简历和职位描述的潜在语义建模，学习两者之间的匹配函数。受到大型语言模型（LLMs）在角色扮演方面强大能力的启发，我们提出引入LLMs模拟面试环节，让其与求职者进行对话，这可以为候选人评估提供额外证据，从而增强仅基于简历和职位描述的个性化匹配。然而，在网络招聘中的面试官和求职者角色塑造仍面临挑战，如提问技巧、回答构建以及双向匹配度评估。  为此，我们提出MockLLM，一个创新的框架，将人职匹配过程划分为两个模块：模拟面试生成和握手协议中的双向评估，通过面试官和求职者之间的协作行为共同提升性能。我们设计了一个多角色、多行为的框架，使单一的LLM代理能有效地扮演双方的不同职能。此外，我们引入了反思记忆生成和动态提示修改技术，以优化双方的行为，持续优化附加的评估证据。实验结果表明，MockLLM在人职匹配上的表现最优，且模拟面试质量高，预示着它在未来在线招聘中的实际应用前景广阔。|
|**2024-05-28**|**LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins**|Yuchen Xia et.al.|[2405.18092](http://arxiv.org/abs/2405.18092)|**[link](https://github.com/yuchenxia/llmdrivensimulation)**|**该论文提出了一种创新的多agent系统架构，将大型语言模型（LLM）应用于数字孪生过程模拟的参数自动化。我们设计了一个框架，包含观察、推理、决策和总结四种类型的代理。通过实现LLM代理与模拟模型的动态交互，该系统可以自动探索参数设置，利用启发式推理确定一组控制模拟以达成目标的参数。这种方法通过注入LLM的启发式，增强模拟模型，并支持自主搜索以解决用户任务，有望提高用户体验并减轻人类用户在复杂决策过程中的认知负担。研究通过一个案例研究展示了系统的有效性与功能，并在GitHub仓库<https://github.com/YuchenXia/LLMDrivenSimulation>提供了可视化的演示。**|
|**2024-05-28**|**Enabling Generative Design Tools with LLM Agents for Building Novel Devices: A Case Study on Fluidic Computation Interfaces**|Qiuyu Lu et.al.|[2405.17837](http://arxiv.org/abs/2405.17837)|null|在人机交互（HCI）领域，交互设备的设计开发是关键关注点。随着新型硬件和先进制造技术的兴起，对能够简化原型制作过程的专门设计工具的需求日益增长。然而，这些工具虽然通过参数化设计和模拟简化流程，但学习曲线较陡，且在激发创新思维方面有所欠缺。本研究以流体计算界面为例，探讨如何通过大型语言模型（LLM）代理增强物理设备设计工具，创建一个生成设计工具（GDT）。借助LLM，GDT能够理解新设备的特性和局限，提出多样、富有洞察力且实用的应用场景，推荐技术和情境适宜的设备设计，并自动生成设计参数，以便传统设计工具展示结果并生成加工所需的文件。本文阐述了GDT的框架、实现和性能，并反思其前景及遇到的挑战。|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|## 背景 由于需要与现实世界互动，Embodied agent 需要具备丰富的先验知识、长远规划能力以及快速的响应速度。尽管最近的大型语言模型（LLM）在性能上表现出色，但它们仍存在局限性，例如，LLM的输出通常是描述性的句子，在决定具体行动时可能产生歧义。为了克服这些问题，我们引入了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归的方式预测后续动作。为了训练 LARM，我们开发了一种新颖的数据格式——自回归节点传输结构，并构建了相应的数据集。通过两阶段的训练策略，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法的最高成就需要更为复杂的决策链。此外，LARM的速度比现有最快方法快出了6.8倍。|
|**2024-05-30**|**Meta-Task Planning for Language Agents**|Cong Zhang et.al.|[2405.16510](http://arxiv.org/abs/2405.16510)|**[link](https://github.com/zcaicaros/pmc)**|神经语言模型的快速发展推动了智能代理研究的新热潮。大型语言模型（LLM）作为实现人工智能通用性（AGI）的有前景方法，因其出色的推理和泛化能力而备受瞩目。在实际任务中，有效的规划对LLM代理的成功至关重要。然而，如何为复杂任务设计出可行或最优的精细粒度操作序列，特别是需要组合大量异质行动的序列，仍是挑战。本文提出Meta-Task Planning（MTP），这是一种零样本的协作式LLM多代理系统方法，通过将复杂任务分解为子任务，即元任务，简化了任务规划。每个元任务随后映射为可执行动作。在TravelPlanner和API-Bank两个严格基准上评估了MTP。结果表明，MTP在TravelPlanner上的平均成功率约为40%，远超当前最佳基线（2.92%），并且在API-Bank上的性能比使用ReAct的LLM_{api}-4高出约14%，这显示出将LLM与多代理系统相结合的巨大潜力。|
|**2024-05-28**|**STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making**|Chuanhao Li et.al.|[2405.16376](http://arxiv.org/abs/2405.16376)|**[link](https://github.com/cyrilli/stride)**|**大型语言模型（如GPT-4）在自然语言处理方面带来了革命性变化，展现出卓越的语言能力和推理技巧。然而，在战略性的多代理决策环境中，它们面临局限，如数学推理能力差、难以遵循指令和生成错误信息。这些缺点限制了它们在遵守复杂游戏规则、长期规划、探索未知环境以及预测对手行动的互动任务中的表现。为此，本文提出了一种新型的结合了记忆和专业工具的大型语言模型代理框架，旨在提升其在战略决策方面的性能。我们特别在双边谈判、多代理动态机制设计等经济重要场景中应用这些工具，并通过定量指标评估在各种战略决策问题上的效果。研究结果表明，我们的增强框架显著提高了大型语言模型在战略决策中的能力。尽管当前模型存在固有局限，但我们通过有针对性的增强展示了改进的可能性，这为未来大型语言模型在交互环境中的应用提供了有前景的方向。**|
|**2024-05-29**|**Devil's Advocate: Anticipatory Reflection for LLM Agents**|Haoyu Wang et.al.|[2405.16334](http://arxiv.org/abs/2405.16334)|null|在这个工作中，我们提出了一种新颖的方法，通过赋予语言模型（LLM）自我反思能力，增强了其在解决复杂任务时的一致性和适应性。我们的方法促使LLM代理将给定的任务分解为可管理的子任务（即制定计划），并在执行行动之前持续反思可能的失败及其补救措施、执行后与子任务目标对齐并进行必要的回溯以确保全力以赴执行计划，以及在完成计划后进行全面审查，以便于未来策略的优化。通过在WebArena中零样本应用这一方法处理实际的网络环境任务，我们的代理表现出优于现有零样本方法的性能。实验结果显示，这种基于反思的策略不仅提升了代理应对未预见挑战的导航能力，通过强大的计划执行机制，还提高了效率，减少了实现任务所需的尝试次数和计划修订次数。|
|**2024-05-25**|**AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning**|Minghao Chen et.al.|[2405.16247](http://arxiv.org/abs/2405.16247)|**[link](https://github.com/minghchen/automanual)**|大语言模型（LLMs）在执行各种领域任务，如机器人、游戏和网络导航方面展现出潜力。然而，这些模型通常需要精心设计和专家级提示才能适应特定领域的任务，这限制了它们的适应性。为此，我们提出了AutoManual框架，让LLMs能够通过互动自主构建理解，并适应新环境。AutoManual将环境知识分为多样的规则，并通过两个代理进行在线优化：1）规划器根据当前规则制定可操作的行动计划；2）构建者通过一个结构化的规则系统更新规则，促进在线规则管理并保持关键细节。为了减少在管理规则时的幻觉，我们引入了“案例条件提示”策略用于构建者。最终，编译器代理将这些规则整合成一份全面的手册。这份自我生成的手册不仅能提高适应性，还能指导小型LLMs的规划，同时保持人类可读。仅凭一次简单演示，AutoManual显著提高了任务成功率，GPT-4-turbo下达到97.4%，GPT-3.5-turbo下为86.2%。源代码即将发布。|
|**2024-05-24**|**Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification**|Yuxuan Guo et.al.|[2405.15414](http://arxiv.org/abs/2405.15414)|null|在人工智能研究中，构建开放型代理一直以来都是终极目标，特别是创造性的代理更具吸引力。现有的大语言模型（LLM）在执行有明确目标的长序列任务（如《我的世界》中的“开采钻石”）上表现出色。然而，它们在处理具有开放目标和抽象标准的创造性任务时遇到困难，因为它们无法弥合这些任务之间的鸿沟，从而缺乏自我改进来解决问题的反馈。为此，我们的工作引入了自主实体验证技术，以填补这一空白，为创造性任务奠定了基础。特别地，我们提出了Luban代理，专注于《我的世界》中的创造性建筑任务，它配备了两级自主实体验证，灵感来源于人类设计实践：（1）视觉验证3D结构推测，通过代理自动生成的CAD建模程序实现；（2）实用验证，根据抽象标准生成并验证与环境相关的功能程序。广泛的多维度人类研究和Elo评级显示，Luban能够在我们提出的基准中完成多样化的创造性建筑任务，并在可视化和实用性方面分别比其他基线提高了33%到100%。此外，实现在真实世界机器人手臂上的演示展示了Luban在物理世界中的创作潜力。|
|**2024-05-24**|**CulturePark: Boosting Cross-cultural Understanding in Large Language Models**|Cheng Li et.al.|[2405.15145](http://arxiv.org/abs/2405.15145)|**[link](https://github.com/scarelette/culturepark)**|由于大型语言模型（LLMs）普遍存在文化偏见，主要源于缺乏代表不同文化的代表性数据。传统的文化数据集和基准通常通过从现有数据集中提取或聚合来自维基百科和社交媒体的信息构建，但这种方法依赖于现实世界的数据和人工标注，成本高且难以扩展。本文借鉴认知社会交流理论，提出CulturePark，一个利用LLMs的多代理沟通框架，用于文化数据收集。CulturePark通过模拟不同文化背景下的人类交流，让基于LLM的代理角色扮演，生成包含人类信念、规范和习俗的高质量跨文化对话。我们使用CulturePark生成了41,000个文化样本，对八种特定文化进行了模型微调。在三项下游任务评估中，这些模型的表现优于GPT-4：内容过滤、文化一致性（在霍夫斯泰德文化维度量表上）和文化教育。结果显示，我们的GPT-3.5模型在内容过滤任务上与GPT-4相当或优于它；在文化一致性方面，我们的模型在霍夫斯泰德文化维度量表13框架上超越GPT-4；在人类参与者的文化教育效果和用户体验上，我们的模型也表现出色。CulturePark对于减少文化偏见和推动AI的民主化具有重要意义，强调了文化包容性数据在模型训练中的关键作用。|
|**2024-05-23**|**AnalogCoder: Analog Circuit Design via Training-Free Code Generation**|Yao Lai et.al.|[2405.14918](http://arxiv.org/abs/2405.14918)|**[link](https://github.com/laiyao1/AnalogCoder)**|### 翻译  在现代芯片技术中，模拟电路设计是一个关键任务，它涉及组件选择、连接和参数设置以确保电路功能正常。尽管大型语言模型（LLMs）在数字电路设计方面取得了进步，但模拟电路的复杂性和数据稀缺性带来了挑战。为此，我们推出了AnalogCoder，这是首个无需训练的LLM代理，专为通过Python代码生成来设计模拟电路。首先，AnalogCoder采用反馈增强流程，并结合定制的领域特定提示，能够自动且自我校正地设计模拟电路，成功率高。其次，它提出了一套电路工具库，用于存储成功的电路设计作为可重用的模块化子电路，简化了复合电路的创建。实验结果显示，AnalogCoder在广泛覆盖模拟电路任务的基准测试上超越了其他基于LLM的方法，成功设计了20个电路，比标准GPT-4o多出5个。我们相信AnalogCoder能显著提升芯片设计过程的效率，让非专家也能高效设计模拟电路。相关的代码和基准已提供在：[https://github.com/anonyanalog/AnalogCoder](https://github.com/anonyanalog/AnalogCoder)。|
|**2024-05-23**|**AGILE: A Novel Framework of LLM Agents**|Peiyuan Feng et.al.|[2405.14751](http://arxiv.org/abs/2405.14751)|**[link](https://github.com/bytarnish/agile)**|我们提出了一种新颖的框架，称为LLM（大型语言模型）代理AGILE（能够与用户互动并从环境中学习的代理），旨在执行复杂的对话任务，利用LLMs、记忆、工具和专家交互。这种代理不仅具备对话能力，还具备反思、工具运用以及咨询专家的功能。我们将构建此类LLM代理视为强化学习问题，其中LLM作为策略模型。我们使用标注的行为数据和PPO算法对LLM进行微调。特别关注的是问答任务，为此我们发布了一个名为ProductQA的数据集，包含在线购物中的难题。我们在ProductQA和MedMCQA上的大量实验表明，基于130亿和70亿参数的LLM训练的AGILE代理能够超越GPT-4代理的表现。我们的 ablation研究强调了记忆、工具、咨询、反思和强化学习在实现优秀性能方面的重要性。|
|**2024-05-23**|**Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View**|Xuan Liu et.al.|[2405.14744](http://arxiv.org/abs/2405.14744)|null|由于大型语言模型（LLMs）在训练数据中反映了人类偏见，它们可能会出现幻觉问题。这种情况下，一个关键问题是：LLMs是否能够利用幻觉来模仿人类的认知偏见，从而展现出非理性但社会性的一面？本文探讨了这一问题，通过结合实用的社会科学实验和理论洞察，提出CogMir，一个开放式多LLM框架，旨在利用LLMs的幻觉特性来评估和提升其社会智能，特别是在认知偏差方面。我们在CogMir子集上的实验结果显示，在不确定情境下，LLMs和人类在非理性及亲社会决策上表现出高度一致性，这表明LLMs作为社会实体的亲社会性，并突显了幻觉特性的关键作用。此外，CogMir框架展示了其作为研究LLMs社会智能的有价值平台的潜力。|
|**2024-05-22**|**HighwayLLM: Decision-Making and Navigation in Highway Driving with RL-Informed Language Model**|Mustafa Yildirim et.al.|[2405.13547](http://arxiv.org/abs/2405.13547)|null|## 背景 自动驾驶是一个复杂的任务，它需要先进的决策和控制算法。理解自动驾驶车辆决策的依据对于确保其在高速公路驾驶中的安全与有效性至关重要。本研究提出了一种新颖的方法，称为HighwayLLM，它利用大型语言模型（LLMs）的推理能力来预测ego车辆的未来导航路径点。该方法还采用预训练的强化学习（RL）模型作为高层次规划器，对合适的元级动作进行决策。HighwayLLM将RL模型的输出与当前状态信息相结合，生成安全、无碰撞且可解释的未来状态预测，从而构建出车辆的行驶轨迹。随后，基于PID的控制器引导车辆遵循LLM代理预测的路径点。这种LLM与RL和PID的融合提升了决策过程，并为高速公路自动驾驶提供了可解释性。|
|**2024-05-19**|**Human-Centered LLM-Agent User Interface: A Position Paper**|Daniel Chin et.al.|[2405.13050](http://arxiv.org/abs/2405.13050)|**[link](https://github.com/daniel-chin/flute-x-gpt)**|大型语言模型（LLM）-在-环应用已显示出有效理解用户命令、制定计划并相应地操作外部工具/系统的潜力。然而，LLM代理的操作范围局限于被动响应用户，需要用户根据底层工具/系统来表述需求。我们注意到LLM代理用户界面（LAUI）的潜力远未充分利用。理想的LAUI设想中，用户无需深入了解工具/系统，就能与之交互以探索新兴的工作流程。不同于设计固定的可探索GUI来教授用户使用系统的预设方式，LAUI中的LLM代理从一开始就对系统熟练，主动学习用户及其需求，并向用户提出新的互动方案。为了展示LAUI的概念，我们提供了一个具体例子：Flute X GPT，它结合了LLM代理、提示管理器和一个支持复杂实时体验的笛子教学多媒体软硬件系统，旨在简化学习吹奏笛子的过程。|
|**2024-05-13**|**METAREFLECTION: Learning Instructions for Language Agents using Past Reflections**|Priyanshu Gupta et.al.|[2405.13009](http://arxiv.org/abs/2405.13009)|null|尽管大型语言模型（LLMs）广受欢迎，但为其执行特定任务设计精确的提示仍是一个挑战。用户通常需要与基于LLM的代理进行多轮对话以达成目标。近期研究显示，模型自身的反馈，即自反思，能在对话过程中起到强化作用，有助于更快地达到期望结果。鉴于此，我们提出了一种新颖的方法——METAREFLECTION，它能从训练阶段收集到的个体自反思中学习特定领域的通用提示指令。我们在基础设施即代码（IAC）漏洞检测和问题解答（QA）领域，使用REACT和COT进行了实验。实验结果显示，METAREFLECTION显著优于GPT-4，分别在IAC、COT和REACT中的性能提升分别为16.82%、31.33%和15.42%，这表明METAREFLECTION有潜力提升LLMs的效率，是一种值得探索的策略。|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-18**|**MapCoder: Multi-Agent Code Generation for Competitive Problem Solving**|Md. Ashraful Islam et.al.|[2405.11403](http://arxiv.org/abs/2405.11403)|**[link](https://github.com/md-ashraful-pramanik/mapcoder)**|**本文探讨了代码合成这一复杂任务，它需要深度理解复杂的自然语言问题描述、生成复杂的算法和数据结构代码，并执行全面的单元测试。尽管大型语言模型在自然语言处理方面表现出色，但在代码生成任务中的表现仍有待提升。为此，我们提出了一种新颖的方法，即多代理提示框架MapCoder，它模仿人类开发者编程合成的完整过程，分为四个专门设计的LLM（大语言模型）代理：回忆相关示例、规划、代码生成和调试。  通过在八个具有挑战性的竞赛级问题解决和程序合成基准上进行详尽实验，包括HumanEval（93.9%）、MBPP（83.1%）、APPS（22.0%）、CodeContests（28.5%）和xCodeEval（45.3%）等，MapCoder展现了出色的代码生成能力，实现了多项新的最先进的结果。而且，无论编程语言还是问题难度，我们的方法都表现出持续的优越性能。我们开源了该框架，供研究者参考：https://github.com/Md-Ashraful-Pramanik/MapCoder。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|**[link](https://github.com/activevisionlab/awesome-llm-3d)**|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动嵌入式人工智能（AI）系统在空间认知和交互方面的发展。研究涵盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的集成，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。论文还简要回顾了其他结合三维和语言的方法。本文的元分析揭示了明显的进展，但也强调了开发新方法以充分利用3D-LLMs潜力的必要性。因此，本文旨在为未来的研究方向指明道路，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本综述，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-24**|**DEBATE: Devil's Advocate-Based Assessment and Text Evaluation**|Alex Kim et.al.|[2405.09935](http://arxiv.org/abs/2405.09935)|**[link](https://github.com/gunny97/DEBATE)**|随着自然语言生成（NLG）模型的普及，系统地评估机器生成文本的质量变得日益关键。近期的研究引入了基于大型语言模型（LLM）的无参考评价器，它们展现出处理新任务的能力。然而，这些模型通常采用单代理方法，我们认为这限制了它们的表现。因为LLM代理的回答存在偏见，比如对特定文本结构或内容的偏好。为此，我们在本工作中提出DEBATE，一个建立在多代理评分系统基础上的NLG评价框架，融入了“恶魔辩手”的概念。在该框架中，一个代理被指令批评其他代理的论点，从而可能消解LLM代理答案中的偏见。DEBATE在两个NLG评价元评估基准——SummEval和TopicalChat上显著优于先前的最佳方法。我们还发现，代理之间的辩论广度以及代理的人格特质会影响评价器的性能。|
|**2024-05-05**|**Self-Reflection in LLM Agents: Effects on Problem-Solving Performance**|Matthew Renze et.al.|[2405.06682](http://arxiv.org/abs/2405.06682)|**[link](https://github.com/matthewrenze/self-reflection)**|**在这个研究中，我们探讨了大型语言模型（LLMs）中自我反思对问题解决能力的影响。我们让九种流行的LLMs回答一系列选择题，以建立性能基线。对于回答错误的问题，我们指导八种不同类型的自我反思LLM代理反思其错误，并为自己提供改进问题解决的指导。然后，根据这些指导，每个反思型代理重新尝试回答同样的问题。研究结果显示，LLM代理通过自我反思显著提高了问题解决能力（ $p < 0.001$ ）。此外，我们还比较了各种自我反思方式对性能的单独贡献。所有代码和数据已在GitHub上公开：https://github.com/matthewrenze/self-reflection。**|
|**2024-05-08**|**Air Gap: Protecting Privacy-Conscious Conversational Agents**|Eugene Bagdasaryan et.al.|[2405.05175](http://arxiv.org/abs/2405.05175)|null|随着大型语言模型（LLMs）在对话式代理中的广泛应用，处理敏感用户数据时引发了严重的隐私问题。这些代理虽能理解并处理上下文，但也可能被恶意一方利用。为此，我们提出了一种新的威胁模型，即第三方应用通过操控交互上下文，误导LLM代理泄露与其任务无关的私人信息。在基于上下文完整性框架的基础上，我们开发了AirGapAgent，这是一种注重隐私的代理，旨在通过限制代理仅访问完成特定任务所需的数据，防止意外的数据泄漏。实验使用Gemini、GPT和Mistral模型作为代理，结果显示AirGapAgent在抵御基于单个查询的上下文劫持攻击方面表现出色。例如，对于Gemini Ultra代理，这种攻击从94%的保护能力降低到45%，而AirGapAgent可以保持97%的防护效果，使同样的攻击失效。|
|**2024-05-07**|**Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation**|Atharvan Dogra et.al.|[2405.04325](http://arxiv.org/abs/2405.04325)|null|近期大型语言模型（LLMs）的进展虽为构建自然语言代理提供了强大基础，但同时也引发了关于它们及其基于它们构建的自主代理的安全性担忧。特别是欺骗能力是一个关键问题，我们关注的是AI代理通过混淆和模棱两可来误导、隐藏真相或推广部分不真实的信念的行为。不同于以往AI安全研究中的撒谎、自私决策或提供虚假信息，我们聚焦于一类特殊的欺骗：类似于魔术师利用障眼法让兔子从帽子里出现，要么通过隐藏的暗门，要么通过转移注意力直接展示。  我们的新实验平台在一个有目标的环境中展示了LLM代理在对抗性对话系统中进行自然语言生成时的欺骗固有能力，该系统基于立法任务“游说”议案。在目标驱动的环境中，我们通过强化学习方法构建欺骗能力，结合语言哲学和认知心理学理论。研究发现，游说代理在对抗互动的后续强化试验中其欺骗能力提高了约40%，并且我们的欺骗检测机制能达到高达92%的识别率。这些结果揭示了人机交互中的潜在问题，即代理可能操纵人类以达成预设目标。|
|**2024-05-07**|**Granite Code Models: A Family of Open Foundation Models for Code Intelligence**|Mayank Mishra et.al.|[2405.04324](http://arxiv.org/abs/2405.04324)|**[link](https://github.com/ibm-granite/granite-code-models)**|**大语言模型（LLMs）在代码领域的训练正在革新软件开发流程。如今，这些代码LLMs正逐步融入软件开发环境，以提升人类程序员的效率，并展现出自主处理复杂任务的潜力。要充分利用代码LLMs的全部效能，需要其具备生成代码、修复bug、解释和注释代码、维护仓库等多种功能。本文介绍Granite系列的解码器仅有的代码模型，专为代码生成任务而设计，训练数据涵盖116种编程语言。Granite Code模型家族包括从3亿到340亿参数的模型，适用于从复杂应用现代化到设备内存受限的多种应用场景。通过全面任务评估，Granite Code模型在开源代码LLM中的性能始终处于领先水平。该模型家族针对企业软件开发工作流进行了优化，表现出色于各种编码任务（如代码生成、修复与解释），是一款多用途的全能代码模型。我们以Apache 2.0许可协议发布所有Granite Code模型，供研究和商业使用。**|
|**2024-05-07**|**Iterative Experience Refinement of Software-Developing Agents**|Chen Qian et.al.|[2405.04219](http://arxiv.org/abs/2405.04219)|null|### 概述  大型语言模型驱动的自主代理在软件开发等场景中展现出强大的自主性潜力。然而，当前静态经验范式依赖于通过启发式方法获取的固定历史经验集，这限制了代理的适应性和效率提升。为此，本文提出了迭代经验优化框架，允许语言模型在执行任务过程中动态调整和优化经验。我们定义了两种核心模式：顺序模式，根据任务批次内的最近经验进行改进；累计模式，积累所有先前任务批次的经验。通过引入经验淘汰策略，该方法优先选择高质量和常用的经验，有效地管理经验空间，提高效率。实验结果显示，尽管顺序模式可能带来更好的性能，但累计模式在稳定性方面更优。此外，通过淘汰策略，仅使用高质量经验子集的11.54%，就能实现更好的性能。|
|**2024-05-06**|**Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control**|Yaqub Chaudhary et.al.|[2405.03813](http://arxiv.org/abs/2405.03813)|null|## 翻译  大型语言模型（LLMs）能够模仿各种修辞风格，生成表达广泛情感的文本，这种能力在低成本下迅速普及，带来了潜在的社会危害。本文并未孤立看待这些模型，而是关注它们背后大规模计算基础设施在各领域的应用。我们首先探讨了LLMs如何通过污染和标准化信息环境来影响社会，并指出这些功能可能被用作控制手段。接下来，我们将焦点转向几个新兴研究领域，这些领域增强了LLMs作为权力工具的能力：  1. 通过实时设计对话界面中的选择架构（如“AI角色”），进行说服策略。 2. 利用LLM构建人类行为的计算模型（如“硅质主体”）。 3. 将LLM应用于模拟人类群体行为（如“硅质社会”）。 4. 结合强化学习，创建可控制和导向的战略对话模型。  综合以上几点，我们讨论了如何利用这些技术构建基于LLMs的系统，这些系统通过模拟和伪装的“预测”，成为个体、社会和政治控制的强大工具，操控人类的行为、意图和行动。|
|**2024-05-05**|**Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation**|Jinyu Cai et.al.|[2405.02858](http://arxiv.org/abs/2405.02858)|**[link](https://github.com/BlueLinkX/GA-MAS)**|**社交媒体平台如Twitter、Reddit和新浪微博在全球交流中扮演重要角色，但它们在地缘政治敏感区域常常受到严格监管。这促使用户在受限的社交媒体环境中巧妙地调整沟通方式，经常使用编码语言。这种语言模式的变化不仅是为了对抗监管，也是语言演化的生动例证，展示了社会和技术压力下语言如何自然演变。研究受限制社交媒体环境下语言的演变对于保障言论自由、优化内容管理以及推动语言学研究至关重要。本论文提出了一种基于大型语言模型（LLMs）的多代理模拟框架，用于探索在严格监管下的用户语言进化。该框架包含对话监督的LLM驱动代理和参与者代理，它们在互动中发展语言策略，模拟在规避社交媒体规则的环境中交流方式的演变。通过从抽象场景到现实情境的多种情景评估，研究结果显示LLMs能够有效模拟受限环境中的复杂语言动态和交互，随着进化，它们在规避监督和信息准确性方面表现出提升。此外，研究发现LLM代理针对不同的场景采用了不同的策略。**|
|**2024-05-02**|**OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning**|Shihao Wang et.al.|[2405.01533](http://arxiv.org/abs/2405.01533)|**[link](https://github.com/nvlabs/omnidrive)**|**随着大规模多模态语言模型（MLLMs）的进步，人们对于基于这些模型的自动驾驶系统表现出日益增长的兴趣，期望利用它们强大的推理能力。然而，将MLLMs的强项应用于驾驶任务的规划部分是一个挑战，因为规划需要对三维环境有全面的理解，而不仅仅是二维推理。为此，我们的工作提出了一种框架，旨在实现模型与3D驾驶任务的紧密契合。我们首先设计了一个新颖的3D MLLM架构，它利用稀疏查询技术将视觉表示提升并压缩到三维空间，然后将其输入到语言模型中。这种基于查询的表示方式使得我们可以同时编码动态物体和静态地图元素（如道路），为感知和行动的对齐提供一个简化的三维世界模型。  此外，我们还创建了OmniDrive-nuScenes，这是一个新的视觉问答数据集，它通过全面的视觉问答任务（如场景描述、交通规则理解、三维定位、反事实推理、决策制定和规划）来考验模型在复杂三维场景中的真正情境意识。大量的实验结果表明，我们的提出的架构有效，并强调了在复杂三维环境中进行推理和规划时，视觉问答任务的重要性。**|
|**2024-05-02**|**CACTUS: Chemistry Agent Connecting Tool-Usage to Science**|Andrew D. McNaughton et.al.|[2405.00972](http://arxiv.org/abs/2405.00972)|**[link](https://github.com/pnnl/cactus)**|**这篇论文介绍了一种名为CACTUS的大型语言模型，它结合了化学信息学工具，旨在提升在化学和分子发现领域的高级推理与问题解决能力。研究者们使用包括Gemma-7b、Falcon-7b、MPT-7b、Llama2-7b和Mistral-7b在内的多款开源大语言模型，对CACTUS进行了广泛的性能评估，通过数千个化学问题的基准测试。结果显示，CACTUS明显优于基础模型，其中Gemma-7b和Mistral-7b无论采用何种提示策略，表现最为出色。论文还探讨了领域特定提示和硬件配置对模型性能的影响，强调了提示工程的重要性，并指出在消费级硬件上部署较小模型可能不会显著牺牲准确性。  CACTUS通过融合开源大语言模型的认知功能与专业工具，能够协助研究人员进行分子性质预测、相似性搜索和药物适用性评估等任务。作为化学信息学领域的重大突破，CACTUS为化学家和分子探索者提供了一个灵活的工具，有望加速科学研究，推动新型有效、安全药物、催化剂和材料的发现。此外，CACTUS与自动化实验平台的集成以及实时数据驱动决策的能力，为自主发现开辟了新的可能。**|
|**2024-04-29**|**Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs**|Bahar Radmehr et.al.|[2404.18978](http://arxiv.org/abs/2404.18978)|null|随着教育环境中对学习者模型日益增长的兴趣，研究重点逐渐转向如何通过强化学习（RL）与大型语言模型（LLMs）相结合，提升在开放性文本学习环境中的通用能力。本文探讨了三种类型的代理：（1）基于RL的代理，使用自然语言表示状态和行动策略以寻找最佳互动方式；（2）基于LLM的代理，利用模型的广泛知识和推理能力通过提示进行操作；（3）混合LLM辅助RL的代理，旨在提高性能和泛化能力。为了支持这些代理的发展和评估，我们提出了PharmaSimText，这是一个源自PharmaSim虚拟药店环境的新基准，专注于诊断对话实践。实验结果显示，RL基础的代理在任务完成方面表现优秀，但在提问质量上有所欠缺；而LLM基础的代理在提问能力上较强，但任务完成度不高。最后，混合LLM辅助RL的代理展示了克服这些局限性的潜力，证实了RL与LLMs结合用于开发开放性学习环境高表现代理的可能性。|
|**2024-04-27**|**CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments**|Kaixuan Huang et.al.|[2404.18021](http://arxiv.org/abs/2404.18021)|null|随着基因组工程技术的兴起，精确修改遗传信息已成为可能，但高效基因编辑系统的构建需要深入理解CRISPR技术及其复杂实验背景。大型语言模型（LLMs）在诸多任务中展现出潜力，但在生物设计问题上往往缺乏特定知识。本文介绍CRISPR-GPT，一个增强型LLM代理，它结合了领域知识和外部工具，以自动化并提升基于CRISPR的基因编辑实验设计过程。CRISPR-GPT利用LLMs的推理能力，协助选择CRISPR系统、设计引导RNA、推荐细胞递送方法、起草协议以及设计验证实验以确认编辑结果。我们展示了CRISPR-GPT如何帮助非专家研究人员从头开始进行基因编辑实验，并通过实际案例验证其有效性。同时，我们探讨了自动化基因编辑设计的伦理和监管问题，强调了负责任和透明使用此类工具的重要性。我们的工作目标是弥合初级生物研究者与CRISPR基因组工程技术之间的鸿沟，展示LLM代理在促进复杂生物发现任务中的潜力。|
|**2024-04-27**|**Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs**|Zhenlan Ji et.al.|[2404.17833](http://arxiv.org/abs/2404.17833)|null|随着大型语言模型（LLMs）驱动的代理在各种商业应用中，特别是在心理健康支持、化学合成和软件开发等领域展现效用，人们发现这些代理在处理复杂任务和长期规划时容易产生错误。为此，本文提出了一种新颖的自动化方法——PDoctor，旨在检测和理解LLM代理的错误规划。PDoctor首先定义了一个领域特定的语言（DSL），用于用户查询，并借助Z3约束求解器生成各种输入，这些输入是描述一系列任务完成需求的自然语言段落。然后，PDoctor从这些需求中提取约束，形成一个测试基准。我们使用三个主流的代理框架和两个强大的LLMs（GPT-3.5和GPT-4）对PDoctor进行了评估，结果显示它能有效识别代理规划中的各种错误，并为开发者和用户提供了有价值的见解和错误特性。最后，我们讨论了可能的替代设计和扩展PDoctor的方向。|
|**2024-04-26**|**PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games**|Qinglin Zhu et.al.|[2404.17662](http://arxiv.org/abs/2404.17662)|**[link](https://github.com/alickzhu/player)**|**随着大型语言模型（LLMs）的最新进展，增强了代理间的通信和社会交互能力。然而，在涉及竞争与合作的动态环境中，利用这些模型进行复杂推理的构建仍然面临挑战，尤其是因为基于信息图的搜索方法存在局限性。为此，我们提出PLAYER*，这是一个基于任意采样式规划器的新框架，它结合了传感器和剪枝技术，构建了一个完全依赖于问题驱动的搜索框架，适用于高难度的推理任务。我们还引入了一种可量化的评估方法，通过多项选择题来测试，并创建了WellPlay数据集，包含1,482个问答对。实验结果表明，PLAYER*在复杂动态环境中的效率和性能优于现有方法，并提供了可量化的对比结果。**|
|**2024-04-24**|**Autonomous LLM-driven research from data to human-verifiable research papers**|Tal Ifargan et.al.|[2404.17605](http://arxiv.org/abs/2404.17605)|**[link](https://github.com/technion-kishony-lab/data-to-paper)**|**随着人工智能推动科学发现的步伐加快，人们还不清楚完全由AI驱动的研究是否可行，以及它能否遵循关键的科学价值观，如透明度、可追溯性和可验证性。为了模拟人类的科学研究实践，我们构建了“数据到论文”（data-to-paper），这是一个自动化平台，引导相互协作的人工智能代理通过完整的分步骤研究流程，同时程序化追踪信息流，并允许人类监督和互动。在自动模式下，仅提供标注数据，该平台就能提出假设，设计研究计划，编写和调试分析代码，生成和解读结果，甚至创建完整且信息可追溯的科研论文。尽管研究新颖性有限，但这一过程展示了AI自主从数据中生成原创定量洞察的能力。对于简单的研究目标，全自动流程能创作出大约80-90%无需重大错误的稿件，然而随着目标复杂性的增加，人类的共同参与对于保证准确性至关重要。此外，生成的论文本身也具有内在的可验证性，因为信息追踪使得结果、方法和数据的链接可以程序化进行。因此，我们的工作表明，AI驱动的科研可以加速科学发现，同时增强而非威胁透明度、可追溯性和可验证性。**|
|**2024-04-11**|**The Future of Scientific Publishing: Automated Article Generation**|Jeremy R. Harper et.al.|[2404.17586](http://arxiv.org/abs/2404.17586)|null|这项研究介绍了一种创新的软件工具，它利用大型语言模型（LLM）提示，实现了从Python代码自动生成学术文章，这对于生物医学信息学和计算机科学领域具有重要意义。选择Python作为基础示例，因其广泛使用和强大的数据分析能力。该方法和框架的灵活性使得其适用于多种GitHub仓库，表明了工具的广泛应用潜力（Harper，2024年）。通过简化传统上耗时的学术写作过程，特别是在整合复杂数据集和代码输出方面，这一突破性进展推动了科研成果的快速传播。开发过程中并未依赖高级语言模型，确保了自动化生成内容的连贯性和完整性。此次探索不仅验证了软件的成功应用和效率，还预示了未来可能集成更先进的LLM，将进一步增强其功能，引领一个科研发现发布更加迅速和易获取的时代。|
|**2024-05-09**|**Large Language Model Agent as a Mechanical Designer**|Yayati Jadhav et.al.|[2404.17525](http://arxiv.org/abs/2404.17525)|null|传统的机械设计方法依赖于专家通过经验引导的修改和有限元分析（FEA）来满足特定需求，但这个过程耗时且高度依赖个人知识。尽管已经开发了许多机器学习模型来简化繁琐的专家驱动迭代过程，但它们通常需要大量训练数据和计算资源。深度学习方法往往局限于其训练领域和任务，限制了跨任务应用。这在自动化效率与资源需求之间形成了权衡。  本研究提出了一种新颖的方法，即将预训练的语言模型（LLMs）与有限元模块结合。有限元模块评估每个设计并提供关键反馈，引导LLMs不断学习、规划、生成和优化设计，无需针对特定领域进行专门训练。我们通过在桁架结构的迭代优化中展示这种框架的有效性，证明它能够根据结构化的反馈和标准调整设计。结果显示，基于LLM的代理成功生成符合自然语言描述的桁架结构设计，成功率高达90%，这取决于所施加的约束条件。通过提示式优化技术，我们展示了LLM代理在接收到解-得分对后，能够根据其内在推理能力迭代优化设计以满足规格要求。  LLM代理能够产生可行的设计并根据其固有的推理能力进行优化，这表明它们有潜力自主发展和实施有效的设计策略。|
|**2024-04-26**|**Ruffle&Riley: Insights from Designing and Evaluating a Large Language Model-Based Conversational Tutoring System**|Robin Schmucker et.al.|[2404.17460](http://arxiv.org/abs/2404.17460)|null|本文讨论并评估了一种新型的对话式辅导系统（Conversational Tutoring Systems，CTS），该系统利用大型语言模型（Large Language Models，LLMs）的最新进展。首先，系统通过自动从课程文本中生成易于编辑的教学脚本，实现AI辅助的内容创作。其次，系统通过两个基于LLM的代理（Ruffle和Riley）以学习教学模式运行，分别扮演学生和教授角色，进行自由形式的对话，遵循典型的人工智能辅导系统的内环和外环结构。我们在两个在线用户研究（N=200）中对比了该系统与简单的问答聊天机器人和阅读活动在支持生物学课程的效果。研究分析了系统使用模式、预后测试成绩以及用户体验调查，结果显示用户对Ruffle&Riley的参与度高，理解力强，并认为提供的支持有帮助。尽管Ruffle&Riley用户的完成时间较长，但在短期学习成效上并未发现显著差异，优于阅读活动。我们的系统架构和用户研究为未来CTS设计者提供了有价值的信息。此外，我们开源我们的系统，以促进基于LLM的学习技术有效教学设计的研究。|
|**2024-04-26**|**A Unified Debugging Approach via LLM-Based Multi-Agent Synergy**|Cheryl Lee et.al.|[2404.17153](http://arxiv.org/abs/2404.17153)|**[link](https://github.com/acceptepapier/unidebugger)**|在软件调试这个耗时的过程中，人们一直在努力实现自动化，包括故障定位和修复生成。近年来，大型语言模型（LLMs）在自动化调试方面展现出巨大潜力。然而，我们发现了传统和基于LLM的调试工具面临三大挑战：1）上游的故障定位不准确会波及下游的修复；2）处理复杂逻辑错误的能力不足；3）忽视程序上下文。针对这些问题，我们提出了首个自动化的、统一的调试框架——FixAgent，通过LLM代理协同。FixAgent能执行端到端的故障定位、修复和分析。  我们的关键洞察是，LLMs能够从人类开发者认可的通用软件工程原则中获益，比如“橡皮鸭调试”，这有助于更好地理解程序功能和逻辑错误。为此，我们设计了三个灵感来源于“橡皮鸭”的解决方案：代理专业化与协同、关键变量跟踪和程序上下文理解，促使LLMs提供明确的解释，并聚焦于关键的程序逻辑信息。在广泛使用的QuixBugs数据集上，FixAgent成功修复了80个bug中的79个，其中9个是之前未解决的。它还在CodeFlaws上合理地修复了1.9倍于最佳修复工具的缺陷，而且无需位置信息，采样率低于0.6%。平均而言，与使用不同LLM的基线模型相比，FixAgent提高了约20%的合理修复和正确修复率，显示出我们设计的有效性。  此外，FixAgent的正确率高达97.26%，表明它有可能克服现有方法的过拟合问题。总结来说，FixAgent是一个有前景的自动化调试框架，旨在提升软件调试的效率和准确性。|
|**2024-04-25**|**Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents**|Giorgio Piatti et.al.|[2404.16698](http://arxiv.org/abs/2404.16698)|**[link](https://github.com/giorgiopiatti/govsim)**|在快速发展的人工智能领域，确保大型语言模型（LLMs）的决策安全是一项重大挑战。本文提出了一种名为“Governance of the Commons Simulation”（GovSim）的模拟平台，旨在研究LLMs中的战略互动和合作决策。通过这个环境，我们探讨了AI代理之间资源分享的动态，强调了伦理考量、战略规划和谈判技巧的重要性。GovSim具有灵活性，支持文本型代理，包括LLMs。利用生成式代理框架，我们创建了一个通用代理，便于整合不同的LLMs。我们的研究发现，在GovSim中，只有15个测试模型中的2个能够实现可持续结果，这表明模型在管理共享资源的能力上存在显著差距。进一步的研究显示，如果移除代理之间的通信能力，它们会过度使用共享资源，突出了合作中沟通的关键性。有趣的是，大多数LLMs缺乏普遍化的假设能力，揭示了它们推理技能的一个重要弱点。我们开源了所有研究结果，包括模拟环境、代理提示以及全面的网络界面，以供进一步研究和讨论。|
|**2024-04-24**|**Online Personalizing White-box LLMs Generation with Neural Bandits**|Zekai Chen et.al.|[2404.16115](http://arxiv.org/abs/2404.16115)|null|随着大型语言模型（LLMs）开始生成个性化的文本内容，如何在不为每位用户创建独特模型的资源消耗下实现高效个性化成了新挑战。本文提出了一种创新的在线方法，利用神经_bandit算法动态优化软指令嵌入，根据用户反馈调整内容，从而提升白盒LLMs开放性文本生成的个性化水平。通过在多个任务上的严谨实验，我们证明了这种方法相对于基础策略有显著性能提升。特别是针对个性化新闻标题生成，NeuralTS带来了高达62.9%的最佳ROUGE分数提升以及2.76%的LLM代理评估分数增长，这表明其效果显著。|
|**2024-04-04**|**Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation**|Mohammadmehdi Ataei et.al.|[2404.16045](http://arxiv.org/abs/2404.16045)|null|## 翻译  在产品开发的关键阶段——需求获取，往往难以全面捕捉用户需求，导致最终产品可能无法满足期望。为此，本文提出了一种新颖的框架，它利用大型语言模型（LLMs）来自动化和增强这一过程。通过生成大量模拟用户（LLM代理），我们可以探索更广泛的用户需求和未预见的使用场景。这些代理通过描述他们的行为、观察和挑战，参与产品体验情景。随后的代理访谈和分析揭示了宝贵的用户需求，包括潜在需求。我们通过三个实验验证了我们的框架：首先，我们探讨了不同方法生成多样化的代理，分析其优缺点，并证明了具有上下文意识的代理生成能带来更大的需求多样性。其次，我们展示了该框架如何有效地模拟富有同情心的领先用户访谈，识别出比传统人类访谈更多的潜在需求。最后，我们展示了如何使用LLMs分析访谈，提取需求并将其分类为潜在或非潜在。我们的研究工作强调了利用LLM代理加速早期产品研发、降低成本和促进创新的潜力。|
|**2024-04-24**|**A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples**|Lihang Pan et.al.|[2404.15974](http://arxiv.org/abs/2404.15974)|null|## 翻译  单个大型语言模型（LLM）在解决复杂任务方面的能力有限。然而，通过连接多个LLM代理构建的网络可以显著提升整体性能。本文介绍了一种人机协作工具——EasyLAN，旨在帮助开发者轻松构建LLM代理网络（LAN）。EasyLAN首先根据任务描述自动生成仅包含一个代理的初始网络。接着，它利用少量训练示例来调整网络。对于每个示例，EasyLAN分析输出与真实结果之间的差距，并找出错误的原因。EasyLAN会采用精心设计的策略来修正这些问题。用户可以介入EasyLAN的工作流程或直接修改LAN。最终，LAN从单个代理发展成多代理的网络。实验结果显示，EasyLAN能够帮助开发者快速构建性能良好的LAN。|
|**2024-04-03**|**Concept-Guided LLM Agents for Human-AI Safety Codesign**|Florian Geissler et.al.|[2404.15317](http://arxiv.org/abs/2404.15317)|null|随着生成人工智能在软件工程，特别是安全工程中的重要性提升，对它的质量要求也随之提高。单纯依赖大型语言模型（LLMs）已不足以满足这些需求。因此，我们提出了一种高效且融合的策略，旨在利用LLMs进行安全分析和人机协同设计，以确保软件系统的安全性。我们开发了一个定制化的LLM代理，结合提示工程、启发式推理和检索增强生成，专注于解决与预定义安全概念相关的任务，并与系统模型图进行交互。决策流程通过一系列微决策进行引导，有助于保持结构化信息。此外，我们还提出了图的口头表述作为系统模型的中间表示，以促进LLM与图的交互。我们通过一个简化自动驾驶系统的示例，展示了选择的提示-响应对，以说明我们的方法如何应用于安全分析。|
|**2024-04-23**|**Aligning LLM Agents by Learning Latent Preference from User Edits**|Ge Gao et.al.|[2404.15269](http://arxiv.org/abs/2404.15269)|**[link](https://github.com/gao-g/prelude)**|**我们研究基于用户对语言模型编辑的互动学习语言代理。在诸如写作助手的常见场景中，用户与语言代理交互，根据上下文生成响应，并可能选择性地编辑代理的响应以反映他们的潜在偏好，同时提高准确性。这种编辑反馈是自然产生的，适合用于提升代理与用户偏好的契合度，降低后续用户的编辑成本。为此，我们提出PRELUDE框架，它根据历史编辑数据推断用户的潜在偏好，并据此设计一个提示策略，引导未来的响应生成，避免了昂贵且难以扩展的微调过程，还能保持在其他任务上的性能。  此外，学习描述性的偏好有助于增强可解释性，用户可以查看和调整学习到的偏好。然而，用户偏好可能复杂多变，受情境影响，因此学习起来具有挑战性。为解决这一问题，我们提出CIPHER算法，它利用大型语言模型（LLM）根据用户编辑推断给定情境下的用户偏好。未来，CIPHER会从历史中的k个最接近的上下文中检索推断出的偏好，综合生成响应。我们在总结和电子邮件写作两个互动环境中使用GPT-4模拟用户进行评估，与直接使用用户编辑但不学习描述性偏好的算法，以及学习全局无上下文偏好的算法进行了比较。  在两项任务中，CIPHER都实现了最低的编辑距离成本，并且学习到的偏好与真实偏好显示出显著的相似性。**|
|**2024-04-22**|**A Survey on Self-Evolution of Large Language Models**|Zhengwei Tao et.al.|[2404.14387](http://arxiv.org/abs/2404.14387)|**[link](https://github.com/alibabaresearch/damo-convai)**|**## 概述  大型语言模型（LLMs）在众多领域和智能代理应用中取得了显著进步。然而，依赖人类或外部模型监督的现有LLMs在处理复杂任务和多样性增加时可能会遇到成本高昂和性能瓶颈的问题。为此，自我进化方法应运而生，这种策略允许LLMs自主获取、精炼并从自身生成的经验中学习，借鉴人类经验学习过程，有望推动LLMs向超级智能发展。本文全面综述了LLMs中的自我进化方法。首先，我们提出一个概念框架，将进化过程划分为迭代循环的四个阶段：经验获取、经验细化、更新和评估。其次，我们分类探讨LLMs和基于LLM的代理的进化目标，并对相关文献进行总结，提供每个模块的分类和见解。最后，我们指出了当前的挑战，并提出了未来研究方向，为加速自演进LLMs的发展提供关键洞见。**|
|**2024-04-21**|**A Survey on the Memory Mechanism of Large Language Model based Agents**|Zeyu Zhang et.al.|[2404.13501](http://arxiv.org/abs/2404.13501)|**[link](https://github.com/nuster1128/llm_agent_memory_survey)**|**随着大型语言模型（LLMs）在科研和工业界的广泛关注，基于LLMs的智能代理因其自我进化能力而备受瞩目，这对于解决需要长期复杂交互的现实问题至关重要。支持agent-environment交互的关键要素是代理的记忆机制。尽管已有众多有前景的记忆设计被提出，但这些研究分散在多篇论文中，缺乏全面的综述来系统性地总结和比较，未能提炼出通用且有效的设计模式以启发后续研究。为此，本论文旨在填补这一空白，我们提出一份关于LLM基代理记忆机制的全面调查。首先，我们将探讨记忆在LLM代理中的“是什么”以及“为什么需要”。然后，我们系统回顾了关于记忆模块的设计和评估方法的研究。此外，我们还会展示记忆模块在各种应用中扮演的重要角色。最后，我们会分析现有工作的局限，并指出重要的未来研究方向。为了跟踪该领域最新进展，我们创建了一个GitHub仓库：\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}。**|
|**2024-04-18**|**From Language Models to Practical Self-Improving Computer Agents**|Alex Sheng et.al.|[2404.11964](http://arxiv.org/abs/2404.11964)|null|我们提出了一种简单直接的方法，用于创建能够执行各种计算机任务的人工智能代理，并通过自我改进来发展工具和增强功能，以解决日益复杂的任务。鉴于大型语言模型（LLMs）已显示出从非参数增强中获益，近期的研究大量集中在开发软件，以赋予LLMs各种能力。我们建议，通过适当的提示工程，一个LLM代理可以系统地生成软件来增强自身，而不是依赖人类工程的静态软件开发。  我们通过一些案例研究展示了这一点：仅通过终端访问，我们引导LLM代理添加了检索、互联网搜索、网页导航和文本编辑功能。该代理有效地利用这些工具解决了问题，例如自动化软件开发和基于网络的任务。这种方法表明，通过连续提问和巧妙的提示设计，LLM能够自主扩展其功能，执行实际的计算机任务。|
|**2024-04-25**|**Automated Social Science: Language Models as Scientist and Subjects**|Benjamin S. Manning et.al.|[2404.11794](http://arxiv.org/abs/2404.11794)|null|我们提出了一种方法，利用大型语言模型（LLM）的最新进展，自动构建和测试社会科学假设。这种方法的关键在于使用结构因果模型。结构因果模型提供了一个陈述假设的语言、构建LLM基础代理的蓝图、实验设计以及数据分析计划。拟合后的结构因果模型可供预测或规划后续实验。我们通过几个场景进行了演示：谈判、保释听证会、求职面试和拍卖。在这些情况下，系统既提出了因果关系，也进行了检验，发现了一些证据，而有些则没有。我们证明，从这些社会互动模拟中获取的洞察并非仅通过直接询问LLM就能获得。当给定每个场景的建议结构因果模型时，LLM在预测估计效应的符号方面表现良好，但无法可靠地预测效应的大小。在拍卖实验中，模拟结果与拍卖理论的预测紧密吻合，但LLM直接提取的清算价格预测不准确。然而，如果模型能基于拟合的结构因果模型进行条件化，LLM的预测会大幅改进。简而言之，LLM知道的比它能立即表达的要多。|
|**2024-04-17**|**AgentKit: Flow Engineering with Graphs, not Coding**|Yue Wu et.al.|[2404.11483](http://arxiv.org/abs/2404.11483)|**[link](https://github.com/holmeswww/agentkit)**|**我们提出了一种直观的大型语言模型提示框架（AgentKit），旨在为多功能代理提供统一的方法。AgentKit通过简单的自然语言提示构建复杂的“思维过程”。其基本单元是节点，包含特定子任务的自然语言指令。用户可以像拼接乐高积木一样连接这些节点，从而明确设计出自然结构化的“思考流程”。例如，在撰写论文时，可能的步骤包括：1）确定核心信息，2）识别研究空白等。AgentKit的模块化特性使得高级功能如即兴的层次化规划、反思和从互动中学习变得可能。由于其直观且模拟人类思考过程的设计，即使没有编程经验的人也能创建和调整基础代理。定量实验显示，使用AgentKit设计的代理在WebShop和Crafter任务上实现了最先进的性能。这些成果表明AgentKit有潜力使LLM代理在更广泛的场景下高效且易于使用。相关代码已开源在GitHub：https://github.com/holmeswww/AgentKit。**|
|**2024-04-15**|**Memory Sharing for Large Language Model based Agents**|Hang Gao et.al.|[2404.09982](http://arxiv.org/abs/2404.09982)|**[link](https://github.com/ghupppp/memorysharingllm)**|**在人工智能领域，大型语言模型（LLMs）通过自然语言提示执行任务的能力是一个重大突破，它减少了对固定答案任务（如常识问题和是非查询）的重新训练或微调需求。然而，在处理开放性挑战如诗歌创作时，基于上下文学习的方法显示出局限，主要源于提供的示例全面性以及模型理解问题内容的能力不足，导致输出往往与预期结果大相径庭。针对这一差距，我们的研究提出了Memory-Sharing（MS）框架，这是一种针对LLM多代理的实时记忆存储和检索系统，旨在增强基于上下文的学习过程。每个“记忆”单元记录了提出的查询及其来自LLM代理的即时响应，从多个类似代理中聚合这些记忆，形成所有代理共享的丰富记忆池。MS框架不仅帮助代理找到特定任务的相关示例，还评估其记忆的潜在利用价值，供其他代理未来应用。在三个不同领域的实证验证显示，MS框架显著提高了代理处理开放性问题的表现。此外，我们还讨论了哪种记忆池和检索策略能更好地支持代理，为MS的未来发展提供了方向。代码和数据可在：https://github.com/GHupppp/MemorySharingLLM 获取。**|
|**2024-05-10**|**Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation**|Ruixin Yang et.al.|[2404.09127](http://arxiv.org/abs/2404.09127)|**[link](https://github.com/minnesotanlp/collaborative-calibration)**|**### 背景  当前的大规模语言模型（LLMs）在不确定性估计方面面临挑战，它们通常校准不良且过度自信，特别是在基于人类反馈的强化学习（RLHF）中。人类的决策和信心不仅源于内在信念，还能通过日常观察进行调整，而现有LLM的校准方法主要关注单个模型的信心估计，未能充分利用“集体智慧”：多个LLM之间的协作表达能力，这可以集体提高准确性和校准。本研究中，我们提出了一种无训练后处理的校准策略——协作校准（Collaborative Calibration），它利用多代理工具增强的LLMs在模拟的群体讨论过程中，共同提升校准能力和推理合理性。  ### 任务  我们在生成式问答任务上展示了协作校准的有效性，覆盖了多个领域，证明了它在整合集体校准后的信心评估和提升模型预测可靠性方面的潜力。**|
|**2024-04-13**|**CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting**|Zukang Yang et.al.|[2404.09077](http://arxiv.org/abs/2404.09077)|**[link](https://github.com/zukangy/kgp-curiousllm)**|**在问答（QA）领域，大型语言模型（LLMs）与外部数据库的融合取得了显著成效。然而，这些方法在处理复杂推理任务时往往力有不逮。为此，我们对一种名为知识图谱提示（KGP）的创新方法进行了优化，该方法结合知识图谱和基于LLM的代理以提升推理和搜索精度。然而，原始的KGP框架需要昂贵的大规模数据微调，并且仍存在LLM的错误推断问题。因此，我们提出了一种融入推理能力的LLM代理，它模仿人类的好奇心，通过提问来更有效地导航搜索过程。这个简单的改进显著提高了LLM在QA任务中的性能，同时避免了初始KGP框架的高成本和延迟。我们的目标是进一步发展这种方法，最终实现更精确、更快捷且成本效益更高的QA解决方案。**|
|**2024-04-13**|**Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation**|Jia Gu et.al.|[2404.09043](http://arxiv.org/abs/2404.09043)|null|随着大型语言模型（LLMs）的飞速发展及其在处理复杂语言任务中的出色表现，越来越多的研究尝试利用LLMs模拟人类的行为决策过程，通常这些过程被表示为马尔可夫决策过程（MDPs）。在这个框架中，动作遵循特定的概率分布，并需要迭代采样。这促使我们探究LLM代理理解概率分布的能力，以通过概率采样指导行为决策并生成行为序列。我们将问题分为两个主要方面：一是已知精确概率分布的模拟，二是模糊概率分布的序列生成。  在已知概率分布的情况下，代理需要根据问题描述提供概率分布的类型和参数，然后给出采样序列。然而，我们的研究显示，LLM代理在这方面的性能不佳，但通过编程工具可以一定程度上提高采样成功率。而在实际情境中，概率分布往往不明确。因此，我们在第二部分让代理调整在线社交网络中的活跃度，并分析行动频率。结果表明，即使借助编程工具，LLM代理依然无法有效地采样概率分布。这意味着在直接将LLM作为模拟人类行为的代理应用之前，还需要谨慎对待。|
|**2024-04-12**|**Strategic Interactions between Large Language Models-based Agents in Beauty Contests**|Siting Lu et.al.|[2404.08492](http://arxiv.org/abs/2404.08492)|null|随着大型语言模型（LLMs）的广泛应用，它们在博弈论框架下的游戏行为理解潜力日益显现。本研究聚焦于通过模拟分析不同类型LLM驱动的代理在经典 Beauty Contest 游戏中的策略互动。借鉴人类实验，我们对LLM代理的策略层次进行类似的评估，发现它们展现出从零级到一级的不同程度推理能力，并在重复游戏中表现出行动趋同。此外，我还探讨了不同类型的代理群体构成如何影响战略行为：高比例的固定策略对手能促进LLM代理的收敛，而混合环境中不同相对策略水平的代理共存会加速所有代理的收敛。更智能的代理可能获得更高的平均收益，但这是以较低智能代理的牺牲为代价的。这些结果不仅揭示了在特定情景下模拟代理的结局，还为理解算法之间的战略互动提供了重要启示。|
|**2024-04-17**|**LLM Agents can Autonomously Exploit One-day Vulnerabilities**|Richard Fang et.al.|[2404.08144](http://arxiv.org/abs/2404.08144)|null|随着大语言模型（LLMs）的威力日益增强，其在良性和恶意用途上的应用也日益广泛。研究人员开始关注它们利用网络安全漏洞的能力。近期的研究探讨了LLMs自主破解网站的可能性，但这些研究主要集中在简单的漏洞上。本工作揭示，LLMs能够自主利用现实世界系统中的单日漏洞。我们收集了一组包含15个被CVE描述为“关键严重性”的一天期漏洞数据。当提供CVE描述时，GPT-4模型能成功利用87%的漏洞，相比之下，其他测试模型（如GPT-3.5、开源LLMs和开源漏洞扫描器ZAP和Metasploit）的表现均为0%。然而，我们的GPT-4模型在没有描述的情况下效率大减，仅能利用7%的漏洞。这些发现对大规模部署高能力LLMs提出了质疑。|
|**2024-04-11**|**WESE: Weak Exploration to Strong Exploitation for LLM Agents**|Xu Huang et.al.|[2404.07456](http://arxiv.org/abs/2404.07456)|null|近期，大型语言模型（LLMs）显示出作为智能代理的强大潜力。然而，现有的研究主要集中在通过精心设计的提示工程或任务特定的微调来提升模型的推理或决策能力，忽视了探索与利用的过程。在处理开放世界交互环境中的复杂任务时，这些方法存在局限性。首先，由于缺乏对环境的全局信息，模型倾向于做出贪婪决策，导致解决方案不理想。另一方面，从环境中获取的无关信息不仅引入噪声，还增加了额外的成本。  为此，本文提出了一种新颖的方法——弱探索强化强利用（Weak Exploration to Strong Exploitation，WESE），旨在增强LLM在解决开放世界交互任务中的表现。具体来说，WESE将探索和利用过程解耦，使用成本效益高的“弱”代理执行探索任务，以获取全局知识。随后，我们引入基于知识图谱的策略来存储这些知识，并提取与任务相关的关键信息，从而提升“强”代理在成功率和效率上的性能。我们的方法适用于各种任务，并在四个互动基准测试中显著提高了成功率和效率。|
|**2024-04-10**|**GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications**|Shishir G. Patil et.al.|[2404.06921](http://arxiv.org/abs/2404.06921)|**[link](https://github.com/ShishirPatil/gorilla)**|**随着大型语言模型（LLMs）的发展，它们不再仅仅是对话系统中的信息提供者，而是开始积极参与到与实际应用和服务的互动中。如今，人类在将LLM生成的输出（如代码、函数或操作）投入现实世界执行前，需要验证其正确性和适用性，这带来了挑战，因为代码理解被广泛认为非常困难。本文研究了人类如何能有效与LLMs协作、委派和监督，特别是在未来。我们主张，在许多情况下，对提出的行动进行“事后验证”（在看到输出后确认其正确性）比之前的“事前验证”更为容易。实现这一目标的核心理念是集成直观的撤销功能，并为LLM生成的动作设定损害约束，作为降低相关风险的有效策略。通过这种方式，人类可以撤销LLM输出的影响，或者确信潜在风险是有限的。我们认为这对于实现LLMs与应用和服务在有限的人类监督下交互至关重要。我们描述了开源运行时Gorilla Execution Engine（GoEX）的设计和实现，该运行时用于执行LLM动作，并提出了一些开放的研究问题，旨在推动LLMs与应用之间以最小的人工干预进行交互。GoEX的源代码已发布在https://github.com/ShishirPatil/gorilla/。**|
|**2024-04-09**|**AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents**|Luca Gioacchini et.al.|[2404.06411](http://arxiv.org/abs/2404.06411)|**[link](https://github.com/nec-research/agentquest)**|**随着大型语言模型（LLMs）的进展，人们追求能够解决复杂、多步骤推理任务的LLM代理。然而，现有的基准往往局限且只关注整体任务成功率。为了解决这些问题，我们提出了AgentQuest框架，它具有以下特点：（i）benchmark和评估指标模块化且易于扩展，通过文档齐全、易用的API；（ii）我们提供了两种新的评估指标，能够在解决任务时可靠地追踪LLM代理的进步。我们通过两个示例展示了这些指标的实用性，通过识别常见失败点并优化代理架构，显著提高了性能。我们希望与研究界共同扩展AgentQuest，并已将其开源在https://github.com/nec-research/agentquest。**|
|**2024-04-15**|**AutoCodeRover: Autonomous Program Improvement**|Yuntong Zhang et.al.|[2404.05427](http://arxiv.org/abs/2404.05427)|**[link](https://github.com/nus-apr/auto-code-rover)**|**在过去几十年里，研究人员在自动化软件开发过程中取得了显著进展，尤其是大型语言模型（LLMs）的应用极大地推动了编程辅助的自动化。然而，软件工程并不仅仅是编码，还包括维护（如修复bug）和演化（如添加功能）等程序改进过程。本文提出了一种自动解决GitHub问题的方法，旨在实现程序自主改进。我们的方法称为AutoCodeRover，它结合了LLMs与高级代码搜索能力，最终生成程序修改或补丁。与AI研究者和从业者近期关注的仅文件级别的软件项目不同，我们的工作侧重于程序表示（抽象语法树），利用类/方法的程序结构来增强LLM对问题根本原因的理解，并通过迭代搜索提供上下文。当测试套件可用时，谱系基线故障定位技术进一步精确了上下文。  在SWE-bench-lite，一个包含300个真实GitHub问题的数据集上，AutoCodeRover的解决方案效果提升，解决了约22-23%的问题。对于全量的SWE-bench，包含2294个GitHub问题，AutoCodeRover解决了大约16%的问题，这比最近报道的来自Cognition Labs的AI软件工程师Devin的表现还要高，而且时间消耗与Devin相当。我们相信，我们的工作流程能够推动自主软件工程的发展，未来LLM自动生成的代码可以被自动地进行优化和改进。**|
|**2024-04-08**|**Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models**|Yutao Ouyang et.al.|[2404.05291](http://arxiv.org/abs/2404.05291)|null|我们提出了一种基于大型语言模型（LLM）的系统，旨在提升四足机器人的问题解决能力，使其能够处理超越短期动作的长期任务。对于四足机器人来说，长期任务极具挑战性，因为它们需要对任务的语义有高层理解，并具备广泛的运动和操纵技能以与环境互动。我们的系统构建了一个高层推理层，利用大型语言模型，从任务描述中生成混合离散-连续的计划，作为机器人代码。它包括多个LLM代理：一个用于构思计划的语义规划器、一个参数计算器，用于预测计划中的参数，以及一个代码生成器，将计划转换为可执行的机器人代码。  在低层次，我们采用强化学习来训练一套运动规划和控制技能，以增强四足机器人的灵活性，使其能进行丰富环境交互。我们在难以用单一技能完成的长期任务上测试了我们的系统。模拟实验和真实世界实验表明，它成功地制定了多步骤策略，并展现出非平凡的行为，例如制作工具或向人类寻求帮助。|
|**2024-04-06**|**Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology**|Dyke Ferber et.al.|[2404.04667](http://arxiv.org/abs/2404.04667)|null|多模态人工智能系统有望通过解析各类医学数据提升临床决策。然而，这些模型在各医学领域的效能尚不明朗，每个领域都有其独特挑战。本文提出了一种利用大型语言模型（LLMs）作为核心推理引擎的新型多模态医疗AI方法。此引擎自主协调并部署一系列专门的医疗AI工具，如文本解读、放射学和病理图像分析、基因数据处理、网络搜索以及医疗指南文档检索。我们在一系列临床肿瘤学场景中验证了该系统，这些场景模拟了典型的患者护理流程。结果显示，系统在选择恰当工具（97%）、得出正确结论（93.6%）、提供完整（94%）和有益（89.2%）治疗建议，以及根据指令引用相关文献（82.5%）方面表现出高能力。这表明LLMs能够有效地规划和执行领域特定模型，以获取或合成新信息，从而充当个性化临床助手。此外，这种架构简化了监管合规性，因为每个组件工具可以单独验证和审批。我们相信，这项工作为医疗领域的更先进LLM代理提供了概念验证。|
|**2024-04-05**|**Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents**|Harsh Kohli et.al.|[2404.04237](http://arxiv.org/abs/2404.04237)|null|大型语言模型（LLMs）的快速进步使其在标准基准测试中频频超越人类表现，推动了众多下游应用的发展，如基于LLMs的代理。然而，这些模型在看似简单的任务中意外地表现不佳，这强调了对更全面和多样化的评估框架的需求，以衡量它们的实际能力。为此，我们聚焦于组合性和条件推理——人类认知的基石，并提出GroundCocoa，这是一个与航班预订这一现实问题相连接的词汇丰富的基准。我们的任务是将用户的详细偏好与以多选形式提供的可用航班选项进行匹配。结果显示，包括最先进的GPT-4 Turbo在内的当前最佳模型，在经过高级提示后，准确率仍不超过67%，显示出显著的性能差距。|
|**2024-04-02**|**Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization**|Yoichi Ishibashi et.al.|[2404.02183](http://arxiv.org/abs/2404.02183)|**[link](https://github.com/tsukushiai/self-organized-agent)**|**## 背景  随着大型语言模型（LLM）代理的最新进展，自动化软件开发的未来正逐渐显现。然而，现有的单代理方法在生成和优化大规模、复杂的代码库时面临上下文长度限制的问题。为解决这一挑战，我们提出了一种新颖的多代理框架——自组织多Agent体系（SoA）。SoA是一个可扩展且高效的多代理系统，它允许独立地生成和修改代码组件，并协同构建整个代码库。SoA的一个关键特性是根据问题复杂性自动增加代理，实现动态可扩展性。这样，整体代码量可以根据代理数量无限增长，而每个代理管理的代码量保持恒定。  我们在HumanEval基准上评估了SoA，并发现与单代理系统相比，SoA中的每个代理处理的代码量明显减少，但总体生成的代码量显著增加。此外，SoA在Pass@1准确率方面比强大的单代理基线提高了5%。**|
|**2024-04-02**|**Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game**|Silin Du et.al.|[2404.01602](http://arxiv.org/abs/2404.01602)|**[link](https://github.com/doslim/evaluate-the-opinion-leadership-of-llms)**|**大型语言模型在社交推理游戏中展现出显著的策略行为，但对它们作为意见领袖的重要性关注不足，这对于多Agent和人机交互场景的实际应用至关重要。意见领袖是指在一个社会群体中对他人信念和行为有显著影响的个体。本研究使用“狼人杀”游戏作为模拟平台，探讨语言模型在扮演Sheriff（治安官）角色时的意见领导能力。Sheriff负责总结论点并提出决策建议，因此它代表了意见领袖的一个可信代理。我们构建了一个整合Sheriff角色的框架，并基于意见领袖的关键特性提出了两个评估指标：第一个衡量意见领袖的可靠性，第二个考察其对其他玩家决策的影响。  我们进行了大量实验，评估不同规模的语言模型，并创建了“狼人杀”问题回答数据集（WWQA），以测试和提升模型对游戏规则的理解。此外，还包含了人类参与者进行进一步分析。研究结果表明，“狼人杀”游戏是一个有效评估语言模型意见领导力的试验场，但目前仅有少数语言模型具备这种能力。**|
|**2024-04-15**|**CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs**|Jingzhe Shi et.al.|[2404.01343](http://arxiv.org/abs/2404.01343)|**[link](https://github.com/jingzheshi/chops)**|**随着企业和软件平台越来越多地采用大型语言模型（如GPT-3.5、GPT-4、GLM-3和LLaMa-2）提供聊天辅助或客户服务推理，现有的基于LLM的客户服务模型在与客户资料集成和执行实际操作方面存在局限。它们倾向于强调多样性而非精确性和错误避免，这对于现实世界的客户服务场景并不理想。因此，我们提出了一种名为CHOPS（结合客户资料的聊天助手）的LLM代理，旨在：（1）高效利用现有数据库或系统查询用户信息，或遵循既定指南与系统交互；（2）提供准确合理的响应并执行系统内的必要操作，同时避免有害操作；（3）通过结合小型和大型LLM以实现性能满意且成本合理的推理。  我们开发了一个实用的数据集，称为CPHOS-dataset，它包括一个数据库、指导文件以及来自CPHOS平台的模拟物理奥林匹克组织服务的问答对。CPHOS是一个面向高中教师和学生的在线平台。我们通过使用CPHOS-dataset进行了广泛的实验，验证了CHOPS架构的性能，目标是展示LLM如何提升或替代人工客户服务。关于我们的提案架构和数据集的代码可在此处获取：<https://github.com/JingzheShi/CHOPS>。**|
|**2024-03-31**|**DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model**|Lirui Zhao et.al.|[2404.01342](http://arxiv.org/abs/2404.01342)|**[link](https://github.com/opengvlab/diffagent)**|**文本到图像（T2I）生成模型近年来备受瞩目，在学术研究和实际应用中大放异彩。例如，Civitai平台，一个T2I创新的聚集地，目前汇集了74,492种独特的模型，这带来了选择最合适的模型和参数的艰巨任务，通常需要多次试验。借鉴大型语言模型（LLMs）工具使用研究的思路，我们推出了DiffAgent，这是一个通过API调用来快速筛选准确选项的LLM代理。DiffAgent采用了一种新颖的两阶段训练框架，称为SFTA，使其能够根据人类偏好精确地将T2I API的响应与用户输入对齐。为了训练和评估DiffAgent的能力，我们构建了DABench，这是一个全面的数据库，涵盖了社区中的各种T2I API。实验结果显示，DiffAgent不仅在选择适当的T2I API方面表现出色，还验证了SFTA训练框架的有效性。相关代码已可在https://github.com/OpenGVLab/DiffAgent获取。**|
|**2024-03-31**|**Algorithmic Collusion by Large Language Models**|Sara Fish et.al.|[2404.00806](http://arxiv.org/abs/2404.00806)|null|随着算法定价的兴起，人们担忧算法间的合谋问题。我们通过实验使用基于大型语言模型（LLMs）的定价代理，特别是GPT-4，进行了探究。研究发现：(1) LLM驱动的定价机制在定价任务上表现出色；(2) 在寡头竞争环境中，LLM定价代理会自发地进行合谋，从而损害消费者利益；(3) 对LLM指令（“提示”）看似微小的变化可能加剧这种合作行为。这些结果同样适用于拍卖场景。我们的研究结果强调了对算法定价进行反垄断监管的必要性，并揭示了针对LLM定价代理特有的监管挑战。|
|**2024-03-31**|**"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents**|Yuki Hou et.al.|[2404.00573](http://arxiv.org/abs/2404.00573)|**[link](https://github.com/tamoharu/Agent-Memory-CHI24)**|在这个研究中，我们提出了一种创新的人类记忆架构，旨在提升基于大型语言模型的对话代理的认知能力。我们的设计使得这些代理能自主检索生成响应所需的必要记忆，从而解决LLMs在时间认知上的局限。我们借鉴了人类的记忆线索召回机制作为触发点，以实现精确且高效的回忆。此外，我们开发了一个数学模型，动态量化记忆巩固过程，考虑了诸如上下文相关性、时间流逝和回忆频率等因素。代理会从用户的交互历史中存储记忆，这些记忆被封装在数据库中，每个记忆都包含了内容和时间关联的语境。这样，通过类似人类识别和回忆过往经历的方式，系统能够战略性地存储记忆，并理解它们对用户在时间线上的重要性。|

<p align=right>(<a href=#updated-on-20250406>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2025-04-03**|**Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models**|Mateusz Pach et.al.|[2504.02821](http://arxiv.org/abs/2504.02821)|null|稀疏自编码器（SAEs）最近被证明可以增强大型语言模型（LLMs）的可解释性和可控性。在这项工作中，我们将SAEs的应用扩展到视觉-语言模型（VLMs），如CLIP，并引入了一个全面的框架来评估视觉表示中的单义性。我们的实验结果表明，针对VLMs训练的SAEs显著提高了单个神经元的单义性，同时表现出与专家定义结构（例如iNaturalist分类法）相一致的分层表示。最值得注意的是，我们展示了通过SAEs对CLIP视觉编码器进行干预可以直接引导多模态LLM（例如LLaVA）的输出，而无需对底层模型进行任何修改。这些发现强调了SAEs作为一种无监督方法在增强VLMs的可解释性和可控性方面的实用性和有效性。|
|**2025-04-03**|**Generative Evaluation of Complex Reasoning in Large Language Models**|Haowei Lin et.al.|[2504.02810](http://arxiv.org/abs/2504.02810)|null|随着强大的大型语言模型（LLMs）展现出超人的推理能力，一个关键问题浮现出来：这些模型是否真正具备推理能力，还是仅仅从其广泛的、基于网络抓取的训练数据集中回忆答案？公开发布的基准测试不可避免地在被纳入后续LLM训练集后受到污染，削弱了它们作为忠实评估工具的可靠性。为了解决这一问题，我们引入了KUMO，这是一种专门用于评估LLMs推理能力的生成性评估框架。KUMO协同结合了LLMs与符号引擎，能够动态生成多样化的多轮推理任务，这些任务部分可观察且难度可调节。通过自动化流程，KUMO持续生成新颖的任务，覆盖开放领域，迫使模型展示真正的泛化能力而非记忆能力。我们在由KUMO创建的涵盖100个领域的5000项任务上评估了23种最先进的LLMs，并将其推理能力与大学生的表现进行对比。研究发现，许多LLMs在简单的推理任务中已超越大学水平的表现，而经过推理扩展的LLMs在复杂的推理挑战中达到了大学水平的表现。此外，LLMs在KUMO任务上的表现与新发布的现实世界推理基准测试结果高度相关，这凸显了KUMO作为可靠持久的评估工具的价值，用以衡量真实的LLM推理能力。|
|**2025-04-03**|**MegaMath: Pushing the Limits of Open Math Corpora**|Fan Zhou et.al.|[2504.02807](http://arxiv.org/abs/2504.02807)|null|数学推理是人类智能的核心，也是大型语言模型（LLMs）高级能力的关键基准。然而，研究社区仍然缺乏一个专门针对数学为中心的LLMs预训练需求的开源、大规模、高质量语料库。我们提出了MegaMath，这是一个通过以下方法从多样化的数学关注来源精心策划的开放数据集：（1）重新审视网络数据：我们从Common Crawl中重新提取了数学文档，并进行了数学导向的HTML优化、基于fasttext的过滤和去重，以获取更高品质的网络数据。（2）回忆数学相关代码数据：我们从大型代码训练语料库Stack-V2中识别出高品质的数学相关代码，进一步增强了数据多样性。（3）探索合成数据：我们从网络数据或代码数据中合成了问答风格文本、数学相关代码以及交错的文本-代码块。通过整合这些策略并通过广泛的消融验证其有效性，MegaMath提供了3710亿个标记，在现有开源数学预训练数据集中数量最多且质量最高。|
|**2025-04-03**|**A Survey of Large Language Models in Mental Health Disorder Detection on Social Media**|Zhuohan Ge et.al.|[2504.02800](http://arxiv.org/abs/2504.02800)|null|心理健康问题的检测和干预是全球重要的研究方向，社交媒体数据已被视为心理健康研究的重要资源。然而，如何利用大型语言模型（LLMs）进行社交媒体上的心理健康问题检测仍面临重大挑战。因此，本文旨在探索LLMs在社交媒体数据分析中的应用潜力，不仅关注常见的心理障碍如抑郁症和焦虑症，还涵盖了精神病性障碍和外化障碍，并从文本数据分析、心理障碍检测等不同维度总结了LLMs的应用方法，揭示了当前研究的主要挑战和不足。此外，本文概述了常用的公开数据集及评估指标。本综述为心理健康领域的研究人员提供了全面的参考框架，同时展示了LLMs在心理健康检测中的巨大潜力，以促进其在未来心理健康干预中的进一步应用。|
|**2025-04-03**|**A Framework for Robust Cognitive Evaluation of LLMs**|Karin de Langis et.al.|[2504.02789](http://arxiv.org/abs/2504.02789)|null|大型语言模型（LLMs）的涌现认知能力已被广泛观察到 但其本质和潜在机制仍未被充分理解。越来越多的研究借鉴认知科学来研究LLMs的认知 但尚未建立标准的方法论和实验流程。为了解决这一问题 我们开发了CognitivEval 这是一个系统评估LLMs人工认知能力的框架 特别强调响应收集的鲁棒性。CognitivEval的关键特性包括：(i) 自动提示排列 和 (ii) 测试同时获取生成结果和模型概率估计。我们的实验表明 这些特性可以带来更稳健的实验结果。使用CognitivEval 我们复制了五个经典的认知科学实验 展示了该框架在各种实验任务中的通用性 并获得了几种最先进的LLMs的认知特征。CognitivEval将公开发布 以促进认知科学界更广泛的协作。|
|**2025-04-03**|**From Consumption to Collaboration: Measuring Interaction Patterns to Augment Human Cognition in Open-Ended Tasks**|Joshua Holstein et.al.|[2504.02780](http://arxiv.org/abs/2504.02780)|null|生成式人工智能尤其是大型语言模型的兴起正在从根本上改变知识工作的认知过程，引发了对其在人类推理和问题解决能力方面影响的关键性思考。随着这些AI系统越来越多地融入工作流程，它们提供了前所未有的机会来增强人类思维，同时通过被动地接受生成的答案而带来认知退化的风险。这种紧张关系在开放性任务中尤为明显，在这些任务中，有效的解决方案需要深入的情境化和领域知识的整合。与具有明确指标的结构化任务不同，衡量人类与LLM在开放性任务中的互动质量面临重大挑战，因为缺乏客观标准且解决方案的发展是迭代式的。为了解决这一问题，我们提出了一种框架，从两个维度分析交互模式：认知活动模式（探索 vs 利用）和认知参与模式（建设性 vs 有害）。该框架提供系统的测量方法，以评估何时LLM成为人类思维的有效工具而非人类认知的替代品，从而推动理论理解和实践指导，开发能够保护并增强人类认知能力的AI系统。|
|**2025-04-03**|**BT-ACTION: A Test-Driven Approach for Modular Understanding of User Instruction Leveraging Behaviour Trees and LLMs**|Alexander Leszczynski et.al.|[2504.02779](http://arxiv.org/abs/2504.02779)|null|自然语言指令通常抽象且复杂，要求机器人执行多个子任务，即使是看似简单的查询也是如此。例如，当用户要求机器人准备牛油果吐司时，该任务涉及多个顺序步骤。此外，此类指令可能含糊不清或对机器人不可行，也可能超出机器人的现有知识范围。虽然大型语言模型（LLMs）提供了强大的语言推理能力以应对这些挑战，但如何有效将其集成到机器人系统中仍然是一个关键问题。为了解决这一问题，我们提出了BT-ACTION，这是一种基于测试驱动的方法，结合了行为树（BT）的模块化结构与LLMs来生成机器人遵循复杂用户指令的一致动作序列，具体是在厨房辅助场景中准备食谱的情境下。我们在一项包含45名参与者的综合用户研究中评估了BT-ACTION，并将其性能与直接LLM提示进行了比较。结果显示，BT-ACTION的模块化设计帮助机器人减少了错误并提高了用户的信任度，参与者明显更倾向于使用BT-ACTION的机器人。代码已公开发布于https://github.com/1Eggbert7/BT_LLM。|
|**2025-04-03**|**How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?**|Andres Algaba et.al.|[2504.02767](http://arxiv.org/abs/2504.02767)|null|科学知识的传播依赖于研究人员如何发现并引用先前的工作。大型语言模型（LLMs）在科学研究过程中的采用引入了新的引用实践层面。然而，LLMs在多大程度上与人类的引用实践一致、它们在不同领域的表现如何以及可能对引用动态产生的影响仍然不清楚。在这里，我们展示了LLMs系统性地强化了引用中的马太效应，即始终倾向于推荐高被引论文。这种模式在不同科学领域中持续存在，尽管各领域在存在率（指生成的参考文献与外部文献计量数据库中的现有记录匹配的比例）方面存在显著差异。分析由GPT-4o为10,000篇论文生成的274,951条参考文献后，我们发现LLM推荐的参考文献偏离传统的引用模式，更偏好较新的、标题较短且作者较少的参考文献。这些生成的参考文献在其内容层面上与每篇论文的内容保持了相当高的语义一致性，并表现出与真实引用相似的网络效应，同时减少了作者自引。这些发现表明LLMs可能会重塑引用实践并影响科学发现的进程，通过反映和放大已有的趋势来塑造科学界如何发现和扩展先前的工作。随着LLMs在科学研究中的整合加深，理解它们在塑造科学共同体发现和构建已有工作的方式中所起的作用至关重要。|
|**2025-04-03**|**Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study**|Aryan Agrawal et.al.|[2504.02733](http://arxiv.org/abs/2504.02733)|null|大型语言模型（LLMs）对输入扰动非常敏感，即使是很小的提示变化也可能导致完全不同的输出。现有增强LLM鲁棒性的方法主要集中在被扰动的数据样本上，而提高对任务级指令扰动的鲁棒性则相对未被充分探索。在这项工作中，我们关注于任务特定指令的字符级和词级编辑，这些编辑会显著降低下游性能。我们尝试了多种技术来增强LLMs的鲁棒性，包括自去噪和表示对齐，并在不同的模型（Llama 3和Flan-T5）、数据集（CoLA、QNLI、SST-2）以及指令（任务导向型和角色导向型）上进行测试。我们发现，平均而言，无论是由冻结的LLM还是微调模型执行的自去噪，在性能提升方面都显著优于替代策略，包括更复杂的基线方法如集成和监督方法。|
|**2025-04-03**|**Why do LLMs attend to the first token?**|Federico Barbero et.al.|[2504.02732](http://arxiv.org/abs/2504.02732)|null|大型语言模型（LLMs）倾向于对序列中的第一个标记给予大量关注，这种现象被称为注意力汇点。许多研究详细探讨了这一现象，并提出了多种利用或缓解该现象的方法。注意力汇点与量化困难、安全问题以及流式注意力等问题相关联。然而，尽管许多研究已经提供了它们出现或不出现的条件，但一个关键问题仍未得到充分解答：为什么LLMs会学习到这样的模式以及这些模式是如何被使用的？在这项工作中，我们从理论上和实证上论证，这种机制为LLMs提供了一种避免过度混合的方法，并将其与现有研究数学上如何在Transformer中传播信息的领域联系起来。我们通过实验验证了我们的理论直觉，并展示了诸如上下文长度、深度和数据打包等选择如何影响汇点行为。我们希望这项研究能为理解LLMs中形成的注意力模式提供一个新的实用视角|
|**2025-04-02**|**Towards Unified Referring Expression Segmentation Across Omni-Level Visual Target Granularities**|Jing Liu et.al.|[2504.01954](http://arxiv.org/abs/2504.01954)|null|指代表达分割（RES）旨在分割与描述性语言表达相匹配的实体掩码。尽管传统RES方法主要关注对象级定位，但现实场景需要更灵活的框架来处理多种目标粒度，如多对象、单对象或部分级参考。由于用户描述目标的方式多样且微妙，这带来了巨大挑战。然而，现有的数据集和模型主要集中在设计对象级目标定位的专业化模型，缺乏必要的数据资源和统一框架以应对更实用的多粒度RES任务。在本文中，我们朝着视觉粒度统一的RES任务迈出了重要一步。为克服数据稀缺性的限制，我们引入了一个新的多粒度指代表达分割（MRES）任务以及RefCOCOm基准数据集，该数据集包含部分级别注释以推动更细粒度的视觉理解。此外，我们创建了MRES-32M，这是最大的视觉接地数据集，包含超过3220万个掩码和100万张图像的描述，专门用于部分级别视觉-语言接地。为了解决多粒度RES的挑战，我们提出了UniRES++，这是一种统一的多模态大型语言模型，整合了对象级和部分级RES任务。UniRES++针对细粒度视觉特征探索进行了针对性设计。通过联合模型架构和参数，UniRES++在多个基准测试中实现了最先进的性能，包括用于MRES的RefCOCOm、用于广义RES的gRefCOCO以及用于经典RES的RefCOCO、RefCOCO+、RefCOCOg。为了促进未来多粒度视觉接地的研究，我们的RefCOCOm基准数据集、MRES-32M数据集和模型UniRES++将在https://github.com/Rubics-Xuan/MRES公开发布|
|**2025-04-02**|**The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data**|Massimiliano Luca et.al.|[2504.01951](http://arxiv.org/abs/2504.01951)|null|随着大型语言模型在跨领域的广泛应用，评估训练数据中存在的统计相关性在多大程度上隐藏了细微且可能令人不安的偏见变得至关重要。性别偏见在大型语言模型中的研究主要集中在工作、爱好和情感等与特定性别相关的方面。在这项研究中，我们引入了一个新的视角：我们调查大型语言模型是否可以根据用户的在线购物历史预测其性别，并分析这些预测是否受到性别偏见和刻板印象的影响。我们使用来自美国用户的历史在线购买数据集，评估六种大型语言模型对性别的分类能力，并分析它们的推理过程以及产品与性别的共现情况。结果表明，尽管模型可以以中等准确度推断性别，但它们的决策往往基于产品类别与性别的刻板印象关联。此外，明确指令要求避免偏见虽然降低了模型预测的确定性，但未能消除刻板印象模式。我们的研究结果强调了大型语言模型中性别偏见的持久性，并突显了制定强有力的偏见缓解策略的必要性。|
|**2025-04-02**|**OpenCodeReasoning: Advancing Data Distillation for Competitive Coding**|Wasi Uddin Ahmad et.al.|[2504.01943](http://arxiv.org/abs/2504.01943)|null|自基于推理的大语言模型问世以来，许多人通过将推理能力提炼到学生模型中取得了显著进展。这些技术在编码任务上显著缩小了推理与标准LLM之间的差距。尽管如此，许多关于提炼推理模型的进展仍然被锁在专有数据集之后，或者缺乏关于数据整理、过滤和后续训练的详细信息。为了解决这一问题，我们构建了一个卓越的监督微调（SFT）数据集，用于实现各种规模模型的最先进的编码能力结果。我们的蒸馏模型仅使用SFT就达到了LiveCodeBench上的61.8%和CodeContests上的24.6%，超过了使用强化学习训练的替代方案。然后我们分析了用于构建数据集的数据来源、代码执行过滤的影响以及指令/解决方案多样性的重要性。我们观察到执行过滤对基准准确性产生了负面影响，因此我们优先考虑指令多样性而非解决方案正确性。最后，我们还分析了这些模型的令牌效率和推理模式。我们将开源这些数据集和蒸馏模型给社区。|
|**2025-04-02**|**Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning Length?**|Celine Lee et.al.|[2504.01935](http://arxiv.org/abs/2504.01935)|null|大语言模型（LLMs）在推理时通常受益于显式推理步骤，但尚不清楚这些额外的推理步骤解决了任务难度的哪些方面。为了解决这个问题，我们使用确定性有限自动机（DFA）构建了一个框架。DFA 提供了一种形式化方法，可以通过可测量的属性（如运行长度和状态空间大小）来表征任务复杂性。我们首先表明，在不同任务和不同规模及训练范式的模型之间，存在一个最优的推理步数，使得生成正确解的概率最大化。然后我们研究了哪些复杂性属性决定了这个关键步数：我们发现，任务实例对应的 DFA 运行长度越长（即需要更多的潜在状态跟踪），推理长度也越长，但令人惊讶的是，DFA 的规模（即状态空间复杂度）并不相关。然后我们展示了这些发现的一个含义：能够预测新问题的最优推理步数并过滤掉非最优长度的答案可以带来一致的准确性提升。|
|**2025-04-02**|**A thorough benchmark of automatic text classification: From traditional approaches to large language models**|Washington Cunha et.al.|[2504.01930](http://arxiv.org/abs/2504.01930)|null|自动文本分类（ATC）在过去十年间取得了显著进展，这主要得益于最近的小型和大型语言模型（SLMs 和 LLMs），这些模型利用了Transformer架构。尽管最近的方法在有效性上有了显著提升，但在文献中仍缺乏对这些新方法与传统文本分类方法（如SVM和逻辑回归）的成本效益进行全面分析的研究。本研究的贡献是双重的：(i) 我们提供了十二种传统和近期ATC解决方案的科学且公正的成本效益比较分析，其中包括五种开源LLMs；(ii) 一个包含22个数据集的大规模基准测试，其训练-验证-测试划分基于折叠交叉验证程序，并附带文档和代码。代码、数据和文档的发布使社区能够复制实验并以更科学的方式推动领域发展。我们的比较实验证明，LLMs在有效性方面优于传统方法（平均高出26%-7.1%）和SLMs（平均高出4.9%-1.9%）。然而，由于微调的原因，LLMs的计算成本显著更高，平均比传统方法慢590倍，比SLMs慢8.5倍。结果表明以下建议：(1) 对于需要最佳效果且能承担相应成本的应用，选择LLMs；(2) 对于资源受限或无法负担大模型微调成本的应用，选择逻辑回归和SVM等传统方法；(3) 对于接近最优的有效性-效率权衡，选择像Roberta这样的SLMs。|
|**2025-04-02**|**Gen-C: Populating Virtual Worlds with Generative Crowds**|Andreas Panayiotou et.al.|[2504.01924](http://arxiv.org/abs/2504.01924)|null|在过去二十年里，研究人员在模拟人类群体方面取得了显著进展，但这些努力大多集中在低层次任务如碰撞避免以及狭窄的行为范围如路径跟随和集群行为上。然而，创建引人入胜的群体场景需要的不仅仅是功能性的移动——它还需要捕捉代理之间、代理与环境之间随时间推移的高层次交互。为了解决这个问题，我们引入了Gen-C，这是一种生成模型，用于自动生成高阶群体行为。Gen-C通过利用大型语言模型（LLM）生成一组有限的群体场景，并通过模拟扩展和泛化这些场景来构建时间扩展图，从而建模虚拟代理的动作和交互。我们的方法使用两个变分图自动编码器并由条件先验网络引导：一个专门用于学习图结构的潜在空间（代理交互），另一个用于节点特征（代理动作和导航）。这种设置使得灵活生成动态的群体交互成为可能。训练好的模型可以基于自然语言进行条件设置，使用户能够从文本描述中合成新的群体行为。我们在两个场景中展示了这种方法的有效性，分别是大学校园和火车站，展示了其在多样化虚拟环境中填充具有复杂交互和高级决策模式的代理的潜力|
|**2025-04-03**|**Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation**|Baban Gain et.al.|[2504.01919](http://arxiv.org/abs/2504.01919)|null|大型语言模型（LLMs）的兴起显著改变了机器翻译（MT）的格局，尤其是在缺乏足够平行语料库、语言工具和计算基础设施的低资源语言和领域中。本文综述了利用LLMs进行机器翻译的最新进展。我们分析了诸如少量提示、跨语言迁移和参数高效微调等技术，这些技术能够有效适应资源匮乏的环境。论文还探讨了使用LLMs生成合成数据的策略，包括回译和词典增强。此外，我们比较了基于LLM的翻译与传统的编码器-解码器模型在不同语言对上的表现，强调了各自的优势和局限性。我们讨论了持续存在的挑战，如幻觉、评估不一致性以及继承的偏见，并评估了新兴的LLM驱动的翻译质量指标。本综述提供了实用的见解，并概述了构建强大、包容且可扩展的MT系统的未来方向。|
|**2025-04-02**|**Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning**|Yinggan Xu et.al.|[2504.01911](http://arxiv.org/abs/2504.01911)|null|大型语言模型（LLMs）在物理学研究中的作用正在不断扩大，通过增强推理、符号操作和数值计算来推动科研进展。然而，确保其输出的可靠性和可解释性仍然是一个重大挑战。在我们的框架中，我们将人工智能与人类科学家之间的协作概念化为三个模块间的动态互动：推理模块、解释模块和AI-科学家交互模块。我们认识到，有效的物理推理需要严格的逻辑一致性、量化的精确性以及与现有理论模型的深度融合，因此引入了解释模块以改进对AI生成输出的理解，这是文献中尚未探讨的领域。该模块包含多个专业化代理，包括总结者、模型构建者、用户界面构建者和测试者，它们共同合作，在物理框架内结构化LLM的输出，从而构建更可解释的科学模型。案例研究表明，我们的方法提高了透明度，促进了验证，并加强了AI辅助推理在科学研究中的应用。|
|**2025-04-02**|**TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving Semi-structured Tables**|Abhilash Shankarampeta et.al.|[2504.01879](http://arxiv.org/abs/2504.01879)|null|人类不断取得新的发现，理解导致这些突破的时间序列事件对于推动科学和社会进步至关重要。这种随着时间推理的能力使我们能够识别未来的步骤，并理解金融和政治决策对我们生活的影响。然而，大型语言模型（LLMs）通常是在静态数据集上进行训练的，这限制了它们进行有效时间推理的能力。为了评估LLMs的时间推理能力，我们推出了TRANSIENTTABLES数据集，该数据集包含从超过14,000个表格中衍生出的3,971个问题，涉及跨越多个时间段的1,238个实体。我们引入了一个基于模板的问题生成管道，利用LLMs来优化模板和问题。此外，我们通过最先进的LLMs建立了基线结果以创建一个基准。我们还介绍了新的建模策略，重点在于任务分解，以提高LLMs的表现。|
|**2025-04-02**|**From Code Generation to Software Testing: AI Copilot with Context-Based RAG**|Yuchen Wang et.al.|[2504.01866](http://arxiv.org/abs/2504.01866)|null|大规模软件开发的快速发展对传统测试方法提出了越来越高的要求，导致效率、准确性和覆盖率方面出现瓶颈。我们提出了一种新的视角，将缺陷检测和减少编码错误视为两个相互关联的问题，其共同目标是在有限资源下减少缺陷。我们将之前关于AI辅助编程的工作（支持代码自动完成和聊天机器人驱动的问答）扩展到软件测试领域，引入了Copilot for Testing，这是一种自动化测试系统，通过基于上下文的检索增强生成（RAG）技术同步检测代码库中的缺陷并更新代码，从而增强大型语言模型（LLM）的能力。我们的评估显示，缺陷检测准确性提高了31.2%，关键测试覆盖度增加了12.6%，用户接受率提高了10.5%，这凸显了AI驱动技术在现代软件开发实践中的变革潜力。|
|**2025-03-31**|**Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation**|Shengqiong Wu et.al.|[2503.24379](http://arxiv.org/abs/2503.24379)|null|为了解决当前视频生成社区中用户意图解释不准确的瓶颈问题，我们提出了Any2Caption，这是一种针对任意条件的新型可控视频生成框架。关键思想是将各种条件解释步骤与视频合成步骤解耦。通过利用现代多模态大型语言模型（MLLMs），Any2Caption能够将文本、图像、视频以及诸如区域、运动和相机姿态等专业提示解释为密集且结构化的描述符，从而为底层视频生成器提供更好的指导。我们还引入了Any2CapIns，这是一个包含337K实例和407K条件的大规模数据集，用于任意条件到描述符的指令微调。全面评估表明，我们的系统在可控性和视频质量方面相对于现有视频生成模型有了显著提升。项目页面：https://sqwu.top/Any2Cap/|
|**2025-03-31**|**Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models**|Rui Wang et.al.|[2503.24377](http://arxiv.org/abs/2503.24377)|**[link](https://github.com/devoallen/awesome-reasoning-economy-papers)**|**大型语言模型（LLMs）的最新进展显著提升了其执行复杂推理任务的能力，从快速直观的思考（系统1）过渡到缓慢深入的推理（系统2）。尽管系统2推理提高了任务准确性，但其慢速思考的本质和低效或不必要的推理行为往往导致巨大的计算成本。相比之下，系统1推理计算效率高但性能欠佳。因此，在性能（收益）与计算成本（预算）之间实现平衡变得至关重要，这引出了推理经济性的概念。本文全面分析了LLMs在后训练阶段和测试时间推理阶段的推理经济性，包括推理效率低下的原因、不同推理模式的行为分析以及实现推理经济性的潜在解决方案。通过提供可操作的见解并指出开放性挑战，我们旨在为改进LLMs的推理经济性提供指导，成为该领域研究人员的重要资源。我们还提供了一个公共存储库以持续跟踪这一快速发展领域的进展。**|
|**2025-03-31**|**Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1**|Yi Chen et.al.|[2503.24376](http://arxiv.org/abs/2503.24376)|**[link](https://github.com/tencentarc/seed-bench-r1)**|**近期链式思维（COT）生成的进展显著提升了大型语言模型（LLMs）的推理能力，强化学习（RL）作为一种有效的后训练方法脱颖而出。多模态大型语言模型（MLLMs）继承了这种推理潜力，但在需要感知和逻辑推理的任务中仍缺乏深入研究。为了解决这一问题，我们引入了SEED-Bench-R1基准，该基准旨在系统性地评估多模态大模型在视频理解上的后训练方法。它包括复杂的现实世界视频和日常规划任务，以多项选择题的形式呈现，要求具备复杂的感知和推理能力。SEED-Bench-R1通过三级广义框架进行评估：分布内、跨环境和跨环境任务场景，并配备了大规模且易于验证的地面真值答案数据集。我们以Qwen2-VL-Instruct-7B为基础模型进行了实验，比较了RL与有监督微调（SFT），结果显示RL在分布内和分布外任务上均表现出更高的数据效率和优越性能，甚至在通用视频理解基准如LongVideoBench上也优于SFT。我们的详细分析表明，RL增强了视觉感知能力，但通常会产生逻辑连贯性较差的推理链。我们还识别出一些关键局限性，例如推理不一致和忽略视觉线索等问题，并建议未来改进基础模型推理、奖励建模以及对噪声信号的RL鲁棒性。**|
|**2025-03-31**|**Effectively Controlling Reasoning Models through Thinking Intervention**|Tong Wu et.al.|[2503.24370](http://arxiv.org/abs/2503.24370)|null|增强推理的大语言模型（LLMs）明确生成中间推理步骤，然后生成最终答案，这帮助模型在复杂问题解决方面表现出色。在本文中，我们展示了这一新兴的生成框架为更精细地控制模型行为提供了独特的机会。我们提出了思维干预（Thinking Intervention），这是一种新的范式，旨在通过战略性地插入或修订特定的思维标记来显式引导LLMs的内部推理过程。我们在多个任务上进行了全面评估，包括IFEval上的指令跟随、SEP上的指令层次结构以及XSTest和SORRY-Bench上的安全性对齐。我们的结果表明，思维干预显著优于基线提示方法，在指令跟随场景中实现了高达6.7%的准确率提升，在推理指令层次方面提高了15.4%，在拒绝不安全指令时开放源码DeepSeek R1模型的拒绝率提升了40.0%。总体而言，我们的工作开辟了控制推理型LLMs的一个有前景的新研究方向。|
|**2025-03-31**|**ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion**|Rana Muhammad Shahroz Khan et.al.|[2503.24354](http://arxiv.org/abs/2503.24354)|null|参数生成作为一种新颖的神经网络开发范式，通过直接合成高质量的模型权重，为传统神经网络训练提供了替代方案。在针对不断更新的大规模语言模型（LLMs）进行低秩适应（LoRA）的应用场景中，该方法有望以高效的方式实现模型适配而无需昂贵的重新训练。然而，现有的方法在同时实现可扩展性和可控性方面存在关键限制。在这篇论文中，我们提出了名为`ORAL`的新框架，这是一种基于条件循环扩散的方法。`ORAL`引入了一种创新的条件机制，结合了模型架构和文本任务规范，能够生成针对具体任务的LoRA参数，并能够在不断演化的基础模型之间无缝迁移。我们的方法成功扩展到了数十亿参数量级的语言模型，并保持了良好的可控性。通过在七个语言任务、四个视觉任务和三个多模态任务上使用五种预训练语言模型进行的广泛实验，我们证明了`ORAL`生成的LoRA参数能够达到与常规训练方法相当甚至更优的性能|
|**2025-03-31**|**BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models**|Alok Abhishek et.al.|[2503.24310](http://arxiv.org/abs/2503.24310)|null|在本研究中，我们介绍了BEATS，这是一种用于评估大型语言模型（LLMs）中偏差、伦理、公平性和事实性的新框架。基于BEATS框架，我们提出了一个偏差基准测试，该测试通过29个不同的指标来衡量性能。这些指标涵盖了广泛的特性，包括人口统计学、认知和社会偏差，以及伦理推理、群体公平性和与事实性相关的误导风险等指标。这些指标能够定量评估LLM生成的响应在多大程度上可能延续强化或扩大系统性不平等的社会偏见。为了在这一基准测试中获得高分，LLM必须在其响应中表现出非常公平的行为，这是一项严格的负责任AI评估标准。我们的实验结果表明，37.65%的行业领先模型生成的输出包含某种形式的偏差，这突显了在关键决策系统中使用这些模型的重大风险。BEATS框架和基准提供了一种可扩展且统计严谨的方法来评估LLMs，诊断偏差驱动因素并制定缓解策略。通过BEATS框架，我们的目标是帮助开发更具社会负责性和伦理一致性的AI模型。|
|**2025-03-31**|**A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG**|Arshia Kermani et.al.|[2503.24307](http://arxiv.org/abs/2503.24307)|null|本研究系统比较了三种用于分析心理健康文本的大语言模型（LLMs）方法：提示工程、检索增强生成（RAG）和微调。使用LLaMA 3，我们在两个数据集上评估了这些方法在情感分类和心理健康状况检测任务上的表现。结果显示，微调在准确率上最高（情感分类为91%，心理健康状况检测为80%），但需要大量的计算资源和较大的训练集，而提示工程和RAG提供了更灵活的部署方式，性能适中（准确率为40%-68%）。我们的研究结果为在心理健康应用中实施基于LLM的解决方案提供了实用见解，突出了准确率、计算需求和部署灵活性之间的权衡。|
|**2025-03-31**|**Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning**|Jiacheng Lin et.al.|[2503.24289](http://arxiv.org/abs/2503.24289)|**[link](https://github.com/linjc16/Rec-R1)**|**我们提出了Rec-R1，这是一种通过闭环优化将大型语言模型（LLMs）与推荐系统相结合的通用强化学习框架。与提示和监督微调（SFT）不同，Rec-R1直接优化LLM生成，使用固定黑盒推荐模型的反馈，而不依赖于来自专有模型（如GPT-4o）的合成SFT数据。这种方法避免了数据蒸馏所需的大量成本和努力。为了验证Rec-R1的有效性，我们在两个代表性任务上对其进行了评估：产品搜索和序列推荐。实验结果表明，Rec-R1不仅始终优于基于提示和SFT的方法，而且即使使用简单的检索器（如BM25）也能显著优于强大的判别基线。此外，Rec-R1保留了LLM的通用能力，而SFT往往会影响指令跟随和推理能力。这些发现表明Rec-R1是一种有前景的基础，可用于持续的任务特定适应而不会灾难性遗忘。**|
|**2025-03-31**|**Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality**|Sewoong Lee et.al.|[2503.24277](http://arxiv.org/abs/2503.24277)|**[link](https://github.com/sewoonglee/top-afa-sae)**|**稀疏自编码器（SAEs）已成为现代机械性可解释性的支柱，但现有的基于前 k 个元素激活函数的领先 SAE 方法缺乏对选择超参数 k 的理论支持。SAEs 基于线性表示假设（LRH），该假设认为大型语言模型（LLMs）的表示是线性编码的，并且基于叠加假设（SH），即模型中的特征数量可以超过其维度。我们证明，根据 LRH 和 SH 的正式定义，可以通过一个封闭形式的误差界来近似稀疏特征向量（SAE 学到的密集嵌入的潜在表示）的大小。为了可视化这一点，我们提出了 ZF 图，它揭示了 LLM 隐藏嵌入和 SAE 特征向量之间之前未知的关系，从而实现了对预训练 SAE 特征向量对于给定输入的过激活或欠激活程度的首次经验测量。相应地，我们引入了近似特征激活（AFA），该方法近似于真实稀疏特征向量的大小，并提出了一种新的评估指标，用于评估输入和激活之间的对齐情况。我们还利用 AFA 引入了一种新的 SAE 架构，称为 top-AFA SAE，从而使 SAE 达到以下效果：(a) 更符合理论依据；(b) 不需要调整 SAE 稀疏性超参数。最后，我们实证表明 top-AFA SAE 在重建损失方面与最先进的 top-k SAE 相当，而无需对超参数 k 进行调优。我们的代码可在以下地址获取：https://github.com/SewoongLee/top-afa-sae。**|
|**2025-03-31**|**Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation**|Dun Yuan et.al.|[2503.24245](http://arxiv.org/abs/2503.24245)|null|大型语言模型（LLMs）在通用自然语言处理任务方面取得了显著进展。然而，在像电信这样的领域专用领域中，LLMs仍然面临挑战，这些领域需要专门的知识和对不断发展的标准的适应能力。本文提出了一种结合知识图谱（KG）和检索增强生成（RAG）技术的新框架，以提升LLMs在电信领域的性能。该框架利用知识图谱捕捉网络协议、标准以及其他电信相关实体的结构化、领域特定信息，并全面表示它们之间的关系。通过将知识图谱与RAG相结合，LLMs可以在响应生成过程中动态访问和利用最相关且最新的知识。这种混合方法弥合了结构化知识表示与LLMs生成能力之间的差距，显著提升了准确性、适应性和领域特定理解力。我们的结果表明，KG-RAG框架在解决复杂技术查询方面具有精确性。所提出的KG-RAG模型在常用电信特定数据集的问题回答任务中达到了88%的准确率，而RAG仅用方法为82%，LLM仅用方法为48%。|
|**2025-03-28**|**Q-Insight: Understanding Image Quality via Visual Reinforcement Learning**|Weiqi Li et.al.|[2503.22679](http://arxiv.org/abs/2503.22679)|**[link](https://github.com/lwq20020127/q-insight)**|**图像质量评估（IQA）专注于图像的感知视觉质量，对下游任务如图像重建、压缩和生成起着至关重要的作用。多模态大型语言模型（MLLMs）的快速发展显著拓宽了IQA的范围，使其朝着综合图像质量理解的方向发展，这不仅包括内容分析、退化感知，还包括比较推理，而不仅仅是数值评分。以往基于MLLM的方法通常要么生成缺乏可解释性的数值分数，要么严重依赖于大规模标注数据集的有监督微调（SFT）来提供描述性评估，这限制了它们的灵活性和适用性。在本文中，我们提出了Q-Insight，这是一种基于强化学习的模型，构建于组相对策略优化（GRPO）之上，展示了强大的视觉推理能力，用于图像质量理解，同时只需要有限数量的评分和退化标签。通过精心设计的奖励函数，我们的方法联合优化了评分回归和退化感知任务，有效利用了它们之间的相互促进作用以提升性能。广泛的实验表明，Q-Insight在评分回归和退化感知任务上大幅超越现有的最先进方法，并在零样本迁移至比较推理任务时表现出令人印象深刻的泛化能力。代码将在https://github.com/lwq20020127/Q-Insight发布。**|
|**2025-03-28**|**QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?**|Belinda Z. Li et.al.|[2503.22674](http://arxiv.org/abs/2503.22674)|null|近期大量工作集中在改进大型语言模型（LLMs）在数学和逻辑等推理基准上的表现。然而，过去的研究大多假设任务是定义明确的。在现实世界中，对LLMs的查询往往是未完全指定的，需要通过获取缺失信息来解决。我们将此形式化为约束满足问题（CSP），其中只缺少一个必要的变量赋值。通过这种特殊形式，我们可以严格评估LLMs识别最小必要问题的能力，并量化每个问题的难度水平。我们提出了QuestBench，这是一个由一系列可以通过最多提出一个问题来解决的未完全指定推理任务组成的数据集，其中包括：(1) Logic-Q：带有缺失命题的逻辑推理任务，(2) Planning-Q：部分观察初始状态的部分观测PDDL规划问题，(3) GSM-Q：人工注释的小学数学问题，其中有一个缺失的变量赋值，以及(4) GSME-Q：GSM-Q的一个版本，其中人类注释员将文字题转化为方程。LLM的任务是从一组选项中选择正确的澄清问题。虽然最先进的模型在GSM-Q和GSME-Q上表现出色，但在Logic-Q和Planning-Q上的准确率仅为40%-50%。分析表明，能够解决定义明确的推理问题的能力可能不足以在我们的基准上取得成功：即使模型可以解决完全指定的问题版本，它们在识别正确问题时仍遇到困难。此外，在Planning-Q领域，LLMs往往不会采取谨慎态度，即使明确提供了预测“不确定”的选项。这凸显了深入研究模型信息获取能力的必要性。|
|**2025-03-28**|**Exploring the Effectiveness of Multi-stage Fine-tuning for Cross-encoder Re-rankers**|Francesca Pezzuti et.al.|[2503.22672](http://arxiv.org/abs/2503.22672)|**[link](https://github.com/fpezzuti/multistage-finetuning)**|**点对点的交叉编码器在经过微调后可以非常有效地进行段落重排序。典型的交叉编码器作为重排序器进行微调的过程需要大量的手动标记数据、对比学习目标以及一组启发式采样的负样本。最近的一种替代方法是通过蒸馏目标来训练模型模仿高度有效的大型语言模型的排名。这些微调策略可以单独应用或者顺序应用。在这项工作中，我们系统地研究了点对点交叉编码器在单阶段独立微调或两阶段顺序微调时的有效性。我们的实验表明，使用对比学习对点对点交叉编码器进行微调的效果确实与多阶段方法微调的模型效果相当。代码可供重现使用，详见 https://github.com/fpezzuti/multistage-finetuning。**|
|**2025-03-28**|**Unicorn: Text-Only Data Synthesis for Vision Language Model Training**|Xiaomin Yu et.al.|[2503.22655](http://arxiv.org/abs/2503.22655)|**[link](https://github.com/yu-xm/unicorn)**|**训练视觉语言模型（VLM）通常需要大规模高质量的图像-文本对，但收集或合成此类数据成本高昂。相比之下，文本数据丰富且廉价，因此提出了一个问题：是否可以仅从文本中合成高质量的多模态训练数据？为了解决这个问题，我们提出了一种跨集成的三阶段多模态数据合成框架，生成了两个数据集：Unicorn-1.2M 和 Unicorn-471K-Instruction。在第一阶段：多样化标题数据合成中，通过使用大型语言模型（LLMs）扩展稀疏的标题种子，构建了120万语义上多样化的高质量标题。在第二阶段：指令微调数据生成中，进一步处理了471K个标题以生成多轮指令微调任务，支持复杂推理。最后，在第三阶段：模态表示转移中，这些文本标题表示被转化为视觉表示，从而产生多样化的合成图像表示。这一三阶段过程使我们能够构建用于预训练的Unicorn-1.2M和用于指令微调的Unicorn-471K-Instruction，而无需依赖真实图像。通过消除对真实图像的依赖同时保持数据质量和多样性，我们的框架为VLMs训练提供了一种经济高效且可扩展的解决方案。代码可在https://github.com/Yu-xm/Unicorn.git获取。**|
|**2025-03-28**|**Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users**|Antonia Karamolegkou et.al.|[2503.22610](http://arxiv.org/abs/2503.22610)|null|本文探讨了多模态大型语言模型（MLLMs）作为视觉障碍人士辅助技术的有效性。我们通过用户调查识别了采用模式和用户面临的关键挑战。尽管这些模型的采用率很高，但我们的研究结果揭示了与上下文理解、文化敏感性和复杂场景理解相关的担忧，特别是对于可能完全依赖它们进行视觉解释的个人而言。基于这些结果，我们设计了五个以用户为中心的任务，包括图像和视频输入，其中引入了一项新型任务——光学布拉ille识别。系统评估十二种MLLMs的结果表明，要克服与文化背景、多语言支持、布拉ille阅读理解、辅助物体识别以及幻觉相关的局限性，仍需进一步改进。本研究为多模态人工智能在无障碍领域的未来发展提供了重要见解，强调了开发更具包容性、稳健性和可信度的视觉辅助技术的需求。|
|**2025-03-28**|**On the Alignment of Post-Publication Reviews & Bibliometric and Altmetric Impact -- A Case Study on Expert Statements from the Science Media Center Germany**|Dirk Tunger et.al.|[2503.22594](http://arxiv.org/abs/2503.22594)|null|在学术出版和同行评审的背景下，本研究探讨了发表后专家评估、其一致程度与被评估研究后续科学界及公众认可之间的关系。以德国科学媒体中心的专家声明作为数据集，我们分析了Research in Context评论，考察质化发表后评估与文献计量以及替代计量指标之间的契合度。我们利用大型语言模型将非结构化的专家评论转化为结构化的评分方案。此外，我们将这些评估与Web of Science的引用次数以及来自Altmetric Explorer的替代影响指标（如Altmetric关注度分数、新闻提及和Mendeley读者统计数据）进行相关性分析。我们调查了正面或负面的发表后评审与高或低引用次数或替代计量指标之间的关联。|
|**2025-03-28**|**LLM-enabled Instance Model Generation**|Fengjunjie Pan et.al.|[2503.22587](http://arxiv.org/abs/2503.22587)|null|在基于模型的工程领域，模型是系统设计和分析的重要组成部分。传统上，这些模型的创建是一个需要深厚建模专业知识以及对目标系统丰富领域知识的手动过程。随着生成式人工智能的快速发展，大型语言模型（LLMs）展示了在自动化模型生成方面的潜力。这项工作探索了使用LLMs生成实例模型，特别关注从Ecore元模型和自然语言规范生成XMI实例模型。我们观察到当前的LLMs难以直接生成有效的XMI模型。为了解决这一问题，我们提出了一种两步方法：首先，使用LLMs生成包含所有必要实例模型信息的简化结构化输出，即概念实例模型；然后，将此中间表示编译成有效的XMI文件。概念实例模型是格式无关的，允许通过不同的编译器转换为各种建模格式。我们已经通过几个LLMs证明了所提出方法的可行性，包括GPT-4o、o1-preview、Llama 3.1（8B和70B）。结果表明，所提出的方法显著提高了LLMs在实例模型生成任务中的可用性。值得注意的是，在所提出的框架内，较小的开源模型Llama 3.1 70B展示了与专有GPT模型相当的性能。|
|**2025-03-28**|**Historical Ink: Exploring Large Language Models for Irony Detection in 19th-Century Spanish**|Kevin Cohen et.al.|[2503.22585](http://arxiv.org/abs/2503.22585)|**[link](https://github.com/historicalink/ironydetection)**|**本研究探讨了大型语言模型（LLMs）在增强数据集和提高19世纪拉丁美洲报纸中讽刺检测能力方面的应用。采用了两种策略来评估BERT和GPT-4o模型捕捉讽刺微妙之处的有效性，通过多类别和二元分类任务进行评估。首先，我们实施了旨在丰富情感和上下文线索的数据集增强措施，但这些措施对历史语言分析的影响有限。其次，半自动化标注过程有效地解决了类别不平衡问题，并通过高质量的标注扩充了数据集。尽管讽刺的复杂性带来了挑战，这项工作通过两个关键贡献推动了情感分析的发展：引入了一个新的历史西班牙语数据集，用于情感分析和讽刺检测，并提出了一种半自动化标注方法，其中专家知识对于改进LLMs的结果至关重要，并且通过整合历史和文化背景作为核心特征得到进一步丰富。**|
|**2025-03-28**|**Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific Methods for Low-Resource Machine Translation**|Sarubi Thillainathan et.al.|[2503.22582](http://arxiv.org/abs/2503.22582)|null|微调多语言序列到序列大型语言模型（msLLMs）在开发神经机器翻译（NMT）系统以应对低资源语言（LRLs）方面显示出巨大潜力。然而，在极端低资源的NMT场景下，传统的单阶段微调方法难以取得良好效果，尤其是在训练数据非常有限的情况下。本文通过提出两种适应msLLMs的方法为人工智能领域做出了贡献：（1）连续预训练（CPT），在此方法中，msLLM进一步使用领域特定的单语数据进行训练，以弥补LRLs表示不足的问题；（2）中间任务迁移学习（ITTL），该方法通过使用领域内和领域外的并行数据对msLLM进行微调，以提升其在不同领域和任务中的翻译能力。作为工程领域的应用实例，这些方法被应用于Sinhala、Tamil和英语（六个语言方向）的特定领域极低资源设置下的NMT系统中（数据集样本数少于100,000）。实验结果表明，与标准的单阶段微调基线相比，这两种方法在所有翻译方向上的平均BLEU分数提升了1.47分。此外，多模型集成进一步提高了性能，额外增加了BLEU分数。|
|**2025-03-28**|**Niyama : Breaking the Silos of LLM Inference Serving**|Kanishk Goel et.al.|[2503.22562](http://arxiv.org/abs/2503.22562)|null|大规模语言模型（LLMs）的广泛应用催生了多样化且具有不同延迟要求的应用场景。现有的LLM服务框架依赖于孤立的基础设施和粗粒度的工作负载隔离——交互式与批处理，这导致资源利用效率低下，并且对精细的质量保证（QoS）支持不足。这种状况带来了运营效率低下、过度配置以及流量激增时的糟糕负载管理等问题。我们提出了Niyama，这是一种新型的QoS驱动推理服务系统，能够实现在共享基础设施上高效地协同调度各种工作负载。Niyama引入了细粒度的QoS分类，使应用程序可以指定精确的延迟需求，并根据实时系统状态动态调整调度决策。利用LLM推理可预测的执行特性，Niyama实现了一种动态分块机制，以提高整体吞吐量同时保持严格的QoS保证。此外，Niyama采用了一种混合优先级策略来平衡公平性和效率，并通过选择性请求降级在过载情况下实现优雅的服务降级。我们的评估表明，与当前的独立部署相比，Niyama增加了32%的服务容量，同时保持了QoS保证。特别是在极端负载下，我们的系统比现有策略减少了十倍的服务水平协议（SLO）违规情况。|
|**2025-03-27**|**Video-R1: Reinforcing Video Reasoning in MLLMs**|Kaituo Feng et.al.|[2503.21776](http://arxiv.org/abs/2503.21776)|**[link](https://github.com/tulerfeng/video-r1)**|**受DeepSeek-R1通过基于规则的强化学习（RL）成功激发推理能力的启发，我们推出了Video-R1，这是首次系统性探索R1范式以激发多模态大型语言模型（MLLMs）中的视频推理。然而，直接使用GRPO算法对视频推理进行RL训练存在两个主要挑战：(i) 缺乏对视频推理的时间建模；(ii) 高质量视频推理数据的稀缺性。为了解决这些问题，我们首先提出了T-GRPO算法，该算法鼓励模型利用视频中的时间信息进行推理。此外，我们没有仅仅依赖视频数据，而是将高质量的图像推理数据纳入训练过程。我们构建了两个数据集：Video-R1-COT-165k用于SFT冷启动，Video-R1-260k用于RL训练，这两个数据集都包含了图像和视频数据。实验结果表明，Video-R1在视频推理基准如VideoMMMU和VSI-Bench以及通用视频基准如MVBench和TempCompass等上取得了显著改进。特别是，Video-R1-7B在视频空间推理基准VSI-Bench上的准确率达到35.8%，超过了商用专有模型GPT-4o。所有代码、模型和数据均已发布。**|
|**2025-03-27**|**MemInsight: Autonomous Memory Augmentation for LLM Agents**|Rana Salama et.al.|[2503.21760](http://arxiv.org/abs/2503.21760)|null|大型语言模型（LLM）代理已发展为能够智能处理信息、做出决策并与用户或工具进行交互。一个关键能力是集成长期记忆功能，使这些代理能够依赖历史交互和知识。然而，记忆规模的增长以及语义结构化的需求带来了重大挑战。在这项工作中，我们提出了一种自主记忆增强方法MemInsight，以增强语义数据表示和检索机制。通过利用对历史交互的自主增强，LLM代理被证明可以提供更准确和情境化的响应。我们在三个任务场景中实证验证了所提出方法的有效性：会话推荐、问答和事件总结。在LLM-REDIAL数据集上，MemInsight将推荐的说服力提高了多达14%。此外，它在LoCoMo检索中的召回率比RAG基线高出34%。我们的实证结果展示了MemInsight在多个任务中增强LLM代理上下文性能的潜力。|
|**2025-03-27**|**GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics**|Arsham Gholamzadeh Khoee et.al.|[2503.21735](http://arxiv.org/abs/2503.21735)|null|确保软件发布决策的可靠性和有效性在汽车系统等安全关键领域尤为重要。传统上，对发布验证数据（通常以表格形式呈现）的手动分析方法容易导致延迟和高昂成本。大型语言模型（LLMs）提供了一种有前景的替代方案，但它们面临分析推理能力不足、上下文理解薄弱、处理超出范围的查询以及对结构化测试数据处理一致性差等问题，这些限制阻碍了其在安全关键场景中的直接应用。本文介绍了一个名为GateLens的LLM工具，用于分析汽车领域的表格数据。GateLens将自然语言查询转换为关系代数（RA）表达式，并生成优化的Python代码。它在基准数据集上的表现优于基线系统，实现了更高的F1分数，并且能够更稳健地处理复杂的模糊查询。消融研究确认了关系代数模块的关键作用，当省略该模块时性能急剧下降。工业评估表明，GateLens将分析时间减少了80%以上，同时保持了高准确率和可靠性。通过展示的结果可以看出，GateLens在不依赖少量样本示例的情况下实现了高性能，展示了其在不同查询类型和公司角色中的强大泛化能力。与一家汽车公司部署GateLens所获得的见解为将AI集成到发布验证等关键工作流程提供了实用指导。结果显示，通过自动化测试结果分析，GateLens能够加快、增强并使发布决策更加可靠，从而推动汽车系统的软件可扩展性和可靠性。|
|**2025-03-27**|**Effective Skill Unlearning through Intervention and Abstention**|Yongce Li et.al.|[2503.21730](http://arxiv.org/abs/2503.21730)|**[link](https://github.com/trustworthy-ml-lab/effective_skill_unlearning)**|**大规模语言模型（LLMs）在各个领域展示了卓越的能力。理解其能力背后的机制并对其实施控制对于开发更好的模型变得日益重要。本文专注于LLMs中的技能遗忘问题，即在保留整体能力的同时消除特定技能。我们提出了两种轻量级、无需训练的LLMs技能遗忘技术。首先，我们观察到当模型展示不同技能时每个前馈层（FFL）的神经元预激活分布存在差异。此外，我们发现触发相同技能的查询在FFL键空间中聚集，并且可以通过超立方体将其与其他查询分离。基于这些观察，我们提出了两种轻量级、无需训练的通过“干预”和“弃用”实现技能遗忘的方法：\texttt{Neuron Adjust} 和 \texttt{Key Space Detection}。我们在七种不同语言的数学解题、Python编程和理解技能的遗忘任务上评估了我们的方法。结果表明，这些方法在指定技能的遗忘能力方面表现出色。具体而言，\texttt{Key Space Detection} 在大多数遗忘任务中实现了超过80%的相对性能下降，并且在被遗忘技能和其他技能以及模型的一般知识（MMLU）上的相对性能下降不到10%。我们的代码可在https://github.com/Trustworthy-ML-Lab/effective_skill_unlearning获取。**|
|**2025-03-27**|**Collab: Controlled Decoding using Mixture of Agents for LLM Alignment**|Souradip Chakraborty et.al.|[2503.21720](http://arxiv.org/abs/2503.21720)|null|大型语言模型（LLMs）的对齐对于其在应用中的安全和可信部署至关重要。从人类反馈的强化学习（RLHF）已成为一种有效的方法来使LLMs与人类偏好和更广泛的目标对齐，但这种方法需要更新数十亿个模型参数，这在计算上非常昂贵。相比之下，可控解码提供了一种在推理时对齐模型的机制，而无需重新训练。然而，单代理解码方法往往难以适应多样化的任务，因为这些任务固有的复杂性和变异性。为了加强针对目标任务的测试时间性能，我们提出了一种基于混合代理解码策略的方法，利用现有的预先对齐的LLM策略。我们将每个先前的策略视为一个代理，并在代理协作的精神下开发了解码方法，该方法在多个代理之间动态选择最合适的LLM以进行推理时间的对齐。通过令牌级别的选择策略，在每个步骤中动态选择最合适的LLM，这种策略切换机制确保了在每个步骤中最佳模型的选择，从而实现在解码过程中不同LLM之间的高效协作和对齐。我们提出的算法的理论分析表明，相对于给定的预对齐模型，该方法在目标任务的表现上达到了最优。我们在开放源代码的对齐模型上进行了广泛的实证评估，涵盖了不同的任务和偏好，结果表明这种方法优于单代理解码基线。值得注意的是，我们的方法在平均奖励方面比当前最先进的解码策略提高了1.56倍，在GPT-4基于胜率和平局率的指标上提升了71.89%|
|**2025-03-27**|**Enhancing Repository-Level Software Repair via Repository-Aware Knowledge Graphs**|Boyang Yang et.al.|[2503.21710](http://arxiv.org/abs/2503.21710)|null|仓库级软件修复面临着在问题描述和代码补丁之间建立语义桥梁的挑战。现有的方法大多依赖于大型语言模型（LLMs），但这些方法存在语义模糊、对结构化上下文理解有限以及推理能力不足的问题。为了解决这些局限性，我们提出了KGCompass，它有两个创新点：（1）一种新的仓库感知知识图谱（KG），能够准确地将仓库中的工件（如问题和拉取请求）与代码库实体（如文件、类和函数）链接起来，使我们能够有效地缩小庞大的搜索空间，仅聚焦于最相关的20个函数，并提供准确的候选错误位置和上下文信息；（2）路径引导修复机制，利用从知识图谱中挖掘的实体路径，通过这些路径可以增强LLMs，使其获得相关上下文信息，从而生成精确的补丁及其解释。实验结果表明，在SWE-Bench-Lite数据集上，KGCompass在开源方法中实现了最先进的修复性能（45.67%）和函数级定位准确性（51.33%），每次修复成本仅为0.20美元。我们的分析显示，在成功定位的错误中，有69.7%需要通过知识图谱进行多跳遍历，否则基于LLM的方法难以准确定位错误。KGCompass构建的知识图谱是语言无关的，并且可以增量更新，这使其成为实际开发环境中实用的解决方案|
|**2025-03-27**|**LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning**|Hui Wang et.al.|[2503.21683](http://arxiv.org/abs/2503.21683)|null|近年来，大型语言模型（LLMs）在自然语言处理（NLP）方面取得了显著进展，在生成、理解与推理方面表现出强大能力，并已在教育、智能决策和游戏等领域得到应用。然而，有效利用LLMs进行五子棋战略规划和决策仍面临挑战。本研究旨在开发一种基于LLMs的五子棋AI系统，模拟人类下国际象棋的学习过程。该系统旨在理解和应用五子棋策略与逻辑以做出理性决策。研究方法包括让模型“读取棋盘”、“理解规则”、“选择策略”和“评估局面”，并通过自我对弈和强化学习增强其能力。结果表明，这种方法显著提高了选点的选择性，解决了产生非法局面的问题，并通过并行局面评估减少了处理时间。经过大量自我对弈训练，模型的五子棋能力得到了明显提升。|
|**2025-03-27**|**JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human Self-Destructive Behavior Content in Jirai Community**|Yunze Xiao et.al.|[2503.21679](http://arxiv.org/abs/2503.21679)|null|本文介绍了JiraiBench，这是首个双语基准测试，用于评估大型语言模型在检测中文和日文社交媒体社区中的自我毁灭性内容方面的有效性。我们专注于跨越多种自我毁灭行为的“地雷”（Jirai）在线亚文化，包括药物过量、饮食失调和自残等。我们提出了一个综合的评估框架，结合了语言和文化两个维度。我们的数据集包含10,419条中文帖子和5,000条日文帖子，并在三个行为类别上进行了多维标注，达到了显著的注释者间一致性。对四个最先进的模型进行的实验评估显示，基于指令的语言存在显著的性能差异，出乎意料的是，处理中文内容时日文提示的表现优于中文提示。这种新兴的跨文化迁移表明，在某些检测任务中，文化接近性有时可以超过语言相似性。通过微调模型进行的跨语言迁移实验进一步证明了在这两种语言系统之间进行知识迁移的可能性，而无需针对目标语言进行明确训练。这些发现强调了在多语言内容审核中采用文化导向方法的必要性，并为开发更有效的检测系统以保护脆弱的在线社区提供了实证依据。|
|**2025-03-27**|**How do language models learn facts? Dynamics, curricula and hallucinations**|Nicolas Zucchet et.al.|[2503.21676](http://arxiv.org/abs/2503.21676)|null|大型语言模型在预训练过程中积累了大量知识，但其获取知识的动态机制仍知之甚少。本研究通过一个合成的事实回忆任务调查了语言模型的学习动态，揭示了三个关键发现：首先，语言模型经历了三个阶段的学习，在精确掌握事实知识之前表现出性能平台期。从机制上讲，这一平台期与支持回忆的注意力电路的形成相吻合。其次，训练数据分布对学习动态有显著影响，不平衡的分布会导致较短的平台期。最后，幻觉与知识同时出现，且通过微调将新知识整合到模型中具有挑战性，因为这会迅速破坏其现有的参数记忆。我们的研究结果强调了数据分布在知识获取中的重要性，并提出了加速神经网络训练的新数据调度策略。|
|**2025-03-27**|**Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base**|Satvik Verma et.al.|[2503.21674](http://arxiv.org/abs/2503.21674)|**[link](https://github.com/claudwq/Intelligent-IoT-Attack-Detection-Design-via-LLM-with-Feature-Ranking-Based-Knowledge-Base-Design)**|**物联网（IoT）设备的广泛采用带来了显著的网络安全挑战，特别是分布式拒绝服务（DDoS）攻击的频率和复杂性不断增加。传统机器学习技术在检测此类攻击时往往因混合且不断演化的模式而表现不足。为了解决这一问题，我们提出了一种新颖的框架，该框架利用了设备端大型语言模型（ODLLMs），并结合了微调和知识库（KB）集成以实现智能的IoT网络攻击检测。通过实施特征排名技术和构建适合模型容量的长短期知识库，所提出的框架确保了高效且准确的DDoS攻击检测，同时克服了计算和隐私限制。仿真结果表明，优化后的框架在各种攻击类型中实现了卓越的准确性，尤其是在边缘计算环境中使用紧凑型模型时。这项工作为实时IoT安全提供了一个可扩展且安全的解决方案，推动了边缘智能在网络安全领域的应用。**|
|**2025-03-26**|**Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark**|Sondos Mahmoud Bsharat et.al.|[2503.20786](http://arxiv.org/abs/2503.20786)|**[link](https://github.com/vila-lab/mobile-mmlu)**|大型语言模型（LLMs）在移动设备上的部署兴趣日益增加，用于设备端AI应用。移动用户与LLMs的交互方式与桌面用户不同，形成了独特的期望和数据偏差。当前的基准数据集主要针对服务器和桌面环境，缺乏专门设计用于移动环境的大规模数据集。此外，移动设备在存储和计算资源上受到严格限制，限制了模型的大小和能力，因此需要优化效率并优先考虑知识。为了解决这些挑战，我们引入了Mobile-MMLU，这是一个专为移动智能设计的大规模基准数据集。它包含80个移动相关领域的16,186个问题，旨在评估LLM在现实移动场景中的性能。一个具有挑战性的子集Mobile-MMLU-Pro提供了与MMLU-Pro规模相当但比我们的完整集合显著更难的高级评估。这两个基准使用多选、顺序不变的问题，专注于实际的移动交互，如食谱建议、旅行规划和日常任务。该数据集强调关键的移动特定指标，如推理延迟、能耗、内存使用和响应质量，为模型在移动约束下的性能提供全面见解。此外，它优先考虑隐私和适应性，评估模型在设备端处理、保护用户隐私和适应个性化使用模式方面的能力。Mobile-MMLU系列为开发和比较移动优化的LLMs提供了一个标准化框架，推动了生产力和决策在移动计算环境中的进步。我们的代码和数据可在https://github.com/VILA-Lab/Mobile-MMLU获取。|
|**2025-03-26**|**MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look in Mathematical Diagrams**|Yanpeng Sun et.al.|[2503.20745](http://arxiv.org/abs/2503.20745)|null|图表作为一种基础的可视化语言，通过结构化的符号、形状和空间布局来表示复杂概念及其相互关系。与自然图像不同，其固有的符号性和抽象性给多模态大型语言模型（MLLMs）带来了显著挑战。然而，现有的基准测试混淆了感知和推理任务，使得难以评估MLLMs是否真正理解数学图表，而不仅仅是表面的模式识别。为了解决这一问题，我们引入了MATHGLANCE，这是一个专门设计用来隔离并评估MLLMs在数学感知方面能力的基准。MATHGLANCE包含1200张图片和1600个精心策划的问题，涵盖了四个感知任务：形状分类、对象计数、关系识别和对象定位，涉及平面几何、立体几何以及图形表示等多个领域。我们的评估显示，MLLMs理解图表的能力明显受限，尤其是在细粒度的定位任务上。为此，我们构建了GeoPeP，这是一个包含20万组结构化几何图像-文本对的数据集，明确标注了几何基元和精确的空间关系。在GeoPeP上训练MLLM可以显著提高感知准确性，从而大幅改善数学推理能力。我们的基准和数据集为评估和推进多模态数学理解设定了重要标准，为未来的MLLM研究提供了宝贵的资源和见解。|
|**2025-03-26**|**Dynamic Motion Blending for Versatile Motion Editing**|Nan Jiang et.al.|[2503.20724](http://arxiv.org/abs/2503.20724)|null|文本引导的运动编辑能够实现高级语义控制和迭代修改，超越了传统关键帧动画。现有方法依赖于有限的预收集训练三元组，这严重限制了其在多样化编辑场景中的适用性。我们引入了MotionCutMix，在线数据增强技术，通过基于输入文本混合身体部位运动来动态生成训练三元组。尽管MotionCutMix有效扩展了训练分布，但其组合性质引入了更大的随机性和潜在的身体部位不协调。为了建模这种丰富的分布，我们提出了MotionReFit，这是一种带有运动协调器的自回归扩散模型。自回归架构有助于通过分解长序列进行学习，而运动协调器则减轻了运动组合产生的伪影。我们的方法可以直接从高级人类指令处理空间和时间上的运动编辑，而不依赖额外的规格或大型语言模型。通过广泛的实验，我们展示了MotionReFit在文本引导的运动编辑任务上达到了最先进的性能。|
|**2025-03-26**|**From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect Extraction for Aspect-Based Sentiment Analysis with Large Language Models**|Nikita Neveditsin et.al.|[2503.20715](http://arxiv.org/abs/2503.20715)|null|本研究考察了大型语言模型（LLMs）在基于方面的情感分析（ABSA）中的表现，重点关注新领域中的隐式方面提取。我们使用一个合成的体育反馈数据集，评估开源权重LLMs提取方面-极性对的能力，并提出了一种度量方法以促进生成模型方面的评估。研究结果揭示了LLMs在ABSA任务中的潜力与局限性。|
|**2025-03-27**|**Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy**|Yinan Sun et.al.|[2503.20673](http://arxiv.org/abs/2503.20673)|null|多模态大型语言模型的快速发展在视觉感知和理解方面取得了显著进展，将多种任务整合到一个视觉问答框架中。然而，这些模型容易出现幻觉，这限制了它们作为人工智能系统的可靠性。尽管这个问题在自然语言处理和图像描述方面得到了广泛研究，但在低级视觉感知与理解（HLPU）领域，特别是针对图像质量评估任务的幻觉问题，仍然缺乏深入研究。我们认为这种幻觉源于模型缺乏清晰的自我意识。为了解决这一问题，我们首先引入了HLPU指令数据库，这是首个专门针对低级视觉任务幻觉现象的指令数据库。该数据库包含约20万组问答对，并分为四个子集，每个子集涵盖不同类型的指令。随后，我们提出了自省失效消除（SAFEQA）模型，利用图像特征、显著区域特征和质量特征来提升模型在低级视觉任务中的感知和理解能力。此外，我们还提出了增强自省偏好的优化（ESA-PO）框架，以提高模型对知识边界的认知，从而减少幻觉的发生。最后，我们在低级视觉任务上进行了全面的实验，结果表明我们的方法显著提升了模型在这些任务中的自省能力并减少了幻觉。值得注意的是，我们的方法不仅提高了模型的准确性和自省能力，还在各种评估指标上优于闭源模型。|
|**2025-03-26**|**TAMA: A Human-AI Collaborative Thematic Analysis Framework Using Multi-Agent LLMs for Clinical Interviews**|Huimin Xu et.al.|[2503.20666](http://arxiv.org/abs/2503.20666)|null|主题分析（TA）是一种广泛使用的定性方法，用于揭示无结构文本数据中的潜在含义。TA在医疗保健领域提供了宝贵的见解，但其资源消耗巨大。大型语言模型（LLMs）已被引入以执行TA，然而其在医疗保健中的应用尚未被探索。在此，我们提出了TAMA：一种使用多智能体LLMs进行临床访谈的人机协作主题分析框架。我们利用多智能体系统通过智能体之间的结构化对话来实现可扩展性和连贯性，并协调心脏专家在TA中的专业知识。使用来自患有异常主动脉起源冠状动脉（AAOCA）的儿童父母的访谈记录，我们证明TAMA在现有LLM辅助TA方法中表现更优，实现了更高的主题命中率、覆盖率和独特性。TAMA通过利用多智能体LLM系统与人机协作集成，在提高质量的同时显著减少了手动工作量，显示出在临床环境中自动主题分析的强大潜力|
|**2025-03-26**|**Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging**|Han Wu et.al.|[2503.20641](http://arxiv.org/abs/2503.20641)|**[link](https://github.com/hahahawu/long-to-short-via-model-merging)**|大语言模型（LLMs）从系统1到系统2推理的转变标志着在处理复杂任务方面通过深思熟虑、迭代思考取得了显著进展。然而，这种进步往往以效率为代价，因为模型倾向于过度思考，生成冗余的推理步骤，而这些步骤并未带来与输出质量成比例的提升。长到短（L2S）推理作为一种有前景的解决方案应运而生，旨在平衡推理深度与实际效率。尽管现有的方法如监督微调（SFT）、强化学习（RL）和提示工程已显示出潜力，但它们要么计算成本高昂，要么不稳定。模型合并则提供了一种经济高效且稳健的替代方案，通过整合系统1模型的快速思维能力和系统2模型的系统性推理能力来实现这一目标。在这项工作中，我们对用于L2S推理的模型合并进行了全面的实证研究，探索了多种方法，包括基于任务向量、奇异值分解（SVD）以及基于激活信息的合并。我们的实验表明，模型合并可以将平均响应长度减少高达55%，同时保持或甚至提高基线性能。此外，我们还发现模型规模与合并效果之间存在较强的关联，并通过在1.5B/7B/14B/32B模型上的广泛评估验证了这一点。进一步的研究表明，合并后的模型具备自我批判和自我修正的能力，并能根据任务复杂度调整响应长度。我们的研究结果强调了模型合并作为L2S推理的一种高效且有效的范式，提供了克服过度思考问题的实用解决方案，同时保留了系统2推理的稳健性。这项工作可以在GitHub上找到：https://github.com/hahahawu/Long-to-Short-via-Model-Merging。|
|**2025-03-26**|**Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions**|Alessandro Maisto et.al.|[2503.20623](http://arxiv.org/abs/2503.20623)|null|角色扮演游戏（RPG）是一种玩家相互互动以创造叙事的游戏。玩家在RPG中的角色主要基于玩家与角色之间的互动。这种新兴的共享叙事形式，主要是口头的，在近年来引起了越来越多的关注。特别是许多作者研究了将大型语言模型（LLM）作为游戏中的参与者。在本文中，我们旨在发现当要求大型语言模型（LLM）生成RPG会话而没有人为干预时，其语言在多大程度上表现出口头或书面特征。我们将对生成文本的词汇和句法特征进行语言学分析，并将其结果与对话、人类RPG会话的转录本以及书籍的分析进行比较。我们发现LLMs表现出一种与其他文本类别（包括口头对话、人类RPG会话和书籍）都不同的模式。我们的分析显示了训练如何影响LLMs的表达方式，并为这些工具的叙事能力提供了重要的指示。|
|**2025-03-26**|**What to Retrieve for Effective Retrieval-Augmented Code Generation? An Empirical Study and Beyond**|Wenchao Gu et.al.|[2503.20589](http://arxiv.org/abs/2503.20589)|null|由于复杂的代码依赖性和大型语言模型（LLMs）在处理长上下文方面的局限性，仓库级代码生成仍然具有挑战性。尽管检索增强生成（RAG）框架被广泛采用，但不同检索信息源——上下文代码、API和相似代码片段——的有效性尚未经过严格分析。通过在两个基准上的实证研究，我们证明上下文代码和潜在的API信息显著提升了LLM的表现，而检索到的相似代码往往引入噪声，导致结果下降多达15%。基于初步结果，我们提出了AllianceCoder，这是一种新颖的上下文化集成方法，利用思维链提示将用户查询分解为实现步骤，并通过语义描述匹配检索API。通过在CoderEval和RepoExec上的大量实验，AllianceCoder达到了最先进的性能，在Pass@1上比现有方法提高了多达20%。|
|**2025-03-27**|**LLPut: Investigating Large Language Models for Bug Report-Based Input Generation**|Alif Al Hasan et.al.|[2503.20578](http://arxiv.org/abs/2503.20578)|null|失效输入在诊断和分析软件错误中起着至关重要的作用。错误报告通常包含这些输入，开发人员从中提取它们以促进调试。由于错误报告是自然语言编写而成的，以往的研究利用了各种自然语言处理（NLP）技术来进行自动输入提取。随着大型语言模型（LLMs）的出现，一个重要的研究问题浮现出来：生成式LLMs在从错误报告中提取相关输入方面的表现如何？在本文中，我们提出了LLPut，这是一种实证评估技术，用于评估三个开源生成式LLMs——LLaMA、Qwen和Qwen-Coder在从错误报告中提取相关信息的能力。我们在包含206个错误报告的数据集上进行了实验评估，以评估这些模型的准确性和有效性。我们的研究结果提供了对生成式LLMs在自动化错误诊断中的能力和局限性的洞察。|
|**2025-03-25**|**CoLLM: A Large Language Model for Composed Image Retrieval**|Chuong Huynh et.al.|[2503.19910](http://arxiv.org/abs/2503.19910)|**[link](https://github.com/hmchuong/CoLLM)**|**组合图像检索（CIR）是一项旨在根据多模态查询检索图像的复杂任务。典型的训练数据由包含参考图像、所需修改的文字描述和目标图像的三元组组成，这类数据获取昂贵且耗时。CIR数据集的稀缺性促使了零样本方法的发展，这些方法利用合成三元组或依赖于视觉语言模型（VLM）和网络爬取的图像-标题对。然而，这些方法存在显著局限：合成三元组存在规模有限、多样性不足以及修改文字不自然的问题，而图像-标题对由于缺乏三元组数据阻碍了多模态查询联合嵌入的学习。此外，现有方法在处理复杂且细微的修改文字时，难以有效融合和理解视觉与语言模态。我们提出了CoLLM，这是一个一站式框架，能够有效解决上述问题。我们的方法从图像-标题对中实时生成三元组，从而实现无手动标注的监督训练。我们利用大规模语言模型（LLM）生成参考图像和修改文字的联合嵌入，以促进更深层次的多模态融合。此外，我们引入了多文本CIR（MTCIR），这是一个包含340万样本的大规模数据集，并改进了现有的CIR基准（CIRR和Fashion-IQ）以增强评估的可靠性。实验结果表明，CoLLM在多个CIR基准和设置下实现了最先进的性能。MTCIR获得了具有竞争力的结果，在某些情况下性能提高了15%。我们改进的基准为CIR模型提供了更可靠的评估指标，推动了这一重要领域的进步。**|
|**2025-03-25**|**A Multi-Agent Framework Integrating Large Language Models and Generative AI for Accelerated Metamaterial Design**|Jie Tian et.al.|[2503.19889](http://arxiv.org/abs/2503.19889)|null|介孔材料因其卓越的机械、电磁和热学性能，在众多应用领域展现出变革性的潜力，但其设计仍受限于劳动密集型的试错方法和有限的数据互操作性。在此，我们引入了CrossMatAgent——一种新颖的多智能体框架，该框架通过将大型语言模型与最先进的生成式人工智能相结合，彻底革新了介孔材料的设计方式。通过协调一组各具专长的智能体——包括模式分析、架构合成、提示工程和监督反馈等任务——我们的系统利用了GPT-4o的多模态推理能力以及DALL-E 3和经过微调的Stable Diffusion XL模型的生成精度。这种集成方法实现了数据增强、提升了设计保真度，并生成了适用于模拟和3D打印的介孔材料图案。全面评估，包括基于CLIP的对齐、SHAP可解释性分析以及在不同负载条件下的力学模拟，证明了该框架能够产生多样化、可重复且实用的设计。CrossMatAgent因此建立了一个可扩展的人工智能驱动范式，弥合了概念创新与实际实现之间的差距，为加速介孔材料的发展铺平了道路。|
|**2025-03-25**|**CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation**|Nengbo Wang et.al.|[2503.19878](http://arxiv.org/abs/2503.19878)|null|大型语言模型（LLMs）通过检索增强生成（RAG）在自然语言处理（NLP）领域取得了革命性进展，RAG通过整合外部知识增强了LLMs的能力。然而，传统的RAG系统存在关键限制，包括因文本分块导致的上下文完整性中断以及对语义相似性的过度依赖。为了解决这些问题，我们提出了CausalRAG，这是一种新颖的框架，它将因果图引入检索过程。通过构建和追踪因果关系，CausalRAG保持了上下文的一致性并提高了检索精度，从而实现了更准确且可解释的响应。我们通过与常规RAG和基于图的RAG方法进行对比评估，证明了CausalRAG的优越性。我们的研究结果表明，以因果推理为基础进行检索为知识密集型任务提供了一种有前景的方法。|
|**2025-03-25**|**SLA-Awareness for AI-assisted coding**|Kishanthan Thangarajah et.al.|[2503.19876](http://arxiv.org/abs/2503.19876)|null|在开发环境中集成AI辅助编码工具可以大大减少开发时间，使开发人员能够更多地专注于软件工程的创造性和关键性方面，通过使用代码大型语言模型（CodeLLMs）。这些编码助手自动化了重复且耗时的编码任务，如代码生成、代码补全、代码总结和代码转换。响应能力是这些编码助手的关键要求，以保持实时交互性，从而不会妨碍开发人员的工作流程。不同的编码任务具有独特的特性和延迟要求：对于代码补全任务，时间到第一个令牌（TTFT）延迟至关重要，而对于代码转换任务，端到端（E2E）延迟则至关重要。同时管理这些不同的需求并优化资源使用带来了重大挑战。现有的工作采用模型即服务（Model-as-a-Service）范式来服务单个CodeLLMs，但由于缺乏端到端延迟感知，无法有效管理并发编码任务和CodeLLM推理调用序列的延迟需求。另一个挑战是在共享集群环境中部署服务系统时保持高资源利用率。为了解决这些挑战，我们提出了编码助手任务编排器（CATO），这是一个运行时系统，旨在服务于多样化的编码任务，满足延迟要求并最大化资源利用率。我们的实验表明，当所有类型的编码任务同时被服务时，对于TTFT关键任务，CATO将整体吞吐量提高率和资源利用率分别提高了10%和41.1%。对于代码总结任务，P95 E2E延迟减少了18%，而代码生成任务的P95 TTFT延迟减少了14%，相比最先进的系统有所改善。|
|**2025-03-25**|**Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking**|Xiaoyu Tian et.al.|[2503.19855](http://arxiv.org/abs/2503.19855)|null|最近大型语言模型（LLMs）如OpenAI-o1和DeepSeek-R1在测试时扩展方面的进展表明，延长推理过程可以显著提升模型性能。然而，当前的模型仍然受到处理长文本以及强化学习（RL）训练效率低下的限制。为了解决这些问题，我们提出了一种简单但有效的测试时扩展方法——多轮思考。该方法通过利用前一轮的答案作为后续轮次的提示来逐步优化模型的推理过程。广泛的实验结果表明，该方法在多个模型上（包括QwQ-32B和DeepSeek-R1）均能稳定提高性能，在AIME 2024、MATH-500、GPQA-diamond和LiveCodeBench等基准数据集上均有显著表现。例如，QwQ-32B在AIME 2024数据集上的准确率从第一轮的80.3%提升到了第二轮的82.1%，而DeepSeek-R1也从79.7%提升到了82.0%。这些结果证明了多轮思考是一种广泛适用且简便的方法，能够稳定地增强模型性能，展示了其在未来测试时扩展技术发展中的潜力。请根据以下提示重新回答：{原问题提示} 助手的上一轮答案是：<answer>{上一轮答案}</answer>，请重新作答。|
|**2025-03-25**|**Towards Online Multi-Modal Social Interaction Understanding**|Xinpeng Li et.al.|[2503.19851](http://arxiv.org/abs/2503.19851)|**[link](https://github.com/sampson-lee/onlinemmsi)**|多模态社会互动理解（MMSI）在人机交互系统中至关重要。在现实场景中，AI代理需要提供实时反馈。然而，现有的模型通常依赖于过去和未来的上下文信息，这阻碍了它们在实际问题中的应用。为了解决这一差距，我们提出了一个在线MMSI设置，其中模型必须仅使用历史信息（如记录的对话和视频流）来解决MMSI任务。为了解决缺少有用未来上下文的问题，我们开发了一种新的框架，名为Online-MMSI-VLM，它利用两种互补策略：多党对话预测和社会感知视觉提示与多模态大型语言模型。首先，为了丰富语言上下文，多党对话预测以粗到细的方式模拟潜在的未来陈述，预测即将进行的发言者轮次并生成详细的对话内容。其次，为了有效结合视觉社交线索如眼神和手势，社会感知视觉提示通过每帧每个人的人体边界框和关键点突出显示视频中的社会动态。在三个任务和两个数据集上的广泛实验表明，我们的方法达到了最先进的性能，并显著优于基线模型，表明其在在线MMSI中的有效性。代码和预训练模型将在https://github.com/Sampson-Lee/OnlineMMSI公开发布。|
|**2025-03-25**|**FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs**|Carlos Plou et.al.|[2503.19850](http://arxiv.org/abs/2503.19850)|null|信息检索在长达一小时的视频中提出了重大挑战，即使是最先进的视觉语言模型（VLM）也难以应对，尤其是在所需信息局限于少量帧的情况下。长视频数据对VLM提出了挑战，因为它们存在上下文窗口限制并且难以定位包含答案的帧。我们提出的视频代理FALCONEye结合了VLM和大型语言模型（LLM），用于搜索视频中的相关信息并定位包含答案的帧。FALCONEye的独特之处在于：1）所提出的元架构更适合处理长达一小时的视频，与针对短视频的现有最先进技术相比；2）一种新的高效探索算法，利用短片段、字幕和答案置信度来定位信息；3）对VLM答案置信度的最先进的校准分析。我们的代理基于小型VLM和中型LLM构建，可以在标准计算资源上运行。我们还发布了FALCON-Bench，这是一个评估长时间（平均>1小时）视频回答搜索挑战的数据集，突显了开放性问题评估的需求。实验表明，FALCONEye在FALCON-Bench上的表现优于现有技术，并在相关基准测试中表现出相似或更好的性能。|
|**2025-03-25**|**A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950**|Zhao Fang et.al.|[2503.19844](http://arxiv.org/abs/2503.19844)|null|本文比较了大型语言模型（LLMs）和传统的自然语言处理（NLP）工具在对1900年至1950年间中国文本进行词 segmentation、词性标注（POS）和命名实体识别（NER）任务中的表现。历史中文文献由于其表意文字系统、自然词边界缺失以及显著的语言变化而给文本分析带来了挑战。使用来自上海图书馆民国期刊语料库的样本数据集，传统工具如Jieba和spaCy与LLMs（包括GPT-4o、Claude 3.5和GLM系列）进行了对比。结果表明，LLMs在所有指标上均优于传统方法，但计算成本显著更高，这凸显了准确性和效率之间的权衡。此外，LLMs更好地应对了体裁特定的挑战，例如诗歌以及时间变化（即1920年之前与之后的文本），表明其上下文学习能力可以通过减少对领域特定训练数据的需求来推动历史文本的NLP方法进步。|
|**2025-03-25**|**SemEval-2025 Task 9: The Food Hazard Detection Challenge**|Korbinian Randl et.al.|[2503.19800](http://arxiv.org/abs/2503.19800)|null|在这个挑战中，我们探讨了针对文本的食品危害预测任务，这些类别呈现长尾分布。任务分为两个子任务：（1）预测网络文本是否暗示其中一种十种食品危害类别并识别相关的食品类别，以及（2）通过分配具体的标签对危害和产品进行更细粒度的分类。我们的研究结果表明，由大型语言模型生成的合成数据在处理长尾分布时非常有效。此外，我们发现经过微调的仅编码器、编码器-解码器和仅解码器系统在两个子任务上的性能相当。在此挑战期间，我们逐步发布了（根据CC BY-NC-SA 4.0许可）一组新的6,644份人工标注的食品事件报告|
|**2025-03-25**|**PAVE: Patching and Adapting Video Large Language Models**|Zhuoming Liu et.al.|[2503.19794](http://arxiv.org/abs/2503.19794)|**[link](https://github.com/dragonlzm/pave)**|预训练的视频大语言模型（Video LLMs）展现出卓越的推理能力，但将其适配到涉及额外模态或数据类型（如音频、三维信息等）的新任务中仍然具有挑战性。在本文中，我们提出了PAVE，这是一种灵活的框架，用于将预训练的Video LLMs适配到带有侧信道信号的下游任务中，例如音频、三维线索或多视角视频。PAVE引入了轻量级适配器，称为“补丁”，这些适配器向基础模型添加少量参数和操作，而不会修改其架构或预训练权重。通过这种方式，PAVE能够有效地将预训练的基础模型适配以支持多种下游任务，包括视听问答、三维推理、多视角视频识别以及高帧率视频理解。在这些任务中，PAVE显著提升了基础模型的性能，超越了最先进的任务特定模型，同时仅增加了约0.1%的浮点运算（FLOPs）和参数。此外，PAVE支持多任务学习，并且在不同的Video LLMs之间具有良好的泛化能力。我们的代码可在https://github.com/dragonlzm/PAVE获取。|
|**2025-03-24**|**SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding**|Mingze Xu et.al.|[2503.18943](http://arxiv.org/abs/2503.18943)|null|我们介绍了SlowFast-LLaVA-1.5（简称SF-LLaVA-1.5），这是一系列面向长视频理解的视频大语言模型，提供了一种针对长视频理解的高效Token解决方案。该模型家族采用两流慢快机制，能够有效地建模长时间范围内的时序上下文，以满足轻量级、移动端友好的视频大语言模型的需求。我们提供了从10亿到70亿参数规模的模型，并通过优化的训练流程和高质量的数据混合（由公开可用的数据集组成）进行训练。实验结果表明，SF-LLaVA-1.5在广泛的视频和图像基准测试中表现出竞争力，所有模型规模均表现稳健。特别地，SF-LLaVA-1.5在长视频理解任务（如LongVideoBench和MLVU）上取得了最先进的成果，并且在小规模（10亿和30亿参数）下在各种视频基准测试中表现出色。|
|**2025-03-24**|**Video-T1: Test-Time Scaling for Video Generation**|Fangfu Liu et.al.|[2503.18942](http://arxiv.org/abs/2503.18942)|null|随着训练数据、模型规模和计算成本的扩展能力，视频生成在数字创作领域取得了令人印象深刻的结果，使用户能够在各个领域表达创造力。最近，大型语言模型（LLMs）的研究人员将扩展能力延伸到了推理时间，这可以通过使用更多的推理时间计算显著提高模型性能。与通过昂贵的训练成本扩展视频基础模型不同，我们探索了推理时间缩放（TTS）在视频生成中的潜力，旨在回答以下问题：如果允许视频生成模型在推理时使用非平凡量的计算资源，它能在面对具有挑战性的文本提示时多大程度上改善生成质量？在这项工作中，我们将视频生成的测试时间缩放重新解释为一个搜索问题，即从高斯噪声空间采样更好的轨迹以达到目标视频分布。具体而言，我们构建了一个带有测试时间验证器的搜索空间以提供反馈，并设计了启发式算法来指导搜索过程。给定一个文本提示，我们首先通过增加推理时间的噪声候选者来探索一种直观的线性搜索策略。由于全步去噪所有帧同时需要大量的推理时间计算开销，我们进一步设计了一种更高效的视频生成测试时间缩放方法称为Tree-of-Frames（ToF），该方法以自回归的方式自适应地扩展和剪枝视频分支。在文本条件视频生成基准上的大量实验表明，增加推理时间计算资源可以持续显著提高视频的质量。项目页面：https://liuff19.github.io/Video-T1|
|**2025-03-24**|**Exploring Training and Inference Scaling Laws in Generative Retrieval**|Hongru Cai et.al.|[2503.18941](http://arxiv.org/abs/2503.18941)|**[link](https://github.com/HongruCai/SLGR)**|生成式检索作为一种新颖的范式，利用大规模语言模型（LLMs）自回归地生成文档标识符，已崭露头角。尽管前景广阔，其性能和可扩展性的机制仍不甚明了。我们系统地研究了生成式检索在训练和推理中的缩放规律，探索模型大小、训练数据规模以及推理计算如何共同影响检索性能。为了解决缺乏合适度量的问题，我们提出了一种受对比熵和生成损失启发的新评估指标，提供了一个连续的性能信号，使不同生成式检索方法之间的稳健比较成为可能。实验表明，基于n元语法的方法与训练和推理缩放规律表现出较强的契合性，尤其是与更大的LLMs结合时。此外，增加推理计算带来了显著的性能提升，揭示了生成式检索可以从更高的计算预算中显著受益。在这些设置中，LLaMA模型始终优于T5模型，表明较大的解码器-only模型在生成式检索中具有特定优势。综合来看，我们的发现强调了模型大小、数据可用性和推理计算的相互作用可以释放生成式检索的全部潜力，为设计和优化未来的系统提供了新的见解。|
|**2025-03-24**|**Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training**|Brian R. Bartoldson et.al.|[2503.18929](http://arxiv.org/abs/2503.18929)|null|强化学习（RL）是大型语言模型（LLM）后训练的重要组成部分。然而，现有的用于后训练的策略梯度算法本质上与经验回放缓冲区不兼容，而经验回放缓冲区可以通过分布式异策略行为者可扩展地填充以增强探索性随着计算能力的增长。我们提出通过轨迹平衡与异步性（TBA），这是一种大规模可扩展的LLM RL系统来高效获得回放缓冲区的好处。与现有方法不同，TBA将更多计算用于搜索，不断为中央回放缓冲区生成异策略数据。训练节点同时基于奖励或最近性从该缓冲区采样数据，使用轨迹平衡（TB）更新策略，这是一种为GFlowNets引入的寻求多样性的RL目标。TBA具有三个关键优势：（1）解耦训练和搜索，将训练的实时时长加快4倍或更多；（2）通过大规模异策略采样提高多样性；以及（3）稀疏奖励设置下的可扩展搜索。在数学推理、偏好调优和自动化红队（多样化且有代表性的后训练任务）上，TBA在速度和性能方面均优于强基准模型。|
|**2025-03-24**|**FFN Fusion: Rethinking Sequential Computation in Large Language Models**|Akhiad Bercovich et.al.|[2503.18908](http://arxiv.org/abs/2503.18908)|null|我们引入了FFN融合，这是一种减少大型语言模型中顺序计算的架构优化技术，通过识别并利用自然的并行化机会来实现。我们的关键见解是，特别是移除特定注意力层后剩余的馈送网络（FFN）层序列，通常可以以最小的准确度影响进行并行化。我们开发了一种原则性的方法来识别和融合这些序列，将其转化为并行操作，从而显著减少推理延迟同时保持模型行为。通过对Llama-3.1-405B-Instruct应用这些技术，我们创建了Llama-Nemotron-Ultra-253B-Base（Ultra-253B-Base），这是一个高效且即将公开可用的模型，在推理延迟上实现了1.71倍的速度提升，每个令牌的成本降低了35倍，同时在各种基准测试中保持了强劲的表现。通过在从49B到253B参数的不同规模模型上的广泛实验，我们证明了FFN融合在更大规模时变得越来越有效，并能与现有的优化技术如量化和剪枝互补。最有趣的是，我们发现即使包含注意力和FFN层的完整Transformer块有时也可以被并行化，这为神经架构设计指出了新的方向。|
|**2025-03-24**|**xKV: Cross-Layer SVD for KV-Cache Compression**|Chi-Chih Chang et.al.|[2503.18893](http://arxiv.org/abs/2503.18893)|**[link](https://github.com/abdelfattah-lab/xkv)**|大型语言模型（LLMs）具有长上下文窗口的能力，但存储Key和Value状态（KV缓存）会消耗大量内存。近期研究尝试将多个层的KV缓存合并到共享表示中，但这些方法要么需要昂贵的预训练，要么依赖于跨层高每令牌余弦相似度的假设，在实践中通常不成立。我们发现主要奇异向量在多个层的KV缓存中表现出惊人的对齐性。利用这一见解，我们提出了xKV，这是一种简单的后训练方法，它对分组层的KV缓存应用奇异值分解（SVD）。xKV将多个层的KV缓存合并到一个共享的低秩子空间中，显著减少了KV缓存的大小。通过在RULER长上下文基准上广泛评估常用的LLMs（如Llama-3.1和Qwen2.5），xKV比最先进的层间技术实现了高达6.8倍更高的压缩率，并提升了2.7%的准确性。此外，xKV与新兴的多头潜在注意力（MLA）（如DeepSeek-Coder-V2）兼容，在编码任务中获得了3倍的压缩率且没有性能下降。这些结果凸显了xKV在解决长上下文LLM推理内存瓶颈方面的强大能力和通用性。我们的代码已公开发布在：https://github.com/abdelfattah-lab/xKV。|
|**2025-03-24**|**AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration**|Zhexuan Wang et.al.|[2503.18891](http://arxiv.org/abs/2503.18891)|**[link](https://github.com/wangzx1219/agentdropout)**|多智能体系统（MAS）基于大型语言模型（LLMs）在协同问题解决方面展现了巨大的潜力，但它们仍然面临通信效率低下和任务性能次优的重大挑战，因此精心设计智能体的通信拓扑结构显得尤为重要。受管理理论中团队角色动态调整的启发，我们提出了AgentDropout方法，该方法通过优化通信图的邻接矩阵来识别冗余智能体及其在不同通信轮次中的通信，并将其消除以提升标记效率和任务性能。与最先进的方法相比，AgentDropout实现了平均21.6%的提示标记消耗减少和18.4%的完成标记消耗减少，同时任务性能提高了1.14。此外，扩展实验表明AgentDropout在领域迁移性和结构鲁棒性方面取得了显著成果，展示了其可靠性和有效性。我们的代码已开源，地址为https://github.com/wangzx1219/AgentDropout。|
|**2025-03-24**|**Toward building next-generation Geocoding systems: a systematic review**|Zhengcong Yin et.al.|[2503.18888](http://arxiv.org/abs/2503.18888)|null|地理编码系统在科学研究的空间分析以及日常生活中基于位置的服务中被广泛使用。数据质量对后续过程和应用有显著影响，这凸显了下一代系统的需求。为响应这一需求，本文首先检查了地理编码系统在不同场景下输入和输出的演变需求。然后通过将其分解为关键功能组件，详细分析了构建此类系统的现有方法，涵盖了从传统的基于规则的方法到信息检索、自然语言处理和大型语言模型等先进技术。最后，我们根据近期技术进步指出了改进下一代地理编码系统的机遇。|
|**2025-03-24**|**I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders**|Andrey Galichin et.al.|[2503.18878](http://arxiv.org/abs/2503.18878)|**[link](https://github.com/airi-institute/sae-reasoning)**|大型语言模型（LLMs）在自然语言处理方面取得了显著的成就。近期的发展催生了一类新的推理LLMs，例如开源的DeepSeek-R1通过整合深度思考和复杂推理达到了最先进的性能。尽管这些能力令人印象深刻，但这类模型内部的推理机制仍未被探索。在这项工作中，我们采用稀疏自编码器（SAEs）——一种学习神经网络潜在表示的稀疏分解以提取可解释特征的方法——来识别驱动推理的特征。首先，我们提出了一种从SAE表示中提取候选“推理特征”的方法。我们通过实证分析和可解释性方法验证了这些特征，展示了它们与模型推理能力之间的直接相关性。至关重要的是，我们证明了系统地调整这些特征能够增强推理性能，从而提供了对LLMs推理机制的第一个机械性解释。代码可在https://github.com/AIRI-Institute/SAE-Reasoning获取|
|**2025-03-24**|**Reimagining Memory Access for LLM Inference: Compression-Aware Memory Controller Design**|Rui Xie et.al.|[2503.18869](http://arxiv.org/abs/2503.18869)|null|大型语言模型（LLM）推理的效率通常受到内存带宽和容量需求的制约。现有的技术如剪枝、量化以及专家混合/深度分割等方法通过略微降低推理质量来减少内存容量和带宽的消耗。本文提出了一种设计解决方案，进一步缓解了内存瓶颈问题，通过增强AI加速器上的片上内存控制器来实现两个主要目标：（1）通过无损块压缩（例如LZ4和ZSTD）对模型权重和键值（KV）缓存进行压缩，从而显著减少内存容量和带宽使用，同时不牺牲推理质量；（2）通过上下文相关的动态量化使内存带宽和能耗能够按比例缩放。这些目标是通过在片上内存控制器中配备机制以提高权重和KV缓存的细粒度位级可访问性和可压缩性，并通过与LLM相关的内存布局和表示配置实现的。实验结果表明，在公开可用的LLM上，该方法实现了模型权重25.2%的内存占用减少和KV缓存46.9%的内存占用减少。此外，我们的硬件原型在4 GHz、32通道（7 nm工艺）下达到了8 TB/s的吞吐量，且面积开销较小（低于3.8 mm²），这证明了LLM感知内存控制作为高效大规模推理关键手段的可行性。|
|**2025-03-21**|**Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique**|Yansi Li et.al.|[2503.17363](http://arxiv.org/abs/2503.17363)|null|增强大型语言模型（LLMs）的推理能力，特别是针对需要多步逻辑推理的复杂任务，仍然是一个重要的挑战。传统的方法在推理时使用来自过程奖励模型的标量奖励信号来评估候选推理步骤，但这些标量奖励缺乏执行每个步骤所需的细微定性信息。在这篇论文中，我们提出了一种新的推理时间扩展方法——逐步自然语言自批评（PANEL），它利用自生成的自然语言评论作为反馈来指导步骤级搜索过程。通过为每个候选推理步骤生成丰富的人类可读评论，PANEL 保留了关键的定性信息，从而在推理过程中实现更好的决策。这种方法绕过了对特定任务验证器的需求及其相关的训练开销，使其广泛适用于各种任务。在具有挑战性的推理基准测试，包括 AIME 和 GPQA 上的实验结果表明，PANEL 显著提高了推理性能，优于传统的基于标量奖励的方法。我们的代码可在 https://github.com/puddingyeah/PANEL 获取，以支持并鼓励该领域的进一步研究。|
|**2025-03-21**|**OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement**|Yihe Deng et.al.|[2503.17352](http://arxiv.org/abs/2503.17352)|**[link](https://github.com/yihedeng9/openvlthinker)**|近期DeepSeek-R1的进展表明通过可验证奖励的强化学习可以实现大型语言模型（LLMs）复杂的推理能力，包括自我验证和自我修正等高级行为，并显著提升了在挑战性任务如AIME上的表现。受此启发，本研究探讨是否可以在大型视觉语言模型（LVLMs）中成功整合类似的推理能力，并评估其对多模态推理任务的影响。我们采用了一种迭代方法，首先利用轻量级训练数据进行监督微调（SFT），随后通过强化学习（RL）进一步提升模型泛化能力。最初的推理能力是从纯文本R1模型中蒸馏而来的，通过使用来自多样化视觉数据集的高质量图像描述生成推理步骤。随后，迭代的RL训练进一步增强了推理技能，每轮迭代的RL改进模型生成了用于下一轮的精炼SFT数据集。这一迭代过程产生了OpenVLThinker，这是一种在具有挑战性的基准测试如MathVista、MathVerse和MathVision上表现出持续改善的推理性能的LVLM，展示了我们策略在稳健视觉语言推理方面的潜力。代码、模型和数据存储在https://github.com/yihedeng9/OpenVLThinker。|
|**2025-03-21**|**Capturing Individual Human Preferences with Reward Features**|André Barreto et.al.|[2503.17338](http://arxiv.org/abs/2503.17338)|null|强化学习从人类反馈通常使用不区分人的奖励模型来建模偏好。我们认为在可能存在较大分歧的背景下，比如训练大型语言模型时，这可能不是一个好的设计选择。我们提出了一种方法，专门针对个人或群体调整奖励模型。我们的方法基于这样一个观察：个体偏好可以被捕捉为一组通用奖励特征的线性组合。我们展示了如何学习这些特征，并随后利用它们快速适应特定个体，即使其偏好在训练数据中未得到反映。我们通过大型语言模型的实验比较了所提出的架构与非自适应奖励模型以及包括上下文自适应模型在内的其他自适应对手，结果显示，在训练数据存在多少分歧的情况下，我们的模型要么显著优于基线模型，要么以更简单的架构和更稳定的训练达到与之相当的性能|
|**2025-03-21**|**Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs**|Reem Gody et.al.|[2503.17336](http://arxiv.org/abs/2503.17336)|null|大型语言模型（LLMs）在会话式人工智能方面展示了显著的能力，使开放式域响应的聊天机器人成为可能，并且在处理会话方面表现出了先进的能力，例如总结、意图分类和见解生成。然而，这些模型资源密集型，需要大量的内存和计算能力。为了解决这个问题，我们提出了一种具有成本效益的解决方案，该方案针对目标下游应用筛选出感兴趣的会话片段以供LLMs处理，而不是处理每个片段。在这项工作中，我们介绍了一种创新的方法，该方法利用来自LLMs的知识蒸馏来开发一个多意图的过滤器，用于多参与者对话，旨在优化计算能力受限的环境。我们的方法结合了不同的策略来创建一个多样化的多参与者对话数据集，并用目标意图对该数据集进行注释，然后用于微调MobileBERT模型以实现多标签意图分类。该模型在效率和性能之间实现了平衡，能够有效地根据其意图过滤会话片段。通过仅将相关片段传递给LLM进行进一步处理，我们的方法显著降低了整体运营成本，如我们的实验所证明的那样。|
|**2025-03-21**|**CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities**|Yuxuan Zhu et.al.|[2503.17332](http://arxiv.org/abs/2503.17332)|**[link](https://github.com/uiuc-kang-lab/cve-bench)**|**大型语言模型（LLM）代理能够自主开展网络攻击，对现有应用程序构成了重大威胁。这种日益增长的风险凸显了建立现实世界基准以评估LLM代理利用Web应用程序漏洞能力的紧迫性。然而，现有的基准测试要么局限于抽象的夺旗竞赛，要么缺乏全面覆盖。构建一个针对真实世界漏洞的基准需要既具备专业技能来重现漏洞利用，又采用系统方法来评估不可预测的威胁。为了解决这一挑战，我们引入了CVE-Bench，这是一个基于高危常见漏洞和暴露（CVE）的真实网络安全基准。在CVE-Bench中，我们设计了一个沙盒框架，使LLM代理能够在模拟真实世界条件的情景下利用易受攻击的Web应用程序，并提供对其漏洞利用的有效评估。我们的评估表明，最先进的代理框架可以解决高达13%的漏洞。**|
|**2025-03-21**|**LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language**|Kun Chu et.al.|[2503.17309](http://arxiv.org/abs/2503.17309)|**[link](https://github.com/kchu/llm-map)**|双机械手机器人操作提供了显著的灵活性，但也带来了固有的挑战，即在空间和时间上协调两只手的复杂性。现有的研究大多集中在实现人类水平的操作技能，但对长时域任务规划的关注较少。尽管大型语言模型（LLMs）因其出色的上下文学习和零样本生成能力而被应用于各种机器人系统以促进任务规划，但在复杂的机器人任务中，它们仍然存在长时域推理错误和幻觉的问题，缺乏逻辑正确性的保证。先前的工作如LLM+P通过符号规划扩展了LLMs的能力，然而这些方法尚未成功应用于双机械手机器人。双机械手操作带来了新的挑战，不仅需要有效的任务分解还需要高效的任务分配。为了解决这些问题，本文引入了LLM+MAP，这是一种集成LLM推理与多智能体规划的双机械手规划框架，实现了有效且高效的双机械手任务规划。我们在多种不同复杂度的长时域操作任务上进行了模拟实验。我们的方法使用GPT-4o作为后端，并将其性能与直接由GPT-4o、V3以及其他近期强大的推理模型o1和R1生成的计划进行比较。通过分析规划时间、成功率、组间差异以及规划步骤减少率等指标，我们展示了LLM+MAP的优越性能，同时为机器人推理提供了见解。代码可在https://github.com/Kchu/LLM-MAP获取。|
|**2025-03-21**|**Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests**|John Naulty et.al.|[2503.17302](http://arxiv.org/abs/2503.17302)|null|随着软件系统的复杂性不断增加，在开发过程中确保安全性面临着重大挑战。传统的手动代码审计通常成本高昂、耗时且不适合快节奏的工作流程，而自动化工具往往存在较高的误报率，限制了其可靠性。为了解决这些问题，我们引入了Bugdar，这是一种集成到GitHub拉取请求中的AI辅助代码审查系统，能够提供近乎实时的上下文感知漏洞分析。Bugdar利用可微调的大语言模型（LLMs）和检索增强生成（RAGs）技术，为每个代码库提供与特定项目需求和开发者实践相一致的操作性反馈。支持多种编程语言，包括Solidity、Move、Rust和Python，Bugdar展示了卓越的效率，平均处理一个拉取请求的时间为56.4秒或每秒处理30行代码。这比手动审查要快得多，后者可能需要数小时来完成一个拉取请求。通过促进一种主动的安全编码方法，Bugdar减少了对人工审查的依赖，加速了开发周期，并在不牺牲生产力的情况下增强了软件系统的安全态势。|
|**2025-03-21**|**CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement**|Gaifan Zhang et.al.|[2503.17279](http://arxiv.org/abs/2503.17279)|null|句子的含义往往依赖于其出现的上下文。尽管句子嵌入方法取得了进展，但仍不清楚如何最好地根据上下文修改句子嵌入。为了解决这个问题，我们提出了条件感知句子嵌入（CASE），这是一种高效且准确的方法，用于在给定条件下创建句子嵌入。首先，CASE 使用大规模语言模型（LLM）为条件创建嵌入，在池化过程中句子会影响对条件中的标记计算的注意力分数。接下来，学习一个监督下的非线性投影以降低基于LLM的文本嵌入的维度。我们表明CASE 在现有的标准基准数据集上显著优于先前提出的条件语义文本相似度（C-STS）方法。我们发现从条件嵌入中减去嵌入一致地提高了基于LLM的文本嵌入的C-STS性能。此外，我们提出了一种监督降维方法，不仅降低了LLM基嵌入的维度，还显著提升了它们的性能。|
|**2025-03-21**|**SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging**|Aladin Djuhera et.al.|[2503.17239](http://arxiv.org/abs/2503.17239)|null|微调大型语言模型（LLMs）在下游任务时可能会无意中损害其安全性，即使是对良性微调数据集也是如此。我们通过提出SafeMERGE来解决这一挑战，SafeMERGE是一种在微调后阶段的框架，能够在保持任务效用的同时保留安全性。它通过选择性地仅在偏离安全行为时合并微调和安全性对齐的模型层，这一过程使用余弦相似性标准进行测量。我们在GSM8K和PubMedQA任务上评估了SafeMERGE，涉及Llama-2-7B-Chat和Qwen-2-7B-Instruct模型，并探索了不同的合并策略。结果表明，SafeMERGE相较于其他微调和微调后阶段的方法能够持续减少有害输出，同时不会显著牺牲性能，有时甚至还能提升性能。这些结果表明，我们的选择性、子空间引导和逐层合并方法为防止微调LLMs时无意中丧失安全性提供了一种有效的保障，同时在性能上优于更简单的微调后阶段防御方法。|
|**2025-03-21**|**FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs**|Albert Sawczyn et.al.|[2503.17229](http://arxiv.org/abs/2503.17229)|null|大型语言模型（LLMs）经常生成幻觉内容，这对需要高度事实准确性的应用构成了重大挑战。虽然现有的幻觉检测方法通常在句子级别或段落级别上运行，但我们提出了FactSelfCheck，这是一种新颖的黑盒采样方法，能够实现细粒度的事实级别检测。我们的方法将文本表示为知识图谱，其中包含形式为三元组的事实。通过分析多个LLM响应中的事实一致性，我们计算出细粒度的幻觉分数，而无需外部资源或训练数据。我们的评估表明，FactSelfCheck的表现与领先的采样方法具有竞争力，同时提供了更详细的见解。最值得注意的是，我们的事实级方法显著提高了幻觉修正能力，在事实内容方面比基线提高了35%，而句子级SelfCheckGPT仅提高了8%。这种细粒度的检测使我们能够更精确地识别和修正幻觉内容。|
|**2025-03-20**|**Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models**|Yang Sui et.al.|[2503.16419](http://arxiv.org/abs/2503.16419)|**[link](https://github.com/eclipsess/awesome-efficient-reasoning-llms)**|大型语言模型（LLMs）在复杂任务上展现了卓越的能力。近期，大型推理模型（LRMs）如OpenAI o1和DeepSeek-R1在数学和编程等系统性2推理领域通过监督微调（SFT）和强化学习（RL）技术提升了链式思考（CoT）推理的性能。然而，尽管较长的CoT推理序列提高了表现，它们也带来了显著的计算开销，这种现象被称为“过度推理”。本文提供了首份系统性调查，旨在系统地研究和探索实现高效推理在LLMs中的最新进展。总体而言，基于LLMs的内在机制，我们将现有工作分为几个关键方向：（1）基于模型的高效推理，考虑将完整的推理模型优化为更简洁的推理模型或直接训练高效的推理模型；（2）基于推理输出的高效推理，旨在推理过程中动态减少推理步骤和长度；（3）基于输入提示的高效推理，寻求根据输入提示的特性如难度或长度控制来提升推理效率。此外，我们介绍了用于训练推理模型的有效数据使用，探讨了小规模语言模型的推理能力，并讨论了评估方法和基准测试。|
|**2025-03-20**|**The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination**|Yifan Sun et.al.|[2503.16402](http://arxiv.org/abs/2503.16402)|**[link](https://github.com/astral-group/bdc_mitigation_assessment)**|基准数据污染（BDC）——即将基准测试样本包含在训练集中——在大型语言模型（LLM）评估中引发了越来越多的担忧，导致性能评估估计虚高并削弱了评估的可靠性。为了解决这个问题，研究人员提出了各种缓解策略来更新现有的基准，包括修改原始问题或基于它们生成新的问题。然而，对这些缓解策略的有效性进行严格检验的工作仍然不足。在这篇论文中，我们设计了一个系统且受控的流程以及两个新颖的指标——保真度和污染抗性——以提供对现有BDC缓解策略的细致全面的评估。先前的评估方法如准确率下降和准确率匹配仅关注聚合准确率，往往导致不完整或误导性的结论。我们的指标通过强调问题级别的评估结果匹配解决了这一局限性。广泛的实验涉及10个LLM、5个基准、20种BDC缓解策略以及两种污染场景表明，在所有基准上没有任何现有策略显著优于空白情况（即没有基准更新），并且没有一种策略能够在保真度和污染抗性之间有效平衡。这些发现突显了设计更有效的BDC缓解策略的紧迫需求。我们的代码存储库可在https://github.com/ASTRAL-Group/BDC_mitigation_assessment获取。|
|**2025-03-20**|**Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them**|Guanyu Chen et.al.|[2503.16401](http://arxiv.org/abs/2503.16401)|null|大型语言模型（LLMs）和视觉-语言模型（VLMs）能够在各种场景下执行多种推理任务，但它们是否真正参与了任务抽象和基于规则的推理，还是仅仅依赖于记忆和模式匹配？为了解答这个问题，我们提出了一种新的实验方法——误导性微调（Misleading Fine-Tuning, MisFT），以检查LLMs/VLMs是否进行抽象推理。具体来说，通过构建一个与正确运算原则相矛盾的数学表达式数据集，我们将模型微调为学习这些矛盾的规则，并评估其在不同测试域上的泛化能力。通过一系列实验，我们发现当前的LLMs/VLMs能够有效地应用矛盾规则来解决实际的数学应用题和图像表示的数学表达式，这表明模型内部存在一种在推理之前进行抽象的机制。|
|**2025-03-20**|**Deconstructing Long Chain-of-Thought: A Structured Reasoning Optimization Framework for Long CoT Distillation**|Yijia Luo et.al.|[2503.16385](http://arxiv.org/abs/2503.16385)|**[link](https://github.com/elena-luo/SODE)**|近期大型语言模型（LLMs）在长链-of-思（CoT）推理方面展现出显著的推理能力。R1蒸馏方案作为一种有前景的方法，用于训练具有增强推理能力的成本效益模型。然而，其有效性背后的机制尚不清楚。本研究探讨了蒸馏数据的通用性，并确定了使LLM蒸馏中长链推理能力高效转移的关键组件。我们的发现表明，从像Qwen-QwQ这样的教师模型进行长CoT推理蒸馏的有效性在非同构模型上显著下降，挑战了当前蒸馏方法假设的通用性。为了更深入地了解长CoT推理的结构和模式，我们提出了DLCoT（解构长链-of-思），这是一种蒸馏数据增强框架。DLCoT由三个关键步骤组成：（1）数据分割以分解复杂的长CoT结构，（2）通过消除无法解决和冗余的解决方案进行简化，以及（3）优化中间错误状态。我们的方法显著提高了模型性能和令牌效率，促进了高性能LLMs的发展。|
|**2025-03-20**|**LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial Images**|Leyang Wang et.al.|[2503.16376](http://arxiv.org/abs/2503.16376)|null|现代机器学习的成功，尤其是在面部翻译网络中的成功，高度依赖于高质量配对的大规模数据集的可用性。然而，获取足够的数据通常具有挑战性和高成本。受扩散模型在高质量图像合成方面的近期成功以及大型语言模型（LLMs）的进步的启发，我们提出了一个名为LLM辅助配对图像生成（LaPIG）的新框架。该框架利用LLMs生成的描述来构建全面且高质量的可见光和热成像配对图像。我们的方法包括三个部分：使用ArcFace嵌入的可见图像合成、使用潜在扩散模型（LDMs）的热成像转换以及与LLMs的描述生成。我们的方法不仅生成了多视角的配对可见光和热成像图以增加数据多样性，而且在保持身份信息的同时生成了高质量的配对数据。我们在公共数据集上评估了我们的方法，并将其与现有方法进行比较，展示了LaPIG的优越性。|
|**2025-03-20**|**CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners**|Yunzhi Yao et.al.|[2503.16356](http://arxiv.org/abs/2503.16356)|**[link](https://github.com/zjunlp/cake)**|知识编辑（KE）使大型语言模型（LLMs）能够修改过时或错误的信息。虽然现有的KE方法可以更新孤立的事实，但它们在将这些更新推广到依赖于修改后知识的多跳推理任务上存在困难。通过对推理电路——LLMs用于基于知识推断的神经通路——进行分析，我们观察到当前局部化的KE方法，如MEMIT和WISE，由于仅编辑单个或少数模型层，难以有效地将更新后的信息整合到这些推理路径中。为了解决这一局限性，我们提出了CaKE（Circuit-aware Knowledge Editing），一种新的方法，它能够更有效地将更新后的知识整合到LLMs中。CaKE利用由我们的电路分析指导的战略性策划的数据，强制模型使用修改后的知识，促使模型为新整合的知识发展适当的推理电路。实验结果表明，CaKE能够在相关推理任务中实现更准确和一致地使用更新后的知识，在MQuAKE数据集上的多跳推理准确性平均提高了20%，优于现有的KE方法。我们在https://github.com/zjunlp/CaKE发布了代码和数据。|
|**2025-03-20**|**LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates**|Ying Shen et.al.|[2503.16334](http://arxiv.org/abs/2503.16334)|null|最近的研究发现，Transformer基的大语言模型（LLM）的大部分知识被编码在其前馈（FFN）层中，每个FFN层可以解释为子更新的总和，每个子更新对应于FFN值参数矩阵中的加权列向量，这些向量通常编码人类可解释的概念。基于此，我们假设通过根据子更新对输入或目标输出风格的相关性调整其贡献，可以进一步增强和控制模型的性能和行为，并提出了一种新颖且高效的方法LLMBRACES。该方法计算与FFN层中值向量相关的相关性分数，并利用这些分数动态调整子更新的贡献。通过优化子更新的贡献，LLMBRACES改进了预测过程，从而产生更准确可靠的输出，就像“支撑”一样提供支持和稳定性。此外，LLMBRACES还可以扩展以支持对生成特征的条件控制，例如情感，从而实现对LLM输出的细粒度引导。在包括Qwen2.5-1.5B、Llama2-7B和Llama3-8B在内的各种LLM上进行的大量实验证明，LLMBRACES在微调和零样本设置下均优于基线方法，同时需要显著更少的可调参数，最多比LoRA减少75%。此外，LLMBRACES在情感控制生成和毒性降低方面表现出色，突显了其在灵活可控文本生成中的潜力。|
|**2025-03-20**|**OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence**|Long Yuan et.al.|[2503.16326](http://arxiv.org/abs/2503.16326)|null|多模态大型语言模型（LLMs）的快速发展开启了人工智能的新领域，使整合多种大规模数据类型如文本、图像和空间信息成为可能。本文探讨了多模态LLMs在地理空间人工智能（GeoAI）中的潜力，该领域利用空间数据解决地理语义、健康地理学、城市地理学、城市感知以及遥感等领域的问题。我们提出了一种专为地理空间应用设计的多模态LLM（OmniGeo），能够处理和分析异构数据源，包括卫星图像、地理空间元数据和文本描述。通过结合自然语言理解和空间推理的优势，我们的模型提升了指令跟随能力和GeoAI系统的准确性。结果表明，与特定任务模型和现有LLMs相比，我们的模型在各种地理空间任务上表现出色，在多模态性质下表现优异，并在零样本地理空间任务上取得了具有竞争力的结果。代码将在发表后发布。|
|**2025-03-20**|**Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1**|Peiran Gu et.al.|[2503.16304](http://arxiv.org/abs/2503.16304)|null|近年来，大型语言模型（LLMs）在自然语言处理领域取得了显著突破，并逐渐应用于人文与社会科学的研究。由于其强大的文本理解、生成和推理能力，LLMs在人文与社会科学领域具有广泛的应用价值。在人文与社会科学的研究中，LLMs能够分析大规模文本数据并进行推断。本文从低资源语言翻译、教育问答、高等教育学生写作提升、逻辑推理、教育测量与心理测量、公共卫生政策分析以及艺术教育七个方面分析了大型语言模型DeepSeek-R1的表现。我们还将DeepSeek-R1的回答与o1-preview的回答进行了比较。DeepSeek-R1在人文与社会科学领域表现出色，大多数问题的回答准确且合乎逻辑，能够提供合理的分析过程和解释。与o1-preview相比，它能自动生成推理过程并提供更详细的解释，适合初学者或需要深入了解该知识的人士使用，而o1-preview则更适合快速阅读。通过分析发现，LLM在人文与社会科学领域具有广阔的应用潜力，显示出提高文本分析效率、语言交流等领域的巨大优势。LLM强大的语言理解和生成能力使其能够深入探索人文与社会科学领域的复杂问题，并为学术研究和实际应用提供创新工具。|
|**2025-03-21**|**Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens**|Shuqi Lu et.al.|[2503.16278](http://arxiv.org/abs/2503.16278)|**[link](https://github.com/dptech-corp/uni-3dar)**|近年来，大型语言模型及其多模态扩展通过自回归下一词预测展示了生成和理解统一的有效性。然而，尽管三维结构生成与理解（3D GU）任务在科学人工智能中扮演着关键角色，但这些任务大多独立发展，自回归方法仍未得到充分探索。为弥合这一差距，我们引入了Uni-3DAR，这是一种通过自回归预测无缝整合各种3D GU任务的统一框架。在其核心，Uni-3DAR采用了一种新颖的分层标记化方法，利用八叉树压缩三维空间，利用了三维结构的固有稀疏性。然后，它对微观三维结构的精细结构细节进行进一步标记化，捕捉原子类型和精确的空间坐标等关键属性。我们还提出了两种优化措施以提升效率和效果。第一种是两级子树压缩策略，可将八叉树标记序列减少多达8倍。第二种是针对动态变化标记位置定制的掩码下一词预测机制，显著提升了模型性能。通过结合这些策略，Uni-3DAR成功地在一个自回归框架中统一了多种多样化的3D GU任务。广泛的实验验证了其在多个微观3D GU任务中的有效性和多功能性，包括分子、蛋白质、聚合物和晶体。值得注意的是，Uni-3DAR相对于先前最先进的扩散模型实现了高达256%的相对改进，并且推理速度提高了21.8倍。代码已公开发布于https://github.com/dptech-corp/Uni-3DAR。|
|**2025-03-19**|**SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks**|Yifei Zhou et.al.|[2503.15478](http://arxiv.org/abs/2503.15478)|**[link](https://github.com/facebookresearch/sweet_rl)**|大型语言模型（LLM）代理在现实世界任务中需要进行多轮交互。然而，现有的用于优化LLM代理的多轮强化学习算法未能在多轮中有效分配信用同时利用LLM的泛化能力，如何开发此类算法仍不清楚。为研究此问题，我们首先引入了一个新的基准ColBench，在该基准中，LLM代理与人类合作者进行多轮交互以解决后端编程和前端设计中的现实任务。基于此基准，我们提出了一种新颖的RL算法SWEET-RL（带有从训练时信息的Step-Wise评估的RL），该算法使用精心设计的优化目标来训练一个具有访问额外训练时信息的批评模型。该批评模型为改进策略模型提供每步奖励。我们的实验表明，SWEET-RL在ColBench上的成功率和胜率绝对提高了6%，使Llama-3.1-8B能够匹配或超过GPT4-o在实际协作内容创建中的性能。|
|**2025-03-19**|**Cube: A Roblox View of 3D Intelligence**|Foundation AI Team et.al.|[2503.15475](http://arxiv.org/abs/2503.15475)|**[link](https://github.com/roblox/cube)**|基础模型经过大量数据训练后，在文本、图像、音频和视频等领域展示了出色的推理和生成能力。我们罗布乐思的目标是构建一个这样的三维智能基础模型，支持开发者在罗布乐思体验的各个方面进行创作，从生成三维物体和场景到为角色绑定动画以及编写描述对象行为的程序脚本。我们讨论了构建这样一个模型的三个关键设计要求，然后介绍了我们迈向这一目标的第一步。我们认为三维几何形状将是核心数据类型之一，并描述了我们的三维形状标记化解决方案。我们展示了如何使用这种标记化方案应用于文本到形状生成、形状到文本生成以及文本到场景生成等应用。我们还演示了这些应用如何与现有的大型语言模型（LLMs）协作以执行场景分析和推理。最后，我们通过讨论概述了构建统一三维智能基础模型的路径。|
|**2025-03-19**|**From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment**|Jia-Nan Li et.al.|[2503.15463](http://arxiv.org/abs/2503.15463)|**[link](https://github.com/jinaleejnl/alignx)**|大型语言模型（LLMs）的传统对齐方法通常采用一刀切的策略，假定人类偏好是统一的，这从根本上忽视了用户价值观和需求的多样性。本文介绍了一个全面的个性化对齐框架。我们构建了一个系统化的偏好空间，刻画了心理和行为维度，同时提供了多样化的角色表示来进行真实场景中的鲁棒偏好推断。在此基础上，我们推出了\textsc{AlignX}，这是一个超过130万条个性化偏好示例的大规模数据集，并开发了两种互补的对齐方法：一种是基于情境的对齐方法，直接以角色表示为条件；另一种是基于偏好桥梁的对齐方法，通过建模中间的偏好分布。广泛的实验表明，这两种方法在四个基准测试中比现有方法平均提高了17.06%的准确率，同时展现了强大的新偏好适应能力、对有限用户数据的鲁棒性以及精确的偏好可控性。这些结果验证了该框架的有效性，推动了真正面向用户的自适应AI系统的实现。|
|**2025-03-19**|**Visual Position Prompt for MLLM based Visual Grounding**|Wei Tang et.al.|[2503.15426](http://arxiv.org/abs/2503.15426)|**[link](https://github.com/waynetomas/vpp-llava)**|尽管多模态大型语言模型（MLLMs）在各种与图像相关的任务中表现出色，但在精确对齐图像中的坐标与空间信息方面仍面临挑战，尤其是在视觉定位等位置感知任务中。这一局限性主要源于两个关键因素。首先，MLLMs缺乏明确的空间参考，使得将文本描述与精确的图像位置关联起来变得困难。其次，它们的特征提取过程更倾向于优先处理全局上下文而非细粒度的空间细节，导致其定位能力较弱。为了解决这一问题，我们引入了VPP-LLaVA，这是一种配备了视觉位置提示（VPP）的MLLM，以提升其定位能力。VPP-LLaVA整合了两种互补机制。全局VPP通过叠加可学习的轴状嵌入到输入图像上，提供结构化的空间线索。局部VPP则通过结合位置感知查询，专注于细粒度的定位，这些建议表明可能的对象位置。我们还引入了一个VPP-SFT数据集，其中包含60万样本，将高质量的视觉定位数据整合成紧凑格式以实现高效的模型训练。在该数据集上进行训练并结合VPP后，模型在标准定位基准测试中取得了最先进的结果，尽管使用的训练样本数量较少（约60万），而其他MLLMs如MiniGPT-v2则依赖于更大规模的数据集（约2100万样本）。代码和VPP-SFT数据集将在https://github.com/WayneTomas/VPP-LLaVA发布。|
|**2025-03-19**|**Probing the topology of the space of tokens with structured prompts**|Michael Robinson et.al.|[2503.15421](http://arxiv.org/abs/2503.15421)|null|本文提出了一种通用且灵活的方法，用于提示大型语言模型（LLM）揭示其（隐藏的）标记输入嵌入直至同胚。此外，本文还为这种通用LLM提供了强有力的理论依据——数学证明，解释了为什么这种方法应该有效。通过这一方法，我们展示了其有效性，并成功恢复了Llemma-7B的标记子空间。本文的结果不仅适用于LLM，也适用于一般的非线性自回归过程。|
|**2025-03-19**|**EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models**|Yinan Liang et.al.|[2503.15369](http://arxiv.org/abs/2503.15369)|null|多模态大型语言模型在复杂推理任务中表现出色，但在部署过程中对资源受限设备提出了显著挑战。本文提出了一种针对大型视觉-语言模型的自动剪枝方法，以提升多模态推理的效率。传统方法依赖于原始模型的训练数据来为不同网络组件选择合适的剪枝比例，但对于大规模视觉-语言模型而言，这种方法因网络规模庞大的训练语料库而造成不可承受的搜索成本。相比之下，我们的方法仅利用少量样本即可搜索到所需的剪枝策略，通过最大化其在未知训练数据上的泛化能力同时保持模型准确性，从而实现了大型视觉-语言模型在准确性和效率之间的最佳权衡。具体来说，我们利用结构风险最小化原理来构建剪枝策略的泛化差距。基于任务性能和泛化能力，我们在给定的搜索空间内迭代搜索最优剪枝策略，并优化视觉投影器以扩展具有更高性能上限的搜索空间。我们在ScienceQA、Vizwiz、MM-vet和LLaVA-Bench等数据集上针对视觉问答任务进行了广泛的实验。使用仅64个样本进行剪枝策略搜索，EfficientLLaVA在ScienceQA上达到了83.05%的准确率，相较于密集型LLaVA-v1.5-7B模型实现了×1.8的速度提升。|
|**2025-03-19**|**SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation**|Thomas Pickard et.al.|[2503.15358](http://arxiv.org/abs/2503.15358)|null|习语表达在自然语言处理中 presents 独特挑战，因为它们的意义通常无法直接从构成词中推断。尽管大型语言模型（LLMs）近期取得了进展，但习语性仍然是稳健语义表示的重要障碍。我们提出了SemEval-2025任务1：AdMiRe（推进多模态习语性表示）的数据集和任务，该任务挑战社区评估和改进模型在多模态上下文中解释习语表达的能力，并涉及多种语言。参与者在两个子任务中竞争：根据图像与习语或字面意义的对齐程度进行排名，以及预测序列中的下一个图像。最有效的方法通过利用预训练的LLMs和视觉语言模型在混合专家设置中实现了人类水平的性能，使用了多个查询来弥补这些模型在习语性表示方面的弱点。|
|**2025-03-19**|**SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models**|I-Fan Lin et.al.|[2503.15351](http://arxiv.org/abs/2503.15351)|null|在本文中，我们提出了选择与池化结合大型语言模型的方法（SPILL），这是一种直观且具有领域适应性的意图聚类方法，无需微调。现有的基于嵌入的聚类方法依赖于少量标记示例或无监督微调以优化每个新数据集的结果，这使得它们在多个数据集上通用性较差。我们的目标是使现有嵌入器在不进行进一步微调的情况下对新的领域数据集更具通用性。受我们在采样和池化技术有效性方面的理论推导和模拟结果的启发，我们将聚类任务视为一个小规模的选择问题。该问题的良好解决方案与更好的聚类性能相关。因此，我们提出了一种两阶段方法：首先，对于每个话语（称为种子），我们使用现有嵌入器得出其嵌入。然后，我们应用距离度量来选择接近种子的候选者集合。由于嵌入器未针对新数据集进行优化，在第二阶段，我们使用大型语言模型（LLM）进一步从这些候选者中选择与种子具有相同意图的话语。最后，我们将这些选定的候选者与种子合并以获得种子的精炼嵌入。我们发现，我们的方法通常优于直接使用嵌入器，并且其结果与其它最先进的研究相当，即使那些使用更大模型并需要微调的研究也如此，这显示了其强大性和效率。我们的结果显示，我们的方法可以使现有嵌入器在没有额外微调的情况下得到进一步改进，使其更适应新的领域数据集。此外，将聚类任务视为小规模选择问题，为使用LLM根据用户目标定制聚类任务提供了潜力。|
|**2025-03-19**|**TruthLens:A Training-Free Paradigm for DeepFake Detection**|Ritabrata Chakraborty et.al.|[2503.15342](http://arxiv.org/abs/2503.15342)|null|合成图像的泛滥带来了识别和理解篡改视觉内容的重大挑战。当前的假图像检测方法主要依赖于专注于准确性的二元分类模型，往往忽视了可解释性，使用户无法清楚了解为什么一张图像被视为真实或伪造。为了解决这一问题，我们引入了TruthLens，这是一种全新的无需训练的框架，重新定义深度伪造检测为视觉问答（VQA）任务。TruthLens利用最先进的大型视觉语言模型（LVLMs）来观察和描述视觉伪影，并结合大型语言模型（LLMs）如GPT-4的推理能力来分析和汇总证据以做出明智决策。通过采用多模态方法，TruthLens无缝集成了视觉和语义推理，不仅可以对图像进行真实性分类，还可以提供可解释的决策依据。这种透明度增强了信任并提供了有关信号合成内容的伪影的宝贵见解。广泛的评估表明，TruthLens在具有挑战性的数据集上表现出色，不仅实现了高准确性，而且在保持强解释性方面表现突出。通过将深度伪造检测重新定义为一个推理驱动的过程，TruthLens确立了一个打击合成媒体的新范式，结合了前沿性能与可解释性以应对视觉虚假信息日益增长的威胁。|
|**2025-03-19**|**Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs**|Yuqi Zhu et.al.|[2503.15341](http://arxiv.org/abs/2503.15341)|null|链式思维（CoT）推理已被证明是提高大型语言模型（LLMs）在代码生成问题解决能力的有效技术。然而，现有的CoT方法往往表现出“过度思考”的倾向，即LLM在处理任务时倾向于始终应用推理策略，而未能充分考虑任务的实际复杂性。这导致LLM在相对简单的任务或问题上分配了过多的计算资源，甚至在答案已经显而易见的情况下也是如此。此外，这种过度思考可能导致LLM陷入错误的推理路径，从而产生不正确的代码生成结果。在本文中，我们引入了一种名为不确定性感知链式思维（UnCert-CoT）的方法，这是一种基于LLM的代码生成增强方法，通过引入一种不确定性感知的CoT推理机制，集中计算资源于LLM更容易出错的地方。我们提出了两种基于置信度的不确定性度量方法：基于熵的方法和基于概率差分的方法。当不确定性较高时，UnCert-CoT激活CoT解码以生成多个推理路径，并选择最有可能正确的代码作为最终结果。而在不确定性较低时，LLM直接生成代码。这种不确定性判断机制使LLM能够优先处理复杂的任务，同时避免在简单情况下进行不必要的步骤，从而提高了代码生成的整体效率和准确性。我们的实验结果显示，UnCert-CoT显著提升了在具有挑战性的基准测试MHPP（Mostly Hard Python Problems）上的代码生成准确性，在PassRate准确率方面最多提高了6.1%，特别是在传统LLM容易出错的情境下。|
|**2025-03-18**|**Aligning Multimodal LLM with Human Preference: A Survey**|Tao Yu et.al.|[2503.14504](http://arxiv.org/abs/2503.14504)|null|大型语言模型（LLMs）可以通过简单的提示处理各种通用任务，而无需特定任务的训练。多模态大型语言模型（MLLMs）建立在LLMs之上，在处理涉及视觉、听觉和文本数据的复杂任务方面表现出色。然而，与真实性、安全性、类一推理和与人类偏好对齐相关的关键问题仍未得到充分解决。这一差距促成了各种对齐算法的出现，每种算法针对不同的应用场景和优化目标。最近的研究表明，对齐算法是解决上述挑战的强大方法。在这篇论文中，我们旨在全面系统地回顾多模态大型语言模型的对齐算法。具体而言，我们探讨了四个方面：（1）对齐算法覆盖的应用场景，包括一般图像理解、多图像、视频和音频以及扩展的多模态应用；（2）构建对齐数据集的核心因素，包括数据来源、模型响应和偏好注释；（3）用于评估对齐算法的基准；（4）对对齐算法未来发展方向的讨论。这项工作旨在帮助研究人员整理当前领域的进展并激发更好的对齐方法。本文项目页面可在https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment获得。|
|**2025-03-18**|**Engineering Scientific Assistants using Interactive Structured Induction of Programs**|Shraddha Surana et.al.|[2503.14488](http://arxiv.org/abs/2503.14488)|null|我们感兴趣的是构建能够充当领域专家科学助手的软件，以加速解决需要紧急解决方案的复杂问题。本文的重点不是特定的科学问题，而是此类“科学加速器”的软件工程设计。最近“无代码”技术的发展似乎表明，科学家只需通过与大型语言模型（LLM）对话就能假设解决方案。然而，对于复杂的科学问题而言，鉴于当前LLM技术的状态，这似乎不太可能。但使用LLMs快速构建供领域专家使用的程序似乎是可行的，包括以自然语言表达的专家需求。我们提议设计一种交互式的“结构化”归纳编程形式，在这种形式下，软件工程师和LLM可以协作构建用于科学数据分析的“助手”。本文描述了一个名为iStrucInd的简单实现，它通过采用“双向可理解性”协议来实现软件工程师和LLM之间的交互。我们在两个不同的非平凡科学数据分析任务上测试了该工具。具体来说，我们将通过iStrucInd构建的系统与手动构建的系统以及低代码/无代码方法构建的系统在以下维度进行比较：(a) 程序性能；(b) 程序质量；和(c) 编程工作量。结果表明，iStrucInd使软件工程师能够更快地开发出更好的程序，这表明交互式结构化归纳可以在快速构建科学助手方面发挥有用的作用。|
|**2025-03-18**|**Gricean Norms as a Basis for Effective Collaboration**|Fardin Saad et.al.|[2503.14484](http://arxiv.org/abs/2503.14484)|**[link](https://github.com/fardinsaad/gricean-norms)**|有效的的人机协作不仅依赖于人工智能代理遵循明确指令的能力，还依赖其处理模糊、不完整、无效或无关交流的能力。格莱斯的会话和推理规范通过协调不明确的指令与合作原则来促进协作。我们提出了一种整合了格莱斯规范和认知框架（共同基础、相关性理论以及心智论）的规范框架，以应用于基于大型语言模型（LLM）的代理中。该规范框架采用格莱斯的数量、质量、关系和方式准则以及推理作为格莱斯规范来解释不清楚的指令。在这一框架下，我们引入了Lamoids，这些代理由GPT-4驱动并旨在与人类协作。为了评估格莱斯规范在人机协作中的影响，我们对两种Lamoids进行了测试：一种带有规范，另一种没有。在实验中，Lamoids与人类一起在一个网格世界（Doors, Keys, and Gems）中协作，通过解读清晰和不清晰的自然语言指令来实现共同目标。我们的结果表明，带有格莱斯规范的Lamoid在任务准确性上更高，并且生成的响应更清晰、更准确且更具上下文相关性。这种改进源于规范框架，它增强了代理的实用推理能力，促进了有效的人机协作，并使基于LLM的代理能够进行情境感知的沟通|
|**2025-03-18**|**Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM**|Xinyu Fang et.al.|[2503.14478](http://arxiv.org/abs/2503.14478)|**[link](https://github.com/open-compass/creation-mmbench)**|创造力是智力的基本方面之一，涉及在不同背景下生成新颖且适当解决方案的能力。虽然大型语言模型（LLMs）已在其创造性能力方面得到了广泛评估，但多模态大型语言模型（MLLMs）在此领域的评估仍基本未被探索。为了解决这一差距，我们引入了Creation-MMBench，这是一个多模态基准，专门设计用于评估MLLMs在基于图像的真实任务中的创造能力。该基准包含765个测试用例，涵盖51个细粒度任务。为了确保严格评估，我们为每个测试用例定义了实例特定的评估标准，指导对一般响应质量和视觉输入的事实一致性进行评估。实验结果表明，当前开源MLLMs在创意任务中显著低于专有模型的表现。此外，我们的分析表明，视觉微调可能会对基础LLM的创造性能力产生负面影响。Creation-MMBench为推进MLLM创造力提供了宝贵的见解，并为未来多模态生成智能的改进奠定了基础。完整数据和评估代码已发布在https://github.com/open-compass/Creation-MMBench。|
|**2025-03-18**|**EnvBench: A Benchmark for Automated Environment Setup**|Aleksandra Eliseeva et.al.|[2503.14443](http://arxiv.org/abs/2503.14443)|**[link](https://github.com/JetBrains-Research/EnvBench)**|**近年来大型语言模型（LLMs）的快速发展使得研究者能够专注于软件工程领域中的实际存储库级任务。在本文中，我们考虑了一个基础性任务——环境设置，即在系统上配置特定于存储库的开发环境的任务。现有的关于环境设置的研究提出了创新的代理策略，但它们的评估通常基于可能无法捕捉实践中遇到的所有配置挑战的小型数据集。为了解决这一差距，我们引入了一个全面的环境设置基准EnvBench。它包含了329个Python和665个JVM语言（Java、Kotlin）的存储库，并重点关注那些具有真实配置挑战的存储库，排除了可以通过简单的确定性脚本完全配置的项目。为了使基准进一步扩展并用于模型调优，我们实现了两种自动度量方法：一种是针对Python的静态分析检查缺失导入，另一种是针对JVM语言的编译检查。我们通过评估三种环境设置方法来展示该基准的应用，其中包括一个简单的零样本基线和两种代理工作流，使用了两个强大的LLM骨干模型GPT-4o和GPT-4o-mini。最佳方法成功配置了6.69%的Python存储库和29.47%的JVM存储库，表明EnvBench对于当前方法仍然具有挑战性。我们的基准套件可以在https://github.com/JetBrains-Research/EnvBench公开获取。数据集和实验轨迹可在https://jb.gg/envbench获得。**|
|**2025-03-18**|**LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers**|Nikhil Abhyankar et.al.|[2503.14434](http://arxiv.org/abs/2503.14434)|**[link](https://github.com/nikhilsab/llmfe)**|自动化特征工程在提升表格学习任务预测模型性能方面起着关键作用。传统的自动化特征工程技术受限于其对预定义转换的依赖以及固定的手动设计搜索空间，常常忽视了领域知识。最近，利用大型语言模型（LLMs）的进步使得在特征工程过程中整合领域知识成为可能。然而，现有的基于LLM的方法仅使用直接提示或完全依赖验证分数来进行特征选择，未能利用先前特征发现实验的见解或将特征生成与数据驱动的性能之间的有意义推理联系起来。为了解决这些挑战，我们提出了LLM-FE，这是一种新颖的框架，结合了进化搜索和LLMs的领域知识与推理能力，以自动生成表格学习任务的有效特征。LLM-FE将特征工程表述为程序搜索问题，其中LLMs迭代地提出新的特征变换程序，并通过数据驱动的反馈指导搜索过程。我们的结果显示，LLM-FE始终优于最先进的基线方法，在各种分类和回归基准测试中显著提升了表格预测模型的性能。|
|**2025-03-18**|**PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play**|Wei Fang et.al.|[2503.14432](http://arxiv.org/abs/2503.14432)|null|大型语言模型（LLMs）越来越多地与专业外部工具集成，但许多任务需要在文档有限或嘈杂的情况下进行零样本工具使用。现有的解决方案依赖于手动重写或标记数据进行验证，这使得它们在真正的零样本设置中不适用。为了解决这些挑战，我们提出了PLAY2PROMPT，这是一种自动框架，通过与每个工具的迭代试错过程系统地“玩”来探索其输入-输出行为。通过这一过程，PLAY2PROMPT完善了工具文档并生成了使用示例，而无需任何标记数据。这些示例不仅指导LLM推理，还用于进一步增强工具利用。广泛的现实任务实验表明，PLAY2PROMPT在开放和封闭模型上显著提高了零样本工具性能，提供了一种可扩展且有效的领域特定工具集成解决方案。|
|**2025-03-18**|**Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models**|Siwei Zhang et.al.|[2503.14411](http://arxiv.org/abs/2503.14411)|null|时序图神经网络（TGNNs）在时序图建模中表现出色。然而，现实中的时序图通常具有丰富的文本信息，形成了时序文本属性图（TTAGs）。这种动态文本语义与演化图结构的结合带来了更高的复杂性。现有的TGNNs静态地嵌入文本，并严重依赖于优先考虑结构信息的编码机制，忽视了文本语义的时间演化以及语义和结构之间至关重要的相互作用以实现协同增强。为了解决这些问题，我们提出了一个名为{Cross}的新框架，该框架无缝扩展了现有的TGNNs以进行TTAG建模。关键思想是利用先进的大语言模型（LLMs）提取文本空间中的动态语义，并生成统一语义和结构的表达。具体来说，我们在{Cross}框架中提出了一种时序语义提取器，它使LLMs能够提供节点演化上下文中文本邻居的时序语义理解，促进语义动力学。随后，我们引入了语义-结构协同编码器，它与上述提取器协作，通过同时考虑语义和结构信息并鼓励它们相互增强来合成有见地的表示。在四个公开数据集和一个实际工业数据集上的大量实验结果证明了{Cross}的有效性和鲁棒性。|
|**2025-03-18**|**Large Language Models for Virtual Human Gesture Selection**|Parisa Ghanad Torshizi et.al.|[2503.14408](http://arxiv.org/abs/2503.14408)|null|共演讲手势在面对面的人类互动中传达了广泛的意义并发挥了重要作用。这些手势显著影响了接收者的参与度、回忆、理解以及对发言者的态度。同样地，它们也影响了人类与具身虚拟代理之间的互动。因此，选择和动画化有意义的手势已成为设计这些代理的关键重点。然而，自动化这一手势选择过程面临重大挑战。先前的手势生成技术从完全自动化的数据驱动方法到更手动的方法各有不同，前者往往难以产生语境上有意义的手势，而后者需要专门的手势知识，耗时且缺乏通用性。在这篇论文中，我们利用大型语言模型的语义能力开发了一种手势选择方法，该方法可以建议有意义且适当的共演讲手势。首先，我们描述了如何将手势信息编码到GPT-4中。然后，我们进行了一项研究来评估替代提示方法在选择有意义且语境相关的手势以及将其适当地与共演讲话语对齐方面的能力。最后，我们详细说明并展示了这种方法如何在一个虚拟代理系统中实现，从而自动化选择和随后对手势进行动画处理以增强人机交互。|
|**2025-03-18**|**From "Hallucination" to "Suture": Insights from Language Philosophy to Enhance Large Language Models**|Qiantong Wang et.al.|[2503.14392](http://arxiv.org/abs/2503.14392)|null|本文通过语言哲学和精神分析的视角探讨大型语言模型（LLMs）中的幻觉现象。我们借鉴拉康的“符号链”和“缝合点”概念，提出了Anchor-RAG框架作为一种新颖的方法来减轻幻觉问题。与主要依赖试错实验、不断调整数学公式或资源密集型方法（这些方法更注重数量而非质量）不同，我们的方法回归到语言学的基本原理，以分析LLMs中幻觉的根本原因。基于坚实的理论基础，我们推导出的算法和模型不仅能够有效减少幻觉，还能提升LLM的性能并改善输出质量。本文旨在建立一个全面理解LLMs中幻觉现象的理论框架，并挑战当前普遍存在的“猜测和测试”方法和追求速度的竞争心态。我们希望为可解释的LLMs开辟新道路，提供对基于语言的人工智能系统内部运作的更深层次洞察。|
|**2025-03-17**|**MetaScale: Test-Time Scaling with Evolving Meta-Thoughts**|Qin Liu et.al.|[2503.13447](http://arxiv.org/abs/2503.13447)|null|一个对大型语言模型（LLMs）进行复杂推理的关键挑战是它们依赖于从训练数据中匹配推理模式，而不是主动选择最合适的认知策略来解决给定任务。现有的方法强加固定的认知结构，这些结构在特定任务上可以提高性能，但在多样化场景中的适应性较差。为了解决这一局限性，我们引入了METASCALE，这是一种基于元思维的测试时扩展框架——元思维是针对每个任务量身定制的自适应思考策略。METASCALE 初始化一组候选元思维，然后通过多臂老虎机算法结合上置信界选择迭代地选择和评估它们，并由奖励模型指导。为了进一步增强适应性，遗传算法进化高奖励的元思维，在时间推移中不断优化和扩展策略池。通过在推理时动态提出和优化元思维，METASCALE 提升了在广泛任务中的准确性和泛化能力。实验结果表明，MetaScale 在各种标准下始终优于标准推理方法，在Arena-Hard上GPT-4o的胜率提升了11%，在风格控制下比o1-mini高出0.9%。值得注意的是，METASCALE 随着采样预算的增加表现出更好的扩展性，并产生更结构化、专家级别的响应。|
|**2025-03-17**|**Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance**|Noah Y. Siegel et.al.|[2503.13445](http://arxiv.org/abs/2503.13445)|null|随着大型语言模型（LLMs）的能力不断提升，确保其自动生成的解释忠实于其内部决策过程对于安全和监管至关重要。在这项工作中，我们对来自8个家族的62个模型进行了全面的反事实忠实性分析，涵盖了预训练和指令调整的各种变体，并显著扩展了先前关于反事实测试的研究。我们引入了phi-CCT，这是相关反事实测试的一种简化变体，避免了对令牌概率的需求，同时解释了原始测试的大部分方差。我们的研究结果揭示了明显的规模趋势：较大的模型在我们的指标上始终表现出更高的忠实性。然而，当比较指令调整和人类模仿的解释时，我们发现观察到的忠实性差异通常可以归因于解释的简洁性，从而导致真正例/假正例帕累托前沿的转变。虽然指令调整和提示可能会影响这一权衡，但我们发现有限的证据表明它们从根本上扩展了与可比规模的预训练模型相比的解释忠实性前沿。我们的分析突显了指令调整、简洁性和忠实表示模型决策过程之间复杂的关系。|
|**2025-03-17**|**VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning**|Ye Liu et.al.|[2503.13444](http://arxiv.org/abs/2503.13444)|null|视频因其独特的时序维度，需要精确的时序关联理解，其中答案需直接链接到可解释的视觉证据。尽管大型语言模型在推理能力上取得了显著进展，但视频的多模态推理尤其是长时序推理仍处于探索阶段。在这项工作中，我们引入了VideoMind，这是一种新型的视频语言代理，旨在实现时序关联的视频理解。VideoMind包含两项关键创新：(i) 我们确定了视频时序推理的关键能力，并开发了一种基于角色的代理工作流程，包括规划器用于协调不同角色、定位器用于时序定位、验证器用于评估时序区间的准确性以及回答器用于问答。(ii) 为了高效整合这些多样化角色，我们提出了一个新的Chain-of-LoRA策略，通过轻量级的LoRA适配器实现无缝的角色切换，同时避免了多个模型带来的开销，从而平衡了效率和灵活性。在14个公开基准数据集上的广泛实验表明，我们的代理在各种视频理解任务上达到了最先进的性能，在3个基于接地视频问答的任务、6个视频时序接地任务和5个通用视频问答任务上均表现突出，彰显了其在推动视频代理和长时间序列推理方面的有效性。|
|**2025-03-17**|**xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference**|Maximilian Beck et.al.|[2503.13427](http://arxiv.org/abs/2503.13427)|**[link](https://github.com/nx-ai/xlstm-jax)**|近期在解决推理、数学和编码问题方面对大型语言模型（LLMs）的重大突破得益于在推理阶段投入了大量计算预算。因此，推理速度是LLM架构最重要的属性之一，对高效且快速的推理LLM的需求日益增长。最近基于xLSTM架构的LLMs作为一种强大的Transformer替代方案崭露头角，提供了与序列长度呈线性扩展的计算以及恒定内存使用率，这些都是高效推理高度 desirable 的特性。然而，此类基于xLSTM的LLMs尚未扩展到更大规模并针对推理速度和效率进行评估和比较。在这项工作中，我们推出了xLSTM 7B，这是一个70亿参数的LLM，结合了xLSTM的架构优势与针对快速高效推理的特定优化。我们的实验表明，xLSTM 7B在下游任务上的表现与其它类似规模的LLMs相当，同时在推理速度和效率上显著优于Llama和Mamba基线模型。这些结果确立了xLSTM 7B作为最快最高效的70亿参数LLM的地位，为需要大量测试时间计算的任务提供了解决方案。我们的工作突显了xLSTM作为方法构建基础架构的潜力，这些方法依赖于大量LLM推理的使用。我们的模型权重、模型代码和训练代码均为开源。|
|**2025-03-17**|**A Comprehensive Survey on Multi-Agent Cooperative Decision-Making: Scenarios, Approaches, Challenges and Perspectives**|Weiqiang Jin et.al.|[2503.13415](http://arxiv.org/abs/2503.13415)|null|随着人工智能的快速发展，在各种人机竞赛中，智能决策技术在复杂多智能体合作任务场景中已逐渐超越人类水平。多智能体协作决策涉及多个智能体协同工作以完成既定任务并实现特定目标。这些技术广泛适用于自动驾驶无人机导航灾害救援模拟军事对抗等现实场景。本文首先对用于多智能体协作决策的主要仿真环境和平台进行了全面综述。具体而言，我们从任务形式奖励分配以及所采用的基础技术等多个角度对这些仿真环境进行了深入分析。随后，我们对多智能体系统（MAS）的主流智能决策方法算法和模型进行了全面概述。这些方法大致可分为五类：基于规则（主要是模糊逻辑）、基于博弈论、基于进化算法、基于深度多智能体强化学习（MARL）以及基于大型语言模型（LLMs）推理的方法。鉴于MARL和LLMs方法相比传统的规则、博弈论和进化算法具有显著优势，本文重点讨论了利用MARL和LLMs技术的多智能体方法。我们深入探讨了这些方法的方法学分类、优点和缺点。此外，还详细阐述了多智能体协作决策的几个重要研究方向及其潜在挑战。|
|**2025-03-18**|**DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective**|Dengyun Peng et.al.|[2503.13413](http://arxiv.org/abs/2503.13413)|**[link](https://github.com/sfasfaffa/dlpo)**|大型语言模型（LLMs）在各种任务中取得了显著的成功，这主要得益于精心设计的提示。然而，设计和选择这些提示通常需要大量的人力投入，极大地限制了其可扩展性。为了缓解这一问题，最近的研究探索了自动提示优化作为一种有前景的解决方案。尽管如此，现有的方法仍然面临鲁棒性、效率和泛化能力方面的关键挑战。为系统地解决这些问题，我们首先进行了实证分析以识别当前基于反射的提示优化范式的局限性。在此基础上，我们提出了7种受传统深度学习范式启发的创新方法，称为提示优化的深度学习（DLPO），并将这些概念无缝集成到基于文本的梯度优化中。通过这些改进，我们逐步解决了上述挑战，并通过广泛的实验验证了我们的方法。我们希望本研究不仅为未来的研究提供了有价值的指导，还为提示优化中的挑战和潜在解决方案提供了全面的理解。我们的代码可在https://github.com/sfasfaffa/DLPO获取。|
|**2025-03-17**|**Using the Tools of Cognitive Science to Understand Large Language Models at Different Levels of Analysis**|Alexander Ku et.al.|[2503.13401](http://arxiv.org/abs/2503.13401)|null|现代人工智能系统如大型语言模型越来越强大但也越来越难以理解。我们认识到这一问题与历史上理解人类心智的困难具有相似性，因此认为认知科学领域的方法可以用于理解大型语言模型。我们基于Marr的三层次分析法提出了一种应用这些方法的框架。通过回顾与每个层次相关的认知科学研究技术并展示它们揭示大型语言模型行为和内部组织潜在见解的可能性，我们旨在为理解这些新型智能体提供工具包。|
|**2025-03-17**|**MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research**|James Burgess et.al.|[2503.13399](http://arxiv.org/abs/2503.13399)|**[link](https://github.com/jmhb0/microvqa)**|科学研究需要对多模态数据进行复杂的推理，这一挑战在生物学领域尤为突出。尽管最近在多模态大型语言模型（MLLMs）方面取得了进展，但现有的多模态推理基准测试仅达到大学水平的难度，而研究级别的基准测试则强调较低层次的感知，未能满足科学研究所需的复杂多模态推理需求。为填补这一空白，我们引入了MicroVQA，这是一个视觉-问答（VQA）基准测试，旨在评估研究工作流程中三个关键的推理能力：专家图像理解、假设生成和实验设计。MicroVQA由生物学家专家精心设计，包含1042个多项选择题（MCQs），涵盖了多种显微镜成像模式，确保VQA样本能够真实反映科学研究实践。在构建基准测试的过程中，我们发现标准的MCQ生成方法容易导致语言捷径问题，因此开发了一种新的两阶段管道：优化的语言模型提示将问答对结构化为MCQs；然后通过基于代理的“RefineBot”更新它们以消除捷径。对最先进的MLLMs进行基准测试显示，最佳性能可达53%；使用较小语言模型的模型表现仅略低于顶级模型，这表明语言推理比多模态推理更具挑战性；用科学文章进行微调可以提高性能。专家分析链式思维响应表明，最常见的错误是感知错误，其次是知识错误和过度概括错误。这些见解突显了多模态科学推理的挑战，表明MicroVQA是一个有价值的资源，有助于推动AI驱动的生物医学研究。MicroVQA可在https://huggingface.co/datasets/jmhb/microvqa获取，项目页面位于https://jmhb0.github.io/microvqa。|
|**2025-03-17**|**Cream of the Crop: Harvesting Rich, Scalable and Transferable Multi-Modal Data for Instruction Fine-Tuning**|Mengyao Lyu et.al.|[2503.13383](http://arxiv.org/abs/2503.13383)|null|预训练大语言模型（LLMs）在微调（SFT）阶段仅需极少的监督这一假设已被近期的数据整理与选择研究证实（Zhou等，2024）。然而，由于实验设置和验证协议的易变性，它们的稳定性和泛化能力受到影响，未能超越随机采样（Diddee & Ippolito，2024；Xia等，2024b）。基于LLMs构建的多模态LLMs（MLLMs），结合海量的令牌数量及数据来源的异质性，不仅提升了数据选择的重要性，也增加了其复杂性。为了以稳健高效的方式获取多模态指令数据，我们重新定义了质量度量的粒度，将其分解为14个与视觉-语言相关的技能，并引入多模态丰富评分器来评估每个数据候选对象的能力。为了促进多样性，在对齐阶段的目标启发下，我们将交互风格作为多样性指标，并使用多模态丰富样式器识别数据指令模式。通过这种方式，我们的多模态丰富评分器和样式器（mmSSR）确保以多样化形式向用户提供高质量信息。无需依赖嵌入式聚类或贪心采样，mmSSR能够高效扩展到数百万数据，并适应各种预算限制，支持定制化以获取通用或特定能力，同时实现无训练的跨领域数据整理。在超过10种实验设置中，通过14个多模态基准验证，我们展示了相较于随机采样、基线策略以及最先进的选择方法的一致改进，仅用260万数据中的30%即可达到99.1%的完全性能|
|**2025-03-17**|**Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning**|Hai-Long Sun et.al.|[2503.13360](http://arxiv.org/abs/2503.13360)|null|大型语言模型（LLMs）的最新进展显示了其推理能力的增强，从链式思维（CoT）提示发展到先进的产品导向解决方案如OpenAI o1。在我们重新实现该模型的过程中，注意到在需要视觉输入的多模态任务（例如几何问题）中，多模态LLMs（MLLMs）难以在整个推理过程中保持对视觉信息的关注，即MLLMs在推理过程中表现出对视觉信息注意力逐渐下降的问题，导致文本依赖的输出。为了解决这一问题，我们在长链推理过程中进行了图像输入的消融实验。具体来说，我们将推理过程中途截断，然后移除输入图像后重新完成推理过程，发现仅在MathVista测试集的hard子集上准确率下降约2%，这表明模型的文本输出主导了后续的推理过程。受此启发，我们提出了“伴随视觉条件”（TVC）策略，该方法将图像输入集中在关键推理阶段，并通过动态剪枝压缩冗余的视觉标记。这种方法帮助模型在整个推理过程中保持对视觉组件的关注。我们的方法在五个数学推理基准测试中平均表现达到最先进的性能（比之前的SOTA提高了3.4%），证明了TVC在提升多模态推理系统方面的有效性。|
|**2025-03-14**|**ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via Structural-Semantic Instruction Tuning**|Xinyi Wang et.al.|[2503.11617](http://arxiv.org/abs/2503.11617)|**[link](https://github.com/wxy3596/asma-tune)**|分析和理解汇编代码在诸如逆向工程等各种应用中至关重要。然而，汇编代码的信息密度低且缺乏明确的语法结构带来了巨大挑战。虽然基于掩码语言建模（MLM）的方法曾被率先提出，但由于其与自然语言交互的局限性而受到限制。尽管最近基于解码器的大规模语言模型（LLMs）的方法显著增强了语义表示能力，但它们仍然难以捕捉汇编代码中细微且稀疏的语义。本文提出了Assembly Augmented Tuning（ASMA-Tune），这是一种端到端的结构-语义指令微调框架。我们的方法通过投影模块协同编码器架构与解码器驱动的LLMs，以实现全面的代码理解。实验表明，ASMA-Tune在现有基准测试中表现优异，显著提升了汇编代码的理解能力和指令遵循能力。我们的模型和数据集公开于https://github.com/wxy3596/ASMA-Tune。|
|**2025-03-14**|**Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs using Semantic Space**|Zhiliang Chen et.al.|[2503.11586](http://arxiv.org/abs/2503.11586)|**[link](https://github.com/chenzhiliang94/convo-plan-scope)**|大型语言模型（LLMs）被用于聊天机器人或AI助手以与人类用户进行对话。在这些应用中，对话的质量（例如用户参与度、安全性）只能在对话结束时才能确切知道。为了最大化其期望质量，对话规划通过推理对话中的随机转换来选择每一轮的最佳LLM响应。现有的基于模拟的对话规划算法通常通过在每一回合进行大量LLM查询来模拟未来的对话以选择最佳响应。然而，这个过程非常耗时，因此在实时对话中不切实际。本文提出了一种名为SCOPE（Semantic space COnversation Planning with improved Efficiency）的新方法，该方法利用对话的密集语义表示来进行高效的对话规划。特别是，SCOPE对对话语义中的随机转换及其相关奖励进行建模，从而完全在语义空间中进行规划。这使得我们能够在每一轮对话中选择最佳的LLM响应，而无需额外的LLM查询进行模拟。结果表明，SCOPE比传统的基于模拟的规划算法快70倍，适用于各种对话起始语和两种现实世界中的奖励函数，同时在实际规划预算内实现更高的奖励。我们的代码可以在以下地址找到：https://github.com/chenzhiliang94/convo-plan-SCOPE。|
|**2025-03-14**|**Synthesizing Access Control Policies using Large Language Models**|Adarsh Vatsa et.al.|[2503.11573](http://arxiv.org/abs/2503.11573)|null|云计算系统允许管理员编写访问控制策略以管理对私有数据的访问。尽管这些策略是用方便的语言编写的，例如AWS身份与访问管理策略语言，但手动编写的策略往往会变得复杂且容易出错。在本文中，我们研究了大型语言模型（LLMs）是否以及如何能够用于合成访问控制策略。我们的研究重点在于通过零样本提示LLMs，根据访问控制请求规范生成符合要求的、格式正确的访问控制策略。我们考虑了两种场景：一种是请求规范以允许或拒绝的具体列表形式给出，另一种是使用自然语言描述来指定需要允许或拒绝的请求集合。然后我们论证了对于零样本提示，使用基于语法的方法的更精确和结构化的提示是必要的，并通过实验初步验证了我们的方法。|
|**2025-03-14**|**Implicit Bias-Like Patterns in Reasoning Models**|Messi H. J. Lee et.al.|[2503.11572](http://arxiv.org/abs/2503.11572)|null|隐性偏见是指自动或自发的思维过程，这些过程塑造了人们的感知、判断和行为。先前研究大型语言模型（LLMs）中的“隐性偏见”通常采用与人类研究隐性偏见不同的方法，主要关注模型的输出而非模型处理过程。为了研究模型处理过程，我们提出了一种名为推理模型内隐联想测验（RM-IAT）的方法，用于探索推理模型中的隐性偏见模式：即那些通过逐步推理解决复杂任务的语言模型。使用此方法，我们发现推理模型在处理关联不兼容信息时需要更多的标记量，而处理关联兼容信息时则较少。这些发现表明AI系统在处理信息时存在类似于人类隐性偏见的模式。我们讨论了这些类似于隐性偏见的模式在其实际应用部署中的意义。|
|**2025-03-14**|**VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity**|Jing Bi et.al.|[2503.11557](http://arxiv.org/abs/2503.11557)|null|视觉推理是人类认知的核心，使个体能够解释并抽象地理解其环境。尽管最近的多模态大型语言模型（MLLMs）在语言和视觉-语言任务上表现出色，但现有的基准测试主要衡量基于识别的技能，未能充分评估真正的视觉推理能力。为了弥补这一关键差距，我们引入了VERIFY，这是一个专门设计用来严格评估最先进MLLMs视觉推理能力的基准。VERIFY迫使模型主要从视觉信息中进行推理，并提供最少的文字上下文以减少对领域特定知识和语言偏见的依赖。每个问题都附有由人工注释的推理路径，使其成为第一个深入评估模型决策过程的工具。此外，我们提出了新的指标来评估视觉推理的保真度，而不仅仅是准确率，揭示了当前模型推理模式中的重要不平衡。我们对领先MLLMs的全面基准测试揭示了显著的局限性，强调了需要平衡和整体的方法来处理感知和推理。更多提示和测试，请访问我们的项目页面（https://verify-eqh.pages.dev/）。|
|**2025-03-14**|**Potential of large language model-powered nudges for promoting daily water and energy conservation**|Zonghan Li et.al.|[2503.11531](http://arxiv.org/abs/2503.11531)|null|水资源和能源短缺问题日益严重，推动了个人节约行为培养的紧迫性。尽管“助推”理念（例如提供基于使用量的反馈）在鼓励节约行为方面显示出潜力，但其效果常受缺乏针对性和可操作性内容的限制。本研究探讨了利用大型语言模型（LLMs）提供个性化节约建议对节约意图及其理由的影响。通过一项涉及1515名大学生的调查实验，比较了三种虚拟助推场景：无助推、传统助推（仅提供使用统计数据）以及LLM驱动助推（提供使用统计数据和个人化节约建议）。统计分析和因果森林建模结果显示，助推使86.9%-98.0%的参与者提高了节约意图，LLM驱动助推的最大增幅达18.0%，比传统助推高出88.6%。此外，结构方程建模结果表明，接触LLM驱动的助推可以增强自我效能感和结果预期，同时减少对社会规范的依赖，从而提高内在动机去节约资源。这些发现凸显了LLMs在促进个人水资源和能源节约方面的变革潜力，标志着可持续行为干预和资源管理设计的新前沿|
|**2025-03-14**|**HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models**|Ziqin Zhou et.al.|[2503.11513](http://arxiv.org/abs/2503.11513)|null|文本到视频生成面临着显著的挑战，由于视频数据在时间和空间维度上的复杂性，它引入了额外的冗余、突变以及语言和视觉标记之间的域间隙。解决这些挑战需要一个有效的视频编码器，能够高效地编码视频数据，同时保留关键的语义和时空信息，作为文本和视觉之间的重要桥梁。受VQ-VAE-2和传统动画工作流程的启发，我们提出了HiTVideo用于文本到视频生成，采用具有多层离散标记框架的3D因果VAE，将视频内容编码为分层结构的代码本。较高层捕获语义信息并实现更高压缩，而较低层则关注细粒度的时空细节，从而在压缩效率和重建质量之间取得平衡。我们的方法可以高效编码更长的视频序列（例如8秒64帧），与基线编码器相比，每像素比特数（bpp）减少了约70%，同时保持竞争性的重建质量。我们探讨了压缩与重建之间的权衡，强调了高压缩语义标记在文本到视频任务中的优势。HiTVideo旨在解决现有视频编码器在文本到视频生成任务中的潜在限制，力求更高的压缩比并在语言指导下简化大型语言模型的建模，提供了一个可扩展且有前景的框架以推进文本到视频生成。演示页面：https://ziqinzhou66.github.io/project/HiTVideo。|
|**2025-03-14**|**V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning**|Zixu Cheng et.al.|[2503.11495](http://arxiv.org/abs/2503.11495)|null|人类在处理视频推理时遵循一种顺序的空间-时间推理逻辑，首先识别相关的帧（“何时”），然后分析关键物体之间的空间关系（“何地”），最后利用这些关系进行推断（“何事”）。然而，视频大型语言模型（Video-LLMs）是否也能通过这种顺序的空间-时间逻辑来理解视频呢？现有的Video-LLM基准主要集中在评估物体存在性，而忽略了关系推理。因此，很难衡量一个模型是否真正理解视频中的物体交互（动作/事件），还是仅仅依赖于预训练的“记忆”中关于共现性的偏见来生成答案。在这项工作中，我们引入了一个视频空间-时间推理（V-STaR）基准以解决这些问题。核心思想是将视频理解分解为一个反向空间-时间推理（RSTR）任务，同时评估视频中物体的存在、事件发生的时间和地点，同时捕捉潜在的链式思维（CoT）逻辑。为了支持这种评估，我们构建了一个数据集，通过GPT-4驱动的半自动化管道生成粗到细的CoT问题，嵌入明确的推理链条以模仿人类认知。从14个Video-LLMs在我们的V-STaR上的实验结果表明，当前Video-LLMs在稳健且一致的空间-时间推理方面存在显著差距。|
|**2025-03-14**|**A Review of DeepSeek Models' Key Innovative Techniques**|Chengen Wang et.al.|[2503.11486](http://arxiv.org/abs/2503.11486)|null|DeepSeek-V3 和 DeepSeek-R1 是领先的开源大型语言模型（LLMs），适用于通用任务和推理任务，其性能可与来自 OpenAI 和 Anthropic 等公司的最先进的闭源模型相媲美，而训练成本却仅为后者的很小一部分。理解 DeepSeek 成功背后的关键创新技术对于推动 LLM 研究至关重要。本文回顾了驱动这些模型卓越效果和效率的核心技术，包括对变压器架构的改进、多头潜在注意力和专家混合等创新、多令牌预测、算法、框架和硬件的协同设计、分组相对策略优化算法、使用纯强化学习的后训练以及在监督微调和强化学习之间交替进行的迭代训练。此外，我们还确定了一些开放性问题并强调了该快速发展的领域中的潜在研究机会。|
|**2025-03-14**|**Integrating LLMs in Gamified Systems**|Carlos J. Costa et.al.|[2503.11458](http://arxiv.org/abs/2503.11458)|null|在本文中，提出了一种全面的数学框架，用于将大型语言模型（LLMs）整合到游戏化系统中，重点是改善任务动态、用户参与度和奖励系统。通过集成LLMs，可以实现个性化反馈、适应性学习和动态内容创建，这对于提高用户参与度和系统性能至关重要。模拟环境测试了该框架的适应性，并展示了其在商业、医疗保健和教育等多个行业中实际应用的潜力。研究结果表明，LLMs能够提供定制化的体验，从而提高系统的有效性以及用户的留存率。本研究还探讨了该框架旨在解决的问题，强调了其在最大化参与度并促进持续行为改变方面的重要性。|
|**2025-03-13**|**GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing**|Rongyao Fang et.al.|[2503.10639](http://arxiv.org/abs/2503.10639)|**[link](https://github.com/rongyaofang/got)**|**当前的图像生成和编辑方法主要将文本提示作为直接输入进行处理，并未对视觉组成进行推理或明确操作。我们提出了生成链式思维（GoT），这是一种新颖的范式，能够通过显式的语言推理过程来进行生成和编辑，然后再输出图像。这种方法将传统的文本到图像的生成和编辑转变为一种以推理为导向的框架，分析语义关系和空间布局。我们定义了GoT的公式并构建了包含超过900万样本的大规模GoT数据集，其中包含了详细的推理链以捕捉语义-空间关系。为了利用GoT的优势，我们实现了一个统一的框架，该框架结合了Qwen2.5-VL用于推理链生成，以及一个由我们的新型语义-空间引导模块增强的端到端扩散模型。实验表明，我们的GoT框架在生成和编辑任务上都取得了出色的表现，显著优于基线方法。此外，我们的方法还实现了交互式的视觉生成，允许用户明确修改推理步骤以实现精确的图像调整。GoT开创了推理驱动的视觉生成和编辑的新方向，生成的图像更符合人类意图。为了促进未来的研究，我们将数据集、代码和预训练模型公开发布于 https://github.com/rongyaofang/GoT。**|
|**2025-03-13**|**HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model**|Jiaming Liu et.al.|[2503.10631](http://arxiv.org/abs/2503.10631)|null|近期在视觉-语言模型（VLM）用于常识推理方面的进展推动了视觉-语言-动作（VLA）模型的发展，使机器人能够执行通用操作。尽管现有的自回归VLA方法利用大规模预训练知识，但它们会破坏动作的连续性。同时，一些VLA方法通过添加扩散头来预测连续动作，仅依赖于由VLM提取的特征，这限制了其推理能力。在本文中，我们提出了HybridVLA，这是一种统一框架，将自回归和扩散策略的优势无缝集成到单一的大语言模型中，而非简单地将两者连接起来。为了弥合生成差距，提出了一种协作训练方法，直接将扩散建模注入到下一个标记预测中。通过这种方法，我们发现这两种动作预测方式不仅相互强化，而且在不同任务中的表现也有所不同。因此，我们设计了一种协作动作集成机制，以自适应融合这两种预测，从而实现更稳健的控制。在实验中，HybridVLA在各种模拟和真实世界任务中超越了先前最先进的VLA方法，并在单臂和双臂机器人中展示了在以前未见过的配置下的稳定操作能力|
|**2025-03-13**|**UniGoal: Towards Universal Zero-shot Goal-oriented Navigation**|Hang Yin et.al.|[2503.10630](http://arxiv.org/abs/2503.10630)|null|在本文中，我们提出了一个通用的零样本目标导向导航框架。现有的零样本方法基于大型语言模型（LLM）为特定任务构建推理框架，这些方法的整体流程差异很大，无法在不同类型的目标之间进行泛化。为了实现通用的零样本导航，我们提出了一种统一的图表示来统一不同的目标，包括物体类别、实例图像和文本描述。我们将智能体的观测转换为在线维护的场景图。通过这种一致的场景和目标表示，我们保留了大部分结构信息，并能够利用LLM进行显式的基于图的推理。具体而言，我们在每个时间步长之间进行场景图和目标图的图匹配，并根据不同的匹配状态提出不同的策略来生成长期探索目标。当完全不匹配时，智能体首先迭代搜索目标子图；当部分匹配时，智能体利用坐标投影和锚点对齐来推断目标位置；最后，在完全匹配时应用场景图修正和目标验证。我们还提出了黑名单机制以实现各阶段之间的鲁棒切换。在多个基准数据集上的广泛实验表明，我们的UniGoal在三种研究的导航任务中实现了最先进的零样本性能，甚至超过了针对特定任务的零样本方法和监督式通用方法。|
|**2025-03-13**|**From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM**|Kshitij Ambilduke et.al.|[2503.10620](http://arxiv.org/abs/2503.10620)|**[link](https://github.com/utter-project/SpireLM)**|大型语言模型（LLMs）在多种语言和任务上表现出色，展示了强大的性能和泛化能力，这使其成为多模态集成（如图像或语音）的极具吸引力的目标。在这项工作中，我们将现有的LLM扩展到语音模态，通过语音离散化和持续预训练实现。特别地，我们关注多语言LLMs，例如TOWER，因为其预训练设置允许我们将离散化的语音输入视为一种额外的翻译语言。由此产生的开源模型SPIRE能够转录并翻译英语语音输入，同时保持TOWER在与翻译相关任务上的原有性能，证明了在LLM适应过程中将离散化语音输入作为额外语言进行集成是可行的。我们将代码和模型开放给社区使用。|
|**2025-03-13**|**Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search**|Andy Zhou et.al.|[2503.10619](http://arxiv.org/abs/2503.10619)|null|我们引入了Siege，这是一种多轮对抗性框架，从树搜索的角度模拟了大型语言模型（LLM）安全性逐渐退化的过程。与依赖精心设计的单轮提示的越狱方法不同，Siege在每一轮对话中以广度优先的方式扩展多个对抗性提示，利用先前响应中的部分合规性来进一步诱导模型偏离正轨。通过追踪这些逐步显现的安全策略漏洞，并将其重新注入后续查询中，Siege展示了小的妥协如何累积成完全被禁止的输出。在JailbreakBench数据集上的评估表明，Siege在单次多轮运行中对GPT-3.5-turbo实现了100%的成功率，对GPT-4实现了97%的成功率，使用的查询次数少于Crescendo或GOAT等基线方法。这种方法提供了对模型防护措施在连续对话中逐步退化过程的深入见解，强调了对语言模型进行健壮的多轮测试程序的重要性。|
|**2025-03-13**|**Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models**|Andy Zhou et.al.|[2503.10617](http://arxiv.org/abs/2503.10617)|null|在适应多个任务时，大语言模型可能会出现跨技能干扰，即改进某一技能会导致另一技能的退化。尽管方法如LoRA在权重层面施加了正交性约束，但它们并未完全解决隐藏状态表示中的干扰问题。我们提出了组合子空间表示微调（CS-ReFT），这是一种新颖的基于表示的方法，通过学习多个正交子空间变换，每个变换专门针对一种技能，并通过轻量级路由器进行组合。通过在隐藏状态中隔离这些子空间编辑，而不是权重矩阵，CS-ReFT能更有效地防止跨任务冲突。在AlpacaEval基准测试中，将CS-ReFT应用于Llama-2-7B实现了93.94%的胜率，超过了GPT-3.5 Turbo的86.30%，同时仅使用了模型参数的0.0098%。这些结果表明，通过简单路由器实现的专业化表示编辑显著提升了多任务指令跟随能力，并且具有极小的开销。|
|**2025-03-13**|**R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization**|Yi Yang et.al.|[2503.10615](http://arxiv.org/abs/2503.10615)|**[link](https://github.com/Fancy-MLLM/R1-onevision)**|**大型语言模型在复杂文本任务中展示了出色的推理能力。然而，多模态推理，即需要整合视觉和文本信息的能力，仍然是一个重大挑战。现有的视觉-语言模型往往难以有效分析和推理视觉内容，导致在复杂推理任务上的表现不佳。此外，缺乏全面的基准测试阻碍了对多模态推理能力的准确评估。在这篇论文中，我们引入了R1-Onevision，这是一种旨在弥合视觉感知与深度推理之间差距的多模态推理模型。为了实现这一目标，我们提出了一种跨模态推理管道，该管道将图像转换为形式化的文本表示，从而实现精确的语言推理。利用这一管道，我们构建了R1-Onevision数据集，该数据集提供了跨不同领域的详细、逐步的多模态推理注释。我们进一步通过有监督微调和强化学习开发了R1-Onevision模型，以培养其高级推理和强大的泛化能力。为了全面评估不同年级的多模态推理性能，我们引入了R1-Onevision-Bench，这是一个与人类教育阶段对齐的基准，涵盖了从初中到大学及更高级别的考试。实验结果表明，R1-Onevision在多个具有挑战性的多模态推理基准上达到了最先进的性能，超过了GPT-4o和Qwen2.5-VL等模型的性能**|
|**2025-03-13**|**CoSTA $\ast$ : Cost-Sensitive Toolpath Agent for Multi-turn Image Editing**|Advait Gupta et.al.|[2503.10613](http://arxiv.org/abs/2503.10613)|**[link](https://github.com/tianyi-lab/CoSTAR)**|**文本到图像模型如Stable Diffusion和DALL-E 3在多轮图像编辑任务上仍然面临挑战。我们将此类任务分解为一个代理工作流程（路径），该流程通过使用不同成本的AI工具来解决一系列子任务。传统的搜索算法需要昂贵的探索才能找到工具路径。尽管大型语言模型（LLMs）拥有子任务规划的先验知识，但它们可能缺乏对工具能力和成本的准确估计，从而无法确定在每个子任务中应应用哪个工具。我们能否结合LLMs和图搜索的优点以找到成本效益高的工具路径？我们提出了一个三阶段的方法“CoSTA*”，它利用LLMs创建子任务树，这有助于为给定任务修剪AI工具的图，并随后在小的子图上进行A*搜索以找到工具路径。为了更好地平衡总成本和质量，CoSTA*将每个工具在每个子任务上的两种度量结合起来指导A*搜索。每个子任务的输出由视觉语言模型（VLM）进行评估，如果失败则会触发工具在子任务上的成本和质量更新。因此，A*搜索可以快速从失败中恢复并探索其他路径。此外，CoSTA*可以根据需要在子任务之间自动切换模态以实现更好的成本质量权衡。我们构建了一个具有挑战性的多轮图像编辑基准，在该基准上CoSTA*在成本和质量方面均优于最先进的图像编辑模型或代理，并根据用户偏好执行多样化的权衡。**|
|**2025-03-13**|**TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention**|Jinhao Duan et.al.|[2503.10602](http://arxiv.org/abs/2503.10602)|**[link](https://github.com/jinhaoduan/truthprint)**|**对象幻觉（OH）被认为是大型视觉语言模型（LVLMs）中主要的可信性挑战之一。最近，大型语言模型（LLMs）的进步表明内部状态（如隐藏状态）编码了生成响应的整体真实性。然而，关于LVLMs内部状态的功能及其是否可以作为“每令牌”幻觉指示器的研究仍处于初步阶段，这对缓解OH至关重要。在本文中，我们首先对LVLM内部状态与OH问题的关系进行了深入探索，发现（1）LVLM内部状态是高特异性的每令牌幻觉行为指示器。此外，（2）不同的LVLMs在共同的潜在子空间中编码了幻觉的通用模式，这表明存在由各种LVLMs共享的“通用真实方向”。基于这些发现，我们提出了真相引导预干预（TruthPrInt），该方法首先学习LVLM解码的真实方向，然后在解码过程中应用真相引导的推理时干预。我们进一步提出ComnHallu，通过构建和对齐幻觉潜在子空间来增强跨LVLM和跨数据幻觉检测的可转移性。我们在广泛的实验设置中评估了TruthPrInt，包括域内和域外场景，并在流行的LVLMs和OH基准上进行了测试。实验结果表明，TruthPrInt显著优于最先进的方法。代码将在https://github.com/jinhaoduan/TruthPrInt上提供。**|
|**2025-03-13**|**Unlock the Power of Unlabeled Data in Language Driving Model**|Chaoqun Wang et.al.|[2503.10586](http://arxiv.org/abs/2503.10586)|null|近期基于视觉的大型语言模型（VisionLLMs）在自动驾驶领域取得了快速进展。然而，这种进步高度依赖于大规模高质量标注数据，而这类数据的获取成本高昂且耗时费力。为了解决这一问题，我们提出了一种利用大量未标注数据的方法，在半监督学习框架下提升语言驱动模型的表现。具体而言，我们首先引入了一系列基于模板的提示词来提取场景信息，生成问题并基于少量标注数据训练的模型创建伪答案。接着，我们提出了自一致性优化方法以提高这些伪标注的质量，然后将其用于进一步的模型训练。通过利用预训练的VisionLLM（例如InternVL），我们构建了一个强大的语言驾驶模型（LDM），在驾驶场景问答任务中表现出色，超过了先前的最先进方法。在DriveLM基准测试中的广泛实验表明，我们的方法仅使用5%的标注数据就能取得优异表现，与使用完整数据集训练的模型相比竞争力相当。特别是，我们的LDM在使用有限标注数据时达到了44.85%的性能，在使用未标注数据后提升至54.27%，而使用完整数据集训练的模型在DriveLM基准上达到60.68%的性能|
|**2025-03-12**|**MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System**|Jihao Zhao et.al.|[2503.09600](http://arxiv.org/abs/2503.09600)|**[link](https://github.com/IAAR-Shanghai/Meta-Chunking)**|检索增强生成（RAG）作为大型语言模型（LLMs）的有力补充，其管道中的文本分块环节往往被忽视。本文首先提出了一种双指标评估方法，包括边界清晰度和块粘附性，以实现对分块质量的直接量化。通过这一评估方法，我们揭示了传统分块和语义分块在处理复杂上下文细节时的固有限制，从而证明了将LLMs集成到分块过程中的必要性。为了解决LLM驱动方法中计算效率与分块精度之间的权衡问题，我们设计了粒度感知的混合分块器（MoC）框架，该框架包含三个阶段的处理机制。值得注意的是，我们的目标是引导分块器生成结构化的分块正则表达式列表，然后用于从原始文本中提取块。广泛的实验表明，我们提出的指标和MoC框架有效解决了分块任务中的挑战，揭示了分块内核并提升了RAG系统的性能。|
|**2025-03-12**|**How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation**|Ruohao Guo et.al.|[2503.09598](http://arxiv.org/abs/2503.09598)|**[link](https://github.com/octaviaguo/EchoMist)**|随着大规模语言模型（LLMs）在各种场景中的广泛应用，其潜在传播错误信息的问题成为重要的安全关注点。目前的研究主要集中在评估LLMs对明确虚假陈述的处理上，而忽视了错误信息往往以用户互动中的未挑战前提形式隐晦地显现这一现实情况。我们创建了ECHOMIST，这是首个针对隐性错误信息的综合基准测试，其中用户的查询中嵌入了误导性的假设。ECHOMIST基于严格的选取标准，并从真实世界的人工智能与人类对话及社交媒体交互中精心策划数据。我们还引入了一种新的评估指标，用以衡量LLMs是否能够识别并反驳虚假信息，而非放大用户的误解。通过广泛研究多种LLMs（包括GPT-4、Claude和Llama），我们发现当前模型在这项任务上的表现令人担忧，常常未能检测到错误的前提并生成误导性解释。我们的研究结果强调了在LLM安全性研究中加强对隐性错误信息的关注的紧迫性。|
|**2025-03-12**|**SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment**|Katrin Renz et.al.|[2503.09594](http://arxiv.org/abs/2503.09594)|null|将大型语言模型（LLMs）集成到自动驾驶中引起了广泛关注，人们希望借此提高泛化能力和可解释性。然而，现有的方法往往专注于驾驶或视觉-语言理解，但同时实现高驾驶性能和广泛的语言理解仍然具有挑战性。此外，解决视觉-语言理解的主流方法是使用视觉问答。但对于自动驾驶而言，这种方法只有在与动作空间对齐时才有用，否则模型的回答可能与其行为不一致。因此，我们提出了一种可以处理三种不同任务的模型：（1）闭环驾驶，（2）视觉-语言理解，以及（3）语言-动作对齐。我们的模型SimLingo基于视觉语言模型（VLM），仅使用摄像头即可工作，无需昂贵的传感器如激光雷达。SimLingo在广泛使用的CARLA模拟器上的Bench2Drive基准测试中取得了最先进的性能，并赢得了CARLA挑战赛2024的冠军。此外，我们在各种语言相关任务中也取得了强劲的表现，同时保持了较高的驾驶性能。|
|**2025-03-12**|**BIMBA: Selective-Scan Compression for Long-Range Video Question Answering**|Md Mohaiminul Islam et.al.|[2503.09590](http://arxiv.org/abs/2503.09590)|**[link](https://github.com/md-mohaiminul/BIMBA)**|视频问答（VQA）在长视频中面临的关键挑战是从大量冗余帧中提取相关信息并建模长距离依赖关系。自注意力机制为序列建模提供了一般性解决方案 但在应用于长视频中大量时空标记时其计算成本变得难以承受。大多数现有方法依赖于压缩策略来降低计算成本 例如通过稀疏帧采样减少输入长度 或通过空间时间池化压缩传递给大型语言模型（LLM）的输出序列。然而 这些简单的方法过度表示了冗余信息 经常错过重要的事件或快速发生的时空模式。在这项工作中 我们提出了BIMBA 一种高效的状态空间模型 用于处理长格式视频。我们的模型利用选择性扫描算法 学习从高维视频中有效选择关键信息 并将其转换为减少标记序列 以实现高效LLM处理。广泛的实验表明 BIMBA在多个长格式VQA基准测试中实现了最先进的准确性 包括PerceptionTest NExT-QA EgoSchema VNBench LongVideoBench和Video-MME。代码和模型可在https://sites.google.com/view/bimba-mllm公开获取|
|**2025-03-12**|**Cost-Optimal Grouped-Query Attention for Long-Context LLMs**|Yingfa Chen et.al.|[2503.09579](http://arxiv.org/abs/2503.09579)|**[link](https://github.com/thunlp/cost-optimal-gqa)**|**构建高效且有效的基于Transformer的大规模语言模型（LLMs）已成为研究重点，需要在最大化模型语言能力的同时最小化训练和部署成本。现有工作主要描述了模型性能、参数规模和数据规模之间的复杂关系，并搜索了用于训练LLMs的最佳计算分配。然而，这些工作忽略了上下文长度和注意力头配置（组查询注意力中的查询和键值头数量）对训练和推理的影响。在本文中，我们系统地比较了具有不同参数规模、上下文长度和注意力头配置的模型在模型性能、计算成本和内存成本方面的差异。然后，我们将现有的仅基于参数规模和训练计算的扩展方法进行扩展，以指导在训练和推理过程中构建成本最优的LLMs。我们的定量扩展研究表明，当处理足够长的序列时，具有较少注意力头的大模型可以实现更低的损失，同时降低计算和内存成本。我们的研究结果为开发实用的LLMs提供了宝贵的见解，特别是在长上下文处理场景中。我们将公开发布代码和数据。**|
|**2025-03-12**|**Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks**|Lutfi Eren Erdogan et.al.|[2503.09572](http://arxiv.org/abs/2503.09572)|null|大型语言模型（LLMs）在实现简单任务方面已显示出显著的进步。然而，将它们应用于复杂的、多步骤的、长时序的任务仍然是一项挑战。近期的研究通过将高阶规划与低阶执行分离，成功地使模型能够有效地平衡高阶规划目标和低阶执行细节。然而，生成准确的计划仍然很困难，因为LLMs并非专门为此任务训练的。为了解决这个问题，我们提出了Plan-and-Act，这是一种新的框架，将明确的规划引入基于LLM的代理，并引入了一种可扩展的方法来通过一种新颖的合成数据生成方法增强计划生成。Plan-and-Act由一个规划器模型和一个执行器模型组成：前者生成用于实现用户目标的结构化高层次计划，后者将这些计划转化为特定环境中的操作。为了有效训练规划器，我们引入了一种合成数据生成方法，该方法使用标注了真实轨迹的可行计划，并结合了多样化和广泛的示例以增强泛化能力。我们在Web导航作为代表性长时序规划环境的评估中，展示了Plan-and-Act在WebArena-Lite基准测试上达到了最先进的54%成功率。|
|**2025-03-13**|**Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models**|Qiguang Chen et.al.|[2503.09567](http://arxiv.org/abs/2503.09567)|null|近年来，具备长链式思维（Long CoT）的大语言模型（RLLMs），如OpenAI-O1和DeepSeek-R1，在数学和编码等复杂领域展现了令人瞩目的能力。其成功的关键在于长链式思维的应用，这种思维方式增强了推理能力，使模型能够解决更复杂的任务。然而，目前尚缺乏关于长链式思维的全面综述，这限制了我们对其与传统短链式思维（Short CoT）区别的理解，并使有关“过度推理”和“测试时扩展”等问题的讨论变得复杂化。本综述旨在填补这一空白，提供一个统一的视角来理解长链式思维。（1）首先，我们将长链式思维与短链式思维区分开来，并引入一种新的分类方法以归类当前的推理范式。（2）接着，我们探讨长链式思维的关键特征：深度推理、广泛探索以及可行的反思，这些特性使得模型能够处理更复杂的任务并产生比短链式思维更高效、更连贯的结果。（3）然后，我们研究了长链式思维及其特性的出现现象，包括过度推理和测试时扩展，提供了这些过程在实际应用中的见解。（4）最后，我们指出了重要的研究空白并提出了有前景的研究方向，包括多模态推理的整合、效率提升以及增强的知识框架。通过提供结构化的概述，本综述旨在激发未来的研究并推动人工智能逻辑推理的发展。|
|**2025-03-13**|**Large Language Models for Multi-Facility Location Mechanism Design**|Nguyen Thach et.al.|[2503.09533](http://arxiv.org/abs/2503.09533)|null|设计针对多设施选址问题的策略证明机制以优化社会成本一直具有挑战性，因为这需要大量的领域知识并且最坏情况下的保证较差。最近，深度学习模型被提出作为替代方案。然而，这些模型同样需要一些领域知识以及广泛的超参数调整，并且缺乏可解释性，而在实践中当透明度是必要时这一点至关重要。在本文中，我们介绍了一种名为LLMMech的新方法，该方法通过将大型语言模型（LLMs）纳入进化框架来生成可解释、无需超参数、经验上策略证明且接近最优的机制。我们的实验结果表明，在各种问题设置下，LLM生成的机制通常优于现有的手工基线和深度学习模型。此外，这些机制在处理分布外的代理偏好和更大规模的问题实例方面表现出令人印象深刻的泛化能力|
|**2025-03-12**|**Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning**|Bowen Jin et.al.|[2503.09516](http://arxiv.org/abs/2503.09516)|**[link](https://github.com/petergriffinjin/search-r1)**|高效获取外部知识和最新信息对于大型语言模型（LLMs）的有效推理和文本生成至关重要。目前的检索增强和工具使用训练方法要么缺乏复杂的多轮检索灵活性，要么需要大规模的有监督数据。在推理过程中通过提示具有推理能力的高级LLMs来使用搜索引擎并非最优解，因为LLM无法学习如何最佳地与搜索引擎交互。本文介绍了一种名为Search-R1的新模型，它是DeepSeek-R1模型的扩展，在该模型中，LLM通过强化学习（RL）单独学习在逐步推理过程中实时生成（多个）搜索查询的能力，并且具有多轮搜索交互优化的特点。Search-R1利用检索到的标记屏蔽进行稳定的RL训练，并采用简单的基于结果的奖励函数。在七个问答数据集上的实验表明，相比于最先进的基线模型，Search-R1在Qwen2.5-7B上提升了26%，在Qwen2.5-3B上提升了21%，在LLaMA3.2-3B上提升了10%的性能。本文还提供了关于RL优化方法、LLM选择以及检索增强推理中响应长度动态的实证见解。代码和模型检查点可以在https://github.com/PeterGriffinJin/Search-R1获得。|
|**2025-03-12**|**ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning**|Ziyu Wan et.al.|[2503.09501](http://arxiv.org/abs/2503.09501)|null|近期关于大型语言模型（LLMs）推理的研究致力于通过引入元思考来进一步提升其性能，使模型能够监控、评估和控制自身的推理过程，从而实现更适应和有效的解决问题。然而，目前单代理工作缺乏专门设计以获取元思考能力，导致效果不佳。为了解决这一挑战，我们提出了强化元思考代理（ReMA），这是一种新颖的框架，利用多智能体强化学习（MARL）激发元思考行为，促使LLMs思考“如何思考”。ReMA 将推理过程解耦为两个层级的代理：高层次的元思考代理负责生成战略监督和计划，低层次的推理代理则负责详细执行任务。通过与对齐目标的迭代强化学习，这些代理探索并学会协作，从而提高泛化能力和鲁棒性。实验结果表明，ReMA 在复杂的推理任务上优于单代理RL基线，包括具有竞争力的数学基准和LLM作为裁判的基准任务。全面的消融研究进一步揭示了每个不同代理的动态演化过程，为理解元思考推理过程如何增强LLMs的推理能力提供了宝贵的见解。|
|**2025-03-11**|**Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs**|Ariba Khan et.al.|[2503.08688](http://arxiv.org/abs/2503.08688)|null|关于大型语言模型（LLMs）“文化适配性”的研究在人们对跨多元利益相关者表征的兴趣日益增长的背景下应运而生。当前评估文化适配性的方法借鉴了社会科学的方法论，但往往忽视了系统的稳健性检查。本文识别并测试了现有评估方法背后的三个假设：(1) 稳定性：认为文化适配性是LLMs的属性而非评估设计的产物；(2) 演绎性：认为在一个狭窄的问题集上与一种文化的适配可以预测在其他方面对该文化的适配；(3) 可引导性：认为LLMs可以通过提示可靠地表示特定的文化视角。通过实验考察领先LLMs的显性和隐性偏好，我们发现跨展示格式的高度不稳定性、评估维度与保留维度之间的一致性不足以及提示引导下的不稳定行为。我们表明，这些不一致性可能导致评估结果对方法学的小幅变化非常敏感。最后，我们在一个关于评估设计的案例研究中展示了如何使用窄化实验和选择性证据评估来描绘LLMs文化适配属性的不完整图景。总体而言，这些结果揭示了现有评估LLMs文化适配性的方法的重大局限性。|
|**2025-03-11**|**Self-Taught Self-Correction for Small Language Models**|Viktor Moskvoretskii et.al.|[2503.08681](http://arxiv.org/abs/2503.08681)|null|尽管大型语言模型（LLMs）在各种任务上取得了显著的性能，但它们仍然容易出错。一个关键的挑战是使它们能够自我修正。虽然之前的研究依赖于外部工具或大型专有模型，但这项工作探索了通过迭代微调在小型语言模型（SLMs）中实现自我修正的方法，所用数据完全由自身生成。我们介绍了Self-Taught Self-Correction（STaSC）算法，并结合了多种算法设计选择。实验结果表明，在问答任务中，STaSC能够有效学习自我修正，带来显著的性能提升。我们的分析进一步揭示了自我修正的机制以及不同设计选择对学习动态和整体性能的影响。为了支持未来的研究，我们发布了用户友好的代码库和轻量级模型。|
|**2025-03-11**|**Exploring the Word Sense Disambiguation Capabilities of Large Language Models**|Pierpaolo Basile et.al.|[2503.08662](http://arxiv.org/abs/2503.08662)|null|词义消歧（WSD）是计算语言学中的一个经典任务，多年来受到广泛关注。然而，随着大型语言模型（LLMs）的出现，对该任务（在其经典定义下）的兴趣有所减少。在本研究中，我们评估了各种LLMs在WSD任务上的表现。我们将先前的基准XL-WSD扩展，重新设计了两个适合LLM的子任务：1）给定句子中的一个单词，LLM必须生成正确的定义；2）给定句子中的一个单词和一组预定义的含义，LLM必须选择正确的含义。扩展后的基准使用了XL-WSD和BabelNet构建。结果表明，LLMs在零样本学习中表现出色，但无法超越当前最先进的方法。然而，具有中等参数量的微调模型在所有模型中表现最佳，包括最先进的模型。|
|**2025-03-11**|**LightGen: Efficient Image Generation through Knowledge Distillation and Direct Preference Optimization**|Xianfeng Wu et.al.|[2503.08619](http://arxiv.org/abs/2503.08619)|**[link](https://github.com/xianfengwu01/lightgen)**|近期的文生图生成模型主要依赖于大规模数据集和参数量庞大的架构 这些要求极大地限制了缺乏充足计算资源的研究者和开发者的使用 在本文中 我们提出了一个名为LightGen的高效训练范式 它结合了知识蒸馏(KD)和直接偏好优化(DPO)方法 受多模态大语言模型广泛采用的数据知识蒸馏技术的启发 LightGen通过仅包含0.7B参数的掩码自回归(MAR)架构 将最先进的(SOTA)文生图模型的知识蒸馏出来 使用由2M张来自多样化描述的高质量图像组成的紧凑合成数据集 我们证明了数据多样性在决定模型性能方面远比数据量更重要 这种策略大幅降低了计算需求 并将预训练时间从可能的数千GPU天减少到仅仅88GPU天 此外 为了应对合成数据固有的不足 特别是在高频细节和空间准确性方面的缺陷 我们整合了DPO技术 以改进图像保真度和位置准确性 全面的实验表明 LightGen生成的图像质量与最先进的模型相当 同时显著减少了计算资源需求 并扩展了对资源受限环境的适用性 代码可在https://github.com/XianfengWu01/LightGen获取|
|**2025-03-11**|**EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments**|Dongping Li et.al.|[2503.08604](http://arxiv.org/abs/2503.08604)|null|开发由自然语言控制的自主家庭机器人长期以来一直是人类的目标。尽管大型语言模型（LLMs）和具身智能的进步使这一目标更近了一步，但仍存在一些挑战：缺乏统一的基准来评估更复杂的机器人任务、有限的评估方法和指标以及LLMs与移动操作轨迹之间的数据不兼容性。为了解决这些问题，我们引入了具身移动操作在开放环境中的Embodied Mobile Manipulation in Open Environments (EMMOE)，它要求代理解释用户指令并在连续空间中执行长期的日常任务。EMMOE将高级和低级具身任务无缝集成到一个统一框架中，并提出了三个新指标以进行更多样化的评估。此外，我们收集了EMMOE-100，该数据集具有各种任务属性、详细的流程注释、失败后的重新规划以及两个用于LLM训练的子数据集。此外，我们设计了HomieBot，这是一种复杂的代理系统，包括带有直接偏好优化（DPO）的LLM、轻量级导航和操作模型以及多种错误检测机制。最后，我们展示了HomieBot的表现并评估了不同的模型和策略。|
|**2025-03-11**|**NSF-SciFy: Mining the NSF Awards Database for Scientific Claims**|Delip Rao et.al.|[2503.08600](http://arxiv.org/abs/2503.08600)|null|我们提出了NSF-SciFy，这是一个从国家科学基金会（NSF）奖项数据库中衍生出的大规模科学主张提取数据集，包含跨越五十年的40多万份资助摘要。虽然先前的数据集依赖于已发表的文献，但我们利用资助摘要，这提供了独特的优势：它们在出版生效之前捕获了研究生命周期早期阶段的主张。我们还引入了一项新任务，以区分提案中的现有科学主张和抱负的研究意图。使用前沿大型语言模型的零样本提示，我们从材料科学领域的16,000份资助摘要中联合提取了114,000个科学主张和145,000个调查提案，创建了一个聚焦子集称为NSF-SciFy-MatSci。我们使用此数据集评估三个关键任务：（1）技术到非技术摘要生成，模型达到了高BERTScore（0.85+ F1）；（2）科学主张提取，微调模型相比基线模型有100%的相对改进；（3）调查提案提取，显示微调后有90%以上的改进。我们引入了新的LLM基础评估指标以进行稳健的质量评估。作为迄今为止最大的科学主张数据集——估计涵盖了所有由NSF资助的STEM学科中的280万个主张——NSF-SciFy为主张验证和元科学研究提供了新的机会。我们将所有数据集、训练模型和评估代码公开发布，以促进进一步研究。|
|**2025-03-11**|**HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding**|Shehreen Azad et.al.|[2503.08585](http://arxiv.org/abs/2503.08585)|null|尽管多模态大型语言模型（MLLMs）取得了进展，但当前方法在中长视频理解方面仍面临帧数和上下文长度限制的挑战。由于这些限制，这些模型通常依赖于帧采样，这可能导致错过关键信息并缺乏任务相关性。为了解决这些问题，我们引入了HierarQ，这是一种基于任务感知的分层Q-Former框架，通过顺序处理帧来避免帧采样的需求，同时克服LLM的上下文长度限制。我们提出了一个轻量级的双流语言引导特征调制器以在视频理解中融入任务感知能力，其中实体流捕获短上下文中帧级别的对象信息，场景流识别长时间内对象间的交互。每个流都有专用的记忆库，使所提出的分层查询变换器（HierarQ）能够有效捕捉短长期上下文。在10个视频基准数据集上的广泛评估表明，HierarQ在大多数数据集上实现了最先进的性能，证明了其在全面视频分析中的鲁棒性和效率。|
|**2025-03-11**|**RAG-Adapter: A Plug-and-Play RAG-enhanced Framework for Long Video Understanding**|Xichen Tan et.al.|[2503.08576](http://arxiv.org/abs/2503.08576)|null|多模态大型语言模型（MLLMs）在视频理解方面正在快速发展。为了有效评估其视频理解能力，提出了长视频理解基准，如Video-MME和MLVU。然而，这些基准直接使用均匀帧采样进行测试，这会导致显著的信息丢失，并影响评估结果对MLLMs真实能力的反映。为了解决这一问题，我们提出了RAG-Adapter，这是一种即插即用框架，通过采样与给定问题最相关的帧来减少测试过程中的信息丢失。此外，我们引入了分组监督对比学习（GCL）方法，通过在我们构建的MMAT数据集上进行微调，进一步增强RAG-Adapter的采样效果。最后，我们在各种视频理解基准上测试了多个基线MLLMs，发现RAG-Adapter采样始终优于均匀采样（例如，在Video-MME上GPT-4o的准确率提高了9.3个百分点），提供了一种更准确的长视频基准测试方法。|
|**2025-03-11**|**DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process**|Minjun Zhu et.al.|[2503.08569](http://arxiv.org/abs/2503.08569)|null|大型语言模型（LLMs）在科学论文评估中的应用日益增多，尤其是在自动化论文评审方面。然而，现有的基于LLM的评审系统面临诸多挑战，包括领域专业知识有限、幻觉推理以及缺乏结构化评估等。为了解决这些局限性，我们引入了DeepReview，这是一种多阶段框架，旨在通过结构化分析、文献检索和基于证据的论证来模拟专家评审员。利用DeepReview-13K这一经过结构化注释的数据集，我们训练了DeepReviewer-14B，其表现优于拥有更多参数的CycleReviewer-70B。在最佳模式下，DeepReviewer-14B在与GPT-o1和DeepSeek-R1的对抗测试中分别取得了88.21%和80.20%的胜率。我们的工作为基于LLM的论文评审设定了新的基准，所有资源均已公开发布。代码、模型、数据集和演示均可在http://ai-researcher.net获取|
|**2025-03-11**|**Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs**|Wanyong Feng et.al.|[2503.08551](http://arxiv.org/abs/2503.08551)|null|多项选择题（MCQs）的难度是教育评估中的关键因素。预测MCQ难度具有挑战性，因为它需要理解达到正确选项的复杂性以及干扰项（错误选项）的可信度。在本文中，我们提出了一种新的两阶段方法来预测MCQ难度。首先，为了更好地估计每个MCQ的复杂性，我们使用大型语言模型（LLMs）来扩充推理步骤以达到每个选项。我们将不仅MCQ本身而且这些推理步骤作为输入来预测难度。其次，为了捕捉干扰项的可信度，我们从分布中采样知识水平以考虑学生在回答MCQ时的变化。这一设置受到项目反应理论（IRT）的启发，使我们能够估计学生选择每个选项（包括正确和错误选项）的可能性。我们将这些预测与它们的真实值对齐，并使用基于Kullback-Leibler（KL）散度的正则化目标函数，利用估计的可能性来预测MCQ难度。我们在两个具有通过IRT估算的实际数学MCQ和响应数据集上评估了我们的方法。实验结果表明，我们的方法优于所有基线方法，均方误差减少了高达28.3％，决定系数提高了34.6％。我们还定性讨论了我们的新方法如何提高预测MCQ难度的准确性|
|**2025-03-10**|**V2Flow: Unifying Visual Tokenization and Large Language Model Vocabularies for Autoregressive Image Generation**|Guiwei Zhang et.al.|[2503.07493](http://arxiv.org/abs/2503.07493)|**[link](https://github.com/zhangguiwei610/v2flow)**|我们提出了一种名为V2Flow的新颖标记器，它能够生成高保真重建的离散视觉标记，并确保与大型语言模型（LLM）词汇空间的结构和潜在分布对齐。利用这种紧密的视觉词汇耦合，V2Flow能够在现有的LLM上实现自回归视觉生成。我们的方法将视觉标记化问题表述为流匹配问题，旨在学习从标准正态先验到连续图像分布的映射，条件是嵌入在LLM词汇空间中的标记序列。V2Flow的有效性来源于两个核心设计。首先，我们提出了一个视觉词汇重采样器，它将视觉数据压缩成紧凑的标记序列，每个标记表示为LLM词汇上的软分类分布。这使得视觉标记可以无缝地集成到现有的LLM中，以实现自回归视觉生成。其次，我们介绍了一个掩码自回归修正流解码器，采用掩码变换器编码器-解码器来细化视觉标记，使其成为上下文增强的嵌入。这些嵌入随后用于条件化一个专用的速度场以实现精确重建。此外，还引入了自回归修正流采样策略，确保在保持竞争性重建质量的同时具有灵活的序列长度。广泛的实验表明，V2Flow优于主流基于VQ的标记器，并促进了现有LLM上的自回归视觉生成。|
|**2025-03-10**|**LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle Zero-shot Radiology Recognition?**|Bangyan Li et.al.|[2503.07487](http://arxiv.org/abs/2503.07487)|null|最近，多模态大型模型（MLLMs）在各种视觉理解与推理任务中展示了卓越的能力。然而，这些模型在零样本医学疾病识别方面通常表现不佳，因为它们未能充分利用捕获的特征和可用的医学知识。为了解决这一挑战，我们提出了LLaVA-RadZ，这是一种简单而有效的框架，用于零样本医学疾病识别。具体来说，我们设计了一种端到端的训练策略，称为解码侧特征对齐训练（DFAT），以利用MLLM解码器架构的特点，并结合针对不同模态定制的模态特定标记，这有效地利用了图像和文本表示并促进了稳健的跨模态对齐。此外，我们引入了一个领域知识锚定模块（DKAM），以利用大型模型固有的医学知识，这减轻了图像-文本对齐中的类别语义差距。DKAM提高了类别级别的对齐，从而实现了准确的疾病识别。在多个基准数据集上的广泛实验表明，我们的LLaVA-RadZ在零样本疾病识别方面显著优于传统的MLLM，并且相比高度优化的基于CLIP的方法，其性能达到了最先进水平。|
|**2025-03-10**|**GenAIReading: Augmenting Human Cognition with Interactive Digital Textbooks Using Large Language Models and Image Generation Models**|Ryugo Morita et.al.|[2503.07463](http://arxiv.org/abs/2503.07463)|null|认知增强是推进教育的关键，特别是在个性化学习方面。然而，由于对大量文本材料（如叙述和学术教科书）的使用，对其进行个性化仍然具有挑战性，这可能会影响学习者的参与度和理解能力。本研究基于双重编码理论（该理论认为结合文本和视觉信息可以提高理解和记忆），探讨了生成式人工智能（GenAI）在丰富教育材料方面的潜力。我们利用大型语言模型（LLM）生成简洁的文本摘要，并使用图像生成模型（IGM）从文本输入中创建视觉相关的材料。在招募了24名参与者后，我们验证了整合AI生成的补充材料显著提高了学习成果，使阅读后的测试分数提高了7.50%。这些发现强调了GenAI在创造适应性学习环境以增强认知增强方面的变革潜力。|
|**2025-03-10**|**MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning**|Xiangru Tang et.al.|[2503.07459](http://arxiv.org/abs/2503.07459)|**[link](https://github.com/gersteinlab/medagents-benchmark)**|大型语言模型（LLMs）在现有的医学问答基准测试中表现出色。这种高水平的性能使得有意义地评估和区分先进方法变得越来越困难。我们提出了MedAgentsBench，这是一个专注于需要多步临床推理、诊断制定和治疗规划的复杂医学问题的基准测试。这些场景是当前模型仍然难以应对的，尽管它们在标准测试中的表现很强。我们的基准测试基于七个已建立的医学数据集，解决了现有评估中的三个关键局限性：（1）简单问题的普遍存在，即使是基础模型也能取得高分；（2）研究中不一致的采样和评估协议；以及（3）对性能、成本和推理时间之间相互作用的系统分析缺乏。通过使用各种基础模型和推理方法进行实验，我们证明了最新的思考模型DeepSeek R1和OpenAI o3在复杂的医学推理任务中表现出色。此外，先进的基于搜索的代理方法相比传统方法提供了更有前景的性能与成本比。我们的分析揭示了模型家族在复杂问题上的显著性能差异，并确定了不同计算约束下的最优模型选择。我们的基准测试和评估框架可在https://github.com/gersteinlab/medagents-benchmark公开获取。|
|**2025-03-10**|**LLMs syntactically adapt their language use to their conversational partner**|Florian Kandra et.al.|[2503.07457](http://arxiv.org/abs/2503.07457)|null|人们经常观察到人类在对话过程中会调整自己的语言使用方式以适应对方。本文通过实证研究探讨大规模语言模型（LLMs）是否也表现出类似的对话适应行为。我们构建了一个大型语言模型之间的对话语料库，并发现两个语言模型代理在对话过程中最终会做出更加相似的句法选择，证实了现代大规模语言模型至少以一种初级的方式适应其对话伙伴的语言使用。|
|**2025-03-10**|**From Idea to Implementation: Evaluating the Influence of Large Language Models in Software Development -- An Opinion Paper**|Sargam Yadav et.al.|[2503.07450](http://arxiv.org/abs/2503.07450)|null|变压器架构的引入是自然语言处理（NLP）的一个转折点。基于变压器架构的模型，如双向编码器表示来自变压器（BERT）和生成预训练变压器（GPT），在各种应用中，如软件开发和教育，已经获得了广泛的认可。大型语言模型（LLMs）如ChatGPT和Bard对公众开放展示了这些模型的巨大潜力，并鼓励它们被整合到各个领域，例如在软件开发中用于代码生成、调试和文档生成等任务。在这项研究中，我们收集并分析了11位专家关于他们在软件开发中使用LLMs的经验意见，以获得可以指导成功和负责任地整合这些技术的见解。专家的整体意见是积极的，他们指出了提高生产力和减少编码时间等优点。同时，也强调了过度依赖的风险和伦理考虑等潜在的担忧和挑战。|
|**2025-03-10**|**From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector Graphics**|Jaewook Lee et.al.|[2503.07429](http://arxiv.org/abs/2503.07429)|null|大型语言模型（LLMs）的进步为通过自动化支持数学教育中的教师和学生提供了新的可能性。尽管先前的研究集中在生成数学问题和高质量的干扰项上，但可视化在数学学习中的作用仍未得到充分探索。图表对于数学思维和解决问题至关重要，然而手动创建它们既耗时又需要特定领域的专业知识，这限制了其可扩展性。最近关于使用LLMs生成可缩放矢量图形（SVG）的研究提出了一种自动化创建图表的有前景的方法。与基于像素的图像不同，SVG使用XML表示几何图形，允许无缝缩放和适应性。教育平台如Khan Academy和IXL已经使用SVG来显示数学问题和提示。在本文中，我们探讨了使用LLMs通过中间SVG表示生成与文本提示相关的数学图表的可能性。我们研究了三个问题：（1）如何自动生成数学问题解决提示中的图表并评估其质量，（2）SVG是否是数学图表的有效中间表示，以及（3）LLMs生成准确的基于SVG的图表所需的提示策略和格式。我们的贡献包括定义自动生成基于SVG的数学提示图表的任务，开发一种基于LLM提示的流水线，并确定提高图表生成的关键策略。此外，我们引入了一种基于视觉问答的评估设置，并进行消融研究以评估不同的流水线变化。通过自动化数学图表的创建，我们旨在为学生和教师提供准确、概念相关的视觉辅助工具，从而增强问题解决和学习体验。|
|**2025-03-10**|**RePO: ReLU-based Preference Optimization**|Junkang Wu et.al.|[2503.07426](http://arxiv.org/abs/2503.07426)|**[link](https://github.com/junkangwu/repo)**|对齐大型语言模型（LLMs）与人类偏好对于实际部署至关重要，但现有方法如RLHF面临计算和稳定性挑战。虽然DPO建立了一种具有单一超参数β的离线范式，后续方法如SimPO通过双重参数（β，γ）重新引入了复杂性。我们提出了基于ReLU的偏好优化（RePO），这是一种简化的算法，通过两项改进消除了β：(1) 保留SimPO的无参考边距，但通过梯度分析去除β，并(2)采用基于ReLU的最大边距损失，自然过滤掉琐碎的配对。从理论上讲，RePO被表征为SimPO的一个极限情况（β → ∞），其中逻辑加权坍缩为二元阈值处理，形成了0-1损失的凸包络。在AlpacaEval 2和Arena-Hard上的实证结果表明，RePO在多个基础模型上均优于DPO和SimPO，且只需要调整一个超参数。|
|**2025-03-10**|**REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding**|Yan Tai et.al.|[2503.07413](http://arxiv.org/abs/2503.07413)|**[link](https://github.com/MacavityT/REF-VLM)**|**多模态大型语言模型（MLLMs）在经过大规模数据集训练后，能够展现出强大的零样本视觉语言任务处理能力。然而，密集预测任务如语义分割和关键点检测对MLLMs提出了严峻挑战，特别是当这些任务仅通过文本输出时。同时，当前利用潜在嵌入进行视觉任务解码的MLLMs通常在多任务学习和多粒度场景适应性方面表现有限。在这项工作中，我们提出了一种端到端框架REF-VLM，用于统一训练各种视觉解码任务。为了应对复杂的视觉解码场景，我们引入了三元组基参照范式（TRP），该范式通过三元组结构显式地解耦了视觉解码任务中的三个关键维度：概念、解码类型和目标。TRP采用符号分隔符来强制结构化表示学习，从而增强模型输出的可解析性和可解释性。此外，我们构建了一个名为Visual-Task Instruction Following Dataset (VTInstruct)的大规模多任务数据集，其中包含了超过1亿个多模态对话样本，覆盖了25种任务类型。除了文本输入和输出外，VTInstruct还整合了多种视觉提示，如点、框、涂鸦和掩码，并生成由文本和视觉单元（如框、关键点、深度和掩码）组成的输出。不同视觉提示和视觉单元的组合生成了各种各样的任务类型，显著扩展了REF-VLM的应用范围。定性和定量实验表明，我们的REF-VLM在各种标准基准上优于其他MLLMs。代码、数据集和演示可在<https://github.com/MacavityT/REF-VLM>获取。**|
|**2025-03-10**|**Revisiting Noise in Natural Language Processing for Computational Social Science**|Nadav Borenstein et.al.|[2503.07395](http://arxiv.org/abs/2503.07395)|null|计算社会科学（CSS）是一个新兴领域，得益于人类生成内容的空前可用性。然而，由于该领域所探索的理论和数据集的性质，它也面临着独特的挑战，包括高度主观的任务和复杂的非结构化文本语料库。在这些挑战中，一个研究较少的话题是噪声的普遍存在。本论文旨在填补文献中的这一空白，通过一系列相互关联的案例研究来探讨CSS中不同形式的噪声。这些研究包括历史记录OCR处理后的字符级错误、古语、主观性和模糊任务中标注的一致性问题，甚至由大型语言模型在内容生成过程中引入的噪声和偏见。本论文挑战了CSS中噪声本质上是有害或无用的传统观念。相反，它认为某些形式的噪声可以编码有意义的信息，这对于推进CSS研究至关重要，例如个人独特的交流方式或数据集和任务的文化依赖性。此外，本论文强调了处理噪声时需要细致入微的方法，并展示了CSS研究人员在遇到噪声时必须考虑的重要性，表明不同类型的噪声需要不同的策略。|
|**2025-03-07**|**Understanding the Limits of Lifelong Knowledge Editing in LLMs**|Lukas Thede et.al.|[2503.05683](http://arxiv.org/abs/2503.05683)|null|保持大型语言模型的知识最新对于部署至关重要，但代价高昂的再训练仍然是一个挑战。知识编辑提供了一个有希望的替代方案，但这些方法仅在小规模或合成编辑基准上进行测试。在这项工作中，我们旨在将终身知识编辑的研究与实际应用中的大规模编辑联系起来。我们首先介绍了WikiBigEdit；这是一个大型的真实世界Wikidata编辑基准，旨在自动扩展终身学习以实现未来的基准测试。在其第一个版本中，它包含了超过500,000个用于知识编辑的问题-答案对，并附带了一个全面的评估流程。最后，我们使用WikiBigEdit来研究现有知识编辑技术在整合大量真实世界事实方面的能力，并将其能力与通用修改技术（如检索增强和持续微调）进行对比，以全面了解当前终身知识编辑的实际应用范围。|
|**2025-03-07**|**A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval**|Yu Zhang et.al.|[2503.05659](http://arxiv.org/abs/2503.05659)|**[link](https://github.com/tsinghua-fib-lab/llm-agent-for-recommendation-and-search)**|信息技术深刻改变了人类处理信息的方式。在线创建、共享和传播的大量内容使得获取相关信息变得日益困难。在过去的二十年里，搜索和推荐系统（统称为信息检索系统）为了应对这些挑战而得到了显著的发展。最近大型语言模型（LLM）的进步展示了其在各种语言相关任务上超越人类表现的能力，并且具备了通用理解、推理和决策能力。本文探讨了大型语言模型代理在提升搜索和推荐系统方面的变革潜力。我们讨论了LLM代理的动机和作用，并建立了一个分类框架来阐述现有的研究。我们强调了LLM代理在解决当前搜索和推荐系统挑战方面的巨大潜力，并提供了对未来研究方向的见解。本文首次系统地回顾并分类了这些领域关于LLM代理的研究，提供了一种利用这一先进人工智能技术进行信息检索的新视角。为了帮助理解现有研究，我们在以下链接列出了基于代理的大型语言模型模拟的相关论文：https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search。|
|**2025-03-07**|**Learning LLM Preference over Intra-Dialogue Pairs: A Framework for Utterance-level Understandings**|Xuanqing Liu et.al.|[2503.05620](http://arxiv.org/abs/2503.05620)|null|大型语言模型（LLMs）在处理复杂的对话任务时表现出显著的能力，无需进行特定用例的微调。然而，实时分析实时对话需要低延迟处理系统，这使得部署具有数十亿参数的模型变得不切实际，因为存在延迟限制。因此，从业者通常更喜欢参数在数百万的小型模型，这些模型是在高质量的人工注释数据集上训练的。然而，创建这样的数据集既耗时又昂贵。因此，迫切需要结合LLM生成标签的可扩展性和人工注释的精确性，使经过微调的小型模型能够实现比大型模型更高的速度和相当的准确性。在本文中，我们介绍了一个简单而有效的框架来解决这一挑战。我们的方法特别设计用于逐句分类问题，包括意图检测、对话状态跟踪等任务。为了减轻LLMs标签错误的影响——这是学生模型的主要不准确来源，我们提出了一种降噪偏好学习损失。实验结果表明，我们的方法显著提高了逐句对话任务的准确性，包括情感检测（超过2%），对话行为分类（超过1.5%）等。|
|**2025-03-07**|**A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models**|Dong Shu et.al.|[2503.05613](http://arxiv.org/abs/2503.05613)|null|大型语言模型（LLMs）在自然语言处理领域引发了革命，但其内部机制仍然很大程度上不透明。最近，机械性可解释性吸引了研究界的广泛关注，成为理解大型语言模型内在工作原理的一种手段。在各种机械性可解释性方法中，稀疏自动编码器（SAEs）作为一种特别有前途的方法脱颖而出，因为它们能够将大型语言模型内的复杂叠加特征解耦为更易于解释的组件。本文对SAEs作为解释和理解LLMs的一种有前景的方法进行了全面探讨。我们提供了系统性的概述，涵盖了专门针对LLM分析的SAE原理、架构和应用，包括理论基础、实施策略以及稀疏机制方面的最新发展。我们还探讨了如何利用SAEs来解释大型语言模型的内部运作、引导模型行为朝向期望的方向发展，并为未来模型开发更加透明的训练方法。尽管在SAE实现和扩展方面仍存在挑战，但它们继续为理解大型语言模型的内部机制提供了有价值的工具。|
|**2025-03-07**|**R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning**|Huatong Song et.al.|[2503.05592](http://arxiv.org/abs/2503.05592)|null|现有的大型推理模型（LRMs）展示了强化学习（RL）增强大型语言模型（LLMs）复杂推理能力的潜力。尽管它们在诸如数学和编码等具有挑战性的任务上表现出色，但它们通常依赖于内部知识来解决问题，这在时间敏感或知识密集型问题中可能不足，导致不准确和幻觉。为了解决这个问题，我们提出了R1-Searcher，这是一种新颖的两阶段基于结果的RL方法，旨在增强LLMs的搜索能力。这种方法允许LLMs在推理过程中自主调用外部搜索系统以获取额外的知识。我们的框架完全依赖于RL，无需过程奖励或蒸馏即可实现冷启动。实验表明，我们的方法显著优于先前强大的RAG方法，甚至超过了闭源的GPT-4o-mini。|
|**2025-03-07**|**Evaluating open-source Large Language Models for automated fact-checking**|Nicolo' Fontana et.al.|[2503.05565](http://arxiv.org/abs/2503.05565)|null|不断增加的在线虚假信息使得自动化事实核查解决方案的需求日益增长。大型语言模型（LLMs）作为潜在工具出现，但其有效性仍不确定。本研究评估了各种开源LLMs的事实核查能力，重点关注它们在处理具有不同层次上下文信息的陈述时的表现。我们进行了三个关键实验：（1）评估LLMs是否能够识别陈述与事实核查文章之间的语义关系，（2）评估模型在提供相关事实核查文章时验证陈述的准确性，以及（3）测试LLMs利用来自外部知识源（如Google和Wikipedia）的数据进行事实核查的能力。我们的结果显示，LLMs在识别陈述文章连接和验证已事实核查的故事方面表现良好，但在确认事实新闻方面表现不佳，后者的性能被传统的微调模型如RoBERTa超越。此外，引入外部知识并未显著提升LLMs的表现，这要求采用更加针对性的方法。我们的研究结果既突显了LLMs在自动化事实核查中的潜力和局限性，强调了在这些模型能够可靠地替代人工事实核查员之前需要进一步改进的必要性。|
|**2025-03-07**|**Revitalizing Saturated Benchmarks: A Weighted Metric Approach for Differentiating Large Language Model Performance**|Bryan Etzine et.al.|[2503.05551](http://arxiv.org/abs/2503.05551)|null|现有的基准测试正变得饱和，并且由于数据污染和大型语言模型（LLM）能力的提升等因素，难以区分模型性能。本文介绍了一种名为EMDM（增强模型区分度指标）的新颖加权指标，该指标通过整合最终答案和链式思维（CoT）推理的正确性，并根据解决评估数据中给定样本所需的复杂性和推理深度分配权重，从而提升了基准测试的效果。使用基线LLM在两种设置下——无指导设置，模型对测试样本没有先验接触；有指导设置，模型事先知道期望的答案——EMDM能够区分不同难度的实例。这些设置下的CoT和答案正确性被用来优化权重分配的目标函数，从而更细致地评估模型性能。与实现17%区分度的精确匹配（EM）指标相比，EMDM在ARC-Challenge上的区分度达到了46%，证明了其在基于推理和知识需求区分模型方面的有效性。|
|**2025-03-07**|**Leveraging Approximate Caching for Faster Retrieval-Augmented Generation**|Shai Bergman et.al.|[2503.05530](http://arxiv.org/abs/2503.05530)|null|Retrieval增强型生成（RAG）通过整合外部知识提高了大型语言模型（LLM）回答的可靠性。然而，RAG增加了端到端的推理时间，因为从大规模向量数据库中查找相关文档在计算上是昂贵的。为了解决这个问题，我们引入了Proximity，这是一种近似键值缓存，通过利用用户查询中的相似性来优化RAG工作流程。它不是将每个查询独立处理，而是当出现相似查询时重用先前检索到的文档，从而减少对昂贵的向量数据库查询的依赖。我们在MMLU和MedRAG基准上评估了Proximity，结果表明它显著提高了检索效率，同时保持了响应准确性。Proximity将检索延迟减少了高达59%，同时保持了准确性和降低了向量数据库的计算负担。我们还尝试了不同的相似性阈值，并量化了速度与召回率之间的权衡。我们的研究表明，近似缓存是一种可行且有效的策略，用于优化基于RAG的系统。|
|**2025-03-07**|**PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs**|Roberto Cerina et.al.|[2503.05529](http://arxiv.org/abs/2503.05529)|null|本文介绍了一种名为PoSSUM的开源协议，该协议通过多模态大语言模型（LLMs）对社交媒体用户进行无创性民意测验。PoSSUM利用用户的实时帖子、图片和其他数字痕迹创建硅样本，这些样本捕捉到了大语言模型训练数据中缺失的信息。为了获得有代表性的估计结果，PoSSUM采用了多层次回归和后分层（MrP）方法，并结合结构化先验来抵消社交媒体平台的可观察选择偏差。该协议在2024年美国总统选举期间得到验证，在此期间进行了五次PoSSUM民调，并发布在GitHub和X平台上。在最后一次民调中，PoSSUM于10月17日至26日进行，合成样本包括1054名X平台用户，准确预测了50个州中的49个州的结果，并给予共和党候选人65%的胜选概率。值得注意的是，它在各州层面的偏差低于大多数传统民调机构。这些结果表明PoSSUM作为一种全自动的、无创的替代传统调查方法具有潜力。|
|**2025-03-07**|**Cognitive Bias Detection Using Advanced Prompt Engineering**|Frederic Lemieux et.al.|[2503.05516](http://arxiv.org/abs/2503.05516)|null|认知偏差是指判断过程中系统性偏离理性的情况，对生成客观内容构成了显著挑战。本文介绍了一种利用大型语言模型（LLMs）和先进的提示工程技术实现实时认知偏差检测的新方法。所提出的系统通过分析文本数据来识别常见的认知偏差，如确认偏差、循环推理和隐藏假设。通过设计定制的提示，该系统有效利用了LLMs识别和缓解这些偏差的能力，从而提高人类生成内容（如新闻、媒体、报告）的质量。实验结果表明，我们的方法在识别认知偏差方面具有高准确性，为增强内容客观性和降低偏差决策风险提供了有价值的工具。|
|**2025-03-06**|**L $^2$ M: Mutual Information Scaling Law for Long-Context Language Modeling**|Zhuo Chen et.al.|[2503.04725](http://arxiv.org/abs/2503.04725)|**[link](https://github.com/LSquaredM/mutual_info_scaling_law)**|我们严格建立了自然语言中的二分互信息缩放定律，以规范长距离依赖。该缩放定律与传统的两点互信息不同，并且独立于其缩放。它是理解长上下文语言建模的关键。通过这一缩放定律，我们制定了长上下文语言建模（L²M）条件，该条件将模型对有效长上下文长度建模的能力与其存储过去信息的潜在状态大小的增长联系起来。我们的结果通过在变换器和状态空间模型上的实验得到验证。这项工作为大型语言模型向更长上下文长度的发展奠定了理论基础。|
|**2025-03-07**|**Shifting Long-Context LLMs Research from Input to Output**|Yuhao Wu et.al.|[2503.04723](http://arxiv.org/abs/2503.04723)|null|近期在长上下文大型语言模型（LLMs）的研究主要集中在处理扩展的输入上下文上，这使得在长上下文理解方面取得了显著进展。然而，生成长篇文章这一同样重要的方面却相对较少受到关注。本文倡导NLP研究领域应转向解决长篇输出生成的挑战。诸如小说创作、长期规划和复杂推理等任务要求模型能够理解广泛的上下文并生成连贯、内容丰富且逻辑一致的长篇文章。这些需求突显了当前LLM能力中的关键差距。我们强调这一尚未充分探索领域的紧迫性，并呼吁集中努力开发专门用于生成高质量长篇文章的基础LLM，这些模型在现实世界应用中具有巨大潜力。|
|**2025-03-06**|**Enough Coin Flips Can Make LLMs Act Bayesian**|Ritwik Gupta et.al.|[2503.04722](http://arxiv.org/abs/2503.04722)|null|大型语言模型（LLMs）在输入提示中通过少量示例展示出泛化的能力，这一新兴能力被称为即时学习（ICL）。我们研究了LLMs是否利用ICL以符合贝叶斯框架的方式进行结构化推理，或者仅仅依赖于模式匹配。使用有偏硬币投掷的控制设置，我们发现：（1）LLMs通常具有有偏的先验，导致在零样本设置下初始时出现偏差，（2）即时上下文证据比显式偏差指令更有分量，（3）LLMs广泛遵循贝叶斯后验更新，偏差主要由于先验校准不当而非更新过程中的缺陷，以及（4）注意力强度对贝叶斯推理影响甚微。通过足够的有偏硬币投掷演示，LLMs能够以贝叶斯方式更新其先验。|
|**2025-03-06**|**Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining**|Houyi Li et.al.|[2503.04715](http://arxiv.org/abs/2503.04715)|null|大型语言模型（LLMs）在各种任务中的强大能力已经得到广泛认可，但其有效部署需要仔细优化超参数。通过广泛的实证研究，我们在不同配置中进行了网格搜索，发现了控制这些超参数的普遍缩放规律：最优学习率与模型参数和数据量之间遵循幂律关系，而最优批次大小主要随数据量变化。我们的分析揭示了在固定模型和数据量条件下的超参数优化景观呈凸形。这种凸性意味着存在一个最优超参数平台期。我们为社区贡献了一个通用的即插即用最优超参数工具。该工具在测试集上的估计值与通过穷尽搜索找到的全局最优LLM性能相差仅为0.07%。这些规律在模型稀疏性、训练数据分布和模型结构的变化中表现出显著的鲁棒性。据我们所知，这是首次统一不同模型形状和结构的工作，例如专家混合模型和密集变换器，并建立了跨越不同数据分布的最优超参数缩放规律。这一全面优化过程需要大量的计算资源，使用了近一百万个NVIDIA H800 GPU小时来从头开始训练3700个具有不同规模和超参数的LLMs，并总共消耗了大约100万亿个令牌。为了促进可重复性和进一步的研究，我们将逐步通过指定的存储库https://step-law.github.io/发布所有损失测量和模型检查点。|
|**2025-03-06**|**Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size**|Alireza Behtash et.al.|[2503.04704](http://arxiv.org/abs/2503.04704)|null|我们提出了一种新颖的选择性模型量化方法，该方法超越了大型语言模型（LLMs）中依赖于架构和大小的压缩方法的局限性，即熵加权量化（Entropy-Weighted Quantization, EWQ）。通过分析跨变换器块的熵分布，EWQ确定哪些块可以被安全地量化而不会导致显著的性能下降，这种方法独立于模型架构或大小。我们的方法优于均匀量化方法，在保持巨大多任务语言理解（MMLU）准确率在未量化模型的0.5%以内的同时，减少了高达18%的内存使用。我们展示了EWQ在多种架构上的有效性——从16亿到700亿参数——展示了在质量-压缩之间的权衡上一致的改进，不受模型规模或架构设计的影响。EWQ的一个意外发现是它能够减少相对于未量化的模型的困惑度，这表明通过选择性的精度降低存在有益的正则化效果。这种改进在不同的模型族中持续存在，表明层级熵与最优精度需求之间存在基本关系。此外，我们引入了FastEWQ，这是一种快速的熵分布分析方法，无需加载模型权重。该技术利用了在各种架构和规模中持续存在的熵分布的通用特征，使得在保持80%分类准确率的情况下，可以近乎即时地做出量化决策。我们的结果表明，有效的量化策略可以独立于特定的架构选择或模型大小开发，为高效的LLMs部署开辟了新的可能性。|
|**2025-03-06**|**UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets**|Wenyu Wang et.al.|[2503.04693](http://arxiv.org/abs/2503.04693)|null|大型语言模型（LLMs）在训练过程中不可避免地会获取有害信息。LLM无学习旨在消除这些有害信息的影响，同时保持模型的整体性能。现有的无学习方法，以梯度上升方法为代表，主要集中在忘记目标数据上，而忽略了逻辑相关知识对无学习效果的重要影响。通过理论和实验分析，我们首先证明了无学习性能不佳的一个关键原因是模型可以通过与逻辑相关的知识推理重建目标内容。为了解决这个问题，我们提出了通过参数外推改进无学习（UIPE），该方法去除了与遗忘目标高度相关的知识。实验结果表明，UIPE显著提升了各种主流LLM无学习方法在TOFU基准上的性能。|
|**2025-03-06**|**Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases**|Pengcheng Qiu et.al.|[2503.04691](http://arxiv.org/abs/2503.04691)|null|最新的增强推理的大语言模型（推理LLMs），如DeepSeek-R1和OpenAI-o3，在各领域取得了显著成功。然而，这种推理增强在高度专业的医学领域的应用尚未得到明确评估，尤其是在不仅评估最终生成结果的同时，还考察其推理过程的质量方面。在这项研究中，我们提出了MedR-Bench，这是一个推理聚焦的医学评估基准，包含了从病例报告中挖掘出的1,453个结构化的患者案例。我们的基准涵盖了13个身体系统和10种专科疾病，包括常见和罕见疾病。在评估中，我们引入了一个全面的框架，该框架由三个关键的临床阶段组成：评估建议、诊断决策和治疗计划，全面捕捉LLMs在整个患者医疗旅程中的表现。  对于评估指标，我们提出了一种新的代理系统——推理评估器，旨在以可扩展的方式自动化并客观量化自由文本推理响应的效率、准确性和完整性，通过动态搜索和执行交叉引用检查来实现。作为结果，我们评估了五种最先进的推理LLMs，包括DeepSeek-R1、OpenAI-o3-mini等。我们的结果显示，当前的LLMs能够处理相对简单的诊断任务，并且具有足够的批判性评估结果，总体准确率超过85%。然而，它们在更复杂的任务上仍然存在困难，例如评估建议和治疗计划。在推理方面，它们的推理过程通常是可靠的，事实性得分超过了90%，尽管它们经常遗漏关键的推理步骤。我们的研究清楚地揭示了当前临床LLMs进一步发展的方向。|
|**2025-03-06**|**LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable User Satisfaction Estimation in Dialogue**|Sangyeop Kim et.al.|[2503.04675](http://arxiv.org/abs/2503.04675)|null|理解用户对对话系统的满意度，即用户满意度估计（USE），对于评估对话质量和提升用户体验至关重要。然而，现有的USE方法面临着挑战，因为它们对用户不满的根本原因理解有限，并且注释用户意图的成本很高。为了解决这些挑战，我们提出了PRAISE（计划与检索对齐以进行可解释的满意度估计），这是一种可解释的框架，用于有效的用户满意度预测。PRAISE通过三个关键模块运作。策略规划器开发策略，这是分类用户满意度的自然语言标准。特征检索器然后从大型语言模型（LLM）中获取关于用户满意度的知识，并从话语中检索相关特征。最后，评分分析器评估策略预测并分类用户满意度。实验结果表明，PRAISE在三个基准测试中达到了最先进的性能。除了其卓越的性能外，PRAISE还提供了额外的好处。它通过有效对齐话语和策略提供实例级别的解释来增强可解释性。此外，PRAISE在推理阶段消除了对LLM的需求，从而提高了效率。|
|**2025-03-06**|**Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment**|Wen Yang et.al.|[2503.04647](http://arxiv.org/abs/2503.04647)|null|Direct Preference Optimization (DPO)已经成为使大型语言模型（LLMs）与人类偏好对齐的一种重要方法。尽管DPO在对齐英语LLMs方面取得了显著进展，但由于数据稀缺性，多语言偏好对齐受到了阻碍。为了解决这一问题，我们提出了一种新颖的方法，该方法通过隐式奖励从对齐良好的英语模型中捕捉学习到的偏好，并通过迭代训练将其转移到其他语言。具体来说，我们从一个英语DPO对齐模型及其相应的参考模型的logits中推导出一个隐式奖励模型。然后，利用这个奖励模型来标注跨语言指令跟随对中的偏好关系，使用英语指令来评估多语言响应。标注的数据随后用于多语言DPO微调，从而促进从英语到其他语言的偏好知识转移。对Llama3进行两次迭代微调后，在X-AlpacaEval排行榜上所有训练语言的胜率平均提高了12.72%，长度控制胜率提高了5.97%。我们的研究结果表明，利用现有的英语对齐模型可以实现高效且有效的多语言偏好对齐，大大减少了对大量多语言偏好数据的需求。代码可在<https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding>获取。|
|**2025-03-06**|**Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking**|Yijie Xu et.al.|[2503.04636](http://arxiv.org/abs/2503.04636)|null|随着开源大型语言模型（LLM）如Llama3变得越来越强大，开发用于检测其潜在滥用的水印技术变得至关重要。现有的水印方法要么在LLM推理过程中添加水印，这不适合开源LLM，要么主要针对分类LLM而不是最近的生成型LLM。适应这些水印以用于开源LLM的滥用检测仍然是一个开放性挑战。本工作定义了开源LLM的两种滥用场景：知识产权（IP）侵犯和LLM使用违规。然后，我们在这些背景下探索推理时水印蒸馏和后门水印的应用。我们提出了全面的评估方法来评估各种现实中的进一步微调场景对水印的影响以及这些水印对LLM性能的影响。我们的实验表明，后门水印能够有效地检测IP侵犯，而推理时水印蒸馏在这两种场景中都是适用的，但对进一步微调的鲁棒性较差，并且对LLM性能的影响比后门水印更大。探索更多先进的水印方法以用于开源LLM的滥用检测应该是未来的一个重要方向。|
|**2025-03-05**|**The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems**|Richard Ren et.al.|[2503.03750](http://arxiv.org/abs/2503.03750)|null|随着大型语言模型（LLMs）变得越来越有能力并且更具自主性，对其输出的信任需求显著增加。然而，同时也有越来越多的担忧，即这些模型可能会学会为了达到目标而撒谎。为了解决这些问题，关于LLMs“诚实”概念的研究以及旨在减轻欺骗行为的干预措施逐渐兴起。然而，目前对诚实的评估非常有限，并且没有一个基准能够大规模地适用于所有模型。此外，许多声称衡量诚实性的基准实际上只是测量了模型信念的准确性，这掩盖了真正的诚实度。在这项工作中，我们引入了一个大规模的人类收集数据集，用于直接测量诚实性，使我们首次能够将准确性与诚实性区分开来。在一系列不同的LLMs上，我们发现虽然较大的模型在我们的基准测试中获得了更高的准确性，但它们并没有变得更加诚实。令人惊讶的是，尽管大多数前沿LLMs在真实性基准测试中得分较高，但我们发现前沿LLMs在被施压时有相当大的倾向去撒谎，导致在我们的基准测试中的诚实分数较低。我们发现，诸如表示工程干预等简单方法可以提高诚实性。这些结果突显了对可靠评估和有效干预措施的日益增长的需求，以确保LLMs保持可信性。|
|**2025-03-05**|**Process-based Self-Rewarding Language Models**|Shimao Zhang et.al.|[2503.03746](http://arxiv.org/abs/2503.03746)|null|大型语言模型在各种下游任务中表现出色，并已在多个场景中得到广泛应用。为了进一步提高这些模型的性能，使用了人工标注的偏好数据进行训练，但这受限于人类表现的上限。因此，提出了自我奖励方法，其中语言模型通过奖励自己的输出来生成训练数据。然而，现有的自我奖励范式在数学推理场景中效果不佳，甚至可能导致性能下降。在这项工作中，我们提出了一种基于过程的自我奖励管道，该管道引入了长时间思考的推理、逐步的LLM作为裁判以及逐步的偏好优化，从而在自我奖励范式内进行了改进。我们的新范式通过迭代的基于过程的自我奖励成功提升了多个数学推理基准上的模型性能，展示了自我奖励实现超越人类能力的LLM推理的巨大潜力。|
|**2025-03-05**|**Towards Understanding Distilled Reasoning Models: A Representational Approach**|David D. Baek et.al.|[2503.03730](http://arxiv.org/abs/2503.03730)|null|在本文中，我们研究模型蒸馏如何影响大型语言模型（LLMs）中推理特征的发展。为此，我们在Qwen系列模型及其微调变体上训练了一个交叉编码器。我们的结果表明，交叉编码器学习到了与各种类型推理相对应的特征，包括自我反思和计算验证。此外，我们观察到蒸馏模型包含了独特的推理特征方向，这些方向可以用来引导模型进入过度思考或敏锐思考模式。特别是，我们对四种特定的推理类别进行了分析：(a) 自我反思，(b) 演绎推理，(c) 替代推理，以及 (d) 对比推理。最后，我们检查了蒸馏过程导致的特征几何变化，并发现较大规模的蒸馏模型可能发展出更结构化的表示，这与增强的蒸馏性能相关。通过提供有关蒸馏如何改变模型的见解，我们的研究有助于提高AI系统的透明度和可靠性。|
|**2025-03-05**|**Improving LLM Safety Alignment with Dual-Objective Optimization**|Xuandong Zhao et.al.|[2503.03710](http://arxiv.org/abs/2503.03710)|**[link](https://github.com/wicai24/door-alignment)**|现有的大型语言模型（LLMs）在训练过程中的安全性对越狱攻击仍然很脆弱。直接偏好优化（DPO）是一种广泛使用的对齐方法，但在实验和理论背景下都显示出局限性，因为其损失函数对于拒绝学习来说并不理想。通过基于梯度的分析，我们识别出这些不足之处，并提出了一种改进的安全性对齐方法，该方法将DPO目标分解为两个部分：(1) 强健的拒绝训练，即使在生成部分不安全内容时也鼓励模型拒绝，以及(2) 针对有害知识的定向消融。这种方法显著提高了LLM对各种越狱攻击的鲁棒性，包括预填充、后缀和多轮攻击，在分布内和分布外场景中均有效。此外，我们引入了一种通过引入基于奖励的令牌级加权机制来强调关键拒绝令牌的方法，这进一步提升了模型对抗恶意利用的鲁棒性。我们的研究还表明，对越狱攻击的鲁棒性与训练过程中令牌分布的变化以及拒绝和有害令牌的内部表示有关，为未来LLM安全性对齐的研究提供了有价值的方向。代码可在https://github.com/wicai24/DOOR-Alignment获取|
|**2025-03-05**|**Effective LLM Knowledge Learning via Model Generalization**|Mingkang Zhu et.al.|[2503.03705](http://arxiv.org/abs/2503.03705)|null|大型语言模型（LLMs）是在包含大量世界知识的文档上进行训练的。然而，通过自回归预训练获取这些知识的过程仍不甚明了。这种理解上的缺乏极大地阻碍了有效知识的学习，尤其是对于继续预训练以获取最新信息来说，因为这些不断演变的信息往往缺乏像基础性知识那样的多样化重复。在本文中，我们专注于理解和改进LLM的知识学习。我们发现并验证了LLM的知识学习可以被视为隐藏在自回归预训练目标中的隐式监督任务。我们的研究结果表明，针对监督任务提升泛化能力的方法有助于改进LLM的知识学习。基于我们的分析，我们提出了基于格式的数据增强方法来扩展分布样本，这种方法不会像文本改写那样改变文档中嵌入的事实。我们还介绍了锐度感知最小化作为一种有效的优化算法，以更好地提升泛化能力。此外，我们的分析和方法可以很容易地扩展到指令调优。广泛的实验结果验证了我们的发现，并展示了我们的方法在继续预训练和指令调优中的有效性。本文为解释和设计LLM知识学习的有效策略提供了新的视角和见解。|
|**2025-03-05**|**A Practical Memory Injection Attack against LLM Agents**|Shen Dong et.al.|[2503.03704](http://arxiv.org/abs/2503.03704)|null|基于大型语言模型（LLM）的代理在广泛的复杂现实世界应用中展示了强大的能力。然而，当用于演示的过去记录被篡改时，具有受损记忆库的LLM代理可能会轻易产生有害输出。在本文中，我们提出了一种新颖的记忆注入攻击（MINJA），该攻击仅通过查询和输出观察与代理进行交互即可向记忆库中注入恶意记录。这些恶意记录旨在引发一系列导致不当代理行为的恶意推理步骤，从而执行受害用户的问题。具体而言，我们引入了一系列桥梁步骤以链接受害查询和恶意推理步骤。在注入恶意记录的过程中，我们提出了一个指示提示，以引导代理自主生成我们设计的桥梁步骤。我们还提出了一种逐步缩短策略，逐渐移除指示提示，从而使恶意记录在处理受害查询时更容易被检索到。我们的广泛实验跨越了不同的代理，证明了MINJA在损害代理记忆方面的有效性。由于执行所需的资源最少，MINJA使任何用户都能影响代理的记忆，突显了LLM代理的实际风险。|
|**2025-03-05**|**Developing and Utilizing a Large-Scale Cantonese Dataset for Multi-Tasking in Large Language Models**|Jiyue Jiang et.al.|[2503.03702](http://arxiv.org/abs/2503.03702)|null|高质量的数据资源在训练大型语言模型（LLMs）中起着至关重要的作用，特别是在处理低资源语言如粤语时。尽管粤语拥有超过8500万的母语使用者，但由于诸如普通话的主导地位、粤语使用群体缺乏凝聚力、字符编码和输入方法的多样性以及海外粤语使用者倾向于使用英语等因素，粤语在自然语言处理（NLP）领域仍然被视为低资源语言。此外，粤语丰富的口语词汇、英语借词和代码转换特性增加了语料库收集和处理的复杂性。为了解决这些挑战，我们从多种来源收集了粤语文本，包括开源语料库、香港特定论坛、维基百科和Common Crawl数据。我们通过语言过滤、质量过滤、内容过滤和去重等严格的步骤进行数据处理，成功构建了一个超过20亿词元的高质量粤语文本语料库用于训练大型语言模型。我们进一步通过精心设计的粤语任务对模型进行了监督微调（SFT），提高了其处理特定应用的能力。完成训练后，该模型在四个粤语基准测试上达到了最先进的性能，并且在其他主流语言任务上也表现出更好的性能。|
|**2025-03-05**|**Addressing Overprescribing Challenges: Fine-Tuning Large Language Models for Medication Recommendation Tasks**|Zihao Zhao et.al.|[2503.03687](http://arxiv.org/abs/2503.03687)|**[link](https://github.com/zzhustc2016/lamo)**|药物推荐系统在医疗领域引起了广泛关注，因为它们能够根据患者的临床数据提供个性化且有效的药物组合。然而，现有的方法在适应多样化的电子健康记录（EHR）系统和有效利用非结构化数据方面面临挑战，导致其泛化能力有限和性能不佳。最近，人们越来越关注在医学领域利用大型语言模型（LLM）来支持医疗专业人员并提高患者护理水平。尽管出现了专门针对医学的LLM，并且在诸如医学问答等任务上取得了有希望的结果，但它们在临床环境中实际应用特别是在药物推荐方面的研究仍然不足。在本研究中，我们评估了通用和专门针对医学的LLM在药物推荐任务中的表现。我们的研究结果表明，LLM经常遇到过度开药的问题，这导致临床风险增加和药物推荐准确性下降。为了解决这个问题，我们提出了语言辅助药物推荐（LAMO），该方法采用参数高效的微调方法来优化开源LLM在药物推荐场景中的表现。LAMO利用临床笔记中丰富的临床信息，而这些信息在传统方法中往往未被充分利用。由于这种方法，LAMO在内部验证准确率上比先前最先进的方法高出10%以上。此外，时间和外部验证显示LAMO在不同时间和医院环境中的强大泛化能力。另外，一项超出分布范围的药物推荐实验表明，即使在训练数据之外的药物情况下，LAMO也表现出显著的准确性。|
|**2025-03-05**|**Attentive Reasoning Queries: A Systematic Method for Optimizing Instruction-Following in Large Language Models**|Bar Karov et.al.|[2503.03669](http://arxiv.org/abs/2503.03669)|**[link](https://github.com/emcie-co/parlant)**|我们提出了注意力推理查询（ARQs），这是一种新颖的结构化推理方法，通过领域专用的推理蓝图显著提高了大型语言模型在遵循指令方面的能力。尽管这些模型在各种任务中表现出色，但在多轮对话中往往难以遵守复杂的、特定用例的指令，这给关键业务应用带来了挑战。ARQs通过系统化的推理步骤来指导大型语言模型，使用有针对性的问题重新启动关键指令，并在整个完成过程中促进中间推理。在Parlant框架内进行的广泛测试中，ARQs在87个测试场景中达到了90.2%的成功率，超过了链式思考推理（86.1%）和直接响应生成（81.5%）。ARQs在解决持续存在的失效模式如指南重申应用和幻觉预防方面尤为强大。我们的分析还表明，如果精心设计，ARQs可能比自由形式推理更具有计算效率。这些发现表明，结构化推理方法为控制大型语言模型在复杂场景下处理信息和做出决策提供了有效机制。|
|**2025-03-05**|**Analogical Reasoning Inside Large Language Models: Concept Vectors and the Limits of Abstraction**|Gustaw Opiełka et.al.|[2503.03666](http://arxiv.org/abs/2503.03666)|**[link](https://github.com/gucioopielka/concept_vectors)**|类比推理依赖于概念抽象，但目前尚不清楚大型语言模型（LLMs）是否具备这样的内部表征。我们探索了从LLMs激活中提取的蒸馏表示，并发现函数向量（FVs；Todd等，2024）——用于情境学习（ICL）任务的紧凑表示——对于简单的输入变化（例如开放式与多项选择题）并不保持不变，这表明它们捕捉到的信息不仅仅是纯粹的概念。使用表征相似性分析（RSA），我们定位了一小部分注意力头，这些头编码了对词语概念如“反义词”的不变概念向量（CVs）。这些CVs作为特征检测器独立于最终输出工作——这意味着模型可能形成了正确的内部表征，但仍会生成错误的输出。此外，CVs可以因果地引导模型行为。然而，对于更抽象的概念如“前一个”和“下一个”，我们没有观察到不变的线性表示，我们将这一发现与LLMs在这些领域内表现出的泛化问题联系起来。|
|**2025-03-04**|**Wikipedia in the Era of LLMs: Evolution and Risks**|Siming Huang et.al.|[2503.02879](http://arxiv.org/abs/2503.02879)|**[link](https://github.com/hsm316/llm_wikipedia)**|在本文中，我们对大型语言模型（LLMs）对维基百科的影响进行了深入分析，通过现有数据考察了维基百科的演变，并使用模拟来探讨潜在风险。我们首先通过分析页面浏览量和文章内容来研究维基百科的近期变化，并评估LLMs的影响。随后，我们评估了LLMs对与维基百科相关的各种自然语言处理（NLP）任务的影响，包括机器翻译和检索增强生成（RAG）。我们的发现和模拟结果表明，维基百科的文章受到了LLMs的影响，在某些类别中的影响约为1%-2%。如果基于维基百科的机器翻译基准受到LLMs的影响，模型的分数可能会膨胀，各模型之间的比较结果也可能发生变化。此外，如果知识库被LLM生成的内容污染，RAG的有效性可能会降低。尽管LLMs尚未完全改变维基百科的语言和知识结构，但我们认为我们的实证发现表明需要仔细考虑潜在的未来风险。|
|**2025-03-04**|**The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models**|Ke Ji et.al.|[2503.02875](http://arxiv.org/abs/2503.02875)|null|提高大型语言模型（LLMs）的推理能力通常需要使用带有标签的数据进行监督微调或计算成本高昂的采样。我们介绍了无监督前缀微调（UPFT），它利用了前缀自一致性——在各种解决方案路径中共享的初始推理步骤——来增强LLM的推理效率。通过仅训练初始前缀子串（最少8个token），UPFT去除了对带标签数据或详尽采样的需求。在推理基准测试中的实验表明，UPFT与监督方法（如拒绝采样微调）的性能相匹配，同时将训练时间减少了75%，并将采样成本降低了99%。进一步分析表明，错误往往出现在推理过程的后期阶段，并且基于前缀的训练保留了模型的结构知识。这项工作展示了少量无监督微调如何解锁LLMs中的大量推理增益，提供了一种可扩展且资源高效的替代传统方法的方案。|
|**2025-03-04**|**Prompting Generative AI with Interaction-Augmented Instructions**|Leixian Shen et.al.|[2503.02874](http://arxiv.org/abs/2503.02874)|null|生成型人工智能（GenAI）模型的出现，包括大型语言模型和文本到图像模型，不仅因其卓越的能力而显著提升了人类与人工智能之间的协同作用，更因其通过文本提示进行直观交流的方式。尽管直观，基于文本的指令由于自然语言的模糊性和冗余性而存在问题。为了解决这一问题，研究人员探索了增强基于文本的指令的方法，例如通过直接操作来促进精确有效的表达人类意图的交互。然而，交互增强指令的设计策略缺乏系统性研究，阻碍了我们对这些方法的理解和应用。为了全面了解交互增强指令，我们提出了一种框架来分析相关工具在何时、何人、使用什么以及如何应用交互以增强基于文本的指令。值得注意的是，我们确定了应用交互的四个目的，包括限制、扩展、组织和精化文本指令。还总结了每种目的的设计范式，以造福未来的研究人员和从业者。|
|**2025-03-04**|**FairSense-AI: Responsible AI Meets Sustainability**|Shaina Raza et.al.|[2503.02865](http://arxiv.org/abs/2503.02865)|null|在本文中，我们介绍了FairSense-AI：一个旨在检测和缓解文本及图像中偏见的多模态框架。通过利用大规模语言模型（LLMs）和视觉-语言模型（VLMs），FairSense-AI揭示了内容中可能出现的微妙形式的偏见或刻板印象，为用户提供偏差分数、解释性亮点以及公平性增强的自动化建议。此外，FairSense-AI集成了与MIT AI风险库和NIST AI风险管理框架相一致的人工智能风险评估组件，实现了伦理和安全问题的结构化识别。该平台通过模型剪枝和混合精度计算等技术优化了能源效率，从而减少了其环境足迹。通过一系列案例研究和应用，我们展示了FairSense-AI如何通过解决公平性的社会层面以及大型人工智能部署中迫切需要的可持续性来促进负责任的人工智能使用。|
|**2025-03-04**|**Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt Aggregation Framework**|Ziang Zhou et.al.|[2503.02863](http://arxiv.org/abs/2503.02863)|null|大型语言模型（LLMs）通常表现出不一致的置信分数，通常会高估其预测的可靠性。尽管语言化的置信度在大型语言模型（LLMs）中已经引起关注，但先前的研究对通过提示系统地引导置信度得分存在分歧，甚至有研究认为这种提示引起的置信度变化可以忽略不计，表明LLMs的置信度校准对语言干预是刚性的。与这些观点相反，我们首先严格确认了方向性置信度变化的存在，通过对三个模型（包括GPT3.5、LLAMA3-70b、GPT4）进行7个基准测试，证明明确的指令可以以受控方式增强或减弱置信分数。基于这一观察结果，我们提出了一种新的框架，包含三个组件：置信度引导、引导置信度聚合和引导答案选择，命名为SteeringConf。我们的方法，SteeringConf，利用一种置信度操作机制，以多种期望的方向引导LLMs的置信度分数，然后通过一个汇总模块聚合引导后的置信度分数以生成最终预测。我们在7个基准上评估了我们的方法，结果显示在置信度校准和失效检测任务中，它始终优于基线方法。|
|**2025-03-04**|**Privacy and Accuracy-Aware AI/ML Model Deduplication**|Hong Guan et.al.|[2503.02862](http://arxiv.org/abs/2503.02862)|null|随着差分隐私随机梯度下降（DP-SGD）等隐私保护机器学习算法的广泛应用，对私有数据集进行模型训练或微调变得越来越普遍。这种转变导致了需要提供不同隐私保证和实用水平的模型版本以满足各种用户需求。然而，管理大量版本的大模型带来了显著的操作挑战，包括增加的推理延迟、更高的资源消耗和更高的成本。模型去重是许多模型服务和数据库系统广泛使用的技术，旨在支持高性能和低成本的推理查询及模型诊断查询。然而，现有的所有模型去重工作都没有考虑隐私问题，导致某些去重模型的隐私成本无界累积，并且在应用于去重DP训练模型时效率低下。我们首次形式化了DP训练模型的去重问题，并提出了一种新的隐私和准确率感知的去重机制来解决这些问题。我们开发了一种贪婪策略来选择并分配基模型到目标模型，以最小化存储和隐私成本。在去重目标模型时，我们动态安排准确率验证，并应用稀疏向量技术以减少与私有验证数据相关的隐私成本。与不提供隐私保证的基线方法相比，我们的方法将单个模型（包括大型语言模型和视觉变换器）的压缩比提高了多达35倍。我们还观察到由于I/O操作的减少，推理速度提高了高达43倍。|
|**2025-03-04**|**Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers**|Zicong He et.al.|[2503.02851](http://arxiv.org/abs/2503.02851)|**[link](https://github.com/ziconghe2002/hcl-spark)**|大型语言模型（LLMs）存在幻觉现象，这一现象通常与创造力有关。尽管之前的研究主要通过理论或定性角度探索了这种联系，我们的工作采取了一种定量的方法来系统地研究LLMs中的幻觉和创造力之间的关系。鉴于创造力的复杂性质，我们提出了一个针对LLMs的狭义定义，并引入了一个评估框架HCL，在解码过程中量化不同层次上的幻觉和创造力。我们的实证分析揭示了幻觉和创造力之间的一致性权衡，这种权衡在层深度、模型类型和模型大小上保持一致。值得注意的是，在不同的模型架构中，我们在每个模型大小中识别出一个特定的层次，该层次最优地平衡了这种权衡。此外，最优层次往往出现在较大模型的早期层次中，且此时模型的信心也显著更高。这些发现提供了定量视角，为理解LLMs创造力和幻觉之间的相互作用提供了新的见解。实验代码和数据可在<https://github.com/ZicongHe2002/HCL-Spark>获取。|
|**2025-03-04**|**Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs**|Yuzhe Gu et.al.|[2503.02846](http://arxiv.org/abs/2503.02846)|**[link](https://github.com/open-compass/anah)**|大型语言模型（LLMs）在作为各种领域的AI助手时会表现出幻觉（即不真实或不合逻辑的信息）。由于LLM的回答总是包含真实的和虚假的信息，先前基于响应级别偏好学习的事实性对齐方法在训练过程中不可避免地引入了噪音。因此，本文提出了一种基于直接偏好优化（DPO）的细粒度事实性对齐方法，称为Mask-DPO。通过结合句子级别的事实性作为掩码信号，Mask-DPO仅从优选样本中的正确句子中学习，并防止对非优选样本中的事实内容进行惩罚，从而解决了偏好学习中的模糊性问题。广泛的实验结果表明，尽管在训练过程中未见过这些问题及其对应的主题，Mask-DPO可以显著提高LLMs对来自领域内和领域外数据集的问题回答的事实性。仅在ANAH训练集上训练后，Llama3.1-8B-Instruct在ANAH测试集上的得分从49.19%提升到77.53%，甚至超过了Llama3.1-70B-Instruct（53.44%）的得分，同时其FactScore在领域外的传记数据集上的得分也从30.29%提升到39.39%。我们进一步研究了使用不同的训练样本扩展策略时Mask-DPO的泛化性能，发现数据集中主题数量的扩展比问题数量的扩展更有效。我们提出了一个假设，解释了事实性对齐在LLMs上的作用以及这一现象的意义，并进行了概念验证实验来验证这一假设。我们希望该方法和发现能为未来事实性对齐的研究提供方向。|
|**2025-03-04**|**AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation**|Songming Zhang et.al.|[2503.02832](http://arxiv.org/abs/2503.02832)|null|在现代的大语言模型（LLMs）中，LLM对齐至关重要，通常通过诸如来自人类反馈的强化学习（RLHF）和直接偏好优化（DPO）等方法实现。然而，在大多数现有的LLM对齐方法中，响应中的所有标记都是使用稀疏的、基于响应级别的奖励或偏好注释进行优化的。忽视标记级别的奖励可能会错误地惩罚高质量标记或鼓励低质量标记，导致性能不佳和收敛速度慢。为了解决这个问题，我们提出了AlignDistil，这是一种与RLHF等效的用于标记级别奖励优化的蒸馏方法。具体来说，我们将由DPO学习到的奖励引入RLHF目标，并从理论上证明这个目标与一个标记级别的蒸馏过程等价，在该过程中教师分布线性结合了来自DPO模型和参考模型的日志。在此基础上，我们通过构建一个正向和反向DPO模型对比的DPO奖励，进一步缩小了来自DPO模型的奖励与纯奖励模型之间的准确度差距。此外，为了避免不同标记上的欠拟合和过拟合，我们设计了一种标记自适应日志外推机制来为每个标记构建适当的教师分布。实验结果表明，我们的AlignDistil优于现有方法，并展示了由于其标记级别的分布奖励优化而带来的快速收敛速度。|
|**2025-03-04**|**RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration**|Alicia Russell-Gilbert et.al.|[2503.02800](http://arxiv.org/abs/2503.02800)|null|在复杂的工业环境中，异常检测面临着独特的挑战，特别是在数据稀疏和运行条件不断变化的背景下。预测性维护（PdM）在这种环境下要求方法具有适应性、可转移性，并且能够整合特定领域的知识。本文介绍了一种名为RAAD-LLM的新框架，用于自适应异常检测，该框架利用了集成检索增强生成（RAG）的大规模语言模型（LLMs）。这种方法解决了上述PdM挑战。通过有效利用领域特定知识，RAAD-LLM增强了对时间序列数据中异常的检测能力，而无需在特定数据集上进行微调。该框架的适应机制使其能够动态调整其对正常操作条件的理解，从而提高检测准确性。我们通过一个实际应用案例在一个塑料制造工厂以及Skoltech异常基准（SKAB）中验证了这一方法。结果显示相比之前的模型有显著改进，在真实世界数据集上的准确率从70.7提升到了89.1。通过允许用语义丰富输入序列数据，RAAD-LLM融合了多模态能力，促进了模型与工厂操作员之间的更协作决策。总体而言，我们的研究结果支持RAAD-LLM能够革新PdM中的异常检测方法，可能引领各行业异常检测实施方式的范式转变。|
|**2025-02-28**|**LLM Post-Training: A Deep Dive into Reasoning Large Language Models**|Komal Kumar et.al.|[2502.21321](http://arxiv.org/abs/2502.21321)|**[link](https://github.com/mbzuai-oryx/awesome-llm-post-training)**|大型语言模型（LLMs）已经改变了自然语言处理领域，并带来了各种应用。这些模型在庞大的网络规模数据上进行预训练，从而奠定了基础，然而研究界现在正越来越多地转向后训练技术以实现进一步的突破。虽然预训练提供了广泛的语言基础，但后训练方法使LLMs能够精炼其知识，提高推理能力，增强事实准确性，并更有效地与用户意图和伦理考虑保持一致。微调、强化学习和测试时扩展已成为优化LLMs性能、确保稳健性和提高适应各种现实世界任务能力的关键策略。本调查系统地探讨了后训练方法，分析了它们在预训练基础上进一步完善LLMs的作用，解决了诸如灾难性遗忘、奖励黑客攻击和推理时权衡等关键挑战。我们强调了模型对齐、可扩展适应和推理时推理的新方向，并概述了未来的研究方向。我们还提供了一个公共存储库以持续跟踪这一快速发展领域的进展：https://github.com/mbzuai-oryx/Awesome-LLM-Post-training。|
|**2025-02-28**|**FANformer: Improving Large Language Models Through Effective Periodicity Modeling**|Yihong Dong et.al.|[2502.21309](http://arxiv.org/abs/2502.21309)|null|周期性作为最重要的基本特征之一，为促进人类学习范式中的结构化知识获取和系统认知过程奠定了基础。然而，Transformer中周期性建模的潜在缺陷影响了大规模语言模型（LLMs）的学习效率以及从数据中建立底层原则的能力。在本文中，我们证明通过整合有效的周期性建模可以提高LLMs的学习效率和性能。我们引入了FANformer，它将傅里叶分析网络（FAN）集成到注意力机制中以实现高效的周期性建模，通过修改注意力机制的特征投影过程来实现。在语言建模的广泛实验结果表明，当模型规模和训练tokens扩大时，FANformer始终优于Transformer，突显其卓越的学习效率。为了进一步验证FANformer的有效性，我们在1万亿个tokens上进行了预训练，得到了FANformer-1B。与具有相似参数或训练tokens的开源LLMs相比，FANformer-1B在下游任务中表现出显著改进。这些结果使FANformer成为推动LLMs发展的有效且有前景的架构。|
|**2025-02-28**|**Contextualizing biological perturbation experiments through language**|Menghua Wu et.al.|[2502.21290](http://arxiv.org/abs/2502.21290)|**[link](https://github.com/genentech/perturbqa)**|高含量扰动实验使科学家能够以前所未有的分辨率探究生物分子系统，但实验和分析成本构成了广泛采用的重大障碍。机器学习有可能指导扰动空间的高效探索并从这些数据中提取新的见解。然而，当前的方法忽略了相关生物学的语义丰富性，并且其目标与下游的生物学分析不一致。在这篇论文中，我们假设大型语言模型（LLM）是表示复杂生物关系和合理化实验结果的自然媒介。我们提出了PerturbQA，这是一个用于扰动实验结构化推理的基准。与主要探究现有知识的当前基准不同，PerturbQA的灵感来源于扰动建模中的开放问题：预测未见过的扰动下的差异表达和方向变化以及基因集富集。我们评估了最先进的机器学习和统计方法在建模扰动方面的表现，以及标准的LLM推理策略，发现当前的方法在PerturbQA上表现不佳。作为可行性证明，我们介绍了Summer（SUMMarize，检索E，和回答），一个简单的、领域导向的LLM框架，其匹配或超过了当前的最先进水平。我们的代码和数据可在<https://github.com/genentech/PerturbQA>公开获取。|
|**2025-02-28**|**Adaptive Keyframe Sampling for Long Video Understanding**|Xi Tang et.al.|[2502.21271](http://arxiv.org/abs/2502.21271)|null|多模态大型语言模型（MLLMs）通过将视觉输入作为额外的标记注入到大型语言模型（LLMs）中以提供上下文，从而实现了开放世界的视觉理解。然而，当视觉输入从单张图像变为长视频时，上述范式遇到了困难，因为大量的视频标记显著超过了MLLMs的最大容量。因此，现有的基于视频的MLLMs大多建立在从输入数据中采样少量标记的基础上，这可能导致关键信息丢失并产生错误的答案。本文提出了一种简单而有效的算法，称为自适应关键帧采样（AKS）。它插入了一个即插即用模块，即关键帧选择，旨在以固定数量的视频标记最大化有用信息。我们将关键帧选择表述为一个优化问题，涉及（1）关键帧与提示的相关性，以及（2）关键帧对视频的覆盖范围，并提出了一个自适应算法来近似最佳解。在两个长期视频理解基准上的实验验证了自适应关键帧采样在选择信息丰富的关键帧后提高了视频问答（QA）的准确性（超越了强大的基线）。我们的研究揭示了信息预筛选在基于视频的MLLMs中的重要性。代码可在<https://github.com/ncTimTang/AKS>获取。|
|**2025-02-28**|**RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete**|Yuheng Ji et.al.|[2502.21257](http://arxiv.org/abs/2502.21257)|null|近期在多模态大型语言模型（MLLMs）方面的进展展示了其在各种多模态场景中的显著能力。然而，在机器人场景中的应用，特别是在长时间范围的操纵任务中，揭示了这些模型的重大局限性。这些局限性源于当前的MLLM缺乏三个关键的机器人大脑能力：规划能力，包括将复杂的操作指令分解为可管理的小任务；感知先验能力，即识别和解释交互对象的先验信息；以及轨迹预测能力，即预见到达成功执行所需的完整操作轨迹的能力。为了增强从抽象到具体的机器人大脑核心能力，我们引入了ShareRobot，这是一个高质量的异构数据集，标注了任务规划、物体先验和末端执行器轨迹等多维信息。ShareRobot的多样性和准确性经过了三位人类注释者的精心优化。在此数据集的基础上，我们开发了RoboBrain，这是一种基于MLLM的模型，结合了机器人和通用的多模态数据，采用多阶段训练策略，并结合长视频和高分辨率图像以提升其机器人操作能力。广泛的实验表明，RoboBrain在各种机器人任务中实现了最先进的性能，突显了其推进机器人大脑能力的潜力。|
|**2025-02-28**|**Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs**|Xiaomin Li et.al.|[2502.21239](http://arxiv.org/abs/2502.21239)|null|大型语言模型（LLMs）在各种任务中展示了惊人的表现能力，通过编码大量的事实知识。然而，它们仍然容易产生幻觉，生成不正确或误导性的信息，通常伴随着高不确定性。现有的幻觉检测方法主要集中在量化内部不确定性，这是由于模型内部缺失或冲突的知识引起的。然而，幻觉也可能源自外部不确定性，即模糊的用户查询导致了多种可能的解释。在这项工作中，我们引入了一种新的数学度量方法，称为语义体积，用于量化LLMs中的外部和内部不确定性。我们的方法对查询和响应进行扰动，将其嵌入到语义空间中，并计算嵌入向量的Gram矩阵的行列式，以此捕捉这些向量的分散程度作为不确定性的度量。我们的框架提供了一种可泛化且无监督的不确定性检测方法，而无需白盒访问LLMs。我们进行了广泛的实验，分别针对外部和内部不确定性检测，结果表明我们的语义体积方法在这两项任务中始终优于现有基线。此外，我们提供了理论见解，将我们的度量与微分熵联系起来，统一并扩展了先前基于采样的不确定性度量，如语义熵。实证显示，语义体积是一种稳健且可解释的方法，通过系统地检测用户查询和模型响应中的不确定性来提高LLMs的可靠性。|
|**2025-02-28**|**Transforming Tuberculosis Care: Optimizing Large Language Models For Enhanced Clinician-Patient Communication**|Daniil Filienko et.al.|[2502.21236](http://arxiv.org/abs/2502.21236)|null|结核病（TB）是全球因传染病致死的主要原因，低收入和中等收入国家的负担最重。在这些地区，有限的医疗保健服务和高患者与医护人员比例阻碍了有效的患者支持、沟通和治疗完成。为了解决这一问题，我们提议将一个专门的大语言模型整合到一种有效的数字依从性技术中，以增强与治疗支持者的互动沟通。这种由人工智能驱动的方法在人机协作框架内运作，旨在提高患者的参与度并改善TB的治疗结果。|
|**2025-02-28**|**ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs**|Hao Ge et.al.|[2502.21231](http://arxiv.org/abs/2502.21231)|null|长上下文能力的扩展对于大型语言模型（LLMs）至关重要。为了在长上下文训练中摊销内存消耗，通常使用数据并行和上下文并行这两种技术。当前的训练框架主要将这两种技术视为正交的，并建立静态通信组来组织设备作为一个静态网格（例如，二维网格）。然而，无论是在文本、多模态还是强化学习中，LLM训练的序列长度通常各不相同。数据异质性与静态网格之间的不匹配导致了冗余通信和负载不平衡计算，从而降低了训练效率。在这项工作中，我们介绍了ByteScale，这是一种高效、灵活且可扩展的LLM训练框架，适用于大规模混合长短期序列训练。ByteScale的核心是一种新的并行策略，即混合数据并行（HDP），它通过动态网格设计统一了数据间和数据内的划分。特别是，我们构建了一个通信优化器，通过数据感知分片和动态通信消除了短序列中的冗余通信，并通过选择性卸载进一步压缩了长序列的通信成本。此外，我们还开发了一种平衡调度器，通过并行感知数据分配减轻了负载不平衡计算。我们在从7B到141B的不同模型大小以及从256K到2048K的不同上下文长度上评估了ByteScale，实验在一个拥有超过12,000个GPU的生产集群上进行。实验结果表明，ByteScale比最先进的训练系统性能高出多达7.89倍。|
|**2025-03-03**|**ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer**|Omer Goldman et.al.|[2502.21228](http://arxiv.org/abs/2502.21228)|null|为了实现跨语言的公平性能，多语言大语言模型（LLMs）必须能够超越其获取知识的语言进行抽象。然而，当前文献缺乏可靠的方法来衡量LLMs跨语言知识转移的能力。为此，我们提出了ECLeKTic，这是一个多语言封闭问答（CBQA）数据集，以简单、黑盒的方式评估跨语言知识转移能力。我们通过控制12种语言中维基百科文章的存在和缺失情况，检测到信息在不同语言中的覆盖不均。我们在源语言中生成寻求知识的问题，这些问题的答案出现在相关的维基百科文章中，并将其翻译成其他11种语言，而这些语言对应的维基百科缺乏相应的文章。假设维基百科反映了LLM训练数据中的主要知识，要解决ECLeKTic的CBQA任务，模型需要在语言之间转移知识。通过使用8个LLM进行实验，我们表明最先进的模型即使能够在相同语言中准确预测答案，也难以有效地跨语言共享知识。|
|**2025-02-28**|**Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought**|Jianhao Huang et.al.|[2502.21212](http://arxiv.org/abs/2502.21212)|null|链式思考（CoT）提示方法已被证明能显著提升大型语言模型（LLMs）在算术和推理任务中的表现，通过指示模型生成中间推理步骤。尽管CoT取得了显著的经验成功及其在增强表达性方面的理论优势，但CoT训练背后的机制仍鲜有探索。本文研究了在基于上下文权重预测的线性回归任务中，Transformer在CoT目标上的训练动态。我们证明，没有CoT的一层线性Transformer只能实现单一梯度下降（GD）步骤，并且无法恢复真实权重向量，而带有CoT提示的Transformer可以学会自回归地执行多步梯度下降，实现近似精确恢复。此外，我们展示了训练后的Transformer在未见数据上表现出良好的泛化能力。通过我们的技术，我们还展示了循环Transformer在线性回归的上下文学习中相比不带循环的Transformer能显著提高最终性能。实证结果表明，CoT提示带来了显著的性能提升。|
|**2025-02-27**|**R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts**|Zhongyang Li et.al.|[2502.20395](http://arxiv.org/abs/2502.20395)|**[link](https://github.com/tianyi-lab/R2-T2)**|在大型多模态模型（LMMs）中，非语言模态（例如视觉表示）的感知通常不如大型语言模型（LLMs）强大的推理能力，这阻碍了LMMs在具有挑战性的下游任务中的表现。这种弱点最近通过将视觉编码器替换为混合专家（MoE）得到了缓解，该方法提供了多样化的下游任务所需的丰富、多粒度和多样的表示。多模态MoE的性能在很大程度上取决于其路由器，该路由器为每个输入重新加权和混合不同专家的表示。然而，我们发现端到端训练的路由器并不总是为每个测试样本产生最优的路由权重。为了解决这一差距，我们提出了一种新颖且高效的“测试时重路由（R2-T2）”方法，该方法通过将其向正确预测样本的路由权重向量移动来在测试时局部优化路由权重向量。我们提出了三种具有不同优化目标和邻域搜索空间的R2-T2策略。R2-T2在各种具有挑战性的基准任务上始终且极大地提高了最先进的LMMs的表现，而无需训练任何基础模型参数。|
|**2025-02-27**|**Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis**|Jeffrey Yang Fan Chiang et.al.|[2502.20383](http://arxiv.org/abs/2502.20383)|null|近期在Web AI代理方面的进展展示了其在处理复杂的网络导航任务中的显著能力。然而，新兴研究表明，与独立的大型语言模型（LLMs）相比，这些代理表现出更大的脆弱性，尽管它们都基于相同的安全对齐模型。这一差异尤其令人担忧，因为Web AI代理相较于独立的LLMs具有更大的灵活性，这可能使它们暴露于更广泛的对抗性用户输入之下。为了构建一个解决这些问题的框架，本研究调查了导致Web AI代理脆弱性增加的根本因素。值得注意的是，这种差异源于Web AI代理与独立LLMs之间的多方面差异，以及复杂信号——这些细节往往是简单的评估指标（如成功率）所无法捕捉到的。为了解决这些挑战，我们提出了一种组件级分析和更为细致、系统的评估框架。通过这种精细的调查，我们确定了三个关键因素，这些因素放大了Web AI代理的脆弱性；（1）将用户目标嵌入系统提示，（2）多步动作生成，以及（3）观察能力。我们的发现突显了增强AI代理设计中的安全性和鲁棒性的迫切需求，并为有针对性的防御策略提供了可行的见解。|
|**2025-02-27**|**Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers**|Shalev Lifshitz et.al.|[2502.20379](http://arxiv.org/abs/2502.20379)|null|通过在测试时利用更多的计算资源，大型语言模型（LLMs）可以在不进行额外训练的情况下得到改进。在这项工作中，我们提出了一个新的测试时计算扩展维度：扩展验证器的数量。我们引入了多代理验证（MAV）作为测试时计算范式，它结合多个验证器以提高性能。我们建议使用方面验证器（AVs），即提示用于验证输出不同方面的现成LLMs，作为MAV系统中的验证器之一。AVs是MAV的一个方便的构建模块，因为它们可以轻松组合而无需额外训练。此外，我们介绍了BoN-MAV，这是一种简单的多代理验证算法，它将最佳n次采样与多个验证器结合起来。BoN-MAV展示了比自我一致性方法和奖励模型验证更强的扩展模式，并且我们展示了从弱到强的一般化能力，在这种情况下，组合弱验证器甚至可以改进强大的LLMs，并且展示了自我提升能力，其中相同的基模型既用于生成输出也用于验证输出。我们的结果确立了扩展验证器数量作为改善语言模型在测试时性能的一个有前景的新维度。|
|**2025-02-27**|**PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation**|Albert Gong et.al.|[2502.20377](http://arxiv.org/abs/2502.20377)|**[link](https://github.com/kilian-group/phantom-wiki)**|高质量的基准对于评估大型语言模型（LLMs）的推理和检索能力至关重要。然而，策划数据集并不是一个永久的解决方案，因为它们容易出现数据泄露和性能结果虚高。为了解决这些挑战，我们提出了PhantomWiki：一种生成独特且事实一致的文档语料库以及多样化问答对的管道。与之前的工作不同，PhantomWiki既不是一个固定的数据集，也不是基于任何现有数据。相反，每次评估时都会按需生成一个新的PhantomWiki实例。我们通过改变问题难度和语料库大小来分别解耦推理和检索能力，并发现PhantomWiki数据集对前沿LLMs来说是出乎意料地具有挑战性。因此，我们贡献了一个可扩展且抗数据泄露的框架，用于解耦评估推理、检索和工具使用能力。我们的代码可在https://github.com/kilian-group/phantom-wiki获取。|
|**2025-02-27**|**Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization**|Ryan C. Barron et.al.|[2502.20364](http://arxiv.org/abs/2502.20364)|null|Agentic生成式AI由大型语言模型（LLMs）与检索增强生成（RAG）、知识图谱（KGs）和向量存储（VSs）结合驱动，代表了一项适用于法律系统、研究、推荐系统、网络安全和全球安全（包括扩散研究）的变革性技术。这种技术擅长在庞大的非结构化或半结构化数据集中推断关系。法律领域包含复杂的数据，具有广泛的、相互关联的半结构化知识系统和复杂的关联。它包括宪法、法规、条例和判例法。从复杂的网络中提取见解并导航这些法律文件及其关系对于有效的法律研究至关重要。我们在此介绍一种集成了RAG、VS和KG的生成式AI系统，并通过非负矩阵分解（NMF）构建，以增强法律信息检索、AI推理并减少幻觉现象。在法律系统中，这些技术使AI代理能够识别和分析案件、法规和法律先例之间的复杂联系，揭示隐藏的关系并预测法律趋势——这些是确保公正和提高运营效率的关键任务。我们的系统采用网络抓取技术从公开平台如Justia系统地收集法律文本，例如法规、宪法条款和判例法。通过利用先进的语义表示、层次关系和潜在主题发现，该系统弥补了传统基于关键词搜索与上下文理解之间的差距。该框架支持法律文件的聚类、摘要和交叉引用，从而实现对半结构化数据的可扩展、可解释且准确的检索，同时推动计算法学和人工智能的发展。|
|**2025-02-27**|**Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs**|Kuan Lok Zhou et.al.|[2502.20356](http://arxiv.org/abs/2502.20356)|null|大型语言模型（LLMs）在理解创意内容方面表现出显著的局限性，正如Hessel等人（2023年）对《纽约客》漫画配文比赛（NYCCC）的研究所揭示的那样。他们的研究展示了LLMs与人类在幽默理解上的巨大差距，确立了理解和评估创意内容是AI发展中的关键挑战。我们重新审视这一挑战，将其分解为三个组成部分，并系统地改进每个部分：通过改进标注增强视觉理解，利用LLM生成的幽默推理和解释，以及实施有针对性的人类偏好数据对齐。我们的改进方法在配文排名上达到了82.4%的准确率，显著超过了之前的67%基准，并且达到了世界级人类专家在这个领域的表现水平。值得注意的是，尽管通过各种人格提示试图模仿子群体偏好影响甚微，但使用众包偏好进行模型微调证明非常有效。这些发现表明，通过有针对性的对齐特定子群体和个人，可以有效地解决LLMs在创造性判断方面的局限性。最后，我们提出观点认为实现通用人工智能需要系统地收集跨创意领域的个人偏好数据。我们主张，正如人类的创造力深受个体和文化偏好的影响一样，用多样化的个人偏好数据训练LLMs可能是培养真正创造理解的关键。|
|**2025-02-27**|**KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large Language Model**|Kai Zhang et.al.|[2502.20350](http://arxiv.org/abs/2502.20350)|null|药物发现是生物医学自然语言处理（NLP）中的一个关键任务，然而可解释的药物发现仍然探索不足。与此同时，大型语言模型（LLMs）在自然语言理解和生成方面展示了显著的能力。利用LLMs进行可解释的药物发现有可能改进下游任务和实际应用。在本研究中，我们利用开源药物知识图谱、临床试验数据和PubMed出版物构建了一个全面的数据集，用于可解释的药物发现任务，命名为\textbf{expRxRec}。此外，我们介绍了\textbf{KEDRec-LM}，这是一种经过指令调优的LLM，它从丰富的医学知识语料库中提炼知识，用于药物推荐和理由生成。为了鼓励该领域进一步的研究，我们将公开发布\footnote{本提交附带了一份副本}数据集和KEDRec-LM。|
|**2025-02-27**|**Sparse Auto-Encoder Interprets Linguistic Features in Large Language Models**|Yi Jing et.al.|[2502.20344](http://arxiv.org/abs/2502.20344)|null|大型语言模型（LLMs）在需要复杂语言能力的任务上表现出色，例如指代消解和隐喻识别/生成。尽管LLMs具有令人印象深刻的能力，但它们处理和表示语言知识的内部机制仍然很大程度上不透明。先前对语言机制的研究受到粗粒度、不足的因果分析以及狭隘关注的限制。在这项研究中，我们使用稀疏自动编码器（SAEs）进行了一项系统且全面的因果调查。我们从六个维度提取了广泛的语音学、语音学、形态学、句法学、语义学和语用学的语言特征。我们通过构建最小对比数据集和反事实句子数据集来提取、评估和干预这些特征。我们引入了两个指标——特征表示置信度（FRC）和特征干预置信度（FIC）——以衡量语言特征捕捉和控制语言现象的能力。我们的结果揭示了LLMs中存在的语言知识内在表示，并展示了控制模型输出的潜力。这项工作提供了强有力的证据，证明LLMs拥有真正的语言知识，并为未来研究中更可解释和可控的语言建模奠定了基础。|
|**2025-02-27**|**Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners**|Daniele Paliotta et.al.|[2502.20339](http://arxiv.org/abs/2502.20339)|null|近期的研究进展表明，通过在测试时扩展计算资源可以显著提升大型语言模型（LLMs）的性能。一种常见策略是生成多个思维链（CoT）轨迹，并通过各种选择机制聚合它们的输出。这引发了一个基本问题：复杂度较低的模型能否利用其更优的生成吞吐量，在固定计算预算下超越相同规模的Transformer？为了解决这个问题并克服缺乏强大的次二次推理器的问题，我们从预训练的Transformer中提炼出了纯和混合的Mamba模型。这些模型仅经过80亿个令牌的训练，在数学推理数据集上展示了强大的性能和可扩展性，同时在大规模批次和长序列的推理过程中速度更快。尽管由于蒸馏导致了零样本性能下降，但纯和混合的Mamba模型能够在固定时间预算下超越其Transformer教师模型的覆盖率和准确性，从而开启了一种新的推理计算扩展方向。|
|**2025-02-27**|**Expertise Is What We Want**|Alan Ashworth et.al.|[2502.20335](http://arxiv.org/abs/2502.20335)|null|临床决策依赖于专家推理，而这种推理由标准化的循证指南指导。然而，将这些指南转化为自动化临床决策支持系统可能会导致不准确，并且重要的是会丢失细微差别。我们分享了一种名为大型语言专家（LLE）的应用程序架构，该架构结合了大型语言模型（LLM）的灵活性和强大功能以及专家系统的可解释性、可解释性和可靠性。LLM有助于解决专家系统的若干关键挑战，如知识的整合与编码以及数据规范化。相反，类似于专家系统的方法有助于克服LLM的问题，包括幻觉、原子性和低成本更新以及可测试性。  为了突出大型语言专家（LLE）系统的功能，我们构建了一个LLE来协助新诊断癌症患者的诊疗工作。及时启动癌症治疗对患者的最佳结果至关重要。然而，诊断建议的复杂性增加使得初级保健医生难以确保其患者在首次与肿瘤学家会诊前已完成必要的诊疗工作。与许多现实世界的临床任务一样，这些诊疗工作需要分析非结构化的健康记录并应用复杂的临床决策逻辑。在这项研究中，我们描述了构建LLE系统的设计与评估过程，该系统旨在快速识别并建议正确的诊疗方案。该系统表现出高度的临床水平准确性（>95%），并且有效地解决了来自一家大型学术中心乳腺癌和结肠癌患者真实世界数据中发现的差距。|
|**2025-02-26**|**Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing**|Akshat Gupta et.al.|[2502.19416](http://arxiv.org/abs/2502.19416)|null|本研究调查了在大型语言模型（LLMs）中局部更新的影响，特别是在知识编辑任务的背景下——该任务旨在融入或修改特定事实而不改变模型的整体能力。我们首先展示，在不同的后训练干预措施，如连续预训练、全量微调和LORA微调中，更新矩阵的弗罗贝尼乌斯范数总是增加。这种范数的增长对于仅更新模型中子集矩阵的局部知识编辑尤其不利。我们揭示了一种在各种编辑技术中一致存在的现象，包括微调、基于超网络的方法和定位与编辑方法：更新矩阵的范数随着连续更新而不可避免地增加。这种增长会破坏模型平衡，特别是当孤立的矩阵被更新而模型的其余部分保持不变时，这会导致潜在的不稳定性以及下游性能的退化。通过深入研究中间激活向量，我们发现内部激活的范数减小，并伴随着占据这些激活的子空间的变化，这表明这些激活向量现在占据了与未编辑模型相比完全不同的表示空间区域。通过我们的研究，我们强调了连续和局部顺序知识编辑的技术挑战及其对维持模型稳定性和效用的影响。|
|**2025-02-26**|**Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs**|Dayu Yang et.al.|[2502.19411](http://arxiv.org/abs/2502.19411)|**[link](https://github.com/dayuyang1999/awesome-code-reasoning)**|在大型语言模型（LLMs）中，代码和推理相互加强：代码提供了一种抽象、模块化且逻辑驱动的结构以支持推理，而推理则将高层次目标转化为可执行的小步骤，从而推动更高级别的代码智能。本研究考察了代码如何作为一种结构化的媒介来增强推理能力：它提供了可验证的执行路径，强制实施逻辑分解，并允许运行时验证。我们还探讨了推理方面的改进如何将代码智能从基本完成转变为高级功能，使模型能够通过规划和调试解决复杂的软件工程任务。最后，我们识别了关键挑战并提出了未来的研究方向，以加强这种协同作用，最终提高LLM在这两个领域的表现。|
|**2025-02-26**|**Less or More: Towards Glanceable Explanations for LLM Recommendations Using Ultra-Small Devices**|Xinru Wang et.al.|[2502.19410](http://arxiv.org/abs/2502.19410)|null|大型语言模型（LLMs）作为个人AI助手在推荐日常行动方面展现出了巨大的潜力，同时解释性人工智能（XAI）技术也被越来越多地用来帮助用户理解为什么给出某个建议。如今的个人AI助手通常位于超小型设备上，如智能手表，这些设备的屏幕空间非常有限。然而，LLM生成的解释往往过于冗长，使得在这些超小型设备上提供可瞥见的LLM解释变得具有挑战性。为了解决这个问题，我们探索了在提示过程中使用定义的上下文组件对LLM的解释文本进行空间结构化以及根据置信度水平呈现时间自适应的解释给用户。我们进行了用户研究，以了解这些方法在超小型设备上与LLM建议和解释互动时如何影响用户体验。结果表明，结构化的解释减少了用户采取行动的时间和阅读解释时的认知负荷。始终开启的结构化解释增加了用户对AI建议的接受程度。然而，由于缺乏足够的、可读的细节，用户对结构化解释的满意度低于非结构化解释。此外，适应性地呈现结构化解释在提高用户对AI的看法方面不如始终开启的结构化解释有效。结合用户的访谈反馈，这些结果为我们提供了设计启示，即在个性化LLM解释的内容和时机时，需要考虑到它们在超小型设备上的显示。|
|**2025-02-26**|**ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models**|Danae Sánchez Villegas et.al.|[2502.19409](http://arxiv.org/abs/2502.19409)|null|在多模态大型语言模型(MLLMs)中，对图像序列进行推理仍然是一项挑战。尽管最近的模型在预训练过程中结合了多图像数据，但它们仍难以识别序列结构，通常会将图像独立处理。本文介绍了一种名为ImageChain的框架，该框架通过将视觉序列建模为多轮对话来增强MLLMs的顺序推理能力。在ImageChain中，图像与相应的文字描述交替出现，形成一个控制性的对话，明确捕捉时间依赖性和叙事进展。我们的方法针对的是下一场景描述任务，在该任务中，模型根据前面的视觉和文字线索生成上下文感知的下一场景描述。我们证明了我们的方法在下一场景描述任务中提高了性能——在衡量语义相似度的人类标注地面真实数据的SimRate指标上平均提升了3.7%到19%。此外，ImageChain在从漫画到机器人技术的各种领域实现了强大的零样本跨域性能。广泛的实验验证了在多模态、多轮对话设计中进行指令调优是弥合静态图像理解与时间感知推理之间差距的关键。|
|**2025-02-26**|**Learning Code-Edit Embedding to Model Student Debugging Behavior**|Hasnain Heickal et.al.|[2502.19407](http://arxiv.org/abs/2502.19407)|null|提供有效的编程作业反馈在计算机科学教育中可能具有挑战性：学生通过迭代提交代码、执行代码并利用来自编译器或自动评分器的有限反馈进行调试。分析这一过程中的学生调试行为可能会揭示他们知识中的重要见解，并为更好的个性化支持工具提供信息。在这项工作中，我们提出了一种基于编码器-解码器的模型，该模型学习连续学生代码提交之间的有意义的代码编辑嵌入，以捕捉他们的调试行为。我们的模型利用学生代码提交是否通过每个测试用例的信息来微调大型语言模型（LLMs），以学习代码编辑表示。它能够提供个性化的下一步代码建议，同时保持学生的编码风格并提高测试用例的正确性。我们的模型还使我们能够使用聚类技术分析学生代码编辑模式，以揭示常见的学生错误和调试行为。在真实的学生代码提交数据集上的实验结果表明，我们的模型在代码重建和个性化代码建议方面表现出色，同时揭示了学生调试行为中的有趣模式。|
|**2025-02-26**|**General Reasoning Requires Learning to Reason from the Get-go**|Seungwook Han et.al.|[2502.19402](http://arxiv.org/abs/2502.19402)|null|大型语言模型（LLMs）展示了显著的实际应用价值，体现了人工实用智能（AUI）。然而，它们在适应性和稳健性推理方面的能力——即人工通用智能（AGI）的标志——仍然很脆弱。尽管LLMs似乎在常识推理、编程和数学方面取得了成功，但在跨新颖情境理解算法方面仍存在困难。我们的实验显示，在使用神秘编程语言进行的算法任务中，LLMs的推理过度拟合了训练数据，并且其迁移能力有限。我们假设这种有限的迁移能力的核心问题在于LLMs中的推理和知识是耦合的。为了从AUI过渡到AGI，我们提出通过三个关键方向来解耦知识和推理：（1）预训练以从零开始使用强化学习进行推理作为替代，而不仅仅是广泛使用的下一令牌预测预训练；（2）使用合成任务的课程来减轻学习强化学习的“推理先验”的难度，然后可以将其转移到自然语言任务上；（3）使用小的上下文窗口来学习更可泛化的推理函数，以减少对令牌之间虚假相关性的利用。这样一个推理系统与经过训练的检索系统以及一个大的外部记忆库作为知识存储相结合，可以克服现有架构在学习在新场景下推理时的许多局限性。|
|**2025-02-26**|**TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding**|Max Ku et.al.|[2502.19400](http://arxiv.org/abs/2502.19400)|null|理解领域特定的定理通常需要不仅仅是基于文本的推理；有效的沟通通过结构化的视觉解释对于深入理解至关重要。虽然大型语言模型（LLMs）在基于文本的定理推理方面表现出色，但它们生成连贯且具有教育意义的视觉解释的能力仍然是一个未解决的挑战。在这项工作中，我们介绍了TheoremExplainAgent，这是一种生成长篇定理解释视频（超过5分钟）的方法，使用Manim动画。为了系统地评估多模态定理解释，我们提出了TheoremExplainBench，这是一个涵盖了240个来自多个STEM学科的定理的基准，以及5个自动评估指标。我们的结果显示，代理规划对于生成详细的长篇视频是必不可少的，o3-mini代理的成功率为93.8%，总体得分为0.77。然而，我们的定量和定性研究显示，大多数生成的视频在视觉元素布局上存在一些小问题。此外，多模态解释揭示了文本解释未能揭示的更深层次的推理缺陷，这突显了多模态解释的重要性。|
|**2025-02-26**|**DataMan: Data Manager for Pre-training Large Language Models**|Ru Peng et.al.|[2502.19363](http://arxiv.org/abs/2502.19363)|null|大型语言模型（LLMs）的性能提升得益于数据规模的增长，这使得预训练数据的选择变得日益重要。然而，现有的方法依赖于有限的经验法则和人类直觉，缺乏全面清晰的指导原则。为了解决这一问题，我们受到“逆向思维”的启发——即让LLMs自我识别哪些标准对其表现有益。由于其预训练能力与困惑度（PPL）相关，我们从文本困惑度异常的原因中推导出14个质量标准，并引入15个常见应用领域以支持领域混合。在本文中，我们训练了一个数据管理器（DataMan），使其从点对点评分中学习质量评级和领域识别，并使用它来注释一个包含4470亿个标记的预训练语料库的质量等级和领域类型。我们的实验验证了我们的方法，使用DataMan选择300亿个标记来训练一个参数量为13亿的语言模型，该模型在上下文学习（ICL）、困惑度和指令执行能力方面显著优于最先进基线模型。基于总体评分l=5的最佳模型，在使用均匀采样时，其训练数据量要多出50%。我们继续使用DataMan标注的高质量、特定领域的数据进行预训练，以增强特定领域的ICL性能，从而验证DataMan的领域混合能力。我们的研究强调了质量排名的重要性，以及质量标准之间的互补性，且它们与困惑度的相关性较低，分析了困惑度和ICL表现之间的不一致。我们还详细分析了我们的预训练数据集，检查了其组成、质量评分分布和原始文档来源。|
|**2025-02-26**|**Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?**|Yancheng He et.al.|[2502.19361](http://arxiv.org/abs/2502.19361)|**[link](https://github.com/openstellarteam/deltabench)**|近期，类似o1的模型引起了广泛关注，这些模型生成较长的Chain-of-Thought（CoT）推理步骤以改进现有大型语言模型（LLMs）的推理能力。在本文中，为了理解这些长CoT的质量并衡量现有LLMs对这些长CoT的批评能力，我们引入了DeltaBench，其中包括来自不同o1-like模型（例如QwQ、DeepSeek-R1）针对不同推理任务（如数学、代码、一般推理）生成的长CoT，用于测量检测长CoT推理错误的能力。基于DeltaBench，我们首先对生成的长CoT进行细粒度分析，以发现不同o1-like模型的有效性和效率。然后，我们对现有的过程奖励模型（PRMs）和批评模型进行广泛的评估，以检测每个注释过程中的错误，旨在探讨现有PRMs和批评模型的边界和局限性。最终，我们希望DeltaBench能够帮助开发者更好地理解其模型的长CoT推理能力。|
|**2025-02-26**|**Evaluating LLMs and Pre-trained Models for Text Summarization Across Diverse Datasets**|Tohida Rehman et.al.|[2502.19339](http://arxiv.org/abs/2502.19339)|null|文本摘要在自然语言处理中扮演着重要角色，通过将大量文本压缩成简洁且连贯的摘要来实现。随着数字内容的快速增长和对有效信息检索需求的增加，近年来文本摘要已成为研究的重点。本研究全面评估了四种领先的预训练和开源大型语言模型：BART、FLAN-T5、LLaMA-3-8B和Gemma-7B，在五个不同的数据集（CNN/DM、Gigaword、News Summary、XSum和BBC News）上的表现。评估采用了广泛认可的自动指标，包括ROUGE-1、ROUGE-2、ROUGE-L、BERTScore和METEOR，以评估这些模型生成连贯和有用摘要的能力。结果揭示了这些模型在处理不同类型文本时的相对优势和局限性。|
|**2025-02-25**|**DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers**|Xueguang Ma et.al.|[2502.18460](http://arxiv.org/abs/2502.18460)|**[link](https://github.com/facebookresearch/dpr-scale)**|大型语言模型（LLMs）在作为密集检索器进行微调时已经展示了强大的效果和鲁棒性。然而，它们庞大的参数规模带来了显著的推理时间计算挑战，包括对大规模语料库的高编码成本和增加的查询延迟，限制了其实际部署。虽然较小的检索器提供了更好的效率，但它们通常在有限的监督微调数据下无法有效泛化。在这项工作中，我们介绍了DRAMA，这是一种利用LLMs训练较小且可泛化的密集检索器的框架。具体来说，我们采用剪枝后的LLMs作为骨干，并在一个单一阶段的对比学习设置中使用多样化的LLM增强数据进行训练。实验表明，DRAMA在多语言和长上下文能力方面优于传统的基于编码器的检索器，并在多个任务和语言上实现了强大的性能。这些结果突显了将较小检索器的训练与LLMs的快速发展相结合的潜力，弥合了效率和泛化之间的差距。|
|**2025-02-25**|**LLM-Based Design Pattern Detection**|Christian Schindler et.al.|[2502.18458](http://arxiv.org/abs/2502.18458)|null|在不熟悉的代码库中检测设计模式实例仍然是一个具有挑战性但至关重要的任务，这对于提高软件质量和可维护性至关重要。传统的静态分析工具往往难以应对真实世界模式实现的复杂性、多样性和缺乏显式注释的特点。在本文中，我们提出了一种利用大型语言模型自动识别不同代码库中设计模式实例的新方法。我们的方法侧重于识别类在模式实例中扮演的角色。通过提供对软件结构和意图的更清晰洞察，本研究旨在支持开发人员，提高理解能力，并简化诸如重构、维护和遵循最佳实践等任务。|
|**2025-02-25**|**FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response**|Mollie Shichman et.al.|[2502.18452](http://arxiv.org/abs/2502.18452)|null|大型语言模型（LLMs）具有显著的常识推理能力。然而，这些能力在更大的模型中通常是涌现的。这意味着可以本地运行的较小模型在某些推理任务方面帮助较少且能力较弱。为了满足我们的问题空间需求，我们将较小的LLMs微调到灾难领域，因为这些领域涉及复杂的低频物理常识知识。我们介绍了一个创建现场准备指令解码代理（FRIDA）模型的管道，在这个过程中领域专家和语言学家结合他们的知识来制作高质量的种子数据，用于生成合成数据进行微调。我们创建了130个用于合成生成的种子指令，一个包含25000个指令的合成数据集以及119个与一般和地震特定物体功能相关的评估指令。我们对多个LLaMa和Mistral指令调优模型进行了微调，并发现FRIDA模型在其各种规模上都优于其基础模型。然后我们进行了一项消融研究以了解哪种类型的合成数据最能影响性能，结果发现仅训练物理状态和物体功能常识知识就比FRIDA模型在所有数据上训练有所改进。我们得出结论，FRIDA管道能够赋予通用常识，但需要通过信息检索来增强特定领域的知识。|
|**2025-02-25**|**SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution**|Yuxiang Wei et.al.|[2502.18449](http://arxiv.org/abs/2502.18449)|null|近期DeepSeek-R1的发布展示了强化学习（RL）在提升大型语言模型（LLMs）通用推理能力方面的巨大潜力。尽管DeepSeek-R1和其他后续研究主要集中在将RL应用于竞争性编程和数学问题上，本文介绍了一种新的方法SWE-RL，该方法旨在通过强化学习将LLM的推理能力扩展到现实世界中的软件工程领域。利用轻量级的基于规则的奖励（例如，真实答案与LLM生成的答案之间的相似度得分），SWE-RL使LLM能够自主恢复开发者的推理过程和解决方案，通过从大量的开源软件演化数据中学习——这些数据记录了软件整个生命周期的信息，包括代码快照、代码变更以及如问题和拉取请求等事件。基于Llama 3之上训练得到的推理模型Llama3-SWE-RL-70B，在SWE-bench Verified上的解决率达到41.0%——这是一个经过人工验证的包含实际GitHub问题的人类验证集合。据我们所知，这是迄今为止对于中型（<100B）LLMs报告的最佳性能，甚至可以与领先的专有LLMs如GPT-4o相媲美。令人惊讶的是，尽管SWE-RL仅对软件演化数据进行了强化学习，但该模型已经发展出了泛化的推理技能。例如，它在五个领域外的任务上表现有所提升，分别是函数编码、库使用、代码推理、数学和通用语言理解，而监督微调基线则导致了平均性能下降。总体而言，SWE-RL开启了一个新方向，即通过在大量软件工程数据上进行强化学习来提升LLMs的推理能力。|
|**2025-02-25**|**MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning**|Chanwoo Park et.al.|[2502.18439](http://arxiv.org/abs/2502.18439)|null|利用多个大型语言模型（LLMs）构建协作多智能体工作流已经展示了巨大的潜力。然而，大多数先前的研究侧重于提示这些现成的LLMs，依赖它们天生的合作能力，这可能无法如最近所显示的那样提升LLMs的表现。在本文中，我们介绍了一种新的后训练范式MAPoRL（多智能体后协同训练用于协作LLMs的强化学习），以明确激发合作行为并进一步释放多智能体LLM框架的潜力。在MAPoRL中，多个LLMs首先独立生成自己的响应，并进行多轮讨论以共同改进最终答案。最后，MAPoRL验证器通过评分来评估答案和讨论，该评分验证答案的正确性，同时增加激励以鼓励纠正性和有说服力的讨论。该评分作为协同训练奖励，并通过多智能体强化学习最大化。与现有的LLM后训练范式不同，MAPoRL倡导使用强化学习一起对多个LLMs进行协同训练以获得更好的泛化能力。结合分析见解，我们的实验表明，单独训练单个LLMs不足以诱导有效的合作。相比之下，多智能体协同训练可以在基准测试中提升合作性能，并推广到未见过的领域。|
|**2025-02-25**|**TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning**|Frederikus Hudi et.al.|[2502.18431](http://arxiv.org/abs/2502.18431)|**[link](https://github.com/fhudi/textgames)**|推理是大型语言模型（LLMs）的基本能力，使它们能够理解、分析和解决复杂问题。在本文中，我们介绍了TextGames，这是一个专门设计的基准测试，通过要求高级模式识别、空间意识、算术和逻辑推理的高难度文字游戏来评估LLMs。我们的分析探讨了LLMs在单回合和多回合推理中的表现，以及它们利用反馈通过自我反思纠正后续答案的能力。我们的研究结果表明，尽管LLMs在解决大多数简单和中等难度的问题上表现出色，但在更困难的任务上面临显著挑战。相比之下，人类在给予足够时间的情况下能够解决所有任务。此外，我们观察到LLMs通过自我反思在多回合预测中表现有所提高，但它们仍然在序列化、计数和一致地遵循复杂规则方面存在困难。此外，优化用于推理的模型比优先考虑指令跟随的预训练LLMs表现更好，这突显了推理技能在解决高度复杂问题中的关键作用。|
|**2025-02-25**|**OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference**|Xiangyu Zhao et.al.|[2502.18411](http://arxiv.org/abs/2502.18411)|**[link](https://github.com/phoenixz810/omnialign-v)**|近期，在开源多模态大型语言模型(MLLMs)的研究进展主要集中在提升基础能力上，而在与人类偏好对齐方面存在显著差距。本文介绍了一个名为OmniAlign-V的综合数据集，该数据集包含了200K高质量的训练样本，涵盖了多样化的图像、复杂的问题以及多样的响应格式，旨在提高MLLMs与人类偏好的对齐程度。我们还提出了MM-AlignBench，这是一个专门设计用于评估MLLMs是否符合人类价值观的人工标注基准。实验结果表明，使用监督微调(SFT)或直接偏好优化(DPO)方法对MLLMs进行微调可以显著提升其与人类偏好的对齐程度，同时保持或提升了在标准视觉问答(VQA)基准上的性能，从而保留了它们的基础能力。我们的数据集、基准、代码和检查点已在https://github.com/PhoenixZ810/OmniAlign-V发布。|
|**2025-02-25**|**Monte Carlo Temperature: a robust sampling strategy for LLM's uncertainty quantification methods**|Nicola Cecere et.al.|[2502.18389](http://arxiv.org/abs/2502.18389)|null|不确定性量化（UQ）在大型语言模型（LLMs）中的应用对于它们的安全和可靠部署至关重要，特别是在那些错误输出可能导致严重后果的关键应用中。当前的UQ方法通常依赖于使用非零温度采样多次查询模型以生成多样化输出来进行不确定性估计。然而，选择特定温度参数的影响研究不足，我们的分析表明温度在不确定性估计的质量方面起着基础性作用。识别最优温度值的传统方法需要昂贵的超参数优化（HPO），并且必须针对每种新的模型-数据集组合重复此过程。我们提出了一种名为蒙特卡洛温度（MCT）的稳健采样策略，该策略消除了温度校准的需求。我们的分析显示：1）MCT在广泛的温度范围内提供了更稳健的不确定性估计；2）MCT通过替换不依赖于HPO的固定温度策略来提高UQ方法的性能；3）MCT在统计上与理想情况下经过精心调整但计算成本高昂的HPO过程产生的最优温度结果相当。这些发现表明，无需进行温度参数校准即可实现有效的UQ。|
|**2025-02-25**|**How Far are LLMs from Real Search? A Comprehensive Study on Efficiency, Completeness, and Inherent Capabilities**|Minhua Lin et.al.|[2502.18387](http://arxiv.org/abs/2502.18387)|null|搜索在各个领域的解决问题过程中发挥着基础性作用，大多数现实世界中的决策问题都可以通过系统搜索来解决。受最近关于搜索和学习的讨论的启发，我们从三个角度系统地探讨了搜索与大型语言模型（LLMs）之间的互补关系。首先，我们分析了学习如何增强搜索效率，并提出了通过学习进行搜索（SeaL）框架，该框架利用LLMs实现高效且有效的搜索。其次，我们将SeaL扩展到SeaL-C，以确保搜索过程的严格完整性。我们在三个现实世界的规划任务中进行评估，结果表明SeaL实现了接近完美的准确性，同时相比传统方法将搜索空间减少了高达99.1%。最后，我们探讨了LLMs距离实际搜索有多远，通过研究它们是否能够独立发展出搜索能力。我们的分析揭示，尽管当前的LLMs在复杂问题中搜索效率较低，但引入系统搜索策略显著提升了它们的问题解决能力。这些发现不仅验证了我们方法的有效性，也突显了提高LLMs搜索能力以适应现实世界应用的需求。|
|**2025-02-25**|**MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and Deep Learning**|Sepehr Asgarian et.al.|[2502.18371](http://arxiv.org/abs/2502.18371)|null|在广告竞争激烈的环境中，成功的关键在于有效地驾驭和利用消费者、广告商和广告平台之间的复杂互动。这些多方面的互动迫使广告商优化策略以建模消费者行为、增强品牌回忆并定制广告内容。为了解决这些挑战，我们介绍了MindMem，这是一种用于广告记忆度的多模态预测模型。通过整合文本、视觉和听觉数据，MindMem达到了最先进的性能，在LAMBDA数据集上的Spearman相关系数为0.631，在Memento10K数据集上为0.731，持续超越现有方法。此外，我们的分析确定了影响广告记忆度的关键因素，如视频节奏、场景复杂性和情感共鸣。在此基础上，我们引入了MindMem-ReAd（基于MindMem驱动的再生成广告），它利用基于大语言模型的模拟来优化广告内容和投放，从而使得广告记忆度提高了多达74.12%。我们的结果突显了人工智能在广告中的变革潜力，为广告商提供了一个强大的工具，以推动参与度、增强竞争力并在快速变化的市场中最大化影响力。|
|**2025-02-24**|**Introducing Visual Perception Token into Multimodal Large Language Model**|Runpeng Yu et.al.|[2502.17425](http://arxiv.org/abs/2502.17425)|**[link](https://github.com/yu-rp/visualperceptiontoken)**|**为了利用视觉信息，多模态大型语言模型（MLLM）依赖于其视觉编码器的感知过程。视觉感知的完整性和准确性显著影响空间推理、细粒度理解等任务的精度。然而，MLLM仍然缺乏自主控制其自身视觉感知过程的能力，例如，有选择地审查图像的特定区域或关注与特定对象类别相关的信息。在这项工作中，我们提出了视觉感知标记的概念，旨在赋予MLLM一种控制其视觉感知过程的机制。我们设计了两种类型的视觉感知标记，称为区域选择标记和视觉重编码标记。MLLM像生成文本一样自主生成这些标记，并使用它们来触发额外的视觉感知操作。区域选择标记明确标识需要进一步感知的图像中的特定区域，而视觉重编码标记则将其隐藏状态作为控制信号以引导额外的视觉感知过程。广泛的实验表明，这些标记在处理空间推理、提高细粒度理解等方面具有优势。平均而言，视觉感知标记的引入使一个2B模型的性能提高了23.6%，从0.572提高到0.708，甚至超过了7B参数模型13.4%（从0.624提高）。请查看我们的仓库https://github.com/yu-rp/VisualPerceptionToken**|
|**2025-02-24**|**MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs**|Jiarui Zhang et.al.|[2502.17422](http://arxiv.org/abs/2502.17422)|**[link](https://github.com/saccharomycetes/mllms_know)**|**多模态大型语言模型（MLLMs）近年来在视觉识别任务中取得了快速进展。鉴于它们可能被整合到许多关键应用中，理解其视觉感知的局限性至关重要。在这项工作中，我们研究了当回答有关图像的问题时，MLLMs是否能同样有效地感知小的视觉细节和大的视觉细节。我们观察到它们的表现对问题中的视觉主体的大小非常敏感，并进一步通过干预研究证明了这种效应实际上是因果关系。接下来，我们研究了MLLMs在回答视觉问题时的注意力模式，并发现即使在提供错误答案时，它们也总能找到正确的关注点。基于这些发现，我们提出了无需训练的视觉干预方法，利用MLLM自身的内部知识，以注意力图和梯度图的形式来增强其对小视觉细节的感知能力。我们在两个广泛使用的MLLM和七个视觉问答基准上评估了我们提出的方法，结果显示它们可以显著提高MLLMs的准确性而不需要任何训练。我们的结果阐明了将MLLM应用于涉及小细节的视觉识别任务的风险，并表明使用模型的内部状态进行视觉干预是一个有前景的方向以减轻这一风险。**|
|**2025-02-24**|**LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification**|Penghui Yang et.al.|[2502.17421](http://arxiv.org/abs/2502.17421)|**[link](https://github.com/sail-sg/longspec)**|**推测性解码已成为缓解大型语言模型（LLMs）自回归解码高延迟的一种有前景的技术。尽管其前景广阔，推测性解码在LLMs中的有效应用仍面临三个关键挑战：草稿模型的内存需求增加、短训练语料库与长上下文推理之间的分布差异以及注意力实现的低效性。在这项工作中，我们通过解决这些挑战来提升长上下文设置下推测性解码的性能。首先，我们提出了一种具有固定大小键值（KV）缓存的内存高效草稿模型。其次，我们引入了用于短训练数据的新位置索引，使从短上下文训练到长上下文推理的无缝适应成为可能。最后，我们提出了一种创新的注意力聚合方法，该方法结合了前缀计算的快速实现和树掩码处理的标准注意力，有效地解决了树解码的延迟和内存效率问题。我们的方法在各种长上下文任务上取得了显著成果，包括仓库级代码补全、长上下文摘要以及类似o1的长推理任务，展示了在延迟减少方面的显著改进。代码可在<https://github.com/sail-sg/LongSpec>获取。**|
|**2025-02-24**|**The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence**|Tom Wollschläger et.al.|[2502.17420](http://arxiv.org/abs/2502.17420)|null|大型语言模型（LLMs）的安全对齐可以通过精心设计的对抗性输入来规避，但这些攻击如何绕过安全屏障的机制仍不甚明了。先前的研究表明，模型激活空间中的一个单一拒绝方向决定了LLM是否拒绝请求。在本研究中，我们提出了一种新的基于梯度的方法来进行表示工程，并用它来识别拒绝方向。与先前的工作相反，我们发现了多个独立的方向，甚至是由多维概念锥体介导的拒绝。此外，我们展示了正交性本身并不意味着在干预下独立，这促使我们提出了表征独立性的概念，该概念考虑了线性和非线性效应。使用这一框架，我们确定了机制上独立的拒绝方向。我们表明LLMs中的拒绝机制受复杂的空间结构支配，并确认多个不同的机制驱动拒绝行为。我们的基于梯度的方法揭示了这些机制，并可为进一步理解LLMs奠定基础。|
|**2025-02-24**|**From System 1 to System 2: A Survey of Reasoning Large Language Models**|Zhong-Zhi Li et.al.|[2502.17419](http://arxiv.org/abs/2502.17419)|**[link](https://github.com/zzli2022/awesome-slow-reason-system)**|**实现人类水平的智能需要完善从快速、直观的系统1到更慢、更谨慎的系统2推理的过渡。虽然系统1在快速、启发式决策方面表现出色，但系统2依赖逻辑推理以获得更准确的判断和减少偏见。基础大型语言模型（LLM）在快速决策方面表现出色，但由于尚未完全采用真正的系统2思维所特有的逐步分析方法，因此缺乏处理复杂推理的能力。最近，像OpenAI的o1/o3和DeepSeek的R1这样的推理LLM在数学和编码等领域展示了专家级表现，接近于模仿系统的谨慎推理，并展示了类人的认知能力。本综述首先简要概述了基础LLM的进展以及系统2技术的早期发展，探讨了它们的结合如何为推理LLM铺平了道路。接下来，我们将讨论如何构建推理LLM，分析其特征、使高级推理成为可能的核心方法以及各种推理LLM的演变。此外，我们还将介绍推理基准测试，提供代表性推理LLM性能的深入比较。最后，我们将探索推进推理LLM的有前景的方向，并维护一个实时的GitHub仓库来跟踪最新进展。我们希望这篇综述能作为一个有价值的资源，激发创新并推动这一快速发展的领域的进步。**|
|**2025-02-24**|**Reasoning with Latent Thoughts: On the Power of Looped Transformers**|Nikunj Saunshi et.al.|[2502.17416](http://arxiv.org/abs/2502.17416)|null|大型语言模型在推理方面展现出了显著的能力，并且规模定律表明参数数量，尤其是深度轴上的参数数量是主要驱动因素。在这项工作中，我们提出了一个更强的论点——许多推理问题需要较大的深度但不一定需要大量的参数。这开启了循环模型在推理中的新应用。首先，我们展示了对于许多合成推理问题如加法、p-hop 归纳和数学问题，一个 k 层的变压器循环 L 次几乎与一个 kL 层的非循环模型性能相当，并且明显优于一个 k 层的模型。理论结果进一步证实了许多此类推理问题可以通过迭代算法解决，因此可以使用循环模型有效地解决这些问题，且具有接近最优的深度。令人惊讶的是，这些好处也适用于语言模型的实际设置——在许多下游推理任务上，一个循环 L 次的 k 层语言模型可以与甚至优于一个 kL 层的语言模型竞争。实际上，我们的实证分析揭示了一个有趣的现象：循环和非循环模型的表现取决于它们的有效深度，类似于思维链（CoT）推理的推理时间缩放行为。我们进一步阐明了与 CoT 推理的联系，证明循环模型会隐式生成潜在的思想，并且可以用 T 次循环模拟 T 步 CoT。受此发现启发，我们还提出了推理和记忆之间的一个有趣二分法，并设计了一种基于循环的正则化方法，在这两个方面都表现出色。|
|**2025-02-24**|**COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs**|Liming Liu et.al.|[2502.17410](http://arxiv.org/abs/2502.17410)|**[link](https://github.com/lliu606/cosmos)**|**大型语言模型（LLMs）在各个领域都展示了显著的成功，但它们的优化仍然是一个重大挑战，因为它们处于复杂的高维损失景观中。尽管自适应优化器如AdamW被广泛使用，但它们存在一些关键限制，包括无法捕捉坐标之间的相互依赖性以及内存消耗高。后续的研究，以SOAP为例，试图更好地捕捉坐标之间的相互依赖性，但这带来了更大的内存开销，限制了大规模LLMs的可扩展性。另一种方法旨在通过低维投影来减少内存消耗，但这导致了显著的近似误差，从而降低了优化的有效性（例如，在每token效率方面）。在本文中，我们提出了COSMOS，这是一种新颖的混合优化器，它利用梯度矩阵中特征子空间的不同重要性来实现内存效率而不牺牲优化性能。COSMOS的设计基于我们的经验洞察和实际考虑。具体来说，COSMOS将SOAP应用于主要优化动力学的主导特征子空间，并将MUON应用于剩余的特征子空间，这部分特征子空间虽然不那么重要但用SOAP处理计算成本较高。这种混合策略显著减少了内存消耗同时保持了稳健的优化性能，使其特别适用于大规模LLMs。我们提供了在各种数据集和Transformer架构上的数值实验以证明COSMOS的有效性。我们的代码可在https://github.com/lliu606/COSMOS获取。**|
|**2025-02-24**|**Large Language Models are Powerful EHR Encoders**|Stefan Hegselmann et.al.|[2502.17403](http://arxiv.org/abs/2502.17403)|**[link](https://github.com/stefanhgm/ehrshot-benchmark)**|电子健康记录（EHRs）为临床预测提供了丰富的潜力，但其固有的复杂性和异质性对传统机器学习方法提出了显著挑战。专门领域的EHR基础模型在大型未标注EHR数据集上训练后，已显示出在预测准确性和泛化方面的显著改进；然而，其训练受到有限的多样化高质量数据集访问和编码标准及医疗实践不一致性的限制。在这项研究中，我们探讨了使用通用大规模语言模型（LLMs）基于嵌入的方法作为EHR编码器的可能性。通过将患者记录序列化为结构化的Markdown文本，将代码转换为人类可读的描述符，我们利用了预训练于庞大公共语料库上的LLMs广泛的泛化能力，从而绕过了对专有医疗数据集的需求。我们系统地评估了两个最先进的LLM嵌入模型，即GTE-Qwen2-7B-Instruct和LLM2Vec-Llama3.1-8B-Instruct，在来自EHRSHOT基准的15个不同临床预测任务中的表现，并将其与专门的EHR基础模型CLIMBR-T-Base以及传统的机器学习基线进行比较。我们的结果表明，基于LLM的嵌入在许多情况下匹配甚至超过了专业模型的表现，即使是在少量样本设置下也是如此，并且它们的有效性随着底层LLM的大小和可用上下文窗口的增加而提升。总体而言，我们的发现表明，将LLMs重新用于EHR编码提供了一种可扩展且有效的方法来进行临床预测，能够克服传统EHR建模的局限性，并促进更互操作和泛化的医疗应用。|
|**2025-02-24**|**On Relation-Specific Neurons in Large Language Models**|Yihong Liu et.al.|[2502.17355](http://arxiv.org/abs/2502.17355)|**[link](https://github.com/cisnlp/relation-specific-neurons)**|**在大型语言模型（LLM）中，某些神经元可以存储在预训练期间学到的不同知识片段。虽然知识通常表现为关系和实体的组合，但目前尚不清楚是否存在专注于特定关系本身的神经元——独立于任何实体。我们假设这些神经元能够检测输入文本中的关系，并引导涉及此类关系的生成。为了研究这一点，我们对Llama-2系列模型在一组选定的关系上采用了一种基于统计的方法。我们的实验表明存在特定于关系的神经元。我们测量了有选择地停用特定于关系 $r$的候选神经元对LLM处理（1）关系为$r$的事实的能力以及（2）关系为不同关系$r' \neq r$的事实的能力的影响。根据它们编码关系信息的能力，我们提供了以下三个特性的证据。（i）神经元累积性。关系$r$的神经元表现出累积效应，因此停用更大比例的这些神经元会导致$r$ 关系中更多的事实降解。（ii）神经元多功能性。神经元可以在多个密切相关以及较少相关的不同关系之间共享。一些关系神经元可以跨语言转移。（iii）神经元干扰。停用特定于一个关系的神经元可以提高LLM生成其他关系事实的表现。我们将公开我们的代码，网址为https://github.com/cisnlp/relation-specific-neurons。**|
|**2025-02-24**|**How Scientists Use Large Language Models to Program**|Gabrielle O'Brien et.al.|[2502.17348](http://arxiv.org/abs/2502.17348)|null|科学家们在数据收集、生成、统计建模和可视化等关键活动中编写代码。随着能够生成代码的大规模语言模型变得广泛可用，科学家们在研究软件开发过程中可能会越来越多地使用这些模型。我们调查了早期采用代码生成模型的科学家的特点，并对一所公立研究型大学的科学家进行了访谈。通过访谈和用户交互日志审查，我们发现科学家经常将代码生成模型用作导航不熟悉的编程语言和库的信息检索工具。我们展示了他们的验证策略，并讨论了可能由于科学家不知情地影响其参数而导致科学分析潜在漏洞的问题。|
|**2025-02-21**|**Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training**|Jaydeep Borkar et.al.|[2502.15680](http://arxiv.org/abs/2502.15680)|**[link](https://github.com/jaydeepborkar/Assisted-Memorization)**|由于个人身份信息（PII）的敏感性质，其所有者可能有权控制其是否被纳入大型语言模型（LLM）的训练。除了这一点，PII 可能会因为不断发展的数据集整理技术、新的重新训练数据抓取或者新的下游微调阶段而被添加或从训练数据集中移除。我们发现，PII 的记忆量和记忆容易程度是模型的一个动态属性，在整个训练流程中不断发展，并且取决于常见的设计选择变化。我们描述了三种这样的新现象：(1) 在训练后期出现的类似外观的 PII 可以引发对早期出现序列的记忆，我们称之为辅助记忆，在我们的设置中这占到高达三分之一；(2) 添加 PII 可以显著增加其他 PII 的记忆（在我们的设置中最多可达到约7.5倍）；(3) 移除 PII 可能会导致其他 PII 被记忆。模型创建者在训练模型时应考虑这些一阶和二阶隐私风险，以避免新 PII 泄露的风险。|
|**2025-02-21**|**FLEKE: Federated Locate-then-Edit Knowledge Editing**|Zongkai Zhao et.al.|[2502.15677](http://arxiv.org/abs/2502.15677)|**[link](https://github.com/zongkaiz/fleke)**|Locate-then-Edit知识编辑（LEKE）是更新大型语言模型（LLMs）而不进行完整重新训练的关键技术。然而，现有方法假设单一用户设置，在现实世界的多客户端场景中变得低效，其中分散的组织（如医院、金融机构）独立更新重叠的知识，导致冗余的中介知识向量（MKV）计算和隐私问题。为了解决这些挑战，我们引入了联邦Locate-then-Edit知识编辑（FLEKE），这是一个新的任务，使多个客户端能够在保持隐私的同时协作执行LEKE并减少计算开销。为了实现这一点，我们提出了FedEdit，一个两阶段框架，优化MKV选择和重用。在第一阶段，客户端本地应用LEKE并上传计算出的MKVs。在第二阶段，FLEKE不仅依赖于基于服务器的MKV共享，还允许客户端根据余弦相似性检索相关的MKVs，从而实现知识的再编辑并最小化冗余计算。在两个基准数据集上的实验结果表明，FedEdit保留了非联邦LEKE超过96%的性能，同时比基于FedAvg的基线显著提升了大约两倍。此外，我们发现MEMIT在我们的FedEdit框架下的FLEKE任务中表现得比PMET更为一致。我们的代码可在https://github.com/zongkaiz/FLEKE获取。|
|**2025-02-21**|**AutoToM: Automated Bayesian Inverse Planning and Model Discovery for Open-ended Theory of Mind**|Zhining Zhang et.al.|[2502.15676](http://arxiv.org/abs/2502.15676)|**[link](https://github.com/SCAI-JHU/AutoToM)**|理论知识（ToM），即根据行为理解人们心理状态的能力，是开发社会智能代理的关键。目前对理论的理解方法要么依赖于提示大型语言模型（LLMs），这容易出现系统性错误，要么使用刚性的手工制作的贝叶斯理论（BToM）模型，虽然更稳健但无法跨不同领域泛化。在这项工作中，我们介绍了AutoToM，这是一种自动化的贝叶斯理论方法，用于实现开放式的机器理论理解。AutoToM可以在任何领域操作，推断任何心理变量，并进行稳健的任意阶次的理论推理。给定一个理论推理问题，AutoToM首先提出一个初始的BToM模型。然后基于所提出的模型进行自动化的贝叶斯逆向规划，利用LLM作为后端。根据推理的不确定性，它通过迭代地改进模型来引入额外的心理变量和/或结合更多的时序上下文。在多个理论基准上的实证评估表明，AutoToM始终达到最先进的性能，提供了一种可扩展、稳健且可解释的机器理论方法。|
|**2025-02-21**|**Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing**|Shoumik Saha et.al.|[2502.15666](http://arxiv.org/abs/2502.15666)|**[link](https://github.com/ShoumikSaha/ai-polished-text)**|大型语言模型（LLMs）在文本生成中的广泛应用引发了对AI生成内容检测的广泛关注。然而，一个被忽视的挑战是AI精修文本，其中人类编写的文本使用AI工具进行细微的润色。这就引出了一个重要问题：经过轻微润色的文本是否应归类为AI生成的？错误分类可能导致虚假的剽窃指控以及关于在线内容中AI普及程度的误导性声明。在本研究中，我们系统地评估了十一种最先进的AI文本检测器，使用我们的AI精修文本评估（APT-Eval）数据集，该数据集包含11.7K个样本，这些样本在不同程度的AI参与下进行了润色。我们的研究结果表明，检测器经常错误地将经过轻微润色的文本分类为AI生成的，难以区分不同级别的AI参与度，并且对较旧和较小的模型存在偏见。这些局限性突显了需要更精细的检测方法的迫切需求。|
|**2025-02-21**|**Machine-generated text detection prevents language model collapse**|George Drayson et.al.|[2502.15654](http://arxiv.org/abs/2502.15654)|null|随着大型语言模型（LLMs）的日益普及，它们生成的输出正在网络上激增，这可能导致未来机器生成的内容稀释人类撰写的内容。由于网络数据是LLM预训练的主要资源，未来的模型将在未知比例的合成数据上进行训练。这将导致模型崩溃，这是一种退化过程，使模型强化自身的错误并经历模型性能下降。在这项研究中，我们调查了解码策略对模型崩溃的影响，在递归训练过程中分析生成数据的特征、其与人类参考文本的相似性以及由此产生的模型性能。利用导致显著模型退化的解码策略，我们探讨了在无法确定训练数据来源（人类或合成）的情况下如何避免模型崩溃。我们设计了一种新方法，基于从我们的机器生成文本检测器获得的重要性权重重新采样数据分布。我们的方法在一个开放的文本生成任务中验证了两种LLM变体（GPT-2和SmolLM2），证明我们能够成功防止模型崩溃，并且当训练数据集中有足够的人类撰写的数据时，我们的方法可以提高模型性能。|
|**2025-02-21**|**Empowering LLMs with Logical Reasoning: A Comprehensive Survey**|Fengxiang Cheng et.al.|[2502.15652](http://arxiv.org/abs/2502.15652)|null|大型语言模型（LLMs）在各种自然语言任务上取得了显著的成功。然而，近期的研究发现，LLMs在逻辑推理能力方面仍然存在重大挑战。本文总结并分类了主要的挑战为两个方面：（1）逻辑问答，LLMs经常无法在需要复杂演绎、归纳或溯因推理的复杂逻辑问题中生成正确答案，这些问题基于一系列前提和约束条件。（2）逻辑一致性，LLMs容易产生自相矛盾的回答，在不同的问题之间出现冲突。例如，最先进的Macaw问答LLM对问题“喜鹊是鸟类吗？”和“鸟类有翅膀吗？”都回答“是”，但对“喜鹊有翅膀吗？”却回答“否”。为了促进这一研究方向，我们全面调查了最前沿的方法，并提出了这些方法的详细分类。具体来说，为了准确回答复杂的逻辑问题，以前的方法可以根据对外部求解器、提示、预训练和微调的依赖程度进行分类。为了避免逻辑矛盾，我们讨论了各种逻辑一致性的概念和解决方案，包括蕴含、否定、传递性、事实一致性及其组合。此外，我们回顾了常用的基准数据集和评估指标，并讨论了有前景的研究方向，如扩展模态逻辑以考虑不确定性，以及同时满足多个逻辑一致性的高效算法。|
|**2025-02-21**|**Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models**|Anirudh Sundar et.al.|[2502.15639](http://arxiv.org/abs/2502.15639)|null|跨语言表示的对齐是多语言大型语言模型（mLLMs）中的一个理想属性，因为对齐可以提高跨语言任务的表现。通常情况下，对齐需要微调模型，这在计算上非常昂贵，并且可能缺乏大量的语言数据。一种数据高效的替代方法是模型干预——一种操纵模型激活以引导生成向期望方向发展的方法。我们分析了一种流行的干预方法（寻找专家）对mLLMs中跨语言表示对齐的影响。我们确定了针对特定语言需要操作的神经元，并检查了mLLMs在干预前后的嵌入空间。我们表明，修改mLLM的激活会改变其嵌入空间，从而增强跨语言对齐。此外，我们证明了嵌入空间的变化转化为检索任务下游性能的提升，在跨语言检索的top-1准确率上有高达2倍的改进。|
|**2025-02-21**|**The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer**|Marthe Ballon et.al.|[2502.15631](http://arxiv.org/abs/2502.15631)|**[link](https://github.com/MartheBallon/analysis_o3-mini_thinks_harder_not_longer)**|大型语言模型在数学推理方面取得了显著进展，利用了思维链和测试时计算扩展。然而，关于推理令牌使用与准确率提升之间相互作用的许多问题仍未解决。特别是，当比较不同代际的模型时，尚不清楚性能的提升是由于更长的推理链还是更高效的推理。我们系统地分析了Omni-MATH基准上的o1-mini和o3-mini变体的思维链长度，发现o3-mini (m)在不增加推理链长度的情况下实现了更高的准确性。此外，我们展示了即使控制问题难度，随着推理链的增长，所有模型和计算设置下的准确性通常会下降。这种准确性下降在更熟练的模型中显著较小，这表明新一代的推理模型在测试时计算使用上更为有效。最后，我们指出虽然o3-mini (h)相比o3-mini (m)在准确性上有微小提升，但它通过在所有问题上分配更多的推理令牌实现这一目标，即便这些问题是o3-mini (m)已经能够解决的。这些发现为模型能力和推理长度之间的关系提供了新的见解，并对效率、扩展和评估方法论具有影响。|
|**2025-02-21**|**Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing**|Qi Le et.al.|[2502.15618](http://arxiv.org/abs/2502.15618)|**[link](https://github.com/qi-le1/probe_pruning)**|我们引入了探针剪枝（Probe Pruning, PP），这是一种新颖的框架，用于在线、动态地对大型语言模型（LLMs）进行结构化剪枝，并以批处理方式应用。PP 利用了一种见解，即并非所有样本和令牌对模型的输出贡献相同，通过探查每个批次的一小部分可以有效地识别关键权重，从而实现针对不同批次的定制化动态剪枝。它主要包括三个阶段：探针、基于历史信息的剪枝和完整推理。在探针阶段，PP 根据残差重要性选择一小部分关键隐藏状态，运行模型的几个层。在基于历史信息的剪枝阶段，PP 战略性地整合探针状态与历史状态。随后，根据集成状态和 PP 重要性得分（一种专门用于评估每个权重通道在保持性能方面的重要性指标）进行结构性剪枝。在最后阶段，对剩余权重进行完整推理。PP 的一个主要优势是其与现有模型的兼容性，因为它在运行时不需要额外的神经网络模块或微调。对 LLaMA-2/3 和 OPT 模型的全面评估显示，即使使用最小的探针——仅使用 1.5% 的浮点运算（FLOPs）——也能显著提高 LLMs 结构化剪枝的效率。例如，在 WikiText2 上对 LLaMA-2-7B 进行评估时，PP 在 40% 剪枝比例下实现了比最先进的方法低 2.56 倍的每单位运行时间减少所对应性能下降率。我们的代码可在 https://github.com/Qi-Le1/Probe_Pruning 获取。|
|**2025-02-21**|**On the Robustness of Transformers against Context Hijacking for Linear Classification**|Tianle Li et.al.|[2502.15609](http://arxiv.org/abs/2502.15609)|null|基于Transformer的大型语言模型（LLMs）展示了强大的上下文学习能力。然而，它们的预测可能会受到事实正确的上下文干扰，这种现象被称为上下文劫持，揭示了一个显著的鲁棒性问题。为了从理论上理解这一现象，我们探索了基于最近线性Transformer进展的上下文线性分类问题。在我们的设定中，上下文token被设计为事实正确的查询-答案对，其中查询与最终查询相似但标签相反。然后，我们开发了一种关于线性Transformer鲁棒性的通用理论分析，该分析以模型深度、训练上下文长度和劫持上下文token数量为函数。一个关键发现是，训练良好的深层Transformer可以实现更高的鲁棒性，这与经验观察结果一致。我们表明，这种改进是因为深层可以实现更精细的优化步骤，有效地减轻了来自上下文劫持的干扰。这也得到了我们的数值实验的良好支持。我们的研究提供了关于深层架构优势的理论见解，并有助于增强对Transformer架构的理解。|
|**2025-02-20**|**LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention**|Shang Yang et.al.|[2502.14866](http://arxiv.org/abs/2502.14866)|**[link](https://github.com/mit-han-lab/omniserve)**|大型语言模型（LLMs）在处理长序列方面显示出巨大的潜力，但在服务这些长上下文模型时仍然面临挑战，主要由于注意力机制在预填充阶段的计算复杂度呈二次增长以及在解码阶段KV缓存的大内存占用。为了解决这些问题，我们介绍了LServe，这是一种通过混合稀疏注意力加速长序列LLM服务的有效系统。该方法统一了不同硬件友好的结构化稀疏模式，用于预填充和解码注意力，并在一个框架内实现了这一点，跳过了对较不重要令牌的块状计算。LServe展示了静态和动态稀疏性在长上下文LLM注意力中的兼容性。这种设计通过结合这些优化实现了乘法加速。具体而言，我们在预填充和解码阶段将一半的注意力头转换为几乎免费的流媒体头。此外，我们发现仅需要一个常数数量的KV页面即可保持长上下文能力，与上下文长度无关。然后我们设计了一种分层KV页面选择策略，根据查询为中心的相似性动态修剪KV页面。平均而言，LServe在预填充阶段比vLLM加速高达2.9倍，在解码阶段加速1.3-2.1倍，同时保持长上下文精度。代码已发布在https://github.com/mit-han-lab/omniserve。|
|**2025-02-20**|**Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning**|Shuyue Stella Li et.al.|[2502.14860](http://arxiv.org/abs/2502.14860)|**[link](https://github.com/stellalisy/alfa)**|大型语言模型（LLMs）在不确定性下往往无法提出有效的疑问，这使得它们在需要主动收集信息以辅助决策的领域中显得不够可靠。我们提出了ALFA框架，通过（i）将“好”问题的概念分解为一系列基于理论的属性（如清晰度、相关性），（ii）合成特定属性的问题变体，以及（iii）通过基于偏好的优化对模型进行对齐，从而显式地学习沿着这些细粒度属性提出更好的问题。以临床推理作为案例研究，我们引入了MediQ-AskDocs数据集，该数据集由17k个真实世界的临床互动组成，并额外包含了80k个特定属性的偏好配对后续问题，以及一个新的专家注释交互式医疗问答任务来评估提问能力。与最先进的指令调优LLMs相比，采用ALFA对齐的模型在MediQ-AskDocs上的诊断错误减少了56.6%，在问题级别上的胜率为64.4%，并且具有良好的泛化能力。我们的研究结果表明，通过结构化的、细粒度的属性显式引导提问是一种提升LLMs的有效可扩展路径，特别是在专业应用领域中。|
|**2025-02-20**|**FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling**|Weilin Zhao et.al.|[2502.14856](http://arxiv.org/abs/2502.14856)|null|推测性采样作为一种加速大型语言模型（LLMs）自回归生成过程的技术已经崭露头角，它通过采用先起草后验证的机制来在每次前向传递中生成多个标记。尽管最先进的推测性采样方法仅使用一层和一个语言建模（LM）头作为起草模型以实现令人印象深刻的层压缩，但它们的效率增益对于词汇量较大的LLMs（如拥有128k个标记的Llama-3-8B）大大减少。为了解决这个问题，我们提出了FR-Spec，这是一种优化草案候选选择的频率排名推测性采样框架，通过压缩词汇空间来实现。通过将草案搜索限制在一个基于频率优先的标记子集上，我们的方法将LM头计算开销减少了75%，同时确保最终输出分布的等效性。多项数据集上的实验表明，其平均速度比最先进的推测性采样方法EAGLE-2快1.12倍。|
|**2025-02-20**|**Prompt-to-Leaderboard**|Evan Frick et.al.|[2502.14855](http://arxiv.org/abs/2502.14855)|**[link](https://github.com/lmarena/p2l)**|大型语言模型（LLM）的评估通常依赖于准确率或人类偏好等聚合指标，这些指标在用户和提示之间进行平均。这种平均掩盖了模型性能在不同用户和提示之间的变化。为了解决这个问题，我们提出了Prompt-to-Leaderboard（P2L）方法，该方法生成特定于提示的排行榜。核心思想是训练一个以自然语言提示作为输入的LLM，输出布拉德利-特erry系数向量，然后用于预测人类偏好投票。由此产生的提示依赖型排行榜允许进行无监督的任务特定评估、查询到模型的最佳路由、个性化以及自动评估模型的优点和缺点。来自Chatbot竞技场的数据表明，P2L比平均排行榜更好地捕捉了语言模型性能的细微差别。此外，我们的研究结果表明，P2L产生提示特定评估的能力遵循类似于LLM本身的幂律缩放。2025年1月，我们基于此方法训练的路由器在Chatbot竞技场排行榜上获得了第一名。我们的代码可以在以下GitHub链接获取：https://github.com/lmarena/p2l。|
|**2025-02-20**|**GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks**|Jianwen Luo et.al.|[2502.14848](http://arxiv.org/abs/2502.14848)|**[link](https://github.com/ayanami2003/gate)**|大型语言模型（LLMs）在工具制作方面展现出了巨大的潜力，但现有的框架通常难以高效地构建可靠的工具集，并且局限于单任务设置。为了解决这些挑战，我们提出了GATE（基于图的自适应工具进化），这是一种自适应框架，能够动态构建和进化可重用工具的分层图谱，适用于多个场景。我们在开放性任务（Minecraft）、基于代理的任务（TextCraft, DABench）以及代码生成任务（MATH, Date, TabMWP）上评估了GATE。结果表明，GATE在Minecraft中的里程碑完成速度比之前的最先进方法快达4.3倍，在代码生成任务中比现有工具制作方法平均提高了9.23%，在代理任务中提高了10.03%。GATE展示了自适应进化的强大能力，平衡了工具的数量、复杂性和功能性，同时保持了高效率。代码和数据可在<https://github.com/ayanami2003/GATE>获取。|
|**2025-02-20**|**Red-Teaming LLM Multi-Agent Systems via Communication Attacks**|Pengfei He et.al.|[2502.14847](http://arxiv.org/abs/2502.14847)|null|基于大型语言模型的多智能体系统（LLM-MAS）通过消息传递的方式使智能体之间能够进行复杂的协作，从而极大地提升了解决复杂问题的能力。然而，尽管通信框架对于智能体协调至关重要，但它也引入了一个关键但未被充分探索的安全漏洞。在这项工作中，我们介绍了Agent-in-the-Middle（AiTM），这是一种新颖的攻击方式，它通过拦截和操纵智能体之间的消息来利用LLM-MAS中的基本通信机制。与那些仅通过破坏单个智能体而进行的现有攻击不同，AiTM展示了对手如何仅通过操纵在智能体之间传递的消息就能危及整个多智能体系统。为了应对有限控制和角色受限的通信格式所带来的挑战，我们开发了一种具有反射机制的对抗性智能体，该智能体能够生成上下文感知的恶意指令。我们的全面评估涵盖了各种框架、通信结构和现实世界的应用场景，结果表明LLM-MAS容易受到基于通信的攻击，这突显了在多智能体系统中实施稳健安全措施的必要性。|
|**2025-02-20**|**Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation**|Yue Yang et.al.|[2502.14846](http://arxiv.org/abs/2502.14846)|null|推理关于图像中的丰富文本（如图表和文档）是视觉语言模型（VLMs）的关键应用。然而，由于缺乏多样化且包含丰富文本的视觉语言数据，这些模型在这些领域往往表现不佳。为了解决这一挑战，我们提出了CoSyn框架，该框架利用纯文本大型语言模型（LLMs）的编码能力自动生成合成的图文数据。给定描述目标领域的输入文本（例如，“营养成分标签”），CoSyn提示LLM生成用于渲染合成图像的代码（如Python、HTML、LaTeX等）。通过将底层代码作为合成图像的文本表示，CoSyn可以生成高质量的指令调优数据，再次依赖于纯文本的LLM。使用CoSyn，我们构建了一个包含40万张图像和270万行视觉语言指令调优数据的数据集。在七个基准测试中的综合实验表明，基于我们的合成数据训练的模型在公开的竞争性模型（包括Llama 3.2）中达到了最先进的性能，并且超过了专有的模型如GPT-4V和Gemini 1.5 Flash。此外，CoSyn可以生成合成指向数据，使VLMs能够定位输入图像内的信息，展示了其在开发能够在现实世界环境中行动的多模态代理方面的潜力。|
|**2025-02-20**|**Revealing and Mitigating Over-Attention in Knowledge Editing**|Pinzheng Wang et.al.|[2502.14838](http://arxiv.org/abs/2502.14838)|**[link](https://github.com/PinzhengWang322/Reveal_Attention_Drift)**|大型语言模型在各种任务中展示了卓越的性能，但它们仍然表现出由于训练数据中的错误知识而产生的不良错误。为了应对这一问题，出现了知识编辑方法，通过高效地修改非常小比例的参数来精确编辑特定模型知识。然而，这些方法可能导致特定性失败的问题：当与编辑知识相关的内容出现在上下文中时，可能会无意中破坏其他预存知识。我们的初步研究表明，特定性失败主要源于模型的注意力头对与编辑知识相关的实体赋予了过多的注意力分数，从而过度关注上下文中的特定片段，我们将其称为注意力漂移现象。为了缓解这种注意力漂移问题，我们提出了一种简单而有效的方法——选择性注意力漂移限制（SADR），该方法在知识编辑过程中引入了一个额外的正则化项，以限制注意力权重分布的变化，从而防止过度关注编辑实体。在五个常用的强大LLM上的实验表明了我们方法的有效性，在主要的知识编辑任务中，SADR可以显著减轻特定性失败问题。|
|**2025-02-20**|**Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs**|Danni Liu et.al.|[2502.14830](http://arxiv.org/abs/2502.14830)|**[link](https://github.com/dannigt/mid-align)**|尽管大型语言模型通过微调在特定任务应用中表现出显著的能力，但将其益处扩展到多种语言对于广泛的可用性至关重要。然而，有效的跨语言迁移受到LLM在不同语言之间的性能差距以及许多语言中微调数据稀缺的阻碍。通过对来自1,000多种语言对的LLM内部表示进行分析，我们发现中间层表现出最强的跨语言对齐潜力。基于这一发现，我们提出了一种集成到特定任务训练中的中间层对齐目标。我们在槽位填充、机器翻译和结构化文本生成上的实验表明，这种策略在跨语言迁移中展现出一致的改进，特别是在低资源语言中。该方法对对齐语言的选择具有鲁棒性，并能推广到未见过的对齐语言。此外，我们展示了单独训练的对齐模块可以与现有的特定任务模块合并，从而提高跨语言能力而不需进行全面重新训练。我们的代码是公开可获取的（https://github.com/dannigt/mid-align）。|
|**2025-02-20**|**A Survey of Model Architectures in Information Retrieval**|Zhichao Xu et.al.|[2502.14822](http://arxiv.org/abs/2502.14822)|null|这项调查研究了信息检索（IR）领域中模型架构的演变，重点关注两个关键方面：特征提取的主干模型和用于相关性估计的端到端系统架构。此次回顾有意将架构考虑与训练方法分开，以便对IR系统的结构创新进行聚焦分析。我们追踪了从传统的基于术语的方法到现代神经网络方法的发展过程，特别强调了基于变换器的模型以及随后的大规模语言模型（LLM）的影响。最后，我们讨论了新兴的挑战和未来方向，包括性能和可扩展性的架构优化、处理多模态和多语言数据，以及适应超越传统搜索范式的新型应用领域。|
|**2025-02-19**|**Where's the Bug? Attention Probing for Scalable Fault Localization**|Adam Stein et.al.|[2502.13966](http://arxiv.org/abs/2502.13966)|null|确保代码正确性即使在大型语言模型（LLM）变得越来越擅长处理与代码相关任务的情况下仍然是一个具有挑战性的问题。尽管基于LLM的程序修复系统可以通过仅使用用户的错误报告来提出错误修复建议，但它们的有效性从根本上受限于其执行故障定位（FL）的能力，这对人类和LLM来说都是一个难题。现有的FL方法依赖可执行的测试用例，需要在昂贵且通常噪声较大的逐行注释上进行训练，或者需要资源密集型的LLM。在本文中，我们提出了Bug Attention Probe（BAP），这是一种无需任何直接定位标签即可学习最先进的故障定位的方法，并且比传统的FL基线和大型LLM的零样本提示效果更好。我们在各种代码设置中评估了我们的方法，包括来自标准Defects4J数据集的实际Java错误以及其他七个涵盖不同错误类型和语言的数据集。在所有八个数据集上的平均表现，BAP相比最强基线提升了34.6%的top-1准确率，并且比零样本提示GPT-4高出93.4%。BAP在效率方面也显著优于提示，以较小的计算成本超过了大规模开源模型。|
|**2025-02-19**|**Autellix: An Efficient Serving Engine for LLM Agents as General Programs**|Michael Luo et.al.|[2502.13965](http://arxiv.org/abs/2502.13965)|null|大型语言模型（LLM）应用程序正在超越简单的聊天机器人，发展成为能够推理、探索和解决复杂任务的动态通用代理程序。然而，现有的LLM服务系统忽略了程序之间以及调用之间的依赖关系，从而错失了优化的重大机会。我们的分析表明，提交给LLM服务引擎的程序经历了较长的累计等待时间，主要是由于单个LLM请求和程序层面的队首阻塞造成的。为了解决这个问题，我们引入了Autellix，这是一种将程序视为一等公民的LLM服务系统，旨在最大程度地减少程序的端到端延迟。Autellix拦截程序提交的LLM调用，并为调度器提供程序级别的上下文。我们提出了两种调度算法——针对单线程和分布式程序的算法——这些算法根据程序之前完成的调用进行预取和优先级排序。评估表明，在不同的LLM和代理工作负载下，与最先进的系统（如vLLM）相比，Autellix在相同的延迟下将程序的吞吐量提高了4到15倍。|
|**2025-02-19**|**MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads**|Weihao Liu et.al.|[2502.13963](http://arxiv.org/abs/2502.13963)|**[link](https://github.com/NeosKnight233/MuDAF)**|大型语言模型（LLMs）经常因为输入中的无关信息而注意力分散，这严重损害了它们的长上下文处理能力。受最近关于检索头在长上下文事实性方面的有效性研究的启发，我们旨在通过直接改进此类检索头来解决这一注意力分散问题。我们提出了多文档注意力聚焦（MuDAF），这是一种新的方法，通过对比学习显式优化头部级别的注意力分布。根据实验结果，MuDAF可以显著提高LLMs在长上下文问答任务中的表现，特别是在多文档问答中。广泛的评估显示了检索分数和注意力可视化，表明MuDAF在使注意力头更加专注于相关信息并减少注意力分散方面具有巨大潜力。|
|**2025-02-19**|**Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering**|William Jurayj et.al.|[2502.13962](http://arxiv.org/abs/2502.13962)|null|在测试时扩展大型语言模型的计算已显示出在推理基准上令人印象深刻的性能。然而，现有的测试时扩展评估方法强烈假设推理系统应该总是对任何提出的问题给出答案。这忽略了模型是否对其答案有信心以及是否始终提供响应是否合适的问题。为了解决这些问题，我们在推理过程中提取置信分数以阈值化模型响应。我们发现，在推理时增加计算预算不仅有助于模型更正地回答更多问题，还能提高对正确答案的信心。然后我们通过考虑具有非零响应风险的设置来扩展当前评估中的零风险响应范式，并建议在这些设置下报告评估结果的方法。|
|**2025-02-19**|**LIDDIA: Language-based Intelligent Drug Discovery Agent**|Reza Averly et.al.|[2502.13959](http://arxiv.org/abs/2502.13959)|null|药物发现是一个漫长、昂贵且复杂的过程，很大程度上依赖于人类药物化学家的努力，他们可能需要花费数年时间来探索潜在的治疗方法。近年来，人工智能在化学领域的进展旨在加速药物发现中的各个任务；然而，仍然迫切需要一种能够导航药物发现过程的智能代理。为此，我们介绍了LIDDiA，这是一种能够在计算机模拟中智能导航药物发现过程的自主代理。通过利用大型语言模型的推理能力，LIDDiA成为了一种低成本且高度灵活的自主药物发现工具。我们全面检验了LIDDiA，结果表明（1）它能够在超过30个临床相关目标中的70%以上生成符合关键药物标准的分子，（2）它在化学空间中智能地平衡了探索与开发，以及（3）它能够在EGFR上识别出有前景的新型药物候选物，EGFR是癌症的关键靶点。|
|**2025-02-19**|**Neurosymbolic artificial intelligence via large language models and coherence-driven inference**|Steve Huntsman et.al.|[2502.13953](http://arxiv.org/abs/2502.13953)|null|我们设计了一种算法来生成能够客观实例化支持连贯性驱动推理的图集的命题集。然后，我们将大型语言模型（LLMs）在从用自然语言表达的命题（经过直接转换）重建连贯性图的能力上进行基准测试，结果表明优化推理的模型具有前景。结合连贯性驱动推理与神经网络模型的一致性评估，可能会推动机器认知的发展。|
|**2025-02-19**|**Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region**|Chak Tou Leong et.al.|[2502.13946](http://arxiv.org/abs/2502.13946)|null|大型语言模型（LLMs）的安全对齐仍然存在脆弱性，因为它们的初始行为很容易被相对简单的攻击破解。由于在输入指令和初始模型输出之间填充固定模板是现有LLMs的常见做法，我们假设这个模板是其脆弱性的关键因素：LLMs的安全相关决策过度依赖于模板区域的聚合信息，这在很大程度上影响了这些模型的安全行为。我们将此问题称为基于模板的安全对齐。在本文中，我们进行了广泛的实验并验证了基于模板的安全对齐在各种已对齐的LLMs中普遍存在。我们的机制分析表明，当遇到推理时的越狱攻击时，它会导致模型的易受攻击性。此外，我们展示了从模板区域分离安全机制有望减轻越狱攻击中的脆弱性。我们鼓励未来的研究开发更稳健的安全对齐技术，以减少对模板区域的依赖。|
|**2025-02-19**|**LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization**|Guanzheng Chen et.al.|[2502.13922](http://arxiv.org/abs/2502.13922)|**[link](https://github.com/DAMO-NLP-SG/LongPO)**|**大型语言模型（LLMs）通过预训练和对齐展示了显著的能力。然而，优秀的短上下文LLMs在长上下文场景中可能会表现不佳，因为它们的长上下文对齐不足。这一对齐过程仍然具有挑战性，因为人为标注扩展上下文是不切实际的，并且难以平衡短上下文和长上下文的表现。为了解决这些挑战，我们引入了LongPO，它使短上下文LLMs能够自我进化以在长上下文任务中表现出色，通过内部转移短上下文能力。LongPO利用LLMs从自生成的短到长偏好数据中学习，这些数据包括为相同指令生成的配对响应，其中一个是针对长上下文输入生成的，另一个是其压缩后的短上下文对应版本。这种偏好揭示了LLMs在短上下文对齐过程中培养的能力和潜力，在未充分对齐的长上下文场景中可能会减弱。此外，LongPO引入了一种短到长的KL约束，以减轻长上下文对齐过程中短上下文性能下降的问题。当应用于Mistral-7B-Instruct-v0.2从128K到512K的上下文长度时，LongPO完全保留了短上下文性能，并在长上下文和短上下文任务中大大超越了简单的微调（SFT）和拒绝策略优化（DPO）。具体来说，使用LongPO训练的模型在长上下文基准测试中的表现可以与更高级的LLMs（例如GPT-4-128K）相媲美，甚至超过它们，尽管这些模型涉及大量的长上下文标注和更大的参数规模。**|
|**2025-02-19**|**Exploring Code Language Models for Automated HLS-based Hardware Generation: Benchmark, Infrastructure and Analysis**|Jiahao Gai et.al.|[2502.13921](http://arxiv.org/abs/2502.13921)|null|近年来，代码生成领域的进展揭示了利用大型语言模型（LLMs）进行通用编程语言（如Python和C++）编程的潜力，开启了自动化软件开发和提高程序员生产力的新机遇。LLMs在软件编程中的潜力引发了探索自动化硬件生成和自动化的极大兴趣。尽管已经初步尝试将LLMs应用于生成硬件描述语言（HDLs），但在这一方向上仍存在若干挑战。首先，可用的HDL训练数据量远小于软件编程语言的训练数据量。其次，主要针对软件代码设计的预训练LLMs往往会产生更容易出错的HDL设计。第三，生成HDL需要显著更多的令牌，导致成本和能源消耗的效率低下。为了解决这些挑战，本文探讨了利用LLMs生成基于高层次综合（HLS）的硬件设计。尽管文献中已有针对特定领域编程语言的代码生成研究，我们旨在提供实验结果、见解、基准测试和评估基础设施来调查HLS相对于低级HDLs在LLM辅助硬件设计生成方面的适用性。为此，我们首先对预训练模型进行微调，以生成基于HLS的硬件设计，使用收集的数据集，其中包含文本提示和相应的参考HLS设计。然后提出了一种LLM辅助框架，实现端到端硬件代码生成，同时研究了链式思维和反馈循环促进技术对HLS设计生成的影响。由于本研究的时间限制，我们计划在未来评估更先进的推理模型。|
|**2025-02-19**|**Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health**|Xingbo Wang et.al.|[2502.13920](http://arxiv.org/abs/2502.13920)|null|尽管睡眠跟踪设备非常普遍，但许多个体仍难以将数据转化为改善睡眠健康的实际措施。目前的方法通常提供基于数据的建议，但这些方法可能不符合现实生活中的限制和个人背景。我们介绍了HealthGuru，这是一种基于大型语言模型的聊天机器人，旨在通过数据驱动、理论指导和适应性建议来增强睡眠健康，并提供对话式行为改变支持。HealthGuru的多代理框架整合了可穿戴设备数据、情境信息以及情境多臂强盗模型，以提出定制的改善睡眠活动。该系统在促进自然对话的同时，结合了基于数据的见解和理论上的行为改变技术。我们的为期八周的实际环境部署研究中有16名参与者，比较了HealthGuru与一个基准聊天机器人。结果显示，使用HealthGuru的个体在睡眠时长和活动评分方面有所提高，且收到了质量更高的回复，用户的改变行为动机也更高。我们也识别了关于个性化和用户参与度在健康聊天机器人中的挑战和设计考虑因素。|
|**2025-02-18**|**Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization**|Shuo Xing et.al.|[2502.13146](http://arxiv.org/abs/2502.13146)|**[link](https://github.com/taco-group/re-align)**|大型视觉语言模型（VLMs）的出现通过整合视觉模态扩展了单一模态大型语言模型（LLMs）的能力，从而在各种现实场景中解锁了变革性的跨模态应用。尽管它们表现优异，但VLMs容易出现显著的幻觉，尤其是以跨模态不一致的形式出现。基于强化学习从人类反馈（RLHF）在对齐LLMs方面的成功，最近的研究重点放在通过精心策划的数据集上的直接偏好优化（DPO）来缓解这些问题。然而，这些方法通常以一种粗暴的方式引入偏好信号，忽略了视觉信息在对齐过程中的关键作用。在本文中，我们介绍了Re-Align，这是一种新颖的对齐框架，利用图像检索构建双重偏好数据集，有效结合了文本和视觉偏好信号。我们进一步引入了rDPO，这是标准直接偏好优化的一个扩展，在微调过程中加入了额外的视觉偏好目标。我们的实验结果表明，Re-Align不仅比以前的方法更有效地减少了幻觉，还在一般视觉问答（VQA）任务中获得了显著的性能提升。此外，我们展示了Re-Align在广泛的VLM大小和架构上保持了鲁棒性和可扩展性。这项工作代表了在对齐多模态LLMs方面向前迈进了一大步，为更可靠和有效的跨模态应用铺平了道路。我们在https://github.com/taco-group/Re-Align发布了所有代码。|
|**2025-02-18**|**Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation**|Bencheng Liao et.al.|[2502.13145](http://arxiv.org/abs/2502.13145)|**[link](https://github.com/hustvl/mmmamba)**|近期的多模态大型语言模型（MLLMs）在性能上取得了显著成就，但面临着由于其二次计算复杂性、不断增长的关键值缓存需求以及对单独视觉编码器的依赖而带来的部署挑战。我们提出了mmMamba框架，通过从现有的MLLMs逐步蒸馏出线性复杂度的原生多模态状态空间模型，该框架能够在适度的学术计算资源下开发此类模型。我们的方法使得能够直接将训练好的解码器仅MLLM转换为线性复杂度的架构，而无需预训练基于RNN的语言模型或视觉编码器。我们提出了一种播种策略来从训练过的Transformer中构建Mamba，并且提出了一种三阶段的蒸馏配方，这可以在保持多模态能力的同时有效转移来自Transformer的知识。我们的方法还支持灵活的混合架构，这些架构可以结合Transformer层和Mamba层以实现可定制的效率与性能之间的权衡。由基于Transformer的解码器仅HoVLE蒸馏出的mmMamba-linear实现了与现有线性和二次复杂性VLMs的竞争性能，而mmMamba-hybrid进一步显著提高了性能，接近HoVLE的能力。在103K令牌时，mmMamba-linear相比HoVLE展示了20.6倍的速度提升和75.8%的GPU内存减少，而mmMamba-hybrid则实现了13.5倍的速度提升和60.2%的内存节省。代码和模型发布在https://github.com/hustvl/mmMamba|
|**2025-02-18**|**UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models**|Huawei Lin et.al.|[2502.13141](http://arxiv.org/abs/2502.13141)|**[link](https://github.com/huawei-lin/uniguardian)**|大型语言模型（LLMs）容易受到诸如提示注入、后门攻击和对抗性攻击的攻击，这些攻击通过操纵提示或模型生成有害输出。在本文中，我们摒弃传统的深度学习攻击范式，探讨了它们之间的内在关系，并统称为提示触发攻击（PTA）。这引出了一个关键问题：我们能否确定一个提示是良性的还是被污染的？为了解决这个问题，我们提出了UniGuardian，这是首个统一的防御机制，旨在检测LLMs中的提示注入、后门攻击和对抗性攻击。此外，我们引入了一种单次前向策略来优化检测流程，使得攻击检测和文本生成可以在一次前向传递中同时进行。我们的实验验证了UniGuardian能够准确且高效地识别LLMs中的恶意提示。|
|**2025-02-18**|**AIDE: AI-Driven Exploration in the Space of Code**|Zhengyao Jiang et.al.|[2502.13138](http://arxiv.org/abs/2502.13138)|**[link](https://github.com/wecoai/aideml)**|机器学习作为现代人工智能的基础，推动了世界的根本性变革。然而，在这些进展背后是一个复杂且常常繁琐的过程，需要大量的劳动和计算资源进行迭代和实验。开发机器学习模型的工程师和科学家们花费大量时间在试错任务上，而不是构思创新的解决方案或研究假设。为了解决这一挑战，我们介绍了AI驱动探索（AIDE），这是一种由大型语言模型（LLMs）驱动的机器学习工程代理。AIDE将机器学习工程视为代码优化问题，并将试错过程形式化为潜在解决方案空间中的树搜索。通过战略性地重用和改进有前景的解决方案，AIDE有效地将计算资源转化为增强的性能，在多个机器学习工程基准测试中实现了最先进的结果，包括我们的Kaggle评估、OpenAI MLE-Bench和METRs RE-Bench。|
|**2025-02-18**|**Theorem Prover as a Judge for Synthetic Data Generation**|Joshua Ong Jun Leang et.al.|[2502.13137](http://arxiv.org/abs/2502.13137)|null|合成数据在数学推理中的需求日益增加，因为它们有可能提升大型语言模型（LLMs）的数学能力。然而，确保中间推理步骤的有效性仍然是一个重大挑战，影响了数据质量。虽然通过定理证明器进行的形式验证有效验证了LLMs的推理，但数学证明的自形式化仍然容易出错。作为回应，我们引入了迭代自形式化方法，该方法通过迭代地优化定理证明器的形式化来减少错误，从而将Lean证明助手上的执行率从60%提高到87%。在此基础上，我们提出了定理证明器作为裁判（TP-as-a-Judge），这是一种方法，它利用定理证明器的形式化严格评估LLMs的中间推理，有效地将自形式化与合成数据生成结合起来。最后，我们介绍了从定理证明反馈的强化学习（RLTPF），这是一个框架，用定理证明反馈取代了人类反馈的强化学习（RLHF）。在多种LLMs上应用TP-as-a-Judge和RLTPF改进了基准测试，在仅有3,508个样本的情况下，Mistral-7B在MultiArith上的准确率提高了5.56%，Llama-2-7B在SVAMP上的准确率提高了6.00%，Llama-3.1-8B在AQUA上的准确率提高了3.55%。|
|**2025-02-18**|**Learning to Defer for Causal Discovery with Imperfect Experts**|Oscar Clivio et.al.|[2502.13132](http://arxiv.org/abs/2502.13132)|null|在因果发现算法中整合专家知识（例如来自大型语言模型的知识）时，如果这些知识不能保证正确性，则可能会遇到挑战。专家建议可能与数据驱动的结果相矛盾，并且其可靠性根据领域或特定查询的不同而显著变化。现有的基于软约束或预测因果关系中不一致性的方法无法考虑这些专业知识的变化。为了解决这个问题，我们提出了L2D-CD方法，该方法用于评估专家建议的正确性，并最优地将其与数据驱动的因果发现结果结合起来。通过将学习型推迟（L2D）算法适应于成对因果发现（CD），我们学习了一个推迟函数，该函数选择是依赖于使用数值数据的经典因果发现方法还是基于文本元数据的专家建议。我们在著名的Tübingen成对数据集上评估了L2D-CD，并展示了其相对于单独使用的因果发现方法和专家的优越性能。此外，我们的方法识别了专家表现强或弱的领域。最后，我们概述了一种策略，以将此方法推广到具有超过两个变量的图形因果发现中，为该领域的进一步研究铺平了道路。|
|**2025-02-18**|**Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning**|Jingyang Lin et.al.|[2502.13127](http://arxiv.org/abs/2502.13127)|null|近期大型语言模型（LLMs）在处理更长序列方面取得了进展，从2K到2M个token甚至更长。然而，仅仅延长输入序列的长度并不一定能带来有效的长上下文理解。在这项研究中，我们通过监督的方式将思维链（CoT）推理融入LLMs以促进有效的长上下文理解。为此，我们引入了LongFinanceQA，这是一个专为改善长上下文推理而设计的金融领域合成数据集。与现有的长上下文合成数据不同，LongFinanceQA包含了在最终结论之前的中间CoT推理，这鼓励LLMs执行明确的推理，从而提高长上下文理解的准确性和可解释性。为了生成合成的CoT推理，我们提出了属性驱动的主体推理（PAI），这是一种主体框架，它模拟了包括属性提取、检索和总结的人类推理步骤。我们通过评估GPT-4o-mini w/ PAI在Loong基准上的推理能力来验证PAI的效果，结果显示其性能比标准的GPT-4o-mini高出20.0%。此外，我们在LongFinanceQA上对LLaMA-3.1-8B-Instruct进行了微调，结果表明其在Loong金融子集上的表现提高了24.6%。|
|**2025-02-18**|**RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading Premises**|Zenan Zhai et.al.|[2502.13125](http://arxiv.org/abs/2502.13125)|**[link](https://github.com/LibrAIResearch/ruozhibench)**|近期大型语言模型（LLMs）在回答需要复杂推理的问题方面取得了进展。然而，它们识别和回应包含逻辑谬误或故意误导前提的文本的能力研究较少。为解决这一差距，我们引入了RuozhiBench，这是一个双语数据集，包含677个经过精心策划的问题，这些问题包含了各种形式的欺骗性推理，并且是通过大量人力和专家评审精心制作的。我们在RuozhiBench上对来自5个系列的17个LLMs进行了全面评估，使用了开放式和二选一格式，并对评估协议和结果模式进行了广泛分析。尽管这些模型在常规基准测试中得分较高，但在检测和正确推理逻辑谬误方面表现有限，即使是最优模型Claude-3-haiku也仅达到了62%的准确率，而人类的准确率超过了90%。|
|**2025-02-18**|**Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context**|Marion Bartl et.al.|[2502.13120](http://arxiv.org/abs/2502.13120)|null|性别包容性语言的使用旨在确保所有个体，无论性别，都能与某些概念相关联。尽管心理语言学研究已经考察了它在人类认知方面的效果，但对于大规模语言模型（LLMs）如何处理性别包容性语言仍不清楚。鉴于商用LLMs正在越来越多地渗透到日常应用中，审视这些模型是否实际上以中立的方式解释性别包容性语言至关重要，因为它们生成的语言有可能影响其用户的语言习惯。本研究探讨了LLM生成的共指术语是否与给定的性别表达相一致或反映了模型的偏见。通过将法语的心理语言学方法改编为英语和德语，我们发现，在英语中，LLMs通常保持先行词的性别，但表现出潜在的男性偏向。在德语中，这种偏向要强烈得多，并且会.override所有测试过的性别中立化策略。|
|**2025-02-18**|**STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models**|Narun Raman et.al.|[2502.13119](http://arxiv.org/abs/2502.13119)|null|如何判断给定的大规模语言模型（LLM）能否可靠地进行经济推理？大多数现有的LLM基准测试侧重于特定的应用，未能向模型呈现丰富的经济任务。一个显著的例外是Raman等[2024]提出的方法，该方法全面评估了战略决策；然而，这种方法未能解决微观经济学中常见的非战略设置，如供需分析。我们通过将微观经济推理分类为58个不同的元素来弥补这一空白，重点关注供给和需求的逻辑，并在多达10个不同的领域、5个视角以及3种类型上进行研究。在这个组合空间中的基准数据生成由一种新的LLM辅助数据生成协议——auto-STEER提供动力，该协议通过适应手写模板来针对新领域和视角生成一系列问题。由于它提供了一种自动生成新问题的自动化方式，auto-STEER减少了LLMs过度拟合评估基准的风险；我们希望它能作为未来几年内评估和微调模型的有用工具。我们通过一项涉及27个LLMs的案例研究展示了我们基准测试的有效性，这些模型从小型开源模型到当前最先进的模型不等。我们检查了每个模型在我们整个分类学范围内解决微观经济问题的能力，并在各种提示策略和评分指标下展示了结果。|
|**2025-02-17**|**Idiosyncrasies in Large Language Models**|Mingjie Sun et.al.|[2502.12150](http://arxiv.org/abs/2502.12150)|**[link](https://github.com/locuslab/llm-idiosyncrasies)**|在这项工作中，我们揭示并研究了大型语言模型（LLM）的独特模式——这些模式在它们的输出中表现为独特的特征，可以用来区分这些模型。为此，我们考虑了一个简单的分类任务：给定一段特定的文本输出，目标是预测生成该文本的源LLM。我们在各种LLM组之间评估这一合成任务，并发现简单地对现有文本嵌入模型进行微调就能在LLM生成的文本上获得极高的分类准确率。值得注意的是，在涉及ChatGPT、Claude、Grok、Gemini和DeepSeek的五分类问题中，我们在保留的验证数据上达到了97.1%的准确率。我们的进一步研究表明，这些独特模式根植于单词级别的分布。即使在文本被重写、翻译或由外部LLM总结后，这些模式仍然存在，这表明它们也被编码在语义内容中。此外，我们利用LLM作为评判者生成了关于每个模型独特性的详细、开放式描述。最后，我们讨论了研究结果的更广泛影响，特别是对于训练合成数据和推断模型相似性的影响。代码可在https://github.com/locuslab/llm-idiosyncrasies获取。|
|**2025-02-17**|**HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation**|Ling Yang et.al.|[2502.12148](http://arxiv.org/abs/2502.12148)|**[link](https://github.com/gen-verse/hermesflow)**|多模态大型语言模型（MLLMs）的成功应用，如Show-e、Transfusion和Emu3等模型，在统一的图像理解和生成方面取得了显著进展。我们首次揭示了一个普遍现象：多模态大模型的理解能力通常强于其生成能力，并且在这两者之间存在显著差距。基于这一洞见，我们提出了HermesFlow，这是一种简单而通用的框架，旨在无缝弥合MLLMs中的理解与生成之间的差距。具体而言，我们以同源数据作为输入来策划理解与生成的同源偏好数据。通过Pair-DPO和自我迭代优化，HermesFlow利用同源偏好数据有效对齐多模态理解和生成。广泛的实验表明，我们的方法相较于先前的方法具有显著优势，尤其是在缩小多模态理解和生成之间的差距方面。这些发现突显了HermesFlow作为下一代多模态基础模型通用对齐框架的潜力。代码：https://github.com/Gen-Verse/HermesFlow|
|**2025-02-17**|**Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control**|Jinyan Su et.al.|[2502.12145](http://arxiv.org/abs/2502.12145)|**[link](https://github.com/jinyansu1/flare-aug)**|Retrieval-Augmented生成（RAG）作为一种有效减少大型语言模型（LLM）幻觉的方法已经出现。然而，现有的RAG框架通常无差别地应用检索，导致不必要的过度检索或者在需要复杂推理时未能迭代检索。最近的自适应检索策略虽然能够灵活应对这些检索策略，但仅基于查询复杂度进行预测，缺乏用户驱动的灵活性，这使得它们难以满足多样的用户需求。在这篇论文中，我们介绍了一种新的用户可控的RAG框架，该框架能够动态调整准确性和成本之间的权衡。我们的方法利用了两个分类器：一个旨在优先提高准确性，另一个则侧重于提高检索效率。通过一个可解释的控制参数α，用户可以根据具体需求无缝切换到最小成本检索或高精度检索。我们通过实证研究表明，我们的方法有效地平衡了准确性、检索成本和用户可控性，使其成为实际应用中实用且灵活的解决方案。|
|**2025-02-17**|**Small Models Struggle to Learn from Strong Reasoners**|Yuetai Li et.al.|[2502.12143](http://arxiv.org/abs/2502.12143)|null|大型语言模型（LLMs）在复杂的推理任务中表现出色，将其推理能力提炼到较小的模型中已显示出前景。然而，我们发现了一个有趣的现象，我们称之为小模型可学习性差距：参数量小于等于30亿的小模型并不总是能从长链式思维（CoT）推理或从较大模型的提炼中获益。相反，它们在较短、更简单的推理链上进行微调时表现更好，这些推理链更好地符合其内在的学习能力。为了解决这个问题，我们提出了混合提炼（Mix Distillation），这是一种简单而有效的策略，通过结合长链和短链CoT示例或来自大小模型的推理来平衡推理复杂度。我们的实验表明，与单独使用任一类数据训练相比，混合提炼显著提升了小模型的推理性能。这些发现突显了直接强模型提炼的局限性，并强调了适应推理复杂度对于有效传递推理能力的重要性。|
|**2025-02-17**|**SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs**|Yige Xu et.al.|[2502.12134](http://arxiv.org/abs/2502.12134)|null|链思（CoT）推理使大型语言模型（LLMs）能够通过生成中间推理步骤来解决复杂的推理任务。然而，大多数现有的方法集中在硬令牌解码上，这限制了推理在离散词汇空间内的范围，且可能并不总是最优的。尽管最近的努力探索了连续空间推理，但它们往往遭受灾难性遗忘的困扰，限制了其在已经能够在零样本设置下表现良好的最先进LLMs中的适用性。为了解决这一挑战，我们提出了一种新的连续空间推理方法，不需要修改底层的LLM。具体来说，我们采用了一个轻量级的辅助模型来推测性地生成特定实例的软链思令牌作为初始的链思过程，然后通过一个投影模块将其映射到LLM的表示空间。在五个推理基准上的实验结果表明，我们的方法通过有监督的、参数高效的微调增强了LLM的推理性能。|
|**2025-02-17**|**Transformer Dynamics: A neuroscientific approach to interpretability of large language models**|Jesseba Fernando et.al.|[2502.12131](http://arxiv.org/abs/2502.12131)|null|随着人工智能模型在规模和能力上的爆炸性增长，对其内部机制的理解仍然是一个关键挑战。受神经科学中动力系统方法成功的启发，我们在此提出了一种研究深度学习系统计算的新框架。我们重点关注变压器模型中的残差流（RS），将其视为跨层演化的动力系统。我们发现，单个RS单元的激活在各层之间表现出强烈的连续性，尽管RS并不是一个特权基。RS中的激活在各层间加速且变得更加密集，而单个单元则追踪不稳定的周期轨道。在降维空间中，RS遵循一条具有吸引子动态特性的弯曲轨迹，在较低层尤为明显。这些见解将动力系统理论与机械解释性相结合，为“人工智能神经科学”奠定了基础，该领域结合了理论严谨性和大规模数据分析，以推进对现代神经网络的理解。|
|**2025-02-17**|**Scaling Autonomous Agents via Automatic Reward Modeling And Planning**|Zhenfang Chen et.al.|[2502.12130](http://arxiv.org/abs/2502.12130)|null|大型语言模型（LLMs）在各种文本生成任务中展示了显著的能力。然而，LLMs在需要多步决策和环境反馈的问题上仍然存在困难，例如在线购物、科学推理和数学问题求解。与纯文本数据不同，收集大规模决策数据具有挑战性。此外，许多强大的LLMs仅通过API访问，这阻碍了它们因成本和复杂性原因针对代理任务进行微调。为了解决LLM代理的局限性，我们提出了一种框架，可以从环境中自动学习奖励模型而无需人工标注。该模型可以用于评估LLM代理的动作轨迹，并为任务规划提供启发式方法。具体而言，我们的方法涉及使用一个基于LLM的代理随机导航环境，生成多样化动作轨迹。随后，利用另一个LLM分配任务意图并合成每个轨迹的负面响应和正确响应。这些三元组（任务意图、正面响应和负面响应）被用作训练数据以优化能够对动作轨迹进行评分的奖励模型。通过在不同的代理基准上进行评估，证明了我们框架的有效性和泛化能力。总之，我们提出的框架代表了增强LLM代理决策能力的重大进展。通过自动化奖励模型的学习，我们克服了数据稀缺性和API限制的挑战，可能彻底改变LLMs在复杂和交互环境中的应用。这项研究为开发更复杂的AI代理铺平了道路，使其能够解决广泛的实际问题，这些问题需要多步决策。|
|**2025-02-17**|**Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty Quantification for LoRA**|Patryk Marszałek et.al.|[2502.12122](http://arxiv.org/abs/2502.12122)|**[link](https://github.com/gmum/b-lora-xs)**|低秩适应（LoRA）通过将权重更新分解为低秩矩阵，实现了大型语言模型的参数高效微调，显著减少了存储和计算开销。尽管有效，标准LoRA缺乏不确定性量化机制，导致模型过于自信且校准不良。Bayesian LoRA的变体解决了这一局限性，但代价是可训练参数数量显著增加，部分抵消了原有的效率优势。此外，这些模型更难训练，可能会遇到不稳定收敛的问题。在本工作中，我们提出了一种新型的参数高效Bayesian LoRA，证明了在非常低维度的参数空间中可以实现有效的不确定性量化。所提出的方法在保持计算效率的同时，实现了强大的性能、改进的校准和泛化能力。我们的实证结果表明，通过适当的权重空间投影：（1）可以在低维空间中有效地建模不确定性，（2）权重协方差表现出低秩特性。|
|**2025-02-17**|**LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws**|Prasanna Mayilvahanan et.al.|[2502.12120](http://arxiv.org/abs/2502.12120)|null|scaling laws 指导大型语言模型（LLMs）的发展，通过提供模型大小、token数量和计算资源之间的最优平衡估计。最近，损失到损失的scaling laws作为理解并提升LLMs性能的强大工具出现。这些scaling laws关联了预训练数据集和下游任务中的损失。在这项工作中，我们研究了哪些因素最能影响损失到损失的scaling。我们的实验揭示，预训练数据和分词器决定了scaling的趋势。相反，模型大小、优化超参数，甚至像Llama这样的Transformer基础模型与Mamba这样的状态空间模型之间的重要架构差异，影响较小。因此，实践者应仔细选择合适的预训练数据集以实现最佳的下游性能，而架构和其他设置可以根据训练效率自由优化。|
|**2025-02-17**|**PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection**|Jinhe Bi et.al.|[2502.12119](http://arxiv.org/abs/2502.12119)|null|视觉指令调优旨在改进预训练的多模态大语言模型（MLLMs），以提升其在现实世界任务中的表现。然而，视觉指令数据集的迅速扩展带来了显著的数据冗余问题，导致了巨大的计算成本。现有的数据选择方法主要依赖于代理模型或基于损失的指标，这两种方法都需要进行模型推理和反向传播，从而引入了巨大的计算开销。为了解决这一挑战，我们提出了PRISM，这是一种新颖的无需训练的方法，用于高效的多模态数据选择。与现有方法不同的是，PRISM消除了对代理模型、预热预训练以及梯度优化的依赖。相反，它利用皮尔逊相关性分析来量化MLLMs的内在视觉编码属性，通过计算任务特定的相关性分数来识别高价值实例。这不仅实现了高效的数据选择，同时保持了原始性能。在多个MLLM上的实验评估表明，PRISM将视觉指令调优和数据选择所需的总体时间减少到传统方法的30%，同时在八个多模态和三个语言理解基准测试中超越了完全微调的模型，最终性能提升了101.7%。|
|**2025-02-14**|**MM-RLHF: The Next Step Forward in Multimodal LLM Alignment**|Yi-Fan Zhang et.al.|[2502.10391](http://arxiv.org/abs/2502.10391)|null|尽管多模态大型语言模型（MLLMs）取得了显著进展，但大多数最先进的模型尚未经过与人类偏好充分对齐。这种差距存在的原因是当前的对齐研究主要在特定领域（例如，减少幻觉）取得了进展，而更广泛的问题，即对齐模型是否能系统性地提升MLLM能力，仍然在很大程度上未被探索。为此，我们引入了MM-RLHF数据集，其中包含120k条细粒度、人工注释的偏好比较对。这个数据集相对于现有资源是一个重大进步，提供了更大的规模、更多的多样性、更精细的标注粒度和更高的质量。利用该数据集，我们提出了一些关键创新来提高奖励模型的质量和对齐算法的效率。特别值得一提的是，我们引入了一种基于批评的奖励模型，该模型在评分前生成对模型输出的批评，相比传统的标量奖励机制，这种方法提供了更好的可解释性和更有用的反馈。此外，我们提出了动态奖励缩放方法，根据奖励信号调整每个样本的损失权重，从而优化高质量比较对的使用。我们的方法在10个不同的维度和27个基准上进行了严格的评估，结果表明模型性能得到了显著且一致的提升。具体而言，使用MM-RLHF和我们的对齐算法微调LLaVA-ov-7B后，对话能力提高了19.5%，安全性提高了60%。我们已经开源了偏好数据集、奖励模型、训练和评估代码以及奖励建模和安全基准。详情请访问我们的项目页面：https://mm-rlhf.github.io。|
|**2025-02-14**|**Aspect-Oriented Summarization for Psychiatric Short-Term Readmission Prediction**|WonJin Yoon et.al.|[2502.10388](http://arxiv.org/abs/2502.10388)|null|近期大型语言模型（LLMs）的进步使得无需在特定任务数据集上进行有监督训练即可处理冗长文档成为可能。然而，它们在复杂任务中的零样本性能相较于简单的信息提取任务仍然不够理想。一种可行的方法是首先对文档进行总结，然后对总结进行有监督的微调。然而，总结过程不可避免地会导致信息丢失。本研究提出了一种处理长文档总结的方法，旨在捕捉原始文档的不同重要方面。我们假设使用不同面向方面的提示生成的LLM总结包含不同的信息信号，并提出了测量这些差异的方法。我们介绍了有效整合这些不同总结中的信号以用于变压器模型的有监督训练的方法。我们在一个高影响力的任务上验证了我们的假设——从精神病出院后的30天再入院预测，使用来自四家医院的真实世界数据，结果表明我们提出的方法提高了预测患者结果这一复杂任务的性能。|
|**2025-02-14**|**Enhancing Multilingual LLM Pretraining with Model-Based Data Selection**|Bettina Messmer et.al.|[2502.10361](http://arxiv.org/abs/2502.10361)|null|数据集整理已成为强大的大型语言模型（LLM）性能的基础。虽然针对英语和多语种数据集存在各种基于规则的过滤启发式方法，但主要关注英语的基于模型的过滤技术却相对较少。为了应对由于对非英语语言的研究有限而产生的差异，我们提出了一种用于多语种数据集的基于模型的过滤框架，旨在识别一组结构化且富含知识的样本。我们的方法强调透明性、简单性和效率，利用基于Transformer和FastText的分类器来确保我们技术及数据的广泛适用性。我们在FineWeb-2网络爬虫数据集上进行了广泛的消融研究，涵盖不同的语言家族、书写系统和资源可用性，以证明我们方法的有效性。训练一个拥有10亿参数的Llama模型处理700亿和1190亿个令牌，我们的方法可以在仅使用15%的训练令牌的情况下达到基线MMLU得分，并且在其他基准测试中也有所提升。这些发现为我们的方法在其他语言中的普适性提供了有力证据。因此，我们将该框架扩展到20种语言，并发布经过精炼的预训练数据集。|
|**2025-02-14**|**Organize the Web: Constructing Domains Enhances Pre-Training Data Curation**|Alexander Wettig et.al.|[2502.10341](http://arxiv.org/abs/2502.10341)|null|现代语言模型是在由万亿个令牌组成的大型非结构化数据集上进行训练的，这些数据集是通过网络爬虫获取的。非结构化的性质使得难以推断其内容并开发系统的方法来进行数据整理。在本文中，我们通过开发内容分类法并将其组织成领域来拆解单一的网络语料库。我们引入了WebOrganizer，这是一种根据主题和格式组织网页的框架。利用这两种互补的领域概念，我们将预训练数据自动标注，并从大型语言模型中提取出高效的分类器。这使我们可以研究来自不同领域的数据如何混合以改进下游任务中的模型，并展示我们如何结合对有效主题和格式的见解进一步提高性能。我们证明我们的领域混合也可以改善基于质量选择数据的现有方法，而且我们还研究并比较了基于质量的方法将如何隐式改变领域混合。总体而言，我们的工作表明，构建和混合领域为基于质量的数据整理方法提供了有价值的补充，开启了有效的和具有洞察力的预训练数据整理的新途径。|
|**2025-02-14**|**Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering**|Nick Ferguson et.al.|[2502.10338](http://arxiv.org/abs/2502.10338)|null|大型语言模型（LLMs）在自然语言任务上表现出色，但在需要复杂多步推理的问题回答（QA）任务上仍然面临挑战。我们概述了这些任务所需的推理类型，并将其重新归类为元级别推理（类似于高级战略推理或规划）和对象级别推理（体现在较低级别的任务如数学推理中）。介绍了Franklin这一新型数据集，它要求进行元级别和对象级别的推理。该数据集连同其他三个数据集一起被用来评估四种LLMs在需要多步推理的问题回答任务上的表现。来自人类注释研究的结果表明，LLMs在高频次展示出元级别推理能力，但在某些使用到的数据集中，它们在对象级别推理任务上存在困难。此外，有证据表明LLMs发现Franklin数据集中所需的对象级别推理具有挑战性，但它们在与元级别推理要求相关的性能方面表现出色。|
|**2025-02-14**|**LLM-Powered Preference Elicitation in Combinatorial Assignment**|Ermis Soumalias et.al.|[2502.10308](http://arxiv.org/abs/2502.10308)|null|我们研究了大型语言模型（LLMs）作为人类代理的潜力，以简化组合性分配中的偏好获取（PE）。虽然传统方法依赖于迭代查询来捕捉偏好，但LLMs提供了一种一次性替代方案，并减少了人力投入。我们提出了一种框架，使LLMs能够与最先进的机器学习驱动的偏好获取方案协同工作。我们的框架解决了由LLMs引入的新挑战，如响应变化性和增加的计算成本。我们在广为人知的课程分配领域通过实验评估了LLM代理相对于人类查询的效率，并调查了成功所需的模型能力。我们发现，我们的方法可以提高高达20%的分配效率，并且这些结果在不同的LLMs之间稳健，并且对报告的质量和准确性差异具有鲁棒性。|
|**2025-02-14**|**Open-Source AI-Powered Optimization in Scalene: Advancing Python Performance Profiling with DeepSeek-R1 and LLaMA 3.2**|Saem Hasan et.al.|[2502.10299](http://arxiv.org/abs/2502.10299)|null|Python的灵活性和易用性以性能效率低下为代价，要求开发人员依赖分析器来优化执行。SCALENE是一种高性能的CPU、GPU和内存分析器，它能为Python应用程序提供细粒度的洞察，同时运行速度远快于传统分析器。最初，SCALENE集成了OpenAI的API以生成基于AI的优化建议，但其对专有API的依赖限制了可访问性。本研究探讨了使用开源大语言模型（LLMs），如DeepSeek-R1和Llama 3.2，来在SCALENE中生成优化建议的可行性。评估表明，DeepSeek-R1提供的代码优化效果与专有模型相当。我们将DeepSeek-R1集成到SCALENE中，以自动分析性能瓶颈并提出改进建议，从而增强SCALENE的功能，同时保持其开源性质。本研究证明，开源LLMs可以作为AI驱动的代码优化的有效替代方案，为更易获取和更具成本效益的性能分析工具铺平道路。|
|**2025-02-14**|**Are Large Language Models the future crowd workers of Linguistics?**|Iris Ferrazzo et.al.|[2502.10266](http://arxiv.org/abs/2502.10266)|null|数据收集是语言学研究中的核心环节之一，其中从人类参与者处获取数据是一种主要的数据收集策略。参与者的数量在这些研究中可能有很大差异，从少数几个人到众包的规模不等。尽管这两种方法都能提供丰富的数据，但它们也都伴随着许多缺点，例如在任务完成过程中对参与者注意力的控制不足、众包环境下的工作条件不佳以及耗时的实验设计。因此，本研究旨在探讨大型语言模型（LLMs）是否可以在实证语言学流程中克服这些障碍。为此，我们进行了两项再现性案例研究：Cruz（2023）和Lombard等人（2021）。我们使用OpenAI的GPT-4o-mini模型在所提出的框架内再现了原本为人类参与者设计的两个强制性数据收集任务。其零样本提示基线的表现展示了LLMs的有效性和高灵活性，这些模型往往在语言学任务上优于人类信息提供者。第二项再现研究的结果进一步强调了探索额外提示技术的必要性，例如思维链（CoT）提示，在随后的一个跟进实验中，这种提示技术在关键项目和填充项目上表现出与人类表现更高的契合度。鉴于本研究的规模有限，值得进一步探索LLMs在实证语言学及其他人文领域的应用潜力。|
|**2025-02-14**|**Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers**|Aivin V. Solatorio et.al.|[2502.10263](http://arxiv.org/abs/2502.10263)|null|跟踪数据在研究论文中的提及和使用情况对于提高数据的可发现性、质量和生产至关重要。然而，手动识别和分类大量学术文献中的数据集提及既耗资源又不可扩展。本文提出了一种机器学习框架，通过利用大型语言模型（LLMs）、合成数据和两阶段微调过程，实现跨研究领域的数据集提及自动化检测。我们采用从研究论文中进行零样本提取，以LLM作为法官进行质量评估，并使用推理代理进行精炼来生成弱监督合成数据集。Phi-3.5-mini指令模型首先在这个数据集上进行预微调，随后在一个人工标注的子集上进行微调。在推理阶段，基于ModernBERT的分类器高效地过滤数据集提及，减少了计算开销同时保持高召回率。在保留的人工标注样本上进行评估时，我们的微调模型在数据集提取准确性方面优于NuExtract-v1.5和GLiNER-large-v2.1。我们的结果表明，LLM生成的合成数据可以有效解决训练数据稀缺问题，在低资源环境下改善泛化能力。这一框架为实现数据使用的可扩展监控提供了一条途径，增强了透明度，并支持研究人员、资助者和政策制定者识别数据缺口，加强数据可访问性以促进明智决策。|
|**2025-02-14**|**VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models**|Gokul Karthik Kumar et.al.|[2502.10250](http://arxiv.org/abs/2502.10250)|null|视觉语言模型（VLMs）在各种视觉基准测试中表现出色，但通常受限于缺乏高质量的视觉微调数据。为了解决这一挑战，我们引入了VisCon-100K，这是一个源自交错图像文本网页文档的新颖数据集。我们的方法将OBELICS数据集中的45K网页文档转换成100K个图像对话样本。我们利用GPT-4V生成与图像相关的标题，并使用OpenChat 3.5模型将这些标题转化为多样化的自由形式和多项选择问答对。将此数据集用于微调显著提升了VLM在多个基准测试中的性能。与仅关注细粒度视觉内容的方法不同，我们的方法利用了伴随的网页上下文，从而获得更优的结果。我们还发现，所谓的“泄露模态混合”，即对话样本包含可以从图像及其上下文标题中回答的问题，比非泄露的标题和问答对组合表现更好。VisCon-100K数据集在两种流行的VLM方法中显示出强大的性能：一种是仅使用文本的大语言模型（LLM）与使用图像标题数据的视觉编码器对齐（ShareGPT4V-7b），另一种是使用交错图像文本数据进行多模态预训练的LLM（IDEFICS2-8b）。除了发布VisCon-100K数据集外，我们还提供了一个在此数据集上训练的上下文标题生成器，便于未来研究和开源应用的可扩展微调数据生成。使用相同的流程，但用我们训练的上下文标题生成器替代GPT-4V，我们还发布了更大的VisCon-1M数据集。|
|**2025-02-13**|**MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency**|Dongzhi Jiang et.al.|[2502.09621](http://arxiv.org/abs/2502.09621)|null|回答问题时使用链式思维（CoT）显著提升了大型语言模型（LLMs）的推理能力，然而其对大型多模态模型（LMMs）的影响仍缺乏系统的评估和深入的研究。在本文中，我们引入了MME-CoT，这是一个专门用于评估LMMs的CoT推理性能的基准测试，涵盖了数学、科学、OCR、逻辑、时空以及一般场景六个领域。作为该领域的首次全面研究，我们提出了一套综合评估体系，包括三个新的指标，以细粒度地评估推理质量、鲁棒性和效率。利用精心策划的高质量数据和独特的评估策略，我们对最先进的LMMs进行了深入分析，揭示了几个关键见解：1）具有反思机制的模型表现出更优秀的CoT质量，Kimi k1.5的表现优于GPT-4o，并展示了最高的质量结果；2）CoT提示经常导致LMM在感知密集型任务上的性能下降，表明可能存在有害的过度思考行为；3）尽管CoT质量很高，但具有反思功能的LMMs在正常响应和自我修正阶段都表现出显著的低效性。我们希望MME-CoT能成为推进LMMs多模态推理发展的基础。项目页面：<https://mmecot.github.io/>|
|**2025-02-13**|**Exploring the Potential of Encoder-free Architectures in 3D LMMs**|Yiwen Tang et.al.|[2502.09620](http://arxiv.org/abs/2502.09620)|**[link](https://github.com/ivan-tang-3d/enel)**|编码器自由架构在二维视觉领域已初步探索，但在三维理解场景中的应用仍是一个开放性问题。本文首次全面探讨了编码器自由架构在克服三维大型多模态模型（LMM）的挑战方面的潜力。这些挑战包括无法适应变化的点云分辨率以及编码器提供的点特征未能满足大型语言模型（LLM）的语义需求。我们确定了三维LMM去除编码器并使LLM承担三维编码器角色的关键方面：1）我们在预训练阶段提出了LLM嵌入的语义编码策略，探讨了各种点云自监督损失的效果，并提出了混合语义损失以提取高级语义。2）我们在指令调优阶段引入了分层几何聚合策略，这在LLM早期层中引入了归纳偏差，使其关注点云的局部细节。最终，我们提出了首个编码器自由的三维LMM，名为ENEL。我们的7B模型在分类、描述和VQA任务上分别达到了55.0%、50.92%和42.7%的准确率，与目前最先进的模型ShapeLLM-13B相媲美。结果表明，编码器自由架构在三维理解领域替代编码器架构具有巨大潜力。代码已发布于https://github.com/Ivan-Tang-3D/ENEL|
|**2025-02-13**|**Human-LLM Coevolution: Evidence from Academic Writing**|Mingmeng Geng et.al.|[2502.09606](http://arxiv.org/abs/2502.09606)|null|通过分析arXiv论文摘要的统计结果，我们发现一些先前被指出为ChatGPT滥用词汇的使用频率明显下降，例如“delve”，这一现象始于2024年初。与此同时，某些其他被ChatGPT青睐的词汇（如“significant”）的使用频率却持续上升。这些现象表明，学术论文的一些作者已经调整了他们对大型语言模型（LLMs）的使用方式，比如选择输出或修改LLM生成的内容。因此，人类与LLMs之间的这种共同进化和合作给检测现实场景中的机器生成文本带来了新的挑战。通过检查词频来估计LLMs对学术写作的影响仍然是可行的，应该更加关注那些已经被频繁使用的词汇，包括那些使用频率降低的词汇。|
|**2025-02-13**|**SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models**|Yung-Sung Chuang et.al.|[2502.09604](http://arxiv.org/abs/2502.09604)|**[link](https://github.com/voidism/selfcite)**|我们介绍了SelfCite，这是一种新颖的自监督方法，旨在使大型语言模型（LLM）生成高质量、细粒度的句子级引用。与仅依赖昂贵且劳动密集型的标注不同，SelfCite通过上下文消减提供的奖励信号来实现这一点：如果需要引用，从上下文中移除被引用的文本应会阻止相同的回复；如果足够，则保留被引用的文本本身应能保持相同的回复。这种奖励可以指导推理时间的最佳-N采样策略，从而显著提高引用质量，也可以用于偏好优化以直接微调模型以生成更好的引用。SelfCite的有效性通过在LongBench-Cite基准上提升多达5.3个百分点的引用F1分数得以证明，在五个长篇问答任务中均有所体现。|
|**2025-02-13**|**Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs**|Siyan Zhao et.al.|[2502.09597](http://arxiv.org/abs/2502.09597)|**[link](https://github.com/amazon-science/PrefEval)**|大型语言模型（LLMs）越来越多地被用作聊天机器人，但它们根据用户偏好个性化回应的能力仍然有限。我们介绍了PrefEval，这是一个基准测试，用于评估LLMs在长上下文对话环境中推断、记忆和遵循用户偏好的能力。PrefEval包含3000个人工精心策划的用户偏好和查询对，涵盖20个主题。PrefEval中的用户个性化或偏好信息以显性和隐性形式存在，并通过生成任务和分类任务来评估LLM性能。使用PrefEval，我们在具有不同上下文长度（最多10万个标记）的多会话对话中评估了上述偏好跟随能力的10个开源和专有LLM。我们使用各种提示方法、迭代反馈和检索增强生成方法进行基准测试。我们的基准测试工作揭示，最先进的LLMs在对话过程中主动遵循用户偏好的能力面临重大挑战。特别是，在零样本设置下，偏好跟随准确性在仅仅10个轮次（约3000个标记）后就下降到10%以下，大多数评估的模型都是如此。即使采用高级提示和检索方法，偏好跟随在长上下文对话中仍然会恶化。此外，我们表明在PrefEval上微调可以显著提高性能。我们认为PrefEval是一个有价值的资源，用于衡量、理解和提升LLMs的偏好跟随能力，为个性化对话代理铺平道路。我们的代码和数据集可在https://prefeval.github.io/获取。|
|**2025-02-13**|**KIMAs: A Configurable Knowledge Integrated Multi-Agent System**|Zitao Li et.al.|[2502.09596](http://arxiv.org/abs/2502.09596)|null|知识密集型对话在大型语言模型（LLMs）的支持下已经成为最流行和最有帮助的应用之一，能够从不同方面辅助人们。许多当前的知识密集型应用都集中在检索增强生成（RAG）技术上。尽管许多开源的RAG框架促进了基于RAG的应用开发，但它们往往难以处理由主题和格式各异的数据、对话上下文管理以及低延迟响应时间要求所导致的实际场景中的复杂性。本技术报告介绍了一个可配置的知识集成多代理系统KIMAs，以解决这些挑战。KIMAs具有一个灵活且可配置的系统，用于整合各种知识源，并具备1）上下文管理和查询重写机制以提高检索准确性和多轮对话连贯性，2）高效的知识路由和检索，3）简单但有效的过滤和参考生成机制，以及4）优化的并行多代理管道执行。我们的工作提供了一个可扩展的框架，以推进LLMs在现实世界中的部署。为了展示KIMAs如何帮助开发者构建具有不同规模和重点的知识密集型应用，我们演示了如何将该系统配置为三个已经在实践中运行的应用程序，并保持了可靠的性能。|
|**2025-02-13**|**Logical forms complement probability in understanding language model (and human) performance**|Yixuan Wang et.al.|[2502.09589](http://arxiv.org/abs/2502.09589)|null|随着人们越来越有兴趣使用大规模语言模型（LLMs）进行自然语言规划，理解它们的行为成为了一个重要的研究问题。这项工作系统地调查了LLMs执行自然语言逻辑推理的能力。我们引入了一个控制数据集，其中包含命题逻辑和模态逻辑中的假设性和析取性三段论，并将其作为理解LLM性能的测试平台。我们的结果导致了预测LLM行为的新见解：除了输入的概率（Gonen等人，2023；McCoy等人，2024），还应考虑逻辑形式作为正交因素。此外，通过比较LLM和人类行为结果，我们展示了LLM和人类在逻辑推理表现上的相似性和差异性。|
|**2025-02-13**|**Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks**|Qian Wan et.al.|[2502.09577](http://arxiv.org/abs/2502.09577)|null|预写是生成和组织想法以供初稿使用的过程，它结合了非正式、迭代和半结构化的策略，例如视觉图解法，这对与大型语言模型（LLMs）进行轮流的对话式协作提出了挑战。我们介绍了Polymind，这是一款利用多个LLM驱动代理支持预写的视觉图解工具。该系统采用并行协作工作流而非轮流的对话交互。它定义了多个“微任务”来模拟诸如协作写作和团队头脑风暴等协作场景。Polymind使用户能够同时安排多个微任务，而不仅仅是反复提示聊天机器人。用户可以配置和委派定制的微任务，并通过指定任务要求和切换可见性和主动性来管理这些微任务。我们的评估表明，与ChatGPT相比，用户对Polymind的协作具有更多的自定义选项，因此能够在预写过程中快速扩展个性化写作思路。|
|**2025-02-13**|**Zero-shot generation of synthetic neurosurgical data with large language models**|Austin A. Barr et.al.|[2502.09566](http://arxiv.org/abs/2502.09566)|**[link](https://github.com/aabarr/Synthetic-Neurosurgical-Data)**|临床数据对于推进神经外科研究至关重要，但访问这些数据常常受到数据可用性、样本量小、隐私法规以及资源密集型的预处理和去标识程序的限制。合成数据为解决与访问和使用真实世界数据（RWD）相关的问题提供了潜在的解决方案。本研究旨在评估大型语言模型（LLM）GPT-4o在零样本生成神经外科数据方面的能力，并通过条件表格生成对抗网络（CTGAN）进行基准测试。通过比较合成数据集与真实世界神经外科数据来评估保真度（均值、比例、分布和双变量相关性）、效用（机器学习分类器在RWD上的性能）和隐私性（复制RWD中的记录）。尽管没有进行微调或未使用RWD进行预训练，GPT-4o生成的数据集在性能上匹配甚至超过了CTGAN。数据集在单变量和双变量保真度上与RWD高度一致，且不会直接暴露任何真实的患者记录，即使是在放大样本量的情况下。在二元预测任务中，使用GPT-4o生成的数据训练机器学习分类器并在RWD上进行测试，其F1分数为0.706，与在CTGAN数据上训练的分类器性能（0.705）相当，用于预测术后功能状态恶化。GPT-4o展示了生成高保真合成神经外科数据的潜力。这些发现还表明，用GPT-4o合成的数据可以有效扩充样本量较小的临床数据，并训练机器学习模型以预测神经外科结果。有必要进一步研究以改进分布特征的保持并提高分类器性能。|
|**2025-02-13**|**MDCrow: Automating Molecular Dynamics Workflows with Large Language Models**|Quintina Campbell et.al.|[2502.09565](http://arxiv.org/abs/2502.09565)|**[link](https://github.com/ur-whitelab/MDCrow)**|分子动力学（MD）模拟对于理解生物分子系统至关重要，但自动化仍然具有挑战性。最近大型语言模型（LLM）在使用基于LLM的代理自动化复杂科学任务方面取得了成功。在本文中，我们介绍了MDCrow，这是一种能够自动化MD工作流程的代理LLM助手。MDCrow通过超过40个专家设计的工具来处理和处理文件、设置模拟、分析模拟输出，并从文献和数据库中检索相关信息。我们在25个不同子任务和难度的任务上评估了MDCrow的表现，并评估了该代理对难度和提示风格的鲁棒性。\texttt{gpt-4o}能够在低方差的情况下完成复杂任务，紧随其后的是\texttt{llama3-405b}，这是一个有说服力的开源模型。虽然最佳模型的性能不受提示风格的影响，但它对较小的模型有显著影响。|
|**2025-02-12**|**Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples**|Andrianos Michail et.al.|[2502.08638](http://arxiv.org/abs/2502.08638)|null|为了允许领域特定的评估，我们引入了跨语言语义区分（CLSD）任务，这是一个新的跨语言语义搜索任务，只需要感兴趣语言对在目标领域的平行句子对即可。此任务侧重于模型在跨语言环境中将真实平行句子排名高于由大规模语言模型生成的困难负样本的能力。我们在新闻领域内为德法语言对创建了四个CLSD任务实例。通过这项案例研究，我们发现针对检索任务进行微调的模型（例如多语言E5）在使用英语作为枢轴语言时受益，而基于双语挖掘的模型如LaBSE则在直接跨语言时表现最佳。我们还展示了通过我们的干扰项生成策略实现的细粒度相似性分析，表明不同的嵌入模型对不同类型的扰动敏感。|
|**2025-02-12**|**Ensemble based approach to quantifying uncertainty of LLM based classifications**|Srijith Rajamohan et.al.|[2502.08631](http://arxiv.org/abs/2502.08631)|null|大型语言模型（LLMs）的输出是内部模型参数和提供给上下文窗口的输入的函数。这里提出的假设是，在贪婪采样策略下，LLM的输出方差是嵌入在模型参数知识中的概念确定性和输入中的词汇变化性的函数。微调模型会导致对词汇输入变化的敏感性降低。然后将其应用于分类问题，并提出了一种估计预测类别确定性的概率方法。|
|**2025-02-12**|**Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks**|Ang Li et.al.|[2502.08586](http://arxiv.org/abs/2502.08586)|null|近年来，许多机器学习安全文献集中于针对对齐的大语言模型（LLMs）的攻击。这些攻击可能提取私人信息或将模型胁迫成生成有害输出。在实际部署中，LLMs通常作为更大代理管道的一部分，包括记忆系统、检索、网络访问和API调用。这些额外组件引入了漏洞，使这些LLM驱动的代理比孤立的LLMs更容易受到攻击，但与此类代理的安全性相关的研究相对较少。在这篇论文中，我们分析了仅存在于LLM代理中的安全和隐私漏洞。我们首先通过威胁行为者、目标、入口点、攻击者可观测性、攻击策略以及代理管道的基本漏洞来提供攻击分类。然后，我们对流行的开源和商业代理进行了一系列说明性的攻击，展示了其漏洞的直接实际影响。值得注意的是，我们的攻击非常容易实施且不需要对机器学习有任何理解。|
|**2025-02-12**|**QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion in Information Retrieval**|Wonduk Seo et.al.|[2502.08557](http://arxiv.org/abs/2502.08557)|null|查询扩展在信息检索（IR）中被广泛使用，以通过添加额外的上下文信息来改进搜索结果。尽管最近基于大型语言模型（LLM）的方法通过多种提示生成伪相关内容和扩展术语，但它们通常会产生重复且狭窄的扩展，缺乏检索所有相关信息所需的多样化上下文。在本文中，我们介绍了一种新颖有效的查询扩展框架QA-Expand。它首先从初始查询生成多个相关问题，然后生成相应的伪答案作为代理文档。反馈模型进一步重写和过滤这些答案，以确保只将最有信息量的扩充纳入其中。在BEIR和TREC等基准上的大量实验表明，QA-Expand在检索性能上比最先进的方法提高了多达13%，为现代检索挑战提供了一个强大的解决方案。|
|**2025-02-12**|**Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies**|Sunnie S. Y. Kim et.al.|[2502.08554](http://arxiv.org/abs/2502.08554)|null|大型语言模型（LLMs）可能会生成听起来流畅且令人信服的错误回答，这增加了用户过度依赖这些回答的风险。减轻这种过度依赖是关键挑战之一。通过一项让参与者使用包含LLM的应用程序来回答客观问题的思考 aloud 研究，我们确定了影响用户依赖程度的几个LLM回答特征：解释（答案的支持细节）、解释中的不一致性和来源。通过一项大规模的、预先注册的、受控实验（N=308），我们隔离并研究了这些特征对用户依赖程度、准确性及其他指标的影响。我们发现，解释的存在会增加对正确和错误回答的依赖。然而，当提供来源或解释存在不一致性时，用户对错误回答的依赖较少。我们讨论了这些发现对于促进用户对LLMs适当依赖的含义。|
|**2025-02-12**|**LLMs can implicitly learn from mistakes in-context**|Lisa Alazraki et.al.|[2502.08550](http://arxiv.org/abs/2502.08550)|null|从错误中学习是人类智能的一个基本特征。先前的研究表明，大规模语言模型（LLMs）在获得详尽的解释以说明答案为何错误或如何纠正时，也能从中学习。在这项工作中，我们研究了当这些解释没有提供时，LLMs是否能够从数学推理任务中的错误中学习。我们调查了LLMs是否能够仅通过观察错误和正确答案就隐式推断出这样的解释。令人惊讶的是，我们发现当解释从上下文中被消除，仅展示错误答案和正确答案时，LLMs的表现更好。这种方法也在我们的评估中明显优于思维链提示。我们展示了这些结果在不同大小和不同推理能力的LLMs中是一致的。进一步地，我们进行了深入分析，表明用错误答案和正确答案进行提示比引入更多样化的额外问题-答案对到上下文中更能提高性能和更好地泛化。最后，我们展示了由仅观察错误和正确答案的模型生成的新解释与那些借助示例解释生成的解释一样得到了人类的高度评价。我们的结果证明LLMs确实能够在上下文中进行隐式学习。|
|**2025-02-12**|**LLM Pretraining with Continuous Concepts**|Jihoon Tack et.al.|[2502.08524](http://arxiv.org/abs/2502.08524)|null|连续概念混合（CoCoMix）是一种新颖的预训练框架，它结合了离散的下一个token预测与连续的概念。具体来说，CoCoMix从一个预训练的稀疏自动编码器中预测连续概念，并通过与token隐藏表示交织的方式将其混入模型的隐藏状态。通过在多个基准测试中的实验，包括语言建模和下游推理任务，我们表明CoCoMix比标准的下一个token预测、知识蒸馏和插入暂停token更具样本效率并且始终表现更优。我们发现，在端到端框架中结合概念学习和交织是性能提升的关键。此外，CoCoMix通过允许直接检查和修改预测的概念来增强可解释性和可控性，提供了一种透明的方式来指导模型的内部推理过程。|
|**2025-02-12**|**The Paradox of Stochasticity: Limited Creativity and Computational Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data**|Evgenii Evstafev et.al.|[2502.08515](http://arxiv.org/abs/2502.08515)|null|本研究考察了温度设置和模型架构对三个大型语言模型（LLM）生成结构化虚构数据（名称、出生日期）的影响。这三个模型分别是llama3.1:8b、deepseek-r1:8b和mistral:latest。通过系统地测试从0.0到1.0的温度值，增量为0.1，我们进行了330次试验，产生了889个结构化的实体，并验证了其语法一致性。关键发现表明，模型架构显著影响计算效率，mistral:latest和llama3.1:8b的处理速度是deepseek-r1:8b的8倍。出乎意料的是，温度与处理时间之间没有相关性，这挑战了关于随机抽样成本的假设。输出多样性仍然有限，因为模型在所有温度下始终倾向于默认生成常见的姓名原型（例如，“John Doe”和“Jane Smith”），尽管罕见的姓名在中间值（0.3-0.7）处聚集。这些结果表明，在结构化生成任务中，架构优化比温度调整更为重要。研究结果强调了优先考虑模型选择而非超参数调整以提高效率的重要性，并建议需要明确的多样性约束来减轻合成数据管道中的默认输出偏差。|
|**2025-02-12**|**Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation**|Mahnaz Koupaee et.al.|[2502.08514](http://arxiv.org/abs/2502.08514)|**[link](https://github.com/amazon-science/madisse)**|基于大型语言模型（LLM）的忠实性评估器往往被文本的流畅性所迷惑，并且难以识别总结中的错误。我们提出了一种在摘要忠实性评估中的方法，其中多个基于LLM的代理被分配初始立场（无论它们的实际信念如何），并被迫提出理由来证明强加的信念，从而进行多轮辩论以达成一致。均匀分布的初始立场导致了更大的立场多样性，从而使得辩论更有意义并且最终能够识别出更多错误。此外，通过分析最近的忠实性评估数据集，我们观察到通常情况下，摘要并不总是要么忠实于源文档要么不忠实。因此，我们引入了一个新的维度——模糊性，并提出了一个详细的分类法来识别这些特殊情况。实验表明，我们的方法可以帮助识别模糊性，并且在非模糊性摘要上表现更佳。|
|**2025-02-12**|**Measuring Diversity in Synthetic Datasets**|Yuchang Zhu et.al.|[2502.08512](http://arxiv.org/abs/2502.08512)|**[link](https://github.com/bluewhalelab/dcscore)**|大型语言模型（LLMs）被广泛用于为各种自然语言处理（NLP）任务生成合成数据集，例如文本分类和摘要。然而，准确衡量这些合成数据集的多样性——这一对模型性能稳健性至关重要的方面——仍然是一项重大挑战。在本文中，我们介绍了DCScore，这是一种新颖的方法，从分类的角度来衡量合成数据集的多样性。具体而言，DCScore将多样性评估公式化为样本分类任务，利用样本之间的相互关系。我们进一步提供了对DCScore满足的多样性的公理的理论验证，突显了其作为原则性多样性评估方法的角色。在合成数据集上的实验结果表明，DCScore与评估数据集的多个多样性伪真相具有更强的相关性，证明了其有效性。此外，实证和理论证据均表明DCScore相较于现有方法大大降低了计算成本。代码可在以下链接获取：https://github.com/BlueWhaleLab/DCScore。|
|**2025-02-11**|**DarwinLM: Evolutionary Structured Pruning of Large Language Models**|Shengkun Tang et.al.|[2502.07780](http://arxiv.org/abs/2502.07780)|**[link](https://github.com/IST-DASLab/DarwinLM)**|大型语言模型（LLMs）在各种自然语言处理任务中取得了显著的成功。然而，它们巨大的计算成本限制了其广泛应用，特别是在实时应用中。结构化剪枝提供了一种有效的解决方案，通过压缩模型直接提供端到端的速度提升，而不论硬件环境如何。同时，模型的不同组件对剪枝的敏感性各不相同，这要求进行非均匀的模型压缩。然而，剪枝方法不仅要识别出一个有能力的子结构，还要考虑压缩后的训练。为此，我们提出了一个名为\sysname的方法，这是一种针对训练感知的结构化剪枝。\sysname基于进化搜索过程，每一代生成多个子模型并通过突变，选择最适者生存。为了评估后训练的影响，我们在子模型群体中引入了一个轻量级的多步训练过程，在每个选择阶段逐步增加令牌数量并淘汰表现不佳的模型。我们通过对Llama-2-7B、Llama-3.1-8B和Qwen-2.5-14B-Instruct进行广泛的实验验证了我们的方法，实现了结构化剪枝的最先进性能。例如，\sysname在后压缩训练期间所需的数据量仅为ShearedLlama的五分之一。|
|**2025-02-11**|**Auditing Prompt Caching in Language Model APIs**|Chenchen Gu et.al.|[2502.07776](http://arxiv.org/abs/2502.07776)|**[link](https://github.com/chenchenygu/auditing-prompt-caching)**|提示缓存（prompt caching）在大型语言模型（LLMs）中会导致数据依赖的时序差异：缓存的提示处理速度比非缓存的提示快。这些时间上的不同引入了侧信道时序攻击的风险。例如，如果缓存是跨用户共享的，攻击者可以通过快速的API响应时间来识别缓存的提示，从而了解关于其他用户的提示信息。由于提示缓存可能导致隐私泄露，因此API提供商在缓存策略方面的透明度非常重要。为此，我们开发并进行统计审计以检测现实世界中的LLM API提供商的提示缓存情况。我们发现七家API提供商存在全球范围内的缓存共享，包括OpenAI，这可能导致关于用户提示的潜在隐私泄露。由于提示缓存导致的时间差异还可能泄露有关模型架构的信息。具体而言，我们发现证据表明OpenAI的嵌入模型是一种仅解码器的Transformer，而这一信息之前并未公开。|
|**2025-02-11**|**Automatic Robot Task Planning by Integrating Large Language Model with Genetic Programming**|Azizjon Kobilov et.al.|[2502.07772](http://arxiv.org/abs/2502.07772)|null|准确的任务规划对于控制自主系统（如机器人、无人机和自动驾驶车辆）至关重要。行为树（BTs）是定义控制策略中最突出的框架之一，因其模块化、灵活性和可重用性而受到重视。然而，为机器人系统生成可靠且准确的基于BT的控制策略仍然具有挑战性，并且通常需要领域专业知识。在本文中，我们提出了LLM-GP-BT技术，该技术利用大型语言模型（LLM）和遗传编程（GP）来自动化生成和配置BT。LLM-GP-BT技术处理用人类自然语言表达的机器人任务命令，并以计算高效和用户友好的方式将其转换为准确可靠的基于BT的任务计划。所提出的技术通过仿真实验进行了系统的开发和验证，展示了其简化自主系统任务规划的潜力。|
|**2025-02-11**|**Great Power Brings Great Responsibility: Personalizing Conversational AI for Diverse Problem-Solvers**|Italo Santos et.al.|[2502.07763](http://arxiv.org/abs/2502.07763)|null|新手在加入开源软件（OSS）项目时会遇到许多挑战。大型语言模型（LLM），如ChatGPT，已成为回答问题和提供指导的潜在资源，许多开发者现在转向ChatGPT而不是传统的问答网站如Stack Overflow。然而，LLM可能在呈现信息时带有偏见，这对新手尤其有影响，因为他们的解决问题的方式可能没有得到广泛代表。这引发了关于AI驱动支持对OSS项目新手的可访问性的关键问题。本文档概述了适应不同问题解决风格的AI响应以避免偏袒特定子群体的潜力。我们讨论了基于AI角色的提示工程作为与AI交互的一种策略的可能性。本研究邀请进一步的研究来完善基于AI的工具，以更好地支持对OSS项目的贡献。|
|**2025-02-11**|**Scalable Fingerprinting of Large Language Models**|Anshul Nasery et.al.|[2502.07760](http://arxiv.org/abs/2502.07760)|null|模型指纹识别已成为模型所有者在仅通过API访问的情况下识别其共享模型的强大工具。然而，为了降低错误发现率、防止指纹泄漏以及防御试图规避检测的模型用户联盟，我们认为可扩展性是至关重要的，即能够在一个模型中嵌入更多的指纹。因此，我们将可扩展性视为指纹识别方案的关键要求。我们尝试设计了一个比之前考虑的规模大得多的指纹，并引入了一种新方法，称为Perinucleus采样，以生成可扩展、持久且无害的指纹。我们证明这种方案可以在不降低模型效用的情况下，向一个Llama-3.1-8B模型添加24,576个指纹——比现有方案多两个数量级。我们插入的指纹即使在标准的训练后数据上进行有监督微调后仍然存在。我们进一步讨论了指纹识别的安全风险，并从理论和实验上展示了像我们的这种可扩展的指纹识别方案如何缓解这些风险。|
|**2025-02-11**|**Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension**|Wenbo Gong et.al.|[2502.07752](http://arxiv.org/abs/2502.07752)|null|设计高效优化器以满足低内存需求和快速收敛对于大规模语言模型（LLMs）来说是一个重要且具有挑战性的问题。本文通过结构化Fisher信息矩阵（FIM）近似的视角，朝系统设计此类优化器迈出了一步。我们展示了许多最先进的高效优化器可以被视为在Frobenius范数下具有特定结构假设的FIM近似解决方案。基于这些见解，我们提出了两个实用高效优化器的设计建议，包括通过仔细选择结构假设来平衡通用性和效率，以及通过新颖的低秩扩展框架增强具有通用结构的优化器的内存效率。我们通过推导新的内存高效优化器：行和列缩放SGD（RACS）和自适应低维子空间估计（Alice），展示了如何使用每种设计方法。在LLaMA预训练上的实验（最多10亿参数）验证了其有效性，显示出比现有的内存高效基线和Adam更快更好的收敛性，并且几乎没有额外的内存开销。值得注意的是，Alice相比Adam实现了超过2倍的更快收敛速度，而RACS在10亿参数模型上提供了与SGD类似的内存性能。|
|**2025-02-11**|**WHODUNIT: Evaluation benchmark for culprit detection in mystery stories**|Kshitij Gupta et.al.|[2502.07747](http://arxiv.org/abs/2502.07747)|**[link](https://github.com/kjgpta/WhoDunIt-Evaluation_benchmark_for_culprit_detection_in_mystery_stories)**|我们提出了一组新的数据集WhoDunIt，用于评估大型语言模型（LLM）在叙事背景下的演绎推理能力。该数据集由开放领域的推理小说和短篇故事构建而成，挑战LLMs在阅读并理解故事后识别罪犯。为了评估模型的鲁棒性，我们应用了一系列角色级别的名称增强，包括原始名称、名称交换以及用流行话语中的知名真实或虚构实体进行替换。我们还使用了各种提示风格来研究提示对演绎推理准确性的影响。我们通过多次试验对最先进的模型进行了评估，特别是GPT-4o、GPT-4-turbo和GPT-4o-mini，采用多数响应选择以确保可靠性。结果表明，虽然LLMs在未经修改的文本上表现可靠，但在某些广泛认知的名称替换情况下，准确性会降低。此数据集已公开可获取。|
|**2025-02-11**|**The Economics of Large Language Models: Token Allocation, Fine-Tuning, and Optimal Pricing**|Dirk Bergemann et.al.|[2502.07736](http://arxiv.org/abs/2502.07736)|null|我们开发了一个经济框架来分析大型语言模型（LLM）的最优定价和产品设计。我们的框架涵盖了LLM的几个关键特征：处理输入和输出令牌的可变运营成本；通过微调进行定制的能力；以及在任务需求和错误敏感性方面的高维用户异质性。在我们的模型中，垄断卖家通过一系列产品提供多个版本的LLM。最优定价结构取决于任务之间的令牌分配是否可合同化以及用户是否面临规模约束。具有相似总体价值尺度特性的用户选择类似的微调水平和令牌消耗量。最优机制可以通过两部收费制菜单实现，对更密集使用的用户收取更高的加成费用。我们的结果合理化了行业实践中观察到的做法，例如基于模型定制和使用水平的分级定价。|
|**2025-02-11**|**Economics of Sourcing Human Data**|Sebastin Santy et.al.|[2502.07732](http://arxiv.org/abs/2502.07732)|null|AI领域的进展依赖于人类生成的数据，从标注员市场到更广泛的互联网。然而，大型语言模型的广泛应用现在威胁到了这些平台上人类生成数据的质量和完整性。我们认为这个问题超出了立即过滤AI生成内容的挑战——它揭示了数据收集系统设计中的深层次缺陷。现有的系统往往优先考虑速度、规模和效率，而牺牲了人类内在的动力，导致参与度和数据质量下降。我们建议重新思考数据收集系统，使其与贡献者的内在动机保持一致——而不仅仅是依赖外部激励——这可以在维持贡献者信任和长期参与的同时，帮助大规模地获取高质量数据。|
|**2025-02-11**|**Verifying LLM-Generated Code in the Context of Software Verification with Ada/SPARK**|Marcos Cramer et.al.|[2502.07728](http://arxiv.org/abs/2502.07728)|null|大型语言模型（LLMs）在代码生成方面展示了显著的能力，但生成的代码的正确性不能被内在地信任。本文探讨了使用形式化软件验证，特别是用于Ada的SPARK框架，以确保LLM生成代码的可靠性。我们介绍了Marmaragan，这是一种工具，利用LLM生成现有程序的SPARK注释，从而实现代码的形式化验证。该工具在一个精心策划的SPARK程序集上进行了基准测试，通过有选择地移除注释来测试特定功能。Marmaragan与GPT-4o在基准测试中的表现令人鼓舞，在基准案例中有50.7%的正确注释已被生成。这些结果为未来将LLMs的强大功能与形式化软件验证的可靠性相结合奠定了基础。|
|**2025-02-10**|**Rationalization Models for Text-to-SQL**|Gaetano Rossiello et.al.|[2502.06759](http://arxiv.org/abs/2502.06759)|null|我们介绍了一个框架来生成逐步查询（CoT）推理，以增强文本到SQL模型的微调。这些推理包括中间的SQL语句和解释，作为构建最终SQL查询的增量步骤。该过程从手动注释一小部分示例开始，然后在教师模型中使用这些示例进行迭代、动态的少量知识蒸馏过程。随后，训练一个合理化模型以对验证后的分解查询进行广泛合成的CoT注释，从而实现对文本到SQL数据集的扩展。为了评估这种方法，我们在BIRD数据集上微调了带有和不带有这些推理的小型语言模型。结果表明，逐步查询生成提高了执行准确性，特别是在中等复杂度和高度复杂的查询中，同时也增强了可解释性。|
|**2025-02-10**|**Gradient Multi-Normalization for Stateless and Scalable LLM Training**|Meyer Scetbon et.al.|[2502.06742](http://arxiv.org/abs/2502.06742)|null|训练大型语言模型（LLMs）通常依赖于自适应优化器如Adam（Kingma & Ba，2015），这些优化器存储额外的状态信息以加速收敛，但带来了显著的内存开销。最近的努力，例如SWAN（Ma等，2024）通过消除对优化器状态的需求，实现了与Adam相当的性能，方法是应用一个多步骤的预处理过程来处理瞬时梯度。受SWAN成功的启发，我们引入了一种新的无状态优化器设计框架，该框架根据多种范数对随机梯度进行归一化。为此，我们提出了一种简单的交替方案来强制这些范数下的梯度归一化。我们证明了我们的程序可以产生一个固定点，精度可任意设定，并且SWAN是我们方法的一个特例，提供了对其设计的更深入理解。然而，SWAN计算上昂贵的白化/正交化步骤限制了其在大规模LLMs中的实用性。利用我们有原则的视角，我们开发了一个更高效、可扩展且实用的无状态优化器。我们的算法放宽了SWAN的属性，显著降低了其计算成本，同时保持了其内存效率，使其适用于训练大规模模型。在使用多达10亿参数的LLaMA模型进行预训练的实验中，我们的算法比Adam快3倍，并且具有显著降低的内存需求，优于其他内存高效的基线。|
|**2025-02-10**|**VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data**|Thomas Zeng et.al.|[2502.06737](http://arxiv.org/abs/2502.06737)|null|过程奖励模型（PRMs）已被证明通过增加推理时间计算可以有效提升大型语言模型（LLMs）的数学推理能力，但它们主要是在数学数据上进行训练，其在非数学领域的泛化性尚未得到严格研究。为此，本工作首先展示了当前PRMs在其他领域表现不佳。为解决这一局限性，我们引入了VersaPRM，这是一种多领域PRM，它是在使用我们新颖的数据生成和标注方法生成的合成推理数据上进行训练的。VersaPRM在各种不同领域内实现了持续的性能提升。例如，在MMLU-Pro的法律类别中，VersaPRM通过加权多数投票，实现了7.9%的性能提升，超过了Qwen2.5-Math-PRM的1.3%提升。我们还向社区开源了所有与VersaPRM相关的数据、代码和模型。|
|**2025-02-10**|**Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining**|Daouda Sow et.al.|[2502.06733](http://arxiv.org/abs/2502.06733)|null|在大规模语言模型（LLMs）的预训练过程中，使用庞大且异构的数据集对于实现跨多样化的下游任务的最佳性能至关重要。然而，当前的训练范式对所有样本一视同仁，忽略了在整个训练过程中单个样本的重要性或相关性。现有的重新加权策略主要集中在群体层面的数据重要性上，未能利用细粒度的实例层面信息，并且无法根据训练进程动态适应单个样本的重要性。在这篇论文中，我们介绍了新颖的算法，用于动态、实例层面的数据重新加权，旨在提高LLM预训练的效率和效果。我们的方法根据每个训练样本的损失值在线调整其权重，使模型能够在当前训练阶段动态关注更具有信息量或更重要的样本。特别是，我们的框架允许我们系统地制定重新加权策略，以降低冗余或无信息数据的重要性，我们发现这些策略通常表现最好。此外，我们开发了一个新的理论框架来分析基于损失的重新加权对基于梯度优化收敛的影响，提供了对这些策略如何影响收敛界限的首次正式描述。我们在一系列任务上实证验证了我们的方法，包括从预训练70亿和14亿参数的LLMs到较小规模的语言模型和线性回归问题，证明了我们的基于损失的重新加权方法可以实现更快的收敛和显著改进的性能。|
|**2025-02-10**|**Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling**|Runze Liu et.al.|[2502.06703](http://arxiv.org/abs/2502.06703)|**[link](https://github.com/RyanLiu112/compute-optimal-tts)**|测试时间扩展（TTS）是一种通过在推理阶段增加计算量来提升大规模语言模型（LLMs）性能的重要方法。然而，当前研究并未系统分析策略模型、过程奖励模型（PRMs）以及问题难度如何影响TTS。这种缺乏分析的局面限制了对TTS方法的理解和实际应用。本文专注于两个核心问题：（1）在不同策略模型、PRMs和问题难度水平下，最优的测试时间计算扩展方法是什么？（2）额外计算能在多大程度上提高LLMs在复杂任务上的表现，较小的语言模型能否通过这种方法超越较大的模型？通过在MATH-500和具有挑战性的AIME24任务上进行广泛的实验，我们有以下观察：（1）计算最优的TTS策略高度依赖于所选策略模型、PRM和问题难度的选择。（2）采用我们的计算最优TTS策略，极小的策略模型可以超越较大的模型。例如，在MATH-500上，一个1B的LLM可以超过一个405B的LLM。此外，在MATH-500和AIME24上，0.5B的LLM优于GPT-4o，3B的LLM超越了405B的LLM，而7B的LLM则击败了o1和DeepSeek-R1，同时具有更高的推理效率。这些发现表明适应每个任务和模型特性的特定TTS策略的重要性，并表明TTS是增强LLMs推理能力的一个有前景的方法。|
|**2025-02-10**|**Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations**|Rui Chen et.al.|[2502.06669](http://arxiv.org/abs/2502.06669)|null|显著的改进已经在零样本大型语言模型（LLMs）的能力上观察到。由于它们对输入的高度敏感，研究越来越多地集中在通过直接和简单的提示工程来增强LLMs的表现，而不是复杂的领域适应。研究表明，LLMs表现出情绪智力，并且正面和负面情绪都有可能提升任务表现。然而，先前的交互提示主要集中在单一类型的刺激，忽略了比较不同刺激的效果、考察不同任务难度的影响或探索潜在机制。受社会认知理论中自我效能与任务表现之间正相关关系的启发，本文引入了口头效能刺激（VES）。我们的VES包括三种类型的口头提示：鼓励性、挑衅性和批判性，涵盖了六个方面如帮助性和能力。我们进一步对任务难度进行分类，旨在广泛调查不同的VES如何影响语言模型在不同难度水平上的自我效能和任务成就。实验结果表明，三种类型的VES提升了大多数任务上LLMs的表现，并且对于不同的模型最有效的VES也有所不同。在广泛的实验中，我们获得了一些与心理理论一致的发现，为未来的研究提供了新的见解。|
|**2025-02-10**|**Automatic Evaluation of Healthcare LLMs Beyond Question-Answering**|Anna Arias-Duart et.al.|[2502.06666](http://arxiv.org/abs/2502.06666)|null|当前的大语言模型（LLMs）基准测试通常基于开放式或闭合式问答评估，避免了对人工劳动的需求。闭合式测量评估响应的事实性，但缺乏表现力。开放式捕捉模型生成对话响应的能力，但难以评估其正确性。这两种方法通常被单独或一起使用，尽管它们之间的关系仍然理解不足。这项工作专注于医疗领域，在该领域事实性和对话能力都非常重要。它介绍了一套全面的多轴医疗LLM评估体系，探索开放和闭合基准及指标之间的相关性。研究结果包括当前方法论中的盲点和重叠部分。作为一次更新的 sanity 检查，我们发布了一个新的医疗基准——CareQA——它具有开放和闭合两种变体。最后，我们提出了一种新的度量方法用于开放式评估——宽松困惑度——以缓解已识别的局限性。|
|**2025-02-10**|**EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models**|Xingrun Xing et.al.|[2502.06663](http://arxiv.org/abs/2502.06663)|null|现代大型语言模型（LLMs）通过规模定律在大规模模型中实现了智能的紧急增长。最近，由于对云成本、延迟和隐私的关注日益增加，开发紧凑的边缘语言模型变得迫切。不同于受限于规模定律的直接预训练方法，这项工作提出了剪枝感知预训练，专注于保持更大优化模型的性能。其特点包括：1）数据可扩展性：我们在LLM中引入最小参数组并持续优化结构化剪枝，将如LLM-Pruner和SparseGPT等后训练剪枝方法扩展到预训练阶段。2）架构无关性：使用基于显著性驱动的剪枝自动设计LLM架构，这是首次超过现代预训练中的人类设计LLM的水平。我们揭示了通过扩展LLM压缩边界，它能够实现顶级质量的边缘语言模型，称为EfficientLLM。EfficientLLM在常识基准测试中显著优于传统基线，例如参数量在1亿到10亿之间的MobileLLM、SmolLM、Qwen2.5-0.5B、OLMo-1B和Llama3.2-1B。作为首次尝试，EfficientLLM弥合了传统LLM压缩与直接预训练方法之间的性能差距，我们将完全开源该项目，网址为https://github.com/Xingrun-Xing2/EfficientLLM。|
|**2025-02-10**|**Unbiased Evaluation of Large Language Models from a Causal Perspective**|Meilin Chen et.al.|[2502.06655](http://arxiv.org/abs/2502.06655)|null|基准污染已成为LLM评估社区的一个重要问题。先前的代理作为评估者通过让代理参与问题生成来解决这一问题。尽管这些方法取得了成功，但代理作为评估者方法中的偏见仍未得到充分探索。在本文中，我们提出了评估偏见的理论公式，为设计无偏评估协议提供了宝贵的见解。此外，我们通过在最小的代理作为评估者设置上精心设计的探测任务，识别出两种类型的偏见。为了解决这些问题，我们提出了无偏评估者，这是一种提供更全面、无偏且可解释的LLM评估的评估协议。广泛的实验揭示了当前LLMs存在的巨大改进空间。此外，我们证明无偏评估者不仅提供了强有力的基准污染证据，还提供了可解释的评估结果。|
|**2025-02-10**|**In-Context Learning (and Unlearning) of Length Biases**|Stephanie Schoch et.al.|[2502.06653](http://arxiv.org/abs/2502.06653)|null|大型语言模型已经展示了强大的能力来进行上下文学习，其中输入-输出对被添加到提示中作为示例。然而，现有的工作表明，这些模型能够学习词汇和标签偏差，并且这种偏差会负面影响模型的性能和鲁棒性。其他统计数据偏差的影响仍有待探索，这就是本文旨在解决的问题。我们特别研究了长度偏差对上下文学习的影响。我们证明了模型确实会在其预测的上下文窗口中学习长度偏差，并进一步分析了调节模型表现偏差程度的因素。此外，我们还展示了如何利用上下文中的长度信息来抵消模型中编码的长度偏差（例如，通过微调）。这揭示了上下文学习的力量，即在不需要进行昂贵的参数更新的情况下，可以对模型预测行为进行去偏差。|
|**2025-02-07**|**Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray**|Yunhang Shen et.al.|[2502.05177](http://arxiv.org/abs/2502.05177)|**[link](https://github.com/vita-mllm/long-vita)**|建立大型视觉-语言模型的长上下文能力对于视频理解、高分辨率图像理解、多模态代理和推理至关重要。我们介绍了Long-VITA，这是一种简单而有效的大型多模态模型，用于长上下文视觉-语言理解任务。它擅长同时处理和分析超过4000帧或100万个标记的图像、视频和文本模态，并在短上下文多模态任务中提供先进的性能。我们提出了一种有效的多模态训练方案，该方案从大规模语言模型开始，接着通过视觉-语言对齐、通用知识学习以及两个连续的长序列微调阶段进行。我们进一步实现了上下文并行分布式推理和带logits掩码的语言建模头，以在模型推理期间将Long-VITA扩展到无限长的图像和文本输入。关于训练数据，Long-VITA仅基于来自公共数据集的1700万样本构建，并与使用内部数据的最近尖端模型相比，在各种多模态基准测试中展示了最先进的性能。Long-VITA是完全可重现的，并支持用于训练和测试的NPU和GPU平台。我们希望Long-VITA可以作为竞争性的基线，并为开源社区在推进长上下文多模态理解方面提供有价值的见解。|
|**2025-02-07**|**NoLiMa: Long-Context Evaluation Beyond Literal Matching**|Ali Modarressi et.al.|[2502.05167](http://arxiv.org/abs/2502.05167)|**[link](https://github.com/adobe-research/NoLiMa)**|近期的大语言模型（LLMs）支持长达128K到1M个token的长上下文。一种流行的评估方法是针 haystack 测试（NIAH），该测试涉及从长而无关的上下文中检索“针”（相关信息）。这种方法的扩展包括增加干扰项、事实链和上下文推理。然而，在这些基准测试中，模型可以通过在针和干草堆之间利用现有的字面匹配来简化任务。为了解决这个问题，我们引入了NoLiMa，这是一个扩展的NIAH基准测试，其中精心设计了针集，使得问题和针之间的词汇重叠最小化，要求模型推断潜在关联以在干草堆中定位针。我们评估了12种流行的LLMs，它们声称至少支持128K个token的上下文。尽管它们在短上下文（<1K）下表现良好，但随着上下文长度的增加，性能显著下降。例如，在32K的情况下，有10个模型的表现低于其短长度基线的50%。即使是表现最好的GPT-4o也从接近完美的基线99.3%下降到69.7%。我们的分析表明，这些下降主要是由于在缺少字面匹配时，注意力机制在较长上下文中检索相关信息变得更加困难所导致的。|
|**2025-02-07**|**DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails**|Yihe Deng et.al.|[2502.05163](http://arxiv.org/abs/2502.05163)|**[link](https://github.com/yihedeng9/duoguard)**|大型语言模型（LLMs）的快速发展增加了对护栏模型的需求，以确保其负责任的使用，特别是在检测不安全和非法内容方面。虽然有大量的英文安全数据，但由于缺乏开源的安全数据，多语种护栏建模仍探索不足。为了解决这一差距，我们提出了一种新的双玩家强化学习（RL）框架，在该框架中，生成器和护栏模型在对抗性环境中共同进化，以生成高质量的合成数据用于多语种护栏训练。我们在理论上将这种交互形式化为一个双玩家游戏，并证明了其收敛到纳什均衡。实验评估表明，我们的模型\ours相比最先进的模型有显著改进，在英语基准测试中比LlamaGuard3（8B）提高了近10%，同时推理速度提高了4.5倍，且模型大小显著减小（0.5B）。我们在多语种安全任务中取得了实质性进展，尤其是在解决收集的真实数据集中低资源语言的不平衡问题上。消融研究强调了合成数据生成在弥合英语和其他语言之间开源数据不平衡中的关键作用。这些发现建立了一个可扩展且高效的合成数据生成方法，为改善多语种护栏模型以增强LLM安全性铺平了道路。代码、模型和数据将在<https://github.com/yihedeng9/DuoGuard>开源。|
|**2025-02-07**|**A Lightweight Method to Disrupt Memorized Sequences in LLM**|Parjanya Prajakta Prashant et.al.|[2502.05159](http://arxiv.org/abs/2502.05159)|null|大型语言模型（LLMs）在许多任务上表现出令人印象深刻的性能，但存在逐字复现受版权保护内容的风险，从而引发法律和伦理问题。尽管方法如差分隐私或神经元编辑可以减少记忆内容，但它们通常需要昂贵的再训练或者直接访问模型权重，并且可能会降低性能。为了解决这些挑战，我们提出了TokenSwap，这是一种轻量级的、事后处理的方法，它用小型辅助模型（例如DistilGPT-2）的概率替换与语法相关的标记的概率。我们在商业级别的模型如Pythia-6.9b和LLaMA-3-8b上进行了广泛的实验，结果表明我们的方法能有效减少已知的记忆生成案例，最多可减少十倍，同时对下游任务几乎没有影响。我们的方法为实际系统用户提供了一个独特且有效的解决方案。|
|**2025-02-07**|**Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation**|Steffen Eger et.al.|[2502.05151](http://arxiv.org/abs/2502.05151)|**[link](https://github.com/nl2g/transformingsciencellms)**|随着大型多模态语言模型的出现，科学正处于人工智能技术变革的门槛。最近，一系列新的AI模型和工具被提出，有望使全球的研究人员和学者更有效地进行研究。这包括研究周期的所有方面，特别是（1）搜索相关文献；（2）生成研究想法并进行实验；生成（3）基于文本和（4）多模态内容（如科学图表和示意图）；以及（5）基于AI的自动同行评审。在本综述中，我们对这些令人兴奋的最新发展进行了深入概述，这些发展有望从根本上改变科学研究过程。我们的综述涵盖了上述五个方面，包括相关的数据集、方法和结果（包括评估）、以及局限性和未来研究的范围。我们特别关注这些工具的缺点和潜在滥用（如伪造科学、剽窃、损害研究诚信）所带来的伦理问题。我们希望本综述不仅将成为该领域新进入者的参考指南，也将成为“AI4Science”领域新的AI基础倡议的催化剂。|
|**2025-02-07**|**CodeSCM: Causal Analysis for Multi-Modal Code Generation**|Mukur Gupta et.al.|[2502.05150](http://arxiv.org/abs/2502.05150)|**[link](https://github.com/nb15/codeSCM-naacl25)**|在本文中，我们提出了CodeSCM，这是一种结构因果模型（SCM），用于使用大型语言模型（LLMs）分析多模态代码生成。通过对此模型进行干预，我们衡量了不同提示模态（如自然语言、代码和输入-输出示例）对模型的因果影响。CodeSCM引入潜在中介变量以分离多模态代码生成提示中的代码和自然语言语义。利用因果中介分析的原则对这些中介变量进行分析，我们量化了表示模型虚假倾向的直接效应。我们发现除了自然语言指令外，输入-输出示例也显著影响代码生成。|
|**2025-02-07**|**An Annotated Reading of 'The Singer of Tales' in the LLM Era**|Kush R. Varshney et.al.|[2502.05148](http://arxiv.org/abs/2502.05148)|null|## context 帕里-洛德口头程式理论是理解口述叙事诗如何被文盲吟游诗人学习、创作和传承的重大突破。在本文中，我们从大型语言模型（LLMs）和生成式人工智能（AI）的角度提供了对该理论机制的注释性解读。我们指出口头创作与LLM生成之间的相似之处和差异，并评论其对社会和AI政策的影响。 ## task 请你将上述论文摘要翻译为中文，不要输出其他任何无关内容，注意输出的内容中不能包含","字符|
|**2025-02-07**|**Refining Integration-by-Parts Reduction of Feynman Integrals with Machine Learning**|Matt von Hippel et.al.|[2502.05121](http://arxiv.org/abs/2502.05121)|null|积分约化在当前理论粒子物理学和引力波物理学中的前沿计算时常构成瓶颈，而这一过程依赖于启发式方法来选择积分约化恒等式，这些恒等式的质量严重影响性能。本文中，我们研究了使用机器学习技术来寻找改进的启发式方法的可能性。我们使用funsearch，一种基于大型语言模型代码生成的遗传规划变体，以探索可能的方法，然后使用强类型遗传编程来锁定有用的解决方案。两种方法不仅重新发现了最近被整合到积分约化求解器中的最先进启发式方法，还在一个例子中对这一最先进的方法略有改进。|
|**2025-02-07**|**Flexible and Efficient Grammar-Constrained Decoding**|Kanghee Park et.al.|[2502.05111](http://arxiv.org/abs/2502.05111)|null|大型语言模型（LLMs）经常被要求生成遵循精确语法规则的结构化输出，例如代码片段或格式化数据。语法约束解码（GCD）可以保证LLM的输出符合指定的上下文无关语法（CFG）的要求，通过屏蔽会导致输出不符合规则的标记。为了确保准确性，GCD算法必须计算给定的LLM子词分词器如何与由给定的上下文无关语法使用的标记对齐，并基于此信息计算标记掩码。高效地完成这一过程具有挑战性，现有的GCD算法需要数十分钟来预处理常见的语法。我们提出了一种新的GCD算法及其实现，该实现比现有方法快17.71倍，能够在离线预处理方面提供更快的速度，同时保持最先进的在线掩码计算效率。|
|**2025-02-07**|**Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs**|Rohit Saxena et.al.|[2502.05092](http://arxiv.org/abs/2502.05092)|null|理解视觉表示中的时间是一项基本的认知技能，但对于多模态大型语言模型（MLLMs）来说仍然是一个挑战。在这项工作中，我们研究了MLLMs在通过模拟时钟和年历解释时间和日期方面的能力。为此，我们整理了一个结构化的数据集，该数据集包含两个子集：1） $\textit{ClockQA}$，其中包含各种类型的时钟样式——标准、黑表盘、无秒针、罗马数字和指针式时钟——以及与时间相关的问题；2）$\textit{CalendarQA}$ ，其中包括年历图像以及从常见的已知日期（例如圣诞节、新年）到计算得出的日期（例如一年中的第100天或第153天）的问题。我们的目标是分析当面对与时间相关的视觉数据时，MLLMs如何执行视觉识别、数值推理和时间推断。我们的评估表明，尽管取得了最近的进步，可靠地理解时间仍然是MLLMs的一个重大挑战。|
|**2025-02-06**|**Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment**|Zuyan Liu et.al.|[2502.04328](http://arxiv.org/abs/2502.04328)|**[link](https://github.com/ola-omni/ola)**|近期大型语言模型尤其是GPT-4之后的进展引发了开发多模态模型的兴趣，这些模型能够理解更多的信息形式。尽管已经出现了一些开源替代方案，但在性能上仍然落后于专门化的单模态模型。本文介绍了一种名为Ola的多模态语言模型，该模型在图像、视频和音频理解方面达到了与专门化模型相竞争的性能。Ola的核心设计在于其渐进式模态对齐策略，该策略逐步扩展了语言模型支持的信息形式。我们的训练流程首先从最独特的模态开始：图像和文本，然后通过连接语言和音频知识的语音数据以及连接所有模态的视频数据逐步扩展模型的能力。这种渐进学习流程使我们能够保持相对较小的跨模态对齐数据集规模，从而使得从现有的视觉语言模型发展出多模态模型变得容易且成本更低。此外，为了实现类似于GPT-4的高级交互体验，我们进一步设计了一个逐句解码方案以实现流式语音生成。广泛的实验表明，Ola在所有模态上都超越了现有的开源多模态LLM，并且在相似大小的情况下，其性能与最先进的专业模型具有高度竞争力。我们希望将Ola作为一个完全开源的多模态理解解决方案，以推动这一新兴领域未来的研究。模型权重、代码和数据已在https://github.com/Ola-Omni/Ola 开源。|
|**2025-02-06**|**Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions**|Yik Siu Chan et.al.|[2502.04322](http://arxiv.org/abs/2502.04322)|**[link](https://github.com/yiksiu-chan/SpeakEasy)**|尽管进行了广泛的安全对齐努力，大型语言模型（LLMs）仍然容易受到越狱攻击，从而引发有害行为。现有研究主要集中在需要技术专业知识的攻击方法上，但有两个关键问题尚未得到充分探讨：（1）越狱响应是否真的有助于普通用户实施有害行动？（2）在更常见、简单的与人类-LLM交互中是否存在安全漏洞？在本文中，我们证明了当LLM的回答既具有可操作性又具有信息性时，最能促进有害行动——这两个属性在多步骤、多语言交互中很容易被诱发。基于这一见解，我们提出了HarmScore，这是一种衡量LLM回答如何有效促进有害行动的越狱指标，并提出了Speak Easy，一种简单的多步骤、多语言攻击框架。值得注意的是，通过将Speak Easy整合到直接请求和越狱基线中，我们在四个安全基准测试中的开源和专有LLMs上观察到攻击成功率平均绝对增加了0.319，HarmScore平均绝对增加了0.426。我们的工作揭示了一个关键但常常被忽视的漏洞：恶意用户可以轻松利用常见的交互模式来达到有害目的。|
|**2025-02-06**|**ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters**|Kamer Ali Yuksel et.al.|[2502.04315](http://arxiv.org/abs/2502.04315)|**[link](https://github.com/kayuksel/ChamaleonLLM)**|近期大型语言模型（LLMs）在各种任务上展示了显著的性能。然而，这些模型通常以固定权重部署，限制了它们在推理过程中动态适应现实世界数据固有变化的能力。本文介绍了一种名为ChamaleonLLM的新框架，该框架通过利用批次感知聚类和即时生成低秩更新，实现了LLMs在推理时的自适应调整。与传统的微调方法如低秩适应（LoRA）或依赖于预先学习的固定集合的方法不同，我们的方法基于聚类批次的聚合统计信息，在解码器权重上动态生成自适应修改。通过智能地分组相似输入并通过超网络计算上下文感知的低秩更新，ChamaleonLLM实现了显著的性能提升，优于传统的LoRA方法，同时消除了维护多个专家模型的开销。我们的实验突显了该方法作为语言模型推理的一种多功能且高度自适应解决方案的潜力。ChamaleonLLM已开源以确保实验的可重复性：https://anonymous.4open.science/r/ChamaleonLLM/|
|**2025-02-06**|**ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization**|Yinjie Wang et.al.|[2502.04306](http://arxiv.org/abs/2502.04306)|**[link](https://github.com/gen-verse/scoreflow)**|近期的研究利用大规模语言模型多智能体系统解决复杂问题，同时试图减少构建它们所需的手动工作，推动了自动化代理工作流程优化方法的发展。然而，现有方法由于表示能力有限、缺乏适应性和在依赖离散优化技术时的可扩展性差而显得不够灵活。我们通过ScoreFlow解决了这些挑战，ScoreFlow是一个简单但高性能的框架，它利用连续空间中的高效梯度优化。ScoreFlow引入了Score-DPO，这是一种直接偏好优化方法的新变体，考虑到了定量反馈。在涵盖问答、编码和数学推理的六个基准测试中，ScoreFlow比现有基线提高了8.2%。此外，它使较小的模型能够以更低的推理成本超越较大的模型。项目：https://github.com/Gen-Verse/ScoreFlow|
|**2025-02-06**|**Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization**|Yuanye Liu et.al.|[2502.04295](http://arxiv.org/abs/2502.04295)|**[link](https://github.com/henrylau7/cfpo)**|大型语言模型（LLMs）在各种任务中展示了显著的能力，其实际效果往往取决于提示的设计。尽管最近的研究集中在优化提示内容上，但提示格式这一关键但常被忽视的维度却得到了有限的系统性研究。在这篇论文中，我们介绍了内容格式集成提示优化（CFPO），这是一种创新的方法，通过迭代细化过程共同优化提示的内容和格式。CFPO利用自然语言变异来探索内容变化，并采用动态格式探索策略系统地评估不同的格式选项。我们在多个任务和开源LLMs上的广泛评估表明，CFPO相对于仅优化内容的方法表现出可衡量的性能提升。这突显了集成内容格式优化的重要性，并提供了一种实用且与模型无关的方法来增强LLM性能。代码将在<https://github.com/HenryLau7/CFPO>获取。|
|**2025-02-06**|**PILAF: Optimal Human Preference Sampling for Reward Modeling**|Yunzhen Feng et.al.|[2502.04270](http://arxiv.org/abs/2502.04270)|null|随着大型语言模型越来越多地驱动现实世界的应用，将其与人类价值观对齐变得至关重要。基于人类反馈的强化学习（RLHF）已成为关键技术之一，它将偏好数据转化为奖励模型，即使人类价值观的准确表达不可达。实际上，RLHF大多依赖于近似的奖励模型，这些模型可能无法始终如一地引导策略以最大化潜在的人类价值观。我们提出了政策插值学习以实现对齐反馈（PILAF），这是一种新颖的响应采样策略，用于偏好标注，它明确地将偏好学习与最大化潜在的oracle奖励对齐。从优化和统计的角度来看，PILAF在理论上是成立的，并展示了最优性。该方法易于实施，并在迭代和在线RLHF设置中表现出色，在这些设置中反馈整理至关重要。|
|**2025-02-06**|**MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion**|Xintong Hao et.al.|[2502.04235](http://arxiv.org/abs/2502.04235)|null|尽管大型语言模型在各种任务中表现出显著的能力，但它们的持续扩展面临着一个关键挑战：高质量预训练数据的稀缺。虽然模型架构不断发展，自然语言数据却难以实现大规模扩展。为了解决这一瓶颈，我们提出了MASSIVE GENRE-AUDIENCE（MAGA）重构方法，该方法系统地从现有语料库中合成多样化、上下文丰富的预训练数据。这项工作主要做出了三个贡献：（1）我们提出了MAGA重构方法，这是一种轻量且可扩展的预训练语料库扩展方法，并构建了一个包含7700亿个标记的MAGACorpus。（2）我们使用不同的数据预算缩放策略评估了MAGACorpus，在各种模型大小（1.34亿至130亿参数）下均显示出一致的改进，确立了下一代大规模合成预训练语言模型的必要性。（3）通过全面分析，我们研究了提示工程对合成训练崩溃的影响，并揭示了使用验证损失的传统崩溃检测指标的局限性。我们的研究表明，MAGA可以在保持质量的同时大幅扩展训练数据集，为突破数据限制的模型扩展提供了一条可靠的途径。|
|**2025-02-06**|**Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks**|Andreas Happe et.al.|[2502.04227](http://arxiv.org/abs/2502.04227)|null|我们探讨了使用大型语言模型（LLM）驱动的自主系统进行企业网络渗透测试的可行性与有效性。我们介绍了一种新颖的原型系统，该系统由大型语言模型驱动，能够在真实的Active Directory测试环境中攻破账户。我们的研究对原型系统的功能进行了全面评估，并在执行攻击时突出了其优势和局限性。评估使用了一个现实的模拟环境（游戏化Active Directory，GOAD），以捕捉复杂交互、随机结果和时间依赖性，这些特性在实际网络场景中很常见。研究表明，自主的LLM能够进行假设违规仿真，这可能使预算有限的组织更容易获得渗透测试。该原型系统的源代码、跟踪记录及分析日志作为开源发布，以增强集体网络安全并促进未来在LLM驱动的网络安全自动化领域的研究。|
|**2025-02-06**|**Keep It Light! Simplifying Image Clustering Via Text-Free Adapters**|Yicen Li et.al.|[2502.04226](http://arxiv.org/abs/2502.04226)|null|许多竞争性的聚类管道具有多模态设计，利用大型语言模型（LLMs）或其他文本编码器以及文本-图像对，而这些在现实世界的实际应用场景中通常是不可用的。此外，这类框架通常难以训练，并且需要大量的计算资源，这使得广泛应用面临挑战。在这项工作中，我们展示了在深度聚类中，可以使用一种不依赖文本且高度简化的训练流程实现与更复杂的最先进方法相媲美的性能。特别是，我们的方法Simple Clustering via Pre-trained models (SCP)，仅训练一个小型聚类头，同时利用预训练视觉模型的特征表示和正样本数据对。在包括CIFAR-10、CIFAR-20、CIFAR-100、STL-10、ImageNet-10和ImageNet-Dogs在内的基准数据集上的实验表明，SCP实现了高度竞争力的表现。此外，我们提供了一个理论结果，解释了为什么至少在理想条件下，不需要额外的基于文本的嵌入也能在视觉聚类中获得强大的性能。|
|**2025-02-06**|**Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents**|Ilia Karmanov et.al.|[2502.04223](http://arxiv.org/abs/2502.04223)|null|光学字符识别（OCR）技术广泛用于从文档图像中提取文本，从而实现高效的数字化和数据检索。然而，在处理复杂文档时，仅仅提取文本是不够的。全面理解这些文档需要了解其结构——包括格式、公式、表格以及多块和多列的阅读顺序，还包括语义信息以检测元素如脚注和图片说明。这种全面理解对于下游任务如检索、文档问答以及为训练大型语言模型（LLMs）和视觉语言模型（VLMs）进行的数据整理至关重要。为了解决这个问题，我们介绍了'Eclair，一种专门设计用于处理多种文档类型的通用文本提取工具。给定一个图像，'Eclair能够按照阅读顺序提取格式化的文本，同时提供边界框及其对应的语义类别。为了彻底评估这些新功能，我们引入了一个多样化的由人工标注的基准，用于文档级别的OCR和语义分类。'Eclair在这个基准上达到了最先进的准确率，在关键指标上超越了其他方法。此外，我们在已建立的基准上评估了'Eclair，展示了它在多个评估标准上的多样性和实力。|
|**2025-02-05**|**Do Large Language Model Benchmarks Test Reliability?**|Joshua Vendrow et.al.|[2502.03461](http://arxiv.org/abs/2502.03461)|**[link](https://github.com/MadryLab/platinum-benchmarks)**|在部署大型语言模型（LLMs）时，确保这些模型不仅具备能力而且可靠是非常重要的。许多基准测试已经被创建来跟踪LLMs不断增长的能力，然而在衡量其可靠性方面还没有类似的关注。为了理解这一差距所带来的潜在影响，我们研究了当前的基准测试在多大程度上量化了模型的可靠性。我们发现普遍存在的标签错误可能会损害这些评估，掩盖模型持续存在的失败并隐藏不可靠的行为。受此评估可靠性的差距所激励，我们提出了所谓的铂金基准的概念，即经过精心策划以最大限度地减少标签错误和歧义的基准。作为构建此类基准的第一步尝试，我们修订了十五个现有流行基准中的示例。我们在这些铂金基准上评估了一系列模型，结果发现前沿的LLMs在简单的任务如小学水平的数学应用题上仍然表现出失败。进一步分析这些失败揭示了之前未被识别的问题模式，前沿模型在这些问题上始终存在困难。我们在https://github.com/MadryLab/platinum-benchmarks提供了代码。|
|**2025-02-05**|**Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training**|Boyao Wang et.al.|[2502.03460](http://arxiv.org/abs/2502.03460)|null|小型语言模型（SLMs）因其在边缘设备上的广泛应用而受到学术界和工业界的广泛关注。为了获得性能强大的SLMs，传统方法要么从头开始预训练模型，这需要大量的计算资源，要么压缩/剪枝现有的大型语言模型（LLMs），但这会导致性能下降，并且无法与从头预训练相比。在本文中，我们研究了涉及结构化剪枝和模型训练的加速方法族。我们发现1）分层自适应剪枝（Adapt-Pruner）在LLMs中极为有效，显著优于现有的剪枝技术，2）配备进一步训练的自适应剪枝可以生成与从头预训练相当的模型，3）增量剪枝通过在剪枝和训练之间交替进行，每次仅移除一小部分神经元（约5%）带来了非平凡的性能提升。在LLaMA-3.1-8B上的实验结果表明，Adapt-Pruner在常识基准测试中的准确率比传统的剪枝方法如LLM-Pruner、FLAP和SliceGPT平均高出1%-7%。此外，Adapt-Pruner通过从更大的模型中剪枝恢复了MobileLLM-125M到600M在MMLU基准测试中的性能，且使用的token数量减少了200倍，并发现了一个新的1B模型，在多个基准测试中超越了LLaMA-3.2-1B。|
|**2025-02-05**|**A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene Graphs with Large-Language-Models (LLMs)**|Yiye Chen et.al.|[2502.03450](http://arxiv.org/abs/2502.03450)|null|场景图已成为大型语言模型（LLMs）进行 grounded 空间推理的结构化和可序列化环境表示。在这项工作中，我们提出了 SG-RwR，这是一种用于场景图推理和规划的Schema-Guided Retrieve-while-Reason框架。我们的方法采用了两个合作的、编写代码的LLM代理：（1）推理者负责任务规划和信息查询生成，以及（2）检索者根据查询提取相应的图形信息。这两个代理迭代协作，实现了顺序推理和对图形信息的自适应关注。与之前的工作不同，两个代理仅用场景图模式而不是完整的图形数据提示，这通过限制输入标记减少了幻觉，并促使推理者抽象地生成推理跟踪。根据跟踪，检索者基于模式理解程序化地查询场景图数据，从而实现动态和全局的图形关注，增强了推理和检索之间的对齐。通过在多个模拟环境中的实验，我们展示了我们的框架在数值问答和规划任务中超越了现有的基于LLM的方法，并且可以从任务级别的少量样本示例中受益，即使在没有代理级别演示的情况下也是如此。项目代码将发布。|
|**2025-02-05**|**BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving**|Ran Xin et.al.|[2502.03438](http://arxiv.org/abs/2502.03438)|null|近期大型语言模型（LLMs）的进展激发了使用Lean4进行自动定理证明的兴趣，在这种背景下有效的树搜索方法对于探索证明搜索空间至关重要。虽然现有的方法主要依赖于价值函数和蒙特卡洛树搜索（MCTS），但诸如最佳优先搜索（BFS）等较简单方法的潜力尚未得到充分探索。本文研究了BFS是否能够在大规模定理证明任务中实现具有竞争力的性能。我们介绍了\texttt{BFS-Prover}，一个可扩展的专家迭代框架，具有三个关键创新。首先，我们在每个专家迭代轮次中实施策略性数据过滤，排除可以通过束搜索节点扩展解决的问题，以专注于更难的情况。其次，我们通过直接偏好优化（DPO）改进了BFS的样本效率，该方法应用于自动标注编译器错误反馈的状态-策略对，从而精炼LLM的策略以优先考虑有成效的扩展。第三，我们采用长度归一化在BFS中以鼓励探索更深的证明路径。\texttt{BFS-Prover}在MiniF2F测试集上达到了71.31分的成绩，因此挑战了复杂树搜索方法的必要性，表明当适当扩展时，BFS可以实现具有竞争力的性能。|
|**2025-02-05**|**On Fairness of Unified Multimodal Large Language Model for Image Generation**|Ming Liu et.al.|[2502.03429](http://arxiv.org/abs/2502.03429)|null|统一的多模态大型语言模型（U-MLLMs）在端到端的视觉理解和生成任务中展示了令人印象深刻的性能。与仅生成模型（如Stable Diffusion）相比，U-MLLMs可能引发关于其输出偏差的新问题，这可能受到其统一能力的影响。鉴于有害刻板印象传播的风险尚未得到充分探索，这一差距尤为令人担忧。在本文中，我们对最新的U-MLLMs进行了基准测试，发现大多数模型表现出显著的人口统计偏差，如性别和种族偏差。为了更好地理解和缓解这一问题，我们提出了一种定位再修复策略，通过审计来展示单个模型组件如何受偏差影响。我们的分析表明，偏差主要源自语言模型。更有趣的是，我们在U-MLLMs中观察到一种“部分对齐”现象，其中理解偏差似乎很小，但生成偏差仍然很大。因此，我们提出了一种新的平衡偏好模型来平衡人口分布与合成数据。实验表明，我们的方法在保持语义保真度的同时减少了人口统计偏差。我们希望我们的研究结果强调了未来对U-MLLMs进行更全面解释和去偏差策略的需求。|
|**2025-02-05**|**Harnessing Large Language Models for Curated Code Reviews**|Oussama Ben Sghaier et.al.|[2502.03425](http://arxiv.org/abs/2502.03425)|**[link](https://github.com/OussamaSghaier/CuREV)**|在代码审查过程中，生成结构化且相关的评论对于识别代码问题以及促进准确的代码修改至关重要，以确保高效的代码审查过程。精心编写的评论不仅能够简化代码审查本身，而且对于后续任务（如代码精炼）也非常重要，在这些任务中，代码会被修改以满足代码审查评论的要求。尽管已有多种基于AI的方法试图实现自动注释生成，但其有效性仍然受限于训练数据的质量。现有的代码审查数据集通常存在噪声且未经提炼，这限制了AI模型的学习潜力并阻碍了自动化流程。为了解决这些挑战，我们提出了一种数据整理管道，旨在提升最大公开代码审查数据集的质量。我们首先建立了一个评估框架，纳入特定标准和类别，以便对数据集的初始质量进行实证研究。然后，我们采用大型语言模型（LLM）驱动的方法应用我们的数据整理管道来优化数据集。通过同一评估框架进行的比较分析表明，新整理的数据集在评论的清晰度和简洁性方面有显著改进。此外，我们还评估了整理后的数据集对下游任务（特别是注释生成和代码精炼）的影响。我们的研究结果表明，整理后的数据集有助于提高模型性能，从而生成更准确的评论。整理后的注释更有用，因为它们可以导致更准确的代码精炼。|
|**2025-02-05**|**Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts**|Nikta Gohari Sadr et.al.|[2502.03418](http://arxiv.org/abs/2502.03418)|null|零样本提示技术显著提升了大语言模型（LLMs）的性能。然而，我们对为何零样本提示如此有效缺乏清晰的理解。例如，在提示“让我们一步一步地思考”中，“思考”或“一步一步地”哪个更重要？现有的解释方法，如基于梯度和基于注意力的方法，计算成本高且仅限于开源模型。我们引入了ZIP评分（零样本扰动重要性评分），这是一种适用于开放和闭源模型的通用指标，基于系统的输入词扰动。我们的实验涵盖了四个近期LLMs、七个常用提示以及多个任务，揭示了词语重要性的有趣模式。例如，虽然“一步一步地”和“思考”都显示出较高的ZIP评分，但哪个更重要取决于模型和任务。我们通过控制实验验证了该方法，并将结果与人类判断进行了比较，发现专有模型在词语重要性方面更接近人类直觉。这些发现增强了我们对LLM行为的理解，并有助于开发更有效的零样本提示和改进的模型分析。|
|**2025-02-05**|**SPRI: Aligning Large Language Models with Context-Situated Principles**|Hongli Zhan et.al.|[2502.03397](http://arxiv.org/abs/2502.03397)|null|在本文中，我们将大型语言模型与人类价值观对齐的问题视为一个需要复杂的人类监督的任务。由于依赖于人类专业知识来进行上下文特定的指导既耗费资源又耗时，因此这一过程颇具挑战性。先前的工作利用预定义的一套规则或原则来引导模型的行为（Bai等人，2022；Sun等人，2023）。然而，这些原则往往是通用的，这使得它们难以适应每个单独的输入查询或具体上下文。在这项工作中，我们提出了Situated-PRInciples (SPRI)，这是一个框架，旨在无需或几乎无需人工努力的情况下实时地为每个输入查询自动生成指导原则，并利用这些原则来调整每个响应。我们在三项任务上评估了SPRI，结果表明：1）SPRI能够在复杂的领域特定任务中推导出原则，其性能与专家设计的原则相当；2）SPRI生成的原则可以形成实例特定的评分标准，从而优于之前的基于大型语言模型作为裁判的框架；3）使用SPRI生成合成的SFT数据显著提高了真实性。我们在<https://github.com/honglizhan/SPRI-public>发布了我们的代码和模型生成。|
|**2025-02-05**|**LIMO: Less is More for Reasoning**|Yixin Ye et.al.|[2502.03387](http://arxiv.org/abs/2502.03387)|**[link](https://github.com/gair-nlp/limo)**|我们提出了一项基本发现，挑战了我们对大型语言模型中复杂推理如何出现的理解。传统观点认为，复杂的推理任务需要大量的训练数据（超过100,000个样本），而我们证明，通过少量的样例就能有效地激发复杂数学推理能力。通过全面的实验，我们提出的模型LIMO在数学推理方面展示了前所未有的性能。仅使用817个精心策划的训练样本，LIMO在AIME上的准确率达到57.1%，在MATH上达到94.8%，分别优于先前基于SFT模型的6.5%和59.2%，同时仅使用了先前方法所需训练数据的1%。LIMO展示了出色的分布外泛化能力，在10个不同的基准测试中实现了40.5%的绝对提升，优于使用100倍数据训练的模型，挑战了SFT导致记忆而非泛化的观念。基于这些结果，我们提出了Less-Is-More推理假设（LIMO假设）：在预训练期间已全面编码领域知识的基础模型中，通过少量但精心设计的认知过程演示，可以引发复杂的推理能力。该假设指出，复杂数理推理的触发阈值由两个关键因素决定：（1）模型在预训练期间编码的知识基础的完整性；（2）后训练样例作为“认知模板”的有效性，这些模板向模型展示如何利用其知识库来解决复杂数理推理任务。为了促进数据高效推理的可重复性和未来研究，我们将LIMO作为综合开源套件发布在https://github.com/GAIR-NLP/LIMO。|
|**2025-02-05**|**Demystifying Long Chain-of-Thought Reasoning in LLMs**|Edward Yeo et.al.|[2502.03373](http://arxiv.org/abs/2502.03373)|**[link](https://github.com/eddycmu/demystify-long-cot)**|大规模语言模型（LLMs）的推理计算扩展增强了其推理能力，长链条思维（CoTs）使得诸如回溯和纠错等策略得以实现。强化学习（RL）已成为开发这些能力的关键方法，但长CoTs出现的条件仍不明确，并且RL训练需要仔细设计选择。在这项研究中，我们系统地调查了长CoT推理的机制，确定了使模型生成长CoT轨迹的关键因素。通过广泛的有监督微调（SFT）和RL实验，我们提出了四个主要发现：（1）虽然SFT不是严格必要的，但它简化了训练并提高了效率；（2）随着训练计算量的增加，推理能力往往会显现出来，但其发展并不保证，因此奖励塑造对于稳定CoT长度增长至关重要；（3）对可验证奖励信号进行扩展对于RL是至关重要的。我们发现，利用带有过滤机制的嘈杂、从网络提取的解决方案显示出强大的潜力，特别是在分布外（OOD）任务如STEM推理方面；（4）像错误纠正这样的核心能力在基础模型中本身就存在，但通过RL有效激励这些技能以应对复杂任务需要大量的计算，并且衡量它们的出现需要一种细致的方法。这些见解为优化训练策略以增强LLMs中的长CoT推理提供了实用指导。我们的代码可在以下链接获取：https://github.com/eddycmu/demystify-long-cot。|
|**2025-02-04**|**A comparison of translation performance between DeepL and Supertext**|Alex Flückiger et.al.|[2502.02577](http://arxiv.org/abs/2502.02577)|**[link](https://github.com/supertext/evaluation_deepl_supertext)**|随着强大的机器翻译（MT）系统越来越多地基于大型语言模型（LLMs），可靠的品质基准测试需要能够捕捉其利用扩展上下文的能力的方法。本研究通过评估未分段文本上的两个商业MT系统——DeepL和Supertext——的性能来进行比较。我们评估了四个语言方向的翻译质量，由专业翻译人员根据全文档级别的上下文评估段落。虽然段落级评估在大多数情况下并未显示出对这两个系统的强烈偏好，但文档级分析显示，在四种语言方向中有三种更倾向于Supertext，这表明其在较长文本中的表现更为一致。我们倡导采用更多与上下文相关的评估方法，以确保MT品质评估能反映现实世界的可用性。我们将所有评估数据和脚本发布在https://github.com/supertext/evaluation_deepl_supertext上，以便进一步分析和再现。|
|**2025-02-04**|**Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement**|Soheil Abbasloo et.al.|[2502.02573](http://arxiv.org/abs/2502.02573)|null|大型语言模型（LLMs）在众多领域展示了令人印象深刻的性能，为优化问题解决这一重要且复杂的领域带来了革命性的机会。本文探讨了LLMs在处理顺序优化问题（SOPs）方面的表现。我们引入了WorldGen，这是一种动态框架，用于生成具有可控复杂度的未见过的SOPs，以评估LLM的表现。我们的初步观察表明，虽然LLMs在简单的SOPs上表现良好，但随着复杂度的增加，它们的表现显著下降。受此激励，我们重新审视了关于推理的哲学假设，以提高LLM的表现。受到黑格尔辩证法这一有影响力的框架启发，我们提出了ACE方法，展示了如何在不进行再训练或进一步微调的情况下显著提升LLMs在SOP环境中的表现。|
|**2025-02-04**|**Learning the RoPEs: Better 2D and 3D Position Encodings with STRING**|Connor Schenck et.al.|[2502.02562](http://arxiv.org/abs/2502.02562)|null|我们介绍了STRING：可分离的平移不变位置编码。STRING通过一个统一的理论框架扩展了最近提出的并在大型语言模型中广泛使用的一种算法——旋转位置编码。重要的是，STRING在保持低计算成本的同时，仍能提供精确的平移不变性，并且适用于任意维度的令牌坐标。这些特性在机器人领域尤为重要，其中高效的三维令牌表示是关键。我们将STRING集成到具有RGB（-D）输入的视觉变换器中（颜色加上可选的深度），在开放式词汇物体检测和机器人控制器中显示出显著的提升。我们通过严格的数学分析补充了我们的实验，证明了我们方法的普适性。|
|**2025-02-04**|**LLMs for Generation of Architectural Components: An Exploratory Empirical Study in the Serverless World**|Shrikara Arun et.al.|[2502.02539](http://arxiv.org/abs/2502.02539)|null|近期，大型语言模型（LLMs）的能力和普及程度呈指数级增长，在代码生成领域取得了显著进展。然而，这种生成仅限于代码片段。我们的目标是自动生成架构组件，这不仅会加快开发时间，还最终能使我们直接从设计决策过渡到部署阶段，而无需经历开发过程。为此，我们对LLMs生成函数即服务（FaaS）的架构组件的能力进行了探索性研究。由于这类架构组件的规模较小，相较于单体架构和微服务架构，它们更适合当前LLMs的生成。我们通过系统地选择开源的服务器less存储库，屏蔽一个服务器less函数，并利用提供不同上下文信息级别的最新LLMs来生成被屏蔽的函数。我们通过现有存储库中的测试来评估正确性，并使用软件工程（SE）和自然语言处理（NLP）领域的指标来评估代码质量和人工与LLM生成代码之间的相似度。除了我们的发现外，我们还将讨论如何在未来使用生成式人工智能（GenAI）进行架构组件生成的路径。|
|**2025-02-04**|**Adaptive Self-improvement LLM Agentic System for ML Library Development**|Genghan Zhang et.al.|[2502.02534](http://arxiv.org/abs/2502.02534)|**[link](https://github.com/zhang677/pcl-lite)**|ML库通常使用针对特定架构的编程语言（ASPL）编写，这些语言针对特定领域的架构进行了优化，对于高效的机器学习系统至关重要。然而，编写这些高性能的ML库具有挑战性，因为它需要对机器学习算法和ASPL有专家级的知识。另一方面，大型语言模型（LLMs）展示了它们的一般编码能力。然而，当使用LLMs生成使用ASPL的ML库时，仍然存在挑战，因为这项任务即使对经验丰富的程序员来说也很复杂，并且由于ASPL的深奥和不断发展的性质，代码示例有限。因此，LLMs需要在有限的数据下进行复杂的推理才能完成此任务。为了解决这些挑战，我们引入了一个自适应自我改进的智能系统。为了评估我们系统的有效性，我们在典型的ML库基准上构建了一个测试集，并在该基准上使用开源和闭源LLMs生成ASPL代码。我们的结果显示，与单一LLM基线相比，性能提高了高达3.9倍。|
|**2025-02-04**|**Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies**|Han Zhou et.al.|[2502.02533](http://arxiv.org/abs/2502.02533)|null|大型语言模型在被用作相互交互和协作的多个代理时，在解决复杂任务方面表现出色。这些代理通过声明其功能的提示以及协调跨代理交互的拓扑来编程。设计多代理系统（MAS）的提示和拓扑本质上是复杂的。为了自动化整个设计过程，我们首先进行了深入分析以了解构建有效MAS的因素。我们揭示了提示与拓扑一起在实现更有效的MAS设计中起着关键作用。基于这些见解，我们提出了Multi-Agent System Search (MASS)，这是一种MAS优化框架，通过交错其从局部到全局、从提示到拓扑的优化阶段，高效地利用复杂的MAS设计空间，分为三个阶段：1）块级（局部）提示优化；2）工作流拓扑优化；3）工作流级（全局）提示优化，每个阶段都根据从前一阶段迭代优化的提示/拓扑进行条件设置。我们展示了MASS优化的多代理系统比现有各种替代方案表现出了显著的优势。基于MASS发现的系统，我们最终提出了构建有效多代理系统的原理。|
|**2025-02-04**|**Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search**|Maohao Shen et.al.|[2502.02508](http://arxiv.org/abs/2502.02508)|null|大型语言模型（LLMs）在各个领域展示了卓越的推理能力。最近的研究表明，在推理时增加计算量可以提升LLMs的推理能力。这通常涉及由外部LLM验证器引导的推理过程中的大量采样，从而形成一个双玩家系统。尽管有外部指导，这一系统的有效性证明了单一LLM解决复杂任务的潜力。因此，我们提出了一个新的研究问题：我们能否将搜索能力内化以根本性地增强单一LLM的推理能力？这项工作探索了一个正交的方向，即对后训练的LLMs进行自回归搜索（即具有自我反思和自我探索新策略的扩展推理过程）。为了实现这一目标，我们提出了动作思维链（COAT）推理方法，并采用两阶段训练范式：1）小型格式调整阶段，以内化COAT推理格式；2）利用强化学习的大规模自我改进阶段。我们的方法产生了Satori，一个基于开源模型和数据训练的7B LLM。广泛的实证评估表明，Satori在数学推理基准测试中达到了最先进的性能，并且在域外任务上表现出强大的泛化能力。代码、数据和模型将完全开源。|
|**2025-02-04**|**EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization**|Yize Wu et.al.|[2502.02493](http://arxiv.org/abs/2502.02493)|null|speculative解码是一种有效的无损大型语言模型（LLM）推理加速方法。它使用较小的模型生成一个草稿令牌序列，然后由原始基础模型进行验证。在多GPU系统中，可以通过张量并行性（TP）进一步减少推理延迟，而草稿模型的最佳TP大小通常小于基础模型的TP大小，在起草阶段会导致GPU空闲。为了解决这个问题，我们提出了EasySpec，这是一种层并行推测策略，优化了多GPU的利用率。EasySpec打破了起草模型中各层的顺序执行，使得多个层可以在设备之间并行化，尽管这会引入一些近似误差。在每次起草和验证迭代之后，草稿模型的关键值（KV）缓存会在单个前向传递中进行校准，从而以最小的额外延迟防止长期错误累积。我们在几个主流的开源LLM上评估了EasySpec，使用相同系列的较小版本模型作为起草者。结果表明，EasySpec可以实现高达4.17倍的峰值加速比，同时保持基础LLM的原始分布。具体而言，起草阶段最多可以加速1.62倍，最大准确率下降仅为7%，且无需对草稿模型进行训练或微调。|
|**2025-02-04**|**Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study**|Menglong Cui et.al.|[2502.02481](http://arxiv.org/abs/2502.02481)|null|大型语言模型（LLMs）的多语言能力不断得到提升，即使是参数量小于十亿的小型开源模型也展示了快速的性能提升。本文系统地探索了参数量少于十亿的开源LLMs在处理多语种机器翻译（MT）任务中的能力。我们对六种流行的LLMs进行了全面评估，发现像Gemma2-9B这样的模型在多语种翻译方面表现出色。随后，我们在连续预训练阶段引入了平行优先单语种次之（PFMS）的数据混合策略，以进一步提高MT性能，并推出了GemmaX2-28，这是一种达到28种语言顶级多语种翻译性能的90亿参数模型。具体而言，GemmaX2-28在所有评估的语言上始终优于最先进的模型如TowerInstruct和XALMA，并且与Google Translate和GPT-4-turbo相比达到了具有竞争力的性能。|
|**2025-02-04**|**SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency**|Qianhao Yuan et.al.|[2502.02458](http://arxiv.org/abs/2502.02458)|null|多模态大型语言模型（MLLMs）主要分为两种架构，每种架构在训练和推理效率之间都存在权衡：嵌入空间对齐（如LLaVA-1.5）在推理时效率低下，而交叉注意力空间对齐（如Flamingo）在训练时效率低下。在本文中，我们比较了这两种架构，并确定了构建高效MLLMs的关键因素。它们之间的主要区别在于如何将注意力应用于视觉标记，特别是在它们彼此的交互上。为了研究视觉标记之间的注意力是否必要，我们提出了一种新的自注意力机制NAAViT（无视觉标记间注意力），该机制消除了这种类型的注意力。我们的初步实验表明，在LLaVA-1.5上，视觉标记之间的注意力高度冗余。基于这些见解，我们引入了SAISA（自注意力输入空间对齐），一种新颖的架构，增强了训练和推理效率。SAISA直接将视觉特征与NAAViT自注意力块的输入空间对齐，减少了自注意力块和前馈网络（FFNs）中的计算开销。使用与LLaVA-1.5相同的配置，SAISA将推理浮点运算（FLOPs）减少了66%，训练预算减少了26%，同时在准确率方面实现了更优的表现。全面的消融研究进一步验证了SAISA在各种LLMs和视觉编码器上的有效性。代码和模型将在https://github.com/icip-cas/SAISA公开。|
|**2025-01-31**|**Vintix: Action Model via In-Context Reinforcement Learning**|Andrey Polubarov et.al.|[2501.19400](http://arxiv.org/abs/2501.19400)|**[link](https://github.com/dunnolab/vintix)**|**In-Context Reinforcement Learning（ICRL）代表了一种有前景的范式，用于开发通过试错互动在推理时学习的通用代理，类似于大型语言模型如何通过上下文进行适应，但其重点在于最大化奖励。然而，ICRL在超出玩具任务和单一领域设置方面的可扩展性仍然是一个开放的挑战。在这项工作中，我们介绍了迈向ICRL可扩展性的第一步，即引入了一个固定、跨域的模型，能够通过上下文强化学习来学习行为。我们的结果显示，旨在促进ICRL的算法提炼框架提供了一种引人注目且具有竞争力的替代专家提炼方法来构建多功能动作模型。这些发现突显了ICRL作为可扩展方法用于通用决策系统的潜力。代码将在<https://github.com/dunnolab/vintix>发布**|
|**2025-01-31**|**Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game**|Mustafa O. Karabag et.al.|[2501.19398](http://arxiv.org/abs/2501.19398)|**[link](https://github.com/mustafakarabag/llmchameleon)**|**大型语言模型（LLM）驱动的智能体在包含非合作方的环境中变得常见。在这种环境中，智能体需要在决策过程中向对手隐藏信息、向合作者透露信息，并通过推断来识别其他智能体的特征。为了探讨LLM是否具备这些信息控制和决策能力，我们将基于LLM的智能体置于一种基于语言的隐身份游戏中，即“变色龙”游戏。在游戏中，一组互不认识的非变色龙智能体试图识别变色龙智能体而不泄露秘密。该游戏要求无论是作为变色龙还是非变色龙，都需要具备上述的信息控制能力。实验结果表明，虽然非变色龙的LLM智能体能够识别出变色龙，但它们未能向变色龙隐藏秘密，其获胜概率远低于甚至是最简单的策略水平。为了正式解释这种行为，我们分析了一系列从隐藏到透露的信息策略，并提供了非变色龙获胜概率的界限。根据实验结果和对不同策略的理论分析，我们得出结论，基于LLM的非变色龙智能体在与未知身份的智能体互动时，会过度透露信息。我们的研究结果指出了当代LLM的一个弱点，包括GPT-4、GPT-4o、Gemini 1.5和Claude 3.5 Sonnet，在战略交互中。**|
|**2025-01-31**|**Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models**|Alina Shutova et.al.|[2501.19392](http://arxiv.org/abs/2501.19392)|**[link](https://github.com/goodevening13/aquakv)**|高效的现实世界部署大型语言模型（LLMs）依赖于键值（KV）缓存来处理和生成长输出，从而减少重复计算的需求。对于大上下文，KV缓存可能占用数十GB的设备内存，因为它们为每个令牌和层存储向量表示。最近的研究表明，可以通过量化、剪枝或合并来压缩缓存向量，但这些技术往往在更高的压缩率下牺牲质量。在这项工作中，我们旨在通过利用两个观察结果来改进键和值的压缩：1）不同层之间键和值之间的固有依赖关系，以及2）对内部网络状态的高压缩机制。我们提出了AQUA-KV，这是一种自适应量化方法，用于KV缓存，它依赖于紧凑型适配器来利用键和值之间存在的依赖关系，并旨在“最优”压缩无法预测的信息。AQUA-KV显著提高了压缩率，同时保持了最先进的LLM系列的高准确性。在Llama 3.2 LLM上，我们在每值2-2.5位的情况下实现了接近无损的推理，困惑度和LongBench得分的相对误差小于1%。AQUA-KV是一次性、简单且高效的：它可以在单个GPU上校准1-6小时，即使是70B模型也是如此。|
|**2025-01-31**|**Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models**|Wenzhi Fang et.al.|[2501.19389](http://arxiv.org/abs/2501.19389)|**[link](https://github.com/wenzhifang/Federated-Sketching-LoRA-Implementation)**|对大型语言模型（LLMs）在设备上的微调正吸引越来越多的兴趣。近期的研究将低秩适应（LoRA）技术与联邦微调相结合，以缓解与设备模型大小和数据稀缺性相关的问题。然而，计算资源的异质性仍然是一个关键瓶颈：虽然高秩模块通常能提升性能，但不同的设备能力限制了LoRA可行的秩范围。现有的试图解决这个问题的方法要么缺乏分析依据，要么增加了额外的计算开销，留下了巨大的效率和理论上可靠解决方案的空间。为了解决这些挑战，我们提出了联邦速记LoRA（FSLoRA），它利用速记机制使设备能够选择性地更新由服务器维护的全局LoRA模块中的子矩阵。通过调整速记比率，这决定了设备上子矩阵的秩，FSLoRA可以灵活适应设备特定的通信和计算约束。我们提供了FSLoRA的严格收敛性分析，该分析描述了速记比率如何影响收敛速度。通过在多个数据集和LLM模型上进行的综合实验，我们证明了FSLoRA相对于各种基线的优越性能。|
|**2025-02-03**|**SELMA: A Speech-Enabled Language Model for Virtual Assistant Interactions**|Dominik Wagner et.al.|[2501.19377](http://arxiv.org/abs/2501.19377)|null|在这项工作中，我们提出了SELMA，这是一种用于虚拟助手交互的语音增强语言模型，它将音频和文本作为输入传递给大型语言模型（LLM）。SELMA旨在同时处理与虚拟助手交互相关的三个主要任务和两个辅助任务，并在单一端到端模型中实现。我们采用了低秩适应模块来对音频编码器和LLM进行有效的参数训练。此外，我们实施了一种特征池化策略，使系统能够识别全局模式并提高不太依赖于单个序列元素的任务的准确性。实验结果表明，在语音触发（VT）检测、设备定向语音检测（DDSD）和自动语音识别（ASR）方面的表现均显示，我们的方法不仅显著简化了虚拟助手典型的输入处理流程，而且在性能上也优于针对每个单独任务的专用模型。SELMA在VT检测任务上的等错误率相对提升了64%，在DDSD上的提升了22%，同时其词错误率接近基准水平。|
|**2025-01-31**|**We're Different, We're the Same: Creative Homogeneity Across LLMs**|Emily Wenger et.al.|[2501.19361](http://arxiv.org/abs/2501.19361)|null|众多强大的大型语言模型（LLMs）现在可用于写作支持、创意生成等领域。尽管这些模型被宣传为有用的创意助手，但多项研究表明，使用LLM作为创意伙伴会导致创意产出范围变窄。然而，这些研究仅考虑了与单一LLM互动的效果，从而引出一个问题：这种狭窄的创造性是否源于特定的LLM——这显然有有限的输出范围——还是源于LLM作为创意助手的使用本身。为了研究这个问题，我们使用标准化的创造性测试从人类和广泛的LLMs中激发创意回应，并比较响应的总体多样性。我们发现，LLM的响应与其他LLM的响应更为相似，即使在控制了响应结构和其他关键变量之后也是如此。这一关于我们在评估的LLMs中观察到的创造输出显著同质化的发现为关于LLMs和创造力的持续讨论增添了新的维度。如果今天的LLMs行为相似，无论使用哪个模型，将其作为创意伙伴使用可能会使所有用户趋向于有限的“创意”输出。|
|**2025-01-31**|**Mechanical Properties of the Meninges: Large Language Model Assisted Systematic Review of over 25,000 Studies**|Brandon P. Chelstrom et.al.|[2501.19359](http://arxiv.org/abs/2501.19359)|null|准确的脑膜本构模型及其相应的力学性能值对于预测由于创伤性脑损伤导致的脑组织机械损伤至关重要。脑膜由于其复杂的解剖结构和空间变异的力学行为，在当前的有限元（FE）头部模型中经常被过度简化。本研究系统地回顾了每层脑膜的力学性能，以获得有限元建模所需的基准数据，并确定了现有文献中的不足之处。相关研究通过三个阶段进行筛选：广泛的初步搜索过滤、大型语言模型分类器以及人工审核员的手动验证。在最初考虑的超过25,000项研究中，本综述最终包括了关于硬脑膜的47项研究、关于蛛网膜的8项研究和关于软脑膜的7项研究，这代表了迄今为止最大且最全面的关于脑膜力学性能的系统评价。每层脑膜都被发现表现出非线性和速率依赖性，这些特性随物种、年龄、位置和方向而变化。该研究表明，简化线弹性有限元模型中最常使用的软脑膜弹性模量可能低估了一个数量级，并且没有考虑到方向依赖性。未来的研究应重点关注更宽范围的加载速率以及对蛛网膜和软脑膜的年龄效应的研究，因为这些特征相对研究较少，预计会影响有限元预测的准确性。|
|**2025-01-31**|**The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking**|Yuchun Miao et.al.|[2501.19358](http://arxiv.org/abs/2501.19358)|null|这项工作识别了在基于人类反馈的强化学习（RLHF）中的能量损失现象及其与奖励操纵的联系。具体而言，在大型语言模型（LLM）的最后一层的能量损失在RL过程中逐渐增加，能量损失的过度增加是奖励操纵的特征。除了经验分析外，我们还提供了理论基础，证明在温和条件下，能量损失的增加降低了LLM中上下文相关性的上限，这是奖励操纵的一个关键方面，因为减少的上下文相关性通常表示在RL中对奖励模型有利模式的过拟合。为了解决这个问题，我们提出了一种考虑能量损失的PPO算法（EPPO），该算法在奖励计算过程中惩罚LLM最后一层的能量损失增加，以防止能量损失过度增加，从而减轻奖励操纵。我们理论上表明EPPO可以被概念上解释为熵正则化的RL算法，这为其有效性提供了更深入的见解。广泛的实验跨越各种LLM和任务，展示了能量损失现象的普遍性以及EPPO在减轻奖励操纵和提高RLHF性能方面的有效性。|
|**2025-01-31**|**Towards Adaptive Self-Improvement for Smarter Energy Systems**|Alexander Sommer et.al.|[2501.19340](http://arxiv.org/abs/2501.19340)|null|本文介绍了一个用于决策和优化的分层框架，利用大规模语言模型（LLMs）进行自适应代码生成。该方法不直接进行决策，而是通过元策略指导任务生成以及基础策略执行操作任务来生成和优化可执行控制策略。应用于简化的微电网场景中，该方法通过迭代改进电池控制策略实现了高达15%的成本节约。所提出的方法为将基于LLM的工具集成到规划和控制任务中奠定了基础，提供了适用于复杂系统的可调和可扩展解决方案，同时解决了不确定性及可重复性挑战。|
|**2025-01-31**|**Homogeneity Bias as Differential Sampling Uncertainty in Language Models**|Messi H. J. Lee et.al.|[2501.19337](http://arxiv.org/abs/2501.19337)|null|先前的研究表明，大规模语言模型（LLMs）和视觉语言模型（VLMs）在表示边缘化群体时比主流群体更加同质化。然而，这种同质性偏差的机制尚未得到充分探索。我们提出，这种偏差源于推理过程中采样token的概率分布系统性差异。通过分析三个衡量token采样分布不确定性的指标——熵、困惑度和区分概率，我们发现，在某些模型中，特别是GPT-4 Turbo和Llama-3.2，在生成关于边缘化群体（即非裔美国人和女性）的文本时，token的采样更加确定性，与主流群体（即白人和男性）相比。尽管这些发现可能有助于解释某些模型中的同质性偏差，但测试的所有VLMs中并未完全复制这些模式，这表明同质性偏差可能是由多种机制共同导致的。|
|**2025-01-30**|**Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs**|Yue Wang et.al.|[2501.18585](http://arxiv.org/abs/2501.18585)|null|大型语言模型（LLMs）如OpenAI的o1在复杂的推理任务中展示了惊人的能力，通过扩展测试时的计算资源并表现出类似人类的深度思考。然而，我们发现了一种现象，我们称之为浅尝辄止，其中像o1这样的LLMs经常在不同的推理思路之间切换，而没有充分探索有希望的路径以找到正确的解决方案。这种行为导致推理深度不足和性能下降，特别是在具有挑战性的数学问题上。为了系统地分析这一问题，我们在三个具有挑战性的测试集和两个代表性的开源o1-like模型上进行了实验，结果表明频繁的思路切换与错误答案相关。我们引入了一个新的指标来量化浅尝辄止，通过测量错误答案中的token效率。为了解决浅尝辄止的问题，我们提出了一种带有思路切换惩罚TIP的解码策略，该策略可以阻止过早地在不同思路之间切换，鼓励更深入地探索每一条推理路径。实验结果显示，我们的方法提高了具有挑战性数据集上的准确率，且无需对模型进行微调。我们的研究有助于理解o1-like LLMs中的推理低效问题，并提供了一种增强其解决问题能力的实际方案。|
|**2025-01-30**|**Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for Multi-Step Reasoning Over Speed in MATH**|Evgenii Evstafev et.al.|[2501.18576](http://arxiv.org/abs/2501.18576)|null|本研究调查了DeepSeek R1语言模型在30个来自MATH数据集的具有挑战性的数学问题上的表现，这些问题之前因时间限制而无法被其他模型解决。与先前的研究不同，本研究去除了时间限制，探讨了以基于令牌的推理著称的DeepSeek R1架构是否可以通过多步骤过程实现准确的解决方案。该研究在11种不同的温度设置下将DeepSeek R1与四个其他模型（gemini-1.5-flash-8b、gpt-4o-mini-2024-07-18、llama3.1:8b和mistral-8b-latest）进行了比较。结果表明，DeepSeek R1在这类复杂问题上实现了更高的准确性，但生成的令牌显著多于其他模型，证实了其令牌密集型方法。研究结果突显了大型语言模型在数学问题求解中准确性和效率之间的权衡：尽管DeepSeek R1在准确性方面表现出色，但其对大量令牌生成的依赖可能不适用于需要快速响应的应用场景。研究强调了在选择LLM时考虑任务特定需求的重要性，并强调了温度设置在优化性能中的作用。|
|**2025-01-30**|**BounTCHA: A CAPTCHA Utilizing Boundary Identification in AI-extended Videos**|Lehao Lin et.al.|[2501.18565](http://arxiv.org/abs/2501.18565)|null|近年来，特别是多模态大型语言模型（MLLMs）的快速发展使得人工智能能够理解文本、图像、视频和其他多媒体数据，并允许基于人类提供的提示执行各种任务。然而，AI驱动的机器人已经能够绕过大多数现有的CAPTCHA系统，这对Web应用程序构成了重大安全威胁，因此设计新的CAPTCHA机制迫在眉睫。我们观察到人类对视频中的边界和突然变化非常敏感，而目前的AI系统仍然难以有效理解和应对这种情况。基于这一观察，我们设计并实现了一种名为BounTCHA的CAPTCHA机制，该机制利用了人类对视频转换和中断中边界的感知能力。通过利用AI扩展原始视频的能力，并引入意外的变化和转折，我们创建了一个生成用于CAPTCHA目的的短视频的流程。我们开发了一个原型，并进行了实验以收集人类在边界识别中的时间偏差的数据。这些数据作为区分人类用户和机器人的基础。此外，我们还对BounTCHA进行了详细的安全分析，证明其对各种类型的攻击具有弹性。我们希望BounTCHA能够作为一种强大的防御手段，在人工智能驱动的时代保护数百万个Web应用程序。|
|**2025-01-30**|**Semantic Web and Creative AI -- A Technical Report from ISWS 2023**|Raia Abu Ahmad et.al.|[2501.18542](http://arxiv.org/abs/2501.18542)|null|International Semantic Web Research School（ISWS）是一个为期一周的强化项目，旨在让参与者深入了解该领域。本文档报告了参加ISWS 2023的十支学生团队在资深研究员指导下进行的一项协作研究工作。每个团队从不同的角度探讨了创意人工智能的主题，并通过一系列研究问题作为其调查的主要内容。2023年ISWS的重点是语义网技术和创意人工智能的交叉领域。ISWS 2023探索了语义网技术与创意人工智能的各种交叉点。重点之一是大型语言模型作为知识工程支持工具的潜力。参与者还深入探讨了大型语言模型的多方面应用，包括创意内容生产的法律方面、人类参与其中的方式、去中心化方法在多模态生成式人工智能模型中的应用、纳米出版物和人工智能用于个人科学知识图谱、自动故事和叙事完成中的常识知识、用于艺术批评的生成式人工智能、提示工程、自动音乐创作、常识原型设计和概念融合，以及隐性知识的提取。随着大型语言模型和语义技术的不断发展，新的令人兴奋的可能性正在出现：一个未来，其中创造性表达与事实知识之间的界限变得越来越模糊，从而形成一个既具信息性又具启发性的知识世界。|
|**2025-01-30**|**Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges**|Manveer Singh Tamber et.al.|[2501.18536](http://arxiv.org/abs/2501.18536)|**[link](https://github.com/manveertamber/content_injection_attacks)**|**在用户搜索信息时，经常会遇到充斥着误导性或不相关的内容的文本。这种情景揭示了神经信息检索（IR）管道中的一个简单但强大的漏洞：内容注入攻击。我们发现用于检索的嵌入模型、重排序器和大型语言模型（LLM）相关性判断都容易受到这些攻击的影响，在这些攻击中，对手会向段落中插入误导性文本以操纵模型判断。我们确定了两种主要威胁：（1）在段落中插入不相关或有害的内容，使其看起来具有欺骗性的“相关性”，以及（2）将整个查询或关键查询词插入段落以提升其感知的相关性。虽然第二种策略已在先前的研究中有所探讨，但我们提供了迄今为止对第一种威胁的第一项实证分析，展示了最先进的模型如何容易被误导。我们的研究系统地考察了影响攻击成功的因素，如注入内容的位置以及相关与非相关材料之间的平衡。此外，我们探索了各种防御策略，包括对抗性段落分类器、重新训练检索器以降低操纵内容的重要性，以及提示LLM判断者采取更谨慎的方法。然而，我们发现这些对策通常涉及权衡，牺牲有效性以提高攻击的鲁棒性，并且有时会对合法文档造成不利影响。我们的研究结果强调了需要更强的防御措施来应对这些不断演化的对抗性策略，以保持IR系统的可信度。我们将代码和脚本公开，以促进进一步的研究。**|
|**2025-01-30**|**Differentially Private Steering for Large Language Model Alignment**|Anmol Goel et.al.|[2501.18532](http://arxiv.org/abs/2501.18532)|**[link](https://github.com/ukplab/iclr2025-psa)**|**对齐大型语言模型（LLMs）与人类价值观，并使其远离有害行为（如幻觉生成）变得越来越重要。最近，通过激活编辑在推理时引导LLMs实现所需行为已成为一种有效方法来缓解有害生成。激活编辑通过保留来自正面演示（例如，真实的）的信息并最小化来自负面演示（例如，幻觉的）的信息来修改LLMs表示。当这些演示来自私有数据集时，对齐后的LLM可能会泄露这些私有样本中包含的私人信息。在这项工作中，我们首次研究了使用私有数据集对齐LLM行为的问题。我们的工作提出了一个名为“私人引导用于LLM对齐（PSA）”算法，以在保证差分隐私（DP）的情况下编辑LLM激活。我们在七个不同的基准上进行了广泛的实验，使用了不同大小（0.5B到7B）和模型家族（LlaMa、Qwen、Mistral和Gemma）的开源LLMs。我们的结果显示，PSA在保持性能最小损失的情况下实现了LLM对齐的DP保证，包括对齐度量、开放式文本生成质量和通用推理能力。我们还开发了首个成员推断攻击（MIA），用于评估和审计LLM引导过程中经验隐私问题。该攻击专门针对激活编辑设计，并且仅依赖于生成的文本而不依赖其关联的概率。我们的实验结果支持了理论保证，显示我们的PSA算法相比几种现有的非私有技术提供了更好的保障。**|
|**2025-01-30**|**Learn from the Past: Language-conditioned Object Rearrangement with Large Language Models**|Guanqun Cao et.al.|[2501.18516](http://arxiv.org/abs/2501.18516)|null|物体重排是协作机器人的一项重要任务，它们被要求将物体移动到指定的目标状态。确定物体的放置位置是一个主要挑战，它影响着重排过程的效率。当前大多数方法严重依赖于预先收集的数据集来训练模型以预测目标位置，并且受到特定指令的限制，这限制了它们的更广泛应用和有效性。在本文中，我们提出了一种基于大型语言模型（LLM）的语言条件物体重排框架。特别是，我们的方法通过使用过去的成功经验作为参考来推断期望的目标位置，模仿了人类的推理方式。基于LLM对自然语言的强大理解和推理能力，我们的方法可以零样本地处理各种日常物品和自由形式的语言指令。实验结果表明，我们的方法能够有效地执行涉及长顺序指令的机器人重排任务。|
|**2025-01-30**|**Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch**|Arthur Douillard et.al.|[2501.18512](http://arxiv.org/abs/2501.18512)|null|大型语言模型（LLMs）的训练通常分布在大量的加速器上以减少训练时间。由于内部状态和参数梯度需要在每一步梯度计算中进行交换，所有设备需要使用低延迟高带宽通信链路进行共址，以支持所需的高数据交换量。最近，分布式算法如DiLoCo已经放松了这种共址约束：加速器可以被分组为“工作器”，其中工作器之间的同步仅偶尔发生。这反过来意味着工作器之间可以使用较低带宽的通信链路而不影响学习质量。然而，在这些方法中，跨工作器的通信仍然需要与之前相同的峰值带宽，因为同步需要所有参数在所有工作器之间进行交换。在本文中，我们通过三种方式改进了DiLoCo。首先，我们按顺序仅同步参数的子集，而不是一次性同步所有参数，这大大减少了峰值带宽。其次，我们允许工作器在同步期间继续训练，这减少了总耗时。第三，我们将工作器之间交换的数据进行量化，进一步减少了跨工作器的带宽需求。通过正确结合这些修改，我们实验表明我们可以分布训练具有数十亿规模参数的模型，并达到相似的质量，但所需带宽降低了两个数量级。|
|**2025-01-30**|**CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to Sustainability Data Extraction**|Peter J. Bentley et.al.|[2501.18504](http://arxiv.org/abs/2501.18504)|null|大型语言模型（LLM）图像识别是一种从图像中提取数据的强大工具，但准确性取决于在提示中提供足够的线索——需要领域专家进行专门任务。我们介绍了Cue Learning using Evolution for Accurate Recognition (CLEAR)，它结合了LLM和进化计算来自动生成并优化线索，从而提高图像中专门特征识别的准确性。通过自动生成新颖的领域特定表示，然后使用遗传算法优化适当的文本线索来实现这一点。我们将CLEAR应用于实际任务，即从建筑内外部图像中识别可持续性数据。我们研究了使用可变长度表示与固定长度表示的效果，并展示了如何通过从分类估计转换为实值估计来改进LLM一致性。我们表明CLEAR能够实现比专家人类识别和人工编写的提示更高的准确性，在每个任务中的错误率提高了多达两个数量级，并且消融研究证明了解决方案的简洁性。|
|**2025-01-30**|**A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models**|Changshu Liu et.al.|[2501.18482](http://arxiv.org/abs/2501.18482)|null|代码执行推理是衡量大型语言模型（LLMs）在编程任务中的能力的一个新的非功能性指标。目前最先进的框架（如CodeMind或REval）和基准（如CruxEval）通常集中在LLM对给定代码的输入/输出或中间变量状态/值的预测上，而这些代码通常是有限的程序。然而，目前还没有工具可以对结果进行更深入的分析。由于缺乏这样的工具，关于LLM的代码执行推理的观察无法推广到更多的数据集，这阻碍了研究界和从业者设计具有更好代码执行推理能力的下一代LLM。本文介绍了一种名为ExeRScope的工具系列和启发式方法，用于分析代码执行推理框架的结果，以更好地理解代码属性对所研究基准的影响。有了这种工具，分析可以推广到具有相似属性的代码，而无需迫切需要设计更多的基准，这是一个繁琐的工作。|
|**2025-01-29**|**Learning Beyond the Surface: How Far Can Continual Pre-Training with LoRA Enhance LLMs' Domain-Specific Insight Learning?**|Pouya Pezeshkpour et.al.|[2501.17840](http://arxiv.org/abs/2501.17840)|**[link](https://github.com/megagonlabs/insight_miner)**|**大型语言模型（LLMs）在各种任务上展示了卓越的性能，但它们从特定领域数据集中提取和内化更深层次洞察的能力仍需深入探索。在这项研究中，我们调查了持续预训练如何增强LLMs在三种不同形式的洞察学习中的能力：陈述性、统计性和概率性洞察。专注于两个关键领域：医学和金融，我们使用LoRA在两个现有数据集上训练LLMs。为了评估每种洞察类型，我们创建了基准测试来衡量持续预训练如何帮助模型超越表面知识。我们还评估了文档修改对捕捉洞察的影响。结果表明，虽然在原始文档上的持续预训练效果有限，但修改文档以仅保留重要信息显著增强了LLMs的洞察学习能力。**|
|**2025-01-29**|**Leveraging Multimodal LLM for Inspirational User Interface Search**|Seokhyeon Park et.al.|[2501.17799](http://arxiv.org/abs/2501.17799)|**[link](https://github.com/spark-damian/s-ui)**|**在移动用户界面（UI）设计中，启发式搜索——即探索设计以启发新的创意工作——至关重要。然而，探索庞大的UI参考空间仍然是一项挑战。现有的基于AI的UI搜索方法通常会遗漏目标用户或应用程序氛围等关键语义信息。此外，这些模型通常需要视图层次结构之类的元数据，限制了它们的实际应用。我们使用多模态大型语言模型（MLLM）从移动UI图像中提取和解释语义。通过一项形成性研究，我们确定了关键的UI语义，并开发了一种基于语义的UI搜索系统。通过计算和人工评估，我们证明我们的方法显著优于现有的UI检索方法，为UI设计师提供了更加丰富且上下文相关的搜索体验。我们增强了对移动UI设计语义的理解，并突显了MLLM在启发式搜索中的潜力，为未来的研究提供了丰富的UI语义数据集。**|
|**2025-01-29**|**BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights**|Chan-Jan Hsu et.al.|[2501.17790](http://arxiv.org/abs/2501.17790)|null|我们提出了BreezyVoice，这是一种专门针对台湾普通话的文本转语音（TTS）系统，特别强调了音素控制能力以解决该语言中多音字消歧的独特挑战。在此基础上构建CosyVoice，我们引入了 $S^{3}$ 分词器、大型语言模型（LLM）、最优传输条件流匹配模型（OT-CFM）以及从字母到发音的预测模型，生成的语音能够真实地模拟人类的发音。我们的评估表明，BreezyVoice在普通和代码切换上下文中均表现出色，突显了其在生成高保真语音方面的稳健性和有效性。此外，我们还解决了建模小众说话者和多音字消歧中的泛化挑战。我们的方法显著提升了性能，并为神经编解码TTS系统的运作提供了有价值的见解。|
|**2025-01-29**|**AdditiveLLM: Large Language Models Predict Defects in Additive Manufacturing**|Peter Pak et.al.|[2501.17784](http://arxiv.org/abs/2501.17784)|null|在这项工作中，我们研究了大型语言模型在给定一系列工艺参数输入的情况下预测增材制造缺陷模式的能力。为此任务，我们利用了一个工艺参数缺陷数据集来微调一组模型，称为AdditiveLLM，目的是预测可能的缺陷模式，包括钥匙孔效应、融合不足和球化现象。我们比较了不同的输入格式化方法，以衡量模型在我们的稀疏基准数据集和自然语言提示数据集上正确预测缺陷模式的表现。该模型显示出了强大的预测能力，在被要求提供与一组工艺参数相关的缺陷模式时，准确率达到93%。自然语言输入的引入进一步简化了工艺参数选择的任务，使用户能够识别特定于其构建的最佳设置。|
|**2025-01-29**|**2SSP: A Two-Stage Framework for Structured Pruning of LLMs**|Fabrizio Sandri et.al.|[2501.17771](http://arxiv.org/abs/2501.17771)|**[link](https://github.com/fabriziosandri/2ssp)**|我们提出了一种新的两阶段剪枝框架（2SSP），用于剪枝大型语言模型（LLMs），该框架结合了宽度剪枝和深度剪枝两种不同的剪枝策略。第一阶段（宽度剪枝）移除整个神经元及其对应的行和列，旨在保持前馈网络中每个Transformer块的中间状态下的结构连通性。这是基于衡量每个神经元对输出幅度影响的重要性得分来完成的。第二阶段（深度剪枝）则移除整个注意力子模块。这是通过应用一个迭代过程来实现的，该过程移除对给定指标（在我们的案例中是困惑度）影响最小的注意力子模块。我们还提出了一个新的机制来平衡两个阶段相对于所需全局稀疏性的稀疏率。我们在四个LLM家族上测试了2SSP，并使用三种稀疏率（25%，37.5%和50%），测量了在三个语言建模数据集上的困惑度以及在六个下游任务上的性能。我们的方法在三个语言建模任务和六个下游任务中始终优于五种最先进的竞争对手，其剪枝时间提高了两个数量级。代码可在<https://github.com/FabrizioSandri/2SSP>获取。|
|**2025-01-29**|**Hybrid Graphs for Table-and-Text based Question Answering using LLMs**|Ankush Agarwal et.al.|[2501.17767](http://arxiv.org/abs/2501.17767)|null|回答既需要推理又需要从结构化（表格）和非结构化（纯文本）数据源中进行聚合的问题提出了重大挑战。当前的方法依赖于微调和高质量的人工标注数据，而这些数据难以获得。大型语言模型（LLMs）的最新进展在零样本设置下对单源文本数据的多跳问答任务中展示了有希望的结果，但对于多源表格-文本问答的探索仍然有限。在本文中，我们提出了一种新颖的基于混合图的方法来解决表格-文本问答问题，该方法利用了LLMs而不需微调。我们的方法从文本和表格数据构建了一个统一的混合图，并根据输入问题修剪信息，以便简洁地向LLM提供相关上下文。我们在具有挑战性的Hybrid-QA和OTT-QA数据集上使用最先进的LLMs（包括GPT-3.5、GPT-4和LLaMA-3）评估了我们的方法。我们的方法在这两个数据集上实现了最佳的零样本性能，在Hybrid-QA上的精确匹配分数提高了多达10%，在OTT-QA上的精确匹配分数提高了5.4%。此外，与原始上下文相比，我们的方法减少了高达53%的令牌使用量。|
|**2025-01-29**|**On the Partitioning of GPU Power among Multi-Instances**|Tirth Vamja et.al.|[2501.17752](http://arxiv.org/abs/2501.17752)|null|高效的云计算数据中心电源管理对于降低成本、提升性能以及减少环境影响至关重要。GPU在机器学习（ML）和生成式人工智能（GenAI）任务中至关重要，同时也是电力消耗的主要来源。NVIDIA的多实例GPU（MIG）技术通过启用隔离分区并进行分区间资源跟踪，提高了GPU利用率，促进了多个租户之间的GPU共享。然而，由于缺乏硬件支持，准确地在MIG实例之间分配GPU功耗仍然具有挑战性。本文通过开发软件方法来估算每个MIG分区的功耗以解决这一问题。我们分析了NVIDIA GPU利用率指标，并发现很难构建轻量级且准确的方法。因此，我们探索使用基于机器学习的功耗模型以实现精确的分区级功耗估算。研究结果表明，单一通用的离线功耗模型或建模方法并不适用于各种工作负载，特别是在并发MIG使用时，而使用在线模型并结合执行中的工作负载的分区级利用率指标可以显著提高准确性。使用NVIDIA A100 GPU，我们展示了这种方法，用于矩阵乘法和大型语言模型推理等工作的分区级功耗估算，从而有助于透明和公平的碳排放报告。|
|**2025-01-29**|**Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation**|Aitor Arrieta et.al.|[2501.17749](http://arxiv.org/abs/2501.17749)|null|大型语言模型（LLMs）已成为我们日常生活的重要组成部分。然而，它们也带来了一些风险，包括可能损害个人隐私、延续偏见和传播错误信息。这些风险凸显了需要强大的安全机制、伦理指南和全面的测试，以确保其负责任的部署。LLM的安全性是需要在模型部署并供一般用户使用之前进行彻底测试的关键属性。本文报告了来自蒙德拉贡大学和塞维利亚大学的研究人员对OpenAI的新o3-mini LLM进行的外部安全性测试经验，这是作为OpenAI早期访问计划的一部分，用于安全性测试。特别是，我们应用了我们的工具ASTRAL，自动且系统地生成最新的不安全测试输入（即提示），这有助于我们测试和评估不同安全类别的LLMs。我们自动生成并执行了总计10,080个不安全测试输入在一个早期的o3-mini测试版本上。经过手动验证由ASTRAL分类为不安全的测试用例后，我们确定了总共87个实际的LLM不安全行为实例。我们在OpenAI最新LLM的预部署外部测试阶段中揭示了关键见解和发现。|
|**2025-01-29**|**Using Code Generation to Solve Open Instances of Combinatorial Design Problems**|Christopher D. Rosin et.al.|[2501.17725](http://arxiv.org/abs/2501.17725)|**[link](https://github.com/constructive-codes/cpro1)**|**《组合设计手册》记录了许多类型的组合设计，并列出了尚未确定存在性的开放实例。我们开发了一种构造性协议CPro1，该协议使用大型语言模型（LLMs）生成构建组合设计的代码，从而解决其中一些开放实例。该协议从特定类型设计的定义和一个可靠验证所提出设计是否有效的验证器开始。LLM选择策略并在代码中实现它们，而脚手架提供了自动超参数调整和使用验证器的执行反馈。大多数生成的代码会失败，但通过生成许多候选者，该协议自动化地探索各种标准方法（如模拟退火、遗传算法）并实验不同的变体（如成本函数），以找到成功的途径。在对16种不同类型的设计进行测试后，CPro1成功解决了其中6种设计的开放实例：对称和斜向重量矩阵、等距置换数组、填充数组、平衡三进制设计和佛罗伦萨矩形。**|
|**2025-01-29**|**RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts**|Eujeong Choi et.al.|[2501.17715](http://arxiv.org/abs/2501.17715)|**[link](https://github.com/boychaboy/ricota)**|**用户与对话代理（CAs）的互动随着大型语言模型（LLMs）的广泛应用而演变。当用户试图超越预设的边界以探索和建立与这些系统的联系时，存在一个日益增长的担忧，即未经授权的访问或操纵，这通常被称为“越狱”。此外，对于具有高度人性化特质的对话代理，用户倾向于发起亲密的性互动或尝试驯服他们的聊天机器人。为了捕捉并反映这些真实的交互到聊天机器人的设计中，我们提出了RICoTA，这是一个韩国红队数据集，包含了609个挑战LLMs的真实用户制作的对话，旨在捕捉越狱尝试。我们利用了一个在韩国类似Reddit社区中自我发布的用户与聊天机器人的对话，其中包含了特定的测试和游戏意图与社交聊天机器人的交互。通过这些提示，我们旨在评估LLMs识别对话类型和用户测试目的的能力，从而推导出缓解越狱风险的聊天机器人设计启示。我们的数据集将通过GitHub公开提供。**|
|**2025-01-28**|**FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data**|Deren Lei et.al.|[2501.17144](http://arxiv.org/abs/2501.17144)|**[link](https://github.com/derenlei/factcg)**|**先前的研究在训练用于检测大型语言模型（LLMs）幻觉的基于事实性的分类模型时，依赖于公开的自然语言推理（NLI）数据和合成数据。然而，传统的NLI数据集并不适用于文档级别的推理，这对于检测LLM幻觉至关重要。最近的方法涉及通过迭代从文档中移除句子并使用基于LLM的提示来注事实性来进行文档级别的合成数据生成。尽管这种方法是有效的，但对于长文档来说计算成本高昂，并且受限于LLM的能力。在这项工作中，我们分析了现有用于最先进的模型中的合成训练数据与真实的LLM输出声明之间的差异。根据我们的发现，我们提出了一种新的合成数据生成方法CG2C，该方法利用从文档中提取的上下文图上的多跳推理。我们的事实检查模型FactCG展示了使用相同的骨干模型进行更连贯推理的改进性能。实验表明，它甚至在LLM-Aggrefact基准上超过了GPT-4-o，同时模型尺寸小得多。**|
|**2025-01-28**|**ASTRAL: Automated Safety Testing of Large Language Models**|Miriam Ugarte et.al.|[2501.17132](http://arxiv.org/abs/2501.17132)|null|大型语言模型（LLMs）最近引起了广泛关注，因为它们能够理解和生成复杂的人类内容。然而，确保其安全性至关重要，因为它们可能会提供有害和不安全的响应。现有的LLM测试框架解决了各种与安全相关的问题（例如毒品、恐怖主义、虐待动物），但常常面临不平衡和过时数据集的挑战。在本文中，我们提出了ASTRAL工具，该工具自动化生成和执行测试用例（即提示）以测试LLMs的安全性。首先，我们引入了一种新颖的黑盒覆盖标准，用于生成平衡且多样的不安全测试输入，涵盖广泛的不安全类别以及不同的语言写作风格和说服性写作技巧。其次，我们提出了一种基于LLM的方法，利用检索增强生成（RAG）、少量提示策略和网络浏览来生成最新的测试输入。最后，类似于当前的LLM测试自动化技术，我们利用LLMs作为测试预言者来区分安全和不安全的测试输出，从而实现完全自动化的测试方法。我们对知名LLMs进行了广泛评估，揭示了以下关键发现：(i) GPT3.5在充当测试预言者时表现出色，准确检测到不安全响应，甚至超过了更新的LLMs（如GPT-4）以及专门用于检测不安全LLM输出的LLMs（如LlamaGuard）；(ii) 结果证实，我们的方法可以在相同数量的测试输入下比目前使用的静态数据集多发现近两倍的不安全LLM行为；(iii) 我们的黑盒覆盖标准结合网络浏览可以有效地指导LLM生成最新的不安全测试输入，显著增加了不安全LLM行为的数量。|
|**2025-01-28**|**Optimizing Large Language Model Training Using FP4 Quantization**|Ruizhe Wang et.al.|[2501.17116](http://arxiv.org/abs/2501.17116)|null|随着训练大型语言模型（LLMs）的计算需求不断增长，需要更高效的方法。量化训练作为一种有前景的解决方案，通过启用低比特算术运算来降低成本。尽管FP8精度已经证明是可行的，但利用FP4仍是一个挑战，因为存在显著的量化误差和有限的表示能力。本文介绍了首个针对LLMs的FP4训练框架，解决了这些挑战，并提出了两个关键技术：可微量化估计器以实现精确的权重更新，以及异常值钳位和补偿策略以防止激活崩溃。为了确保稳定性，该框架结合了混合精度训练方案和向量级量化。实验结果表明，我们的FP4框架实现了与BF16和FP8相当的准确性，降解最小，并且能够有效扩展到参数量高达13B的LLMs，在最多100B的令牌上进行训练。随着支持FP4的下一代硬件的出现，我们的框架为高效的超低精度训练奠定了基础。|
|**2025-01-28**|**Unlocking Transparent Alignment Through Enhanced Inverse Constitutional AI for Principle Extraction**|Carl-Leander Henneking et.al.|[2501.17112](http://arxiv.org/abs/2501.17112)|null|传统的对齐大型语言模型（LLMs）的方法，如基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO），依赖于隐含的原则，限制了可解释性。宪法人工智能（CAI）提供了一个明确的、基于规则的框架来指导模型输出。在此基础上，我们改进了逆宪法人工智能（ICAI）算法，该算法从偏好数据集中提取宪法。通过改进原则生成、聚类和嵌入过程，我们的方法提高了从合成和真实数据集中提取的原则的准确性和泛化能力。虽然上下文对齐带来了适度的改进，但我们的结果突显了这些原则在促进更透明和适应性强的对齐方法方面的潜力，为未来的进步提供了一个有前景的方向，超越传统的微调方法。|
|**2025-01-28**|**Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on Advanced Mathematical Problem-Solving**|Evgenii Evstafev et.al.|[2501.17084](http://arxiv.org/abs/2501.17084)|null|大型语言模型（LLMs）在许多自然语言任务上表现出色，但在复杂的数学问题解决方面尤其在符号推理和保持输出一致性方面存在困难。本研究使用MATH数据集中945个竞赛级别的问题评估了参数量在7到80亿之间的10个LLMs。重点在于它们生成可执行的Python代码作为其推理过程的一部分的能力，涉及超过9450次代码执行。研究引入了一个使用mistral-large-2411进行评分的评估框架，该框架有助于解决数学符号不一致的问题。同时，研究还考察了逐个输出标记再生对改进结果的影响。研究结果显示，顶级商业模型（gpt-4o-mini，得分为83.7%）与效果最差的开源模型（open-codestral-mamba:v0.1，得分为49.2%）之间存在显著的34.5%性能差距。这种差异在数论等复杂领域尤为明显。虽然逐个标记的再生使模型llama3.1:8b的准确性提高了0.8%，但也减少了36.7%的代码执行时间，这突显了效率和精度之间的权衡。研究还注意到一个一致的趋势，即问题难度越大，所有模型的准确性越低。尽管采用了受控的执行环境，但不到1%的生成代码被认为是不安全的，并且有3.17%的问题在10次尝试后仍未解决，这表明混合推理方法可能是有益的。|
|**2025-01-28**|**Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models**|Minghan Li et.al.|[2501.17039](http://arxiv.org/abs/2501.17039)|null|近年来，大型语言模型（LLMs）在信息检索等多个领域展示了强大的能力。大多数先前的做法涉及利用这些模型单独为每个查询、每个段落或每个文档创建一个嵌入表示，这一策略由检索增强生成（RAG）框架所体现和使用。尽管这种方法已经证明是有效的，但我们认为它未能充分捕捉长文档的细微复杂性，因为它依赖于相对粗粒度的表示。为了解决这一局限性，我们引入了一种新颖的细粒度方法，旨在提高长文档相关性评分的准确性。我们的方法首先将长文档分割成块，然后使用LLM为每一块生成嵌入表示，以便与查询表示进行匹配。在计算相关性得分时，我们通过加权求和的方法聚合查询-块的相关性得分，从而得出查询与整个文档的综合得分。尽管这种方法看似简单，但我们的实验结果表明，它比标准表示方法表现更好，并且实现了显著的嵌入生成延迟减少。此外，通过仔细优化成对损失函数，我们还取得了更好的性能。|
|**2025-01-28**|**Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies**|Manojkumar Parmar et.al.|[2501.17030](http://arxiv.org/abs/2501.17030)|null|大型语言模型（LLMs）在推理、对齐和特定任务性能方面取得了显著进展。然而，确保这些系统无害性仍然是一个关键挑战，尤其是在先进的模型如DeepSeek-R1中。本文研究了以强化学习（RL）为主要方法来减少DeepSeek-R1有害输出的局限性，并将其与有监督微调（SFT）进行比较。虽然RL提高了推理能力，但它面临诸如奖励攫取、泛化失败、语言混合和高计算成本等挑战。我们提出了结合RL和SFT的混合训练方法，以实现稳健的无害性减少。还提出了关于负责任地部署DeepSeek-R1的使用建议和未来方向。|
|**2025-01-28**|**Automated Refactoring of Non-Idiomatic Python Code: A Differentiated Replication with LLMs**|Alessandro Midolo et.al.|[2501.17024](http://arxiv.org/abs/2501.17024)|**[link](https://github.com/alemidolo/gptidiomrefactoring)**|在Python生态系统中，由于惯用构造的表达性、提高生产力甚至效率，这些惯用构造得到了推广，尽管关于熟悉度或可理解性问题存在争议。最近的研究贡献提出了一些方法——基于静态代码分析和转换——以自动识别并执行非惯用代码到惯用代码的重构机会。鉴于大型语言模型（LLM）在代码相关任务中的最新成果，本文呈现了一项复制研究的结果，我们调查了GPT-4在推荐和建议惯用重构操作方面的有效性。我们的结果显示，GPT-4不仅能够有效地识别惯用构造，而且经常超过基准，在现有基线失败的地方提出重构操作。对随机样本的手动分析显示了所获得建议的正确性。我们的发现强调了LLM在实现过去需要复杂代码分析才能完成的任务方面的潜力。|
|**2025-01-28**|**Mobile Manipulation Instruction Generation from Multiple Images with Automatic Metric Enhancement**|Kei Katsumata et.al.|[2501.17022](http://arxiv.org/abs/2501.17022)|**[link](https://github.com/keio-smilab24/mmig)**|我们研究了根据目标物体图像和容器图像生成自由格式移动操作指令的问题。传统的图像描述模型通常针对单个图像进行优化，因此无法生成适当的指令。在这项研究中，我们提出了一种能够处理目标物体和容器的模型，以生成移动操作任务的自由格式指令句子。此外，我们引入了一种新颖的训练方法，该方法有效结合了基于学习和基于n元语法的自动评估指标的分数作为奖励。这种方法使模型能够学习单词之间的共现关系以及适当的同义词。结果表明，我们提出的方法在标准自动评估指标上优于包括代表性多模态大型语言模型在内的基线方法。此外，物理实验显示使用我们的方法来增强语言指令的数据可以提高现有多模态语言理解模型在移动操作中的性能。|
|**2025-01-28**|**Large Language Models for Code Generation: The Practitioners Perspective**|Zeeshan Rasheed et.al.|[2501.16998](http://arxiv.org/abs/2501.16998)|**[link](https://github.com/gpt-laboratory/llm-evaluation)**|大型语言模型（LLMs）作为编码助手已经出现，能够根据自然语言提示生成源代码。随着LLMs在软件开发中的日益普及，学术研究和基于行业的项目正在开发各种工具、基准和指标来评估LLM生成代码的有效性。然而，缺乏通过以实证为基础的方法并结合从业者观点来评估功能、语法和准确性的真实世界应用的解决方案。为了解决这一差距，我们提出并开发了一个多模型统一平台，该平台可以根据自然语言提示生成并执行代码。我们对来自11个国家、四大洲的60名软件从业人员进行了调查，他们在不同的专业角色和领域工作，以评估每个模型的可用性、性能、优势和局限性。结果展示了从业者的反馈和见解，涉及LLMs在软件开发中的使用，包括它们的优势和劣势、基准和指标忽略的关键方面，以及对其实用性的更广泛理解。这些发现可以帮助研究人员和从业者就如何系统地选择和使用LLMs在软件开发项目中做出明智的决策。未来的研究将集中在将更多样化的模型集成到所提出的系统中，纳入更多的案例研究，并进行开发者访谈，以获得有关LLM驱动的软件开发更深入的经验见解。|
|**2025-01-27**|**Evaluating The Performance of Using Large Language Models to Automate Summarization of CT Simulation Orders in Radiation Oncology**|Meiyun Cao et.al.|[2501.16309](http://arxiv.org/abs/2501.16309)|null|本研究旨在利用大型语言模型（LLM）自动生成CT模拟订单的总结并评估其性能。方法：从机构的Aria数据库中收集了607名患者的CT模拟订单。使用本地托管的Llama 3.1 405B模型通过应用程序编程接口（API）服务从CT模拟订单中提取关键词并生成总结。下载的CT模拟订单根据治疗模式和疾病部位被分为七个组。为每个组开发了定制的指令提示，与治疗师合作指导Llama 3.1 405B模型生成总结。相应的总结的真实情况由治疗师通过仔细审查每个CT模拟订单手动得出，并由治疗师进行验证。评价中，治疗师以验证后的真实情况作为参考评估LLM生成的总结的准确性。结果：约98%的LLM生成的总结在准确性方面与人工生成的真实情况相符。我们的评估显示，LLM生成的总结在格式上具有一致性并且可读性增强，相比治疗师生成的总结有所提升。这种方法在所有组别中均表现出一致的性能，不受治疗模式或疾病部位的影响。结论：本研究表明Llama 3.1 405B模型在提取关键词和总结CT模拟订单方面的高精度和一致性，表明LLM在此任务中有巨大潜力，能够帮助减轻治疗师的工作负担并提高工作流程效率。|
|**2025-01-27**|**RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based Video Event Retrieval**|Long Nguyen et.al.|[2501.16303](http://arxiv.org/abs/2501.16303)|null|检索视频中的事件以响应文本查询变得越来越具有挑战性，因为多媒体内容的快速增长。现有的基于文本的视频事件检索方法通常过分关注对象级别的描述，而忽略了上下文信息的重要作用。当查询缺乏足够的上下文时，例如缺少位置细节或存在模糊的背景元素时，这一局限性尤为明显。为了解决这些挑战，我们提出了一种名为RAPID（检索增强并行推断草稿）的新系统，该系统利用大型语言模型（LLMs）和基于提示的学习来语义上纠正和丰富用户查询，并添加相关的上下文信息。这些丰富的查询随后通过并行检索处理，并通过评估步骤选择与原始查询最匹配的结果。我们在自定义开发的数据集上进行了广泛的实验，结果表明RAPID在处理上下文不完整的查询方面显著优于传统检索方法。我们的系统通过参加2024年胡志明市人工智能挑战赛成功验证了其速度和准确性，在该比赛中它从超过300小时的视频中检索到了事件。进一步的评估表明，与比赛组织者提出的基线相比，RAPID表现出了更高的有效性，突显了我们方法的优势和稳健性。|
|**2025-01-27**|**Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With Configurable Depth and Width**|Zheng Liu et.al.|[2501.16302](http://arxiv.org/abs/2501.16302)|null|大型语言模型（LLMs）提供了强大的基础来进行细粒度的文本重排序。然而，由于计算带宽的限制，它们在现实中往往是不可行的。在这项工作中，我们提出了一种名为“套娃重排序器”的灵活架构，旨在根据用户配置定制模型层和每层序列长度。因此，基于LLM的重排序器可以适用于各种现实情况。这种增加的灵活性可能会导致精度损失。为了解决这个问题，我们引入了一系列优化性能的技术。首先，我们提出了级联自蒸馏技术，其中每个子架构学习从其超级组件中保留精确的重排序性能，这些预测可以作为平滑且信息丰富的教师信号加以利用。其次，我们设计了一种因子补偿机制，其中两个协作的低秩适应模块，垂直和水平，被联合用于补偿由任意组合的层和序列压缩导致的精度损失。我们在MSMARCO的数据集上进行了全面的实验，包括所有来自BEIR基准的公开数据集。在我们的实验中，套娃重排序器在各种形式的压缩和不同的应用场景下，显著优于现有方法，同时有效保持了其卓越的性能。|
|**2025-01-27**|**Large Models in Dialogue for Active Perception and Anomaly Detection**|Tzoulio Chamiti et.al.|[2501.16300](http://arxiv.org/abs/2501.16300)|**[link](https://github.com/Tzoulio/Large_Models_Dialogue_for_Active_Perception)**|自主空中监测是一项重要的任务，旨在收集人类可能难以到达区域的信息。与此同时，这项任务经常需要从较远的距离识别异常情况或过去未曾遇到的情况。在本文中，我们提出了一种新颖的框架，该框架利用大型语言模型（LLMs）提供的高级能力主动收集信息并执行异常检测。为此，我们提出了基于模型对话的方法，其中两个深度学习模型进行对话以主动控制无人机提高感知和异常检测的准确性。我们在高保真仿真环境中进行实验，在该环境中，LLM被提供了预定的一组自然语言运动命令映射到可执行代码函数。此外，我们部署了一个多模态视觉问答（VQA）模型负责视觉问答和图像描述。通过让这两个模型进行对话，LLM会提出探索性问题，同时操控无人机飞入场景的不同部分，提供了一种实现主动感知的新方法。通过利用LLMs的推理能力，我们输出了对场景的改进且详细的描述，超越了现有的静态感知方法。除了信息收集外，我们的方法还用于异常检测，结果表明所提出方法在告知和警示潜在危险方面的有效性。|
|**2025-01-27**|**FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers**|Renshan Zhang et.al.|[2501.16297](http://arxiv.org/abs/2501.16297)|null|高分辨率视觉输入的引入使多模态大语言模型（MLLMs）在现实世界任务中的视觉感知能力得到增强。然而，大多数现有的高分辨率MLLMs依赖于基于裁剪的方法来处理图像，这导致了碎片化的视觉编码和冗余标记的急剧增加。为了解决这些问题，我们提出了FALCON模型。FALCON引入了一种新颖的视觉寄存器技术，以同时实现以下两个目标：1）在视觉编码阶段消除冗余标记。为了直接解决视觉编码输出中的冗余问题，我们提出了一种基于寄存器的表示压缩（ReCompact）机制。该机制引入了一组可学习的视觉寄存器，旨在自适应地聚合关键信息并丢弃冗余。这使得编码器能够生成更紧凑的视觉表示，并且输出的标记数量最少，从而无需额外的压缩模块。2）确保视觉编码的连续性。为了解决碎片化视觉输入可能引起的编码错误，我们开发了一种寄存器交互注意力（ReAtten）模块。该模块通过使视觉寄存器之间进行交互，促进了子图像间有效且高效的信 息交换，确保了视觉语义在整个编码过程中的连续性。我们在高分辨率基准数据集上对FALCON进行了全面的实验，涵盖了广泛的场景。实验结果表明，FALCON表现出色，视觉标记减少了9倍和16倍。|
|**2025-01-27**|**Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**|Jing Zhang et.al.|[2501.16282](http://arxiv.org/abs/2501.16282)|null|理解脑部疾病对于准确的临床诊断和治疗至关重要。近期多模态大型语言模型（MLLMs）的研究进展提供了一种通过文本描述来解读医学图像的有前景的方法。然而，先前的研究主要集中在二维医学图像上，忽略了三维图像中更丰富的空间信息，并且单模态方法由于忽视了其他模态中包含的关键临床信息而受到限制。为了解决这一问题，本文提出了一种名为Brain-Adapter的新方法，该方法引入了一个额外的瓶颈层以学习新知识并将其注入到原始预训练的知识中。其主要思想是引入一个轻量级的瓶颈层，在捕捉关键信息的同时减少参数训练的数量，并利用对比语言图像预训练（CLIP）策略在统一表示空间内对齐多模态数据。大量的实验表明，我们的方法在整合多模态数据方面非常有效，显著提高了诊断准确性，同时没有带来高计算成本，这突显了其在增强现实世界诊断工作流程方面的潜力。|
|**2025-01-27**|**Do LLMs Have Visualization Literacy? An Evaluation on Modified Visualizations to Test Generalization in Data Interpretation**|Jiayi Hong et.al.|[2501.16277](http://arxiv.org/abs/2501.16277)|**[link](https://github.com/vaderasu/llm4viz-experiments)**|在本文中，我们评估了两个著名的大型语言模型（LLM）的可视化素养：OpenAI的生成预训练变压器（GPT），即ChatGPT的后端，以及Google的Gemini（以前称为Bard），以建立评估其可视化能力的基准。尽管LLM在生成图表描述、标题和设计建议方面显示出潜力，但它们在评估可视化方面的潜力仍未得到充分探索。从人类那里收集数据进行评估在时间和金钱上一直是可视化研究的瓶颈，如果LLM能够在某种程度上作为评估者发挥作用，它们可能成为一个重要的资源。为了探讨将LLM用于可视化评估过程的可行性，我们研究了这些模型是否具备可视化素养——这是它们在该领域有效应用的关键因素。我们进行了系列实验，使用修改后的53项可视化素养评估测试（VLAT）对GPT-4和Gemini进行了测试。我们的研究结果表明，我们所探索的LLM目前未能达到与VLAT中报告的一般公众相同的可视化素养水平，并且LLM在回答问题时主要依赖其预先存在的知识而不是利用可视化提供的信息。|
|**2025-01-27**|**URAG: Implementing a Unified Hybrid RAG for Precise Answers in University Admission Chatbots -- A Case Study at HCMUT**|Long Nguyen et.al.|[2501.16276](http://arxiv.org/abs/2501.16276)|null|随着人工智能特别是自然语言处理的快速发展，大型语言模型（LLMs）在教育问答系统中变得至关重要，尤其是在大学录取聊天机器人方面。为了增强这些系统，已经开发了检索增强生成（RAG）等先进技术，通过整合特定大学的数据，使LLMs能够提供有关招生和学术咨询的信息性回复。然而，这些增强的RAG技术通常涉及高昂的运营成本，并且需要训练复杂的专门模块，这给实际部署带来了挑战。此外，在教育背景下，提供准确的答案以防止错误信息至关重要，这是一个LLM系统在没有适当策略和方法的情况下难以完成的任务。在本文中，我们介绍了统一RAG（URAG）框架，这是一种混合方法，显著提高了对关键查询的回答准确性。实验结果表明，URAG使我们的内部轻量级模型能够与最先进的商业模型相媲美。此外，为了验证其实际应用性，我们在我们的教育机构进行了案例研究，该研究得到了积极的反馈和赞誉。这项研究不仅证明了URAG的有效性，还突显了它在教育环境中实际实施的可行性。|
|**2025-01-27**|**A foundation model for human-AI collaboration in medical literature mining**|Zifeng Wang et.al.|[2501.16255](http://arxiv.org/abs/2501.16255)|null|系统性文献回顾对于循证医学至关重要，需要对临床试验出版物进行全面分析。然而，由于缺乏在广泛治疗领域和多样化任务中的训练与评估，人工智能（AI）模型在医学文献挖掘中的应用受到了限制。我们在此介绍了LEADS，这是一种用于研究搜索、筛选和从医学文献中提取数据的AI基础模型。该模型是在LEADSInstruct上训练的，其数据点来源于21,335篇系统综述、453,625篇临床试验出版物和27,015个临床试验注册信息，共计633,759个指令数据点。我们展示了LEADS在六个任务上比四个最先进的通用大型语言模型（LLMs）表现更优。此外，LEADS通过在专家请求后提供支持性参考来增强专家工作流程，从而简化了流程，同时保持了高质量的结果。一项涉及来自14个不同机构的16名临床医生和医学研究人员的研究显示，与单独工作的专家相比，在研究选择过程中，使用LEADS的专家召回率达到了0.81，而未使用LEADS的专家召回率为0.77，时间节省了22.6%。在数据提取任务中，使用LEADS的专家准确率达到0.85，而不使用LEADS的准确率为0.80，时间节省了26.9%。这些发现突显了专门的医学文献基础模型相较于通用模型的潜力，表明当将其整合到专家的工作流程中进行医学文献挖掘时，可以带来显著的质量和效率提升。|
|**2025-01-27**|**Multi-Agent Geospatial Copilots for Remote Sensing Workflows**|Chaehong Lee et.al.|[2501.16254](http://arxiv.org/abs/2501.16254)|null|我们提出了GeoLLM-Squad，这是一种地理空间协作者，它在遥感（RS）工作流程中引入了新颖的多智能体范式。与依赖于单一大型语言模型（LLM）的现有单智能体方法不同，GeoLLM-Squad将代理的编排与地理空间任务解决分开，将遥感任务委托给专门的子智能体。基于开源的AutoGen和GeoLLM-Engine框架，我们的工作实现了多样化应用的模块化集成，涵盖城市监测、森林保护、气候分析以及农业研究。我们的结果显示，虽然单一智能体系统难以随着遥感任务复杂性的增加而扩展，但GeoLLM-Squad保持了稳健的性能，在智能体正确性方面比最先进的基线提高了17%。我们的研究结果突显了多智能体AI在推进遥感工作流程方面的潜力。|
|**2025-01-24**|**HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation**|Xin Zhou et.al.|[2501.14729](http://arxiv.org/abs/2501.14729)|**[link](https://github.com/lmd0311/hermes)**|**驾驶世界模型（DWMs）在自动驾驶中变得至关重要，因为它们能够预测未来的场景。然而，现有的DWMs仅限于场景生成，无法结合场景理解，这涉及到对驾驶环境的解释和推理。在这篇论文中，我们提出了一种名为HERMES的统一驾驶世界模型。我们在驾驶场景中通过一个统一框架无缝地整合了三维场景理解和未来场景演化（生成）。具体来说，HERMES利用鸟瞰图（BEV）表示法来整合多视角的空间信息，同时保持几何关系和交互。我们还引入了世界查询，通过因果注意力机制将世界知识融入到BEV特征中，从而实现大语言模型（LLM）中的上下文丰富性，以支持理解和生成任务。我们在nuScenes和OmniDrive-nuScenes数据集上进行了全面研究，验证了我们方法的有效性。HERMES达到了最先进的性能，将生成误差降低了32.4%，并提高了理解指标如CIDEr，提升了8.0%。该模型和代码将在https://github.com/LMD0311/HERMES公开发布。**|
|**2025-01-24**|**Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**|Ipek Baris Schlicht et.al.|[2501.14719](http://arxiv.org/abs/2501.14719)|null|公平获取可靠的健康信息对公共卫生至关重要，但在线健康资源的质量因语言而异，引发了关于大型语言模型（LLMs）在医疗保健领域中不一致性的担忧。在这项研究中，我们考察了大型语言模型对不同语言的健康相关问题的回答一致性，涉及英语、德语、土耳其语和汉语。我们主要扩展了HealthFC数据集，通过按疾病类型分类健康相关问题，并用土耳其语和汉语将其多语言范围扩大。我们揭示了回答中的显著不一致，这可能导致传播医疗保健错误信息。我们的主要贡献有：1）一个包含疾病类别元信息的多语言健康相关查询数据集；2）一种新的基于提示的评估工作流程，通过解析可以实现两种语言之间的子维度比较。我们的发现突显了在多语言环境中部署基于LLM的工具的关键挑战，并强调了改进跨语言对齐以确保准确和公平的医疗保健信息的需求。|
|**2025-01-24**|**Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models**|Naihao Deng et.al.|[2501.14717](http://arxiv.org/abs/2501.14717)|null|近期在自然语言处理领域的进展利用了指令调优来增强大型语言模型（LLMs）以应对与表格相关的任务。然而，之前的工作使用了不同的基础模型和不同的训练数据集，缺乏一个公平的比较基准。为了解决这个问题，我们对Mistral、OLMo和Phi家族的基础模型进行了微调，并使用现有的公共训练数据集。我们的复现结果达到了与现有表格LLMs相当或更优的性能，在Hitab表格问答数据集上建立了新的最先进性能。更重要的是，通过系统的域外评估，我们区分了训练数据和基础模型各自的贡献，提供了它们各自影响的见解。此外，我们在通用基准上评估了表格特定指令调优的效果，揭示了专业化和通用化之间的权衡。|
|**2025-01-24**|**FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing**|James Seale Smith et.al.|[2501.14713](http://arxiv.org/abs/2501.14713)|null|大型语言模型（LLMs）在自然语言处理（NLP）中的迅速普及对能够在内存受限设备上高效部署的技术提出了迫切需求，同时不牺牲性能。我们提出了一种方法来剪枝这些LLMs，该方法根据重要性评分选择性地剪枝模型块，并用低参数替换策略替代它们。具体来说，我们提出了一种原则性的度量标准，使用基于权重共享机制的低秩适配器从模型的未剪枝部分替换每个剪枝块。此外，我们通过输出特征规范化以及基于低秩SVD重构的适配器初始化方案来促进这些替换块的学习。经验评估表明，在压缩率为30%时，我们的方法在5/6个基准测试中取得了最先进的性能，在压缩率为40%时，在6/6个基准测试中取得了最先进的性能。我们还证明了我们的方法可以扩展较小的模型，在仅使用约0.3%的扩展训练令牌的情况下，在6/6个基准测试中提升性能，且额外参数成本极小。|
|**2025-01-24**|**The Karp Dataset**|Mason DiCicco et.al.|[2501.14705](http://arxiv.org/abs/2501.14705)|null|理解大型语言模型（LLMs）的数学推理能力是人工智能研究中的核心议题。这一新兴领域需要创建推理任务的数据集，以用于训练和评估这些模型的性能。为此，我们介绍了Karp数据集：这是首个包含详细NP完全性约简证明的数据集。这些约简的难度不一，从本科课程的简单练习到学术论文中的复杂约简都有所涵盖。我们将最先进的模型在该任务上的表现进行比较，并展示了使用Karp数据集微调对提升推理能力的影响。|
|**2025-01-24**|**Rethinking Table Instruction Tuning**|Naihao Deng et.al.|[2501.14693](http://arxiv.org/abs/2501.14693)|null|近期在表格理解方面的研究主要集中在对大型语言模型（LLMs）进行指令调优以处理与表格相关的任务。然而，现有研究忽视了超参数选择的影响，并且缺乏对这些表格LLMs的域外表格理解能力和通用能力的全面评估。在这篇论文中，我们评估了现有表格LLMs的这些能力，并揭示了它们在域外表格理解和通用能力方面相对于基线模型的显著下降。通过系统分析，我们表明超参数（如学习率）可以显著影响表格特定和通用能力。与现有的表格指令调优工作相反，我们证明较小的学习率和较少的训练实例可以增强表格理解，同时保持通用能力。根据我们的发现，我们介绍了TAMA，这是一种从LLaMA 3.1 8B Instruct指令调优而来的表格LLM，在表格任务上表现与GPT-3.5和GPT-4相当或更优，同时保持了强大的域外泛化能力和通用能力。我们的研究结果强调了通过仔细选择超参数来降低数据标注成本和提高模型开发效率的潜力。|
|**2025-01-24**|**An Empirical Study on LLM-based Classification of Requirements-related Provisions in Food-safety Regulations**|Shabnam Hassani et.al.|[2501.14683](http://arxiv.org/abs/2501.14683)|null|随着工业4.0转型食品行业，实现食品安全法规合规性的软件作用变得日益关键。食品安全法规和其他法律领域一样，大多以技术独立的方式阐述，以确保其持久性和广泛适用性。然而，这种方法导致了法规与现代系统和软件之间的差距，这些系统和软件越来越多地用于实施这些法规。本文旨在实现两个主要目标。首先，我们对食品安全法规进行了基于扎根理论的研究，并开发了一种概念性表征，这些表征与系统和软件需求密切相关。其次，我们检查了两类大型语言模型（LLM）——BERT和GPT在自动分类基于需求相关的食品安全条款方面的有效性。我们的结果显示：(a) 当经过微调时，BERT和GPT家族中表现最佳的模型之间的准确率差异相对较小。然而，在实验中最强大的模型GPT-4o仍然实现了最高的准确率，平均精确率为89%，平均召回率为87%；(b) 使用GPT-4o进行少量样本学习将召回率提高到97%，但精确率降低到65%，这表明微调和少量样本学习之间存在权衡；(c) 尽管我们的训练示例完全来自加拿大法规，基于LLM的分类在美国测试条款上表现一致良好，表明在监管司法辖区方面具有一定程度的普适性；(d) 对于我们的分类任务，LLM显著优于使用长短期记忆（LSTM）网络和自动关键词提取构建的简单基线。|
|**2025-01-24**|**Diffusion based Text-to-Music Generationwith Global and Local Text based Conditioning**|Jisi Zhang et.al.|[2501.14680](http://arxiv.org/abs/2501.14680)|null|基于扩散的文本到音乐模型通过文本描述生成相应的音乐。通常，这类模型使用UNet架构，并根据从预训练的大语言模型或跨模态音频-语言表示模型生成的文本嵌入来条件化。这项工作提出了一种基于扩散的文本到音乐方法，其中UNet根据（i）单模态语言模型（例如T5）通过交叉注意力和（ii）跨模态音频-语言表示模型（例如CLAP）通过特征性线性调制(FiLM)进行条件化。该扩散模型旨在利用来自T5的局部文本表示以及来自CLAP的全局表示。此外，我们提出了修改方案，通过称为均值池化和自注意力池化的机制从T5提取全局和局部表示。这种方法减少了对额外编码器（例如CLAP）的需求以提取全局表示，从而减少了所需的模型参数数量。我们的结果显示，将CLAP全局嵌入与T5局部嵌入结合使用增强了文本适应性（KL=1.47），相比于仅依赖T5局部嵌入的基础模型（KL=1.54）。或者，通过所提出的均值池化方法直接从T5局部嵌入中提取全局文本嵌入，在生成质量方面表现出色（FAD=1.89），尽管在文本适应性上略低（KL=1.51）相较于同时根据CLAP和T5文本嵌入条件化的模型（FAD=1.94和KL=1.47）。我们提出的方法不仅高效而且紧凑，所需参数数量较少。|
|**2025-01-24**|**MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**|Yixing Jiang et.al.|[2501.14654](http://arxiv.org/abs/2501.14654)|**[link](https://github.com/stanfordmlgroup/medagentbench)**|**近期的大语言模型（LLMs）在作为代理方面展现出显著的进步，尤其是在其规划和工具利用能力方面，这超越了它们传统的聊天机器人角色。这些代理能够应对在高层次上指定的任务。然而，目前缺乏一个标准化的数据集来评估LLMs在医疗应用中的代理能力，这使得在复杂的互动医疗环境中评估LLMs具有挑战性。为了解决这一不足，我们引入了MedAgentBench，这是一个广泛的评估套件，旨在评估大型语言模型在医疗记录背景下的代理能力。MedAgentBench包含了由人类医生编写的100个患者特定的临床衍生任务，涵盖10个类别，以及100个患者的现实档案，包含超过700,000个数据元素，一个符合FHIR标准的交互式环境，以及相应的代码库。该环境使用现代EMR系统中使用的标准API和通信基础设施，因此可以轻松迁移到实际的EMR系统中。MedAgentBench提供了一个未饱和的以代理为中心的基准，当前最先进的LLMs在这项基准上表现出一定程度的成功能力。最佳模型（GPT-4o）的成功率为72%。然而，仍然有很大的提升空间，以便为社区提供优化的方向。此外，在任务类别之间存在显著的性能差异。MedAgentBench建立了这一点，并且在https://github.com/stanfordmlgroup/MedAgentBench 公开可用，为模型开发者提供了一个有价值的框架，用于跟踪进展并推动大型语言模型在医学领域内代理能力的持续改进。**|
|**2025-01-24**|**Investigating the (De)Composition Capabilities of Large Language Models in Natural-to-Formal Language Conversion**|Ziyao Xu et.al.|[2501.14649](http://arxiv.org/abs/2501.14649)|**[link](https://github.com/xzy-xzy/dedc)**|**为了实现通用且稳健的自然语言到形式语言转换（N2F），大规模语言模型（LLMs）在面对不熟悉的正式语言时需要具备强大的分解和组合能力，并能够应对组合差距和反直觉符号名称。为调查LLMs是否具备这一基本的N2F分解和组合能力，我们提出了DEDC框架。该框架半自动地执行样本和任务构建，使得可以解耦评估LLMs在N2F中的分解和组合能力。基于此框架，我们评估并分析了最先进的LLMs，主要发现包括：(1) LLMs在分解和组合方面都存在不足；(2) LLMs表现出广泛类型的错误，这些错误可归因于对自然语言理解的不足以及对符号系统的掌握和使用不当；(3) 组合差距和反直觉符号名称均影响LLMs的分解和组合能力。我们的工作为研究LLMs在N2F中的分解和组合基本能力提供了新视角。对缺陷及其原因的详细分析有助于后续改进LLMs。**|
|**2025-01-23**|**CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation**|Guofeng Cui et.al.|[2501.13927](http://arxiv.org/abs/2501.13927)|null|大型语言模型（LLMs）在自然语言处理任务中展现出巨大潜力，但将其应用于机器翻译（MT）仍然面临挑战，这主要是因为它们的预训练数据偏向于英语，并且强化学习从人类反馈（RLHF）的复杂性。直接偏好优化（DPO）作为一种更简单高效的替代方法出现，但其性能严重依赖于偏好数据的质量。为了解决这一问题，我们提出了基于信心奖励驱动的偏好优化（CRPO），这是一种新颖的方法，它结合了奖励分数和模型置信度以改进微调的数据选择。CRPO选择模型不确定或表现不佳的具有挑战性的句子对，从而实现更有效的学习。虽然CRPO主要设计用于LLMs，但它也适用于像NLLB这样的编码器-解码器模型，展示了其多功能性。实证结果表明，CRPO在翻译准确性和数据效率方面均优于现有方法，如RS-DPO、RSO和MBR得分。|
|**2025-01-23**|**Analysis of Indic Language Capabilities in LLMs**|Aatman Vaidya et.al.|[2501.13912](http://arxiv.org/abs/2501.13912)|null|本报告评估了文本输入文本输出的大语言模型（LLMs）理解和生成印度语言的能力。该评估用于识别和优先考虑适合纳入安全基准的印度语言。我们通过审查现有的评估研究和数据集以及支持印度语言的二十八个LLM来进行这项研究。我们根据训练数据、模型和数据许可、访问类型以及模型开发者的不同来分析这些LLM。我们还比较了在评估数据集上各印度语言的表现，并发现显著的性能差异。印地语是模型中最广泛使用的语言。虽然模型性能大致与前五种语言的使用人数成正比，但之后的评估则有所不同。|
|**2025-01-23**|**Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models**|Linh Tran et.al.|[2501.13904](http://arxiv.org/abs/2501.13904)|null|多模态大型语言模型（LLMs）通过整合文本、图像和音频等多种模态，在革新客户支持和运营方面发挥着关键作用。最近提出的联邦提示学习（FPL）方法结合了预训练的多模态LLMs（如视觉-语言模型）与联邦学习，以创建个性化且保护隐私的AI系统。然而，平衡个性化、泛化和隐私这相互竞争的目标仍然是一个重大挑战。过度个性化可能导致过拟合，从而降低泛化能力，而严格的隐私措施（如差分隐私）可能会妨碍个性化和泛化。在本文中，我们提出了一种差分私有联邦提示学习（DP-FPL）方法来解决这一挑战，通过利用低秩适应方案来捕捉泛化，同时保留一个残差项以保持个性化表达能力。为了确保隐私，我们引入了一种新方法，即对本地提示的两个低秩组件应用局部差分隐私，对全局提示应用全局差分隐私。我们的方法减轻了隐私噪声对模型性能的影响，同时在个性化和泛化之间实现了平衡。广泛的实验表明，我们的方法比其他基准更为有效。|
|**2025-01-23**|**Exploring Finetuned Audio-LLM on Heart Murmur Features**|Adrian Florea et.al.|[2501.13884](http://arxiv.org/abs/2501.13884)|null|大型语言模型（LLMs）在识别和分析人类语音、音乐及环境声音方面表现出色，但它们在理解其他类型声音，尤其是生物医学声音方面的潜力尚未得到充分探索，尽管这一领域受到了极大的科学关注。在这项研究中，我们专注于使用音图（即心音）诊断心血管疾病。大多数现有的深度神经网络（DNN）范式仅限于心脏杂音分类（健康与不健康），而无法预测杂音的其他声学特征，如时间、分级、粗糙度、音高和质量等，这些特征对于帮助医生诊断潜在的心脏状况至关重要。我们提出对音频LLM Qwen2-Audio进行微调，并在PhysioNet CirCor DigiScope音图（PCG）数据集上进行训练，评估其在分类11个专家标注的杂音特征上的表现。此外，我们旨在通过探索一种使用音频表示模型SSAMBA的预处理分割算法来实现更具噪声鲁棒性和泛化能力的系统。结果表明，基于LLM的模型在11个特征中的8个上优于最先进的方法，在其余3个特征上表现相当。此外，该LLM成功地对长尾杂音特征进行了分类，这是以前所有方法都无法完成的任务。这些发现强调了音频LLMs作为辅助人类心脏病学家提高心脏病诊断能力的潜力。|
|**2025-01-23**|**The machine learning platform for developers of large systems**|Alexey Naikov et.al.|[2501.13881](http://arxiv.org/abs/2501.13881)|null|机器学习系统以检索增强生成（RAG）的形式自2021年左右开始稳步发展。可以将RAG视为知识转移的一种形式。在研究的案例中，大型计算系统作为RAG的应用点，其中包括大型语言模型（LLM），作为开发团队的合作伙伴。这种方法在开发过程中以及在使用阶段都有其优势。|
|**2025-01-23**|**A RAG-Based Institutional Assistant**|Gustavo Kuratomi et.al.|[2501.13880](http://arxiv.org/abs/2501.13880)|null|尽管大型语言模型（LLMs）在文本生成方面表现出强大的能力，但在需要访问结构化知识库或特定文档的场景中，它们的表现受限，限制了其在知识密集型任务中的有效性。为了解决这一局限性，开发了检索增强生成（RAG）模型，使生成模型能够将其输入与相关的文档片段结合起来。在本文中，我们设计并评估了一种专门针对圣保罗大学的基于RAG的虚拟助手。我们的系统架构包括两个关键模块：检索器和生成模型。我们对两个组件使用了不同类型的模型，并调整了超参数，如块大小和检索到的文档数量。我们最优的检索器模型达到了5个结果内的准确率为30%，而最有效的生成模型得分为22.04%与真实答案对比。值得注意的是，当正确的文档片段提供给LLMs时，准确性显著提高到54.02%，增加了超过30个百分点。相反，如果没有上下文输入，性能下降到13.68%。这些发现突显了数据库访问在提升LLM性能方面发挥的关键作用。它们也揭示了当前语义搜索方法在准确识别相关文档方面的局限性，并强调了LLMs在生成精确响应方面面临的持续挑战。|
|**2025-01-23**|**On the Reasoning Capacity of AI Models and How to Quantify It**|Santosh Kumar Radha et.al.|[2501.13833](http://arxiv.org/abs/2501.13833)|null|近期大型语言模型（LLMs）在GPQA和MMLU等基准测试中取得了高绩效，但它们在更复杂的推理任务中表现出局限性，这凸显了需要更为严格的评估方法。我们提出了一种新的现象学方法，超越传统的准确率指标来探究模型行为的基本机制，建立了一个可能广泛影响我们分析和理解AI系统的方法框架。以多项选择推理任务中的位置偏差作为案例研究，我们展示了如何通过系统性的扰动揭示模型决策制定的根本方面。为了分析这些行为，我们开发了两种互补的现象学模型：一种概率混合模型（PMM），它将模型响应分解为推理、记忆和猜测成分；以及一种信息论一致性（ITC）分析，量化模型信心与策略选择之间的关系。通过在推理基准上进行的控制实验，我们发现当前模型仍难以进行真正的推理，表面上的成功往往依赖于复杂组合的记忆和模式匹配而非真实的逻辑推理。更根本的是，我们证明仅凭准确率常常会夸大模型的推理能力，因为模型的行为可以通过认知策略相空间中的基本机制来表征，揭示了模型在回应查询时如何动态平衡不同的处理方式。这一框架为实际应用提供了定量标准，使应用程序能够基于策略分布而不是汇总性能指标来指定可靠性阈值。|
|**2025-01-23**|**Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing**|Hao Zhang et.al.|[2501.13831](http://arxiv.org/abs/2501.13831)|null|大型语言模型（LLMs）在文本风格转换和语法错误纠正等重写任务中表现出色。尽管这些任务的输入和输出之间存在大量重叠，但解码成本仍然随着输出长度的增加而增加，无论重叠程度如何。Kaneko和Okazaki（2023）提出了一种与模型无关的编辑跨度表示方法，通过利用输入和输出之间的重叠来压缩重写以节省计算。他们在四项重写任务中报告了近80%的输出长度减少率，并且几乎没有影响准确性。在本文中，我们提出了受基于短语的统计机器翻译启发的替代短语表示方法。我们系统地比较了我们的短语表示方法与其跨度表示方法。我们将LLM重写模型应用于自动语音识别（ASR）后编辑任务，并表明我们的仅目标短语编辑表示方法具有最佳的效率-准确性权衡。在LibriSpeech测试集上，我们的方法在编辑跨度模型和完整重写模型之间的WER差距上闭合了50-60%，同时仅损失了编辑跨度模型10-20%的长度减少率。|
|**2025-01-23**|**Hallucinations Can Improve Large Language Models in Drug Discovery**|Shuzhou Yuan et.al.|[2501.13824](http://arxiv.org/abs/2501.13824)|null|关于大型语言模型（LLMs）在药物发现领域中创造性至关重要的应用，人们对其可能产生的幻觉现象提出了担忧，但这一特性是否能提升模型性能值得探索。本文提出假设，认为幻觉现象能够提高LLMs在药物发现中的表现。为了验证这一假设，我们让LLMs以自然语言描述分子的SMILES字符串，然后将这些描述作为提示的一部分来解决药物发现中的具体任务。通过对七种LLM和五个分类任务进行评估，我们的研究结果证实了这一假设：包含幻觉文本的LLMs可以实现更好的性能。特别地，Llama-3.1-8B相比没有幻觉的基线模型，在ROC-AUC上获得了18.35%的提升。此外，GPT-4o生成的幻觉在不同模型中提供了最为一致的改进效果。同时，我们进行了实证分析和案例研究，以探讨影响性能的关键因素及其背后的原因。本研究揭示了幻觉现象对LLMs潜在用途的价值，并为未来利用LLMs进行药物发现的研究提供了新的视角。|
|**2025-01-23**|**Large Language Model driven Policy Exploration for Recommender Systems**|Jie Wang et.al.|[2501.13816](http://arxiv.org/abs/2501.13816)|null|近期推荐系统（RS）的研究引入了强化学习（RL），将推荐问题构架为马尔可夫决策过程（MDP）。然而，基于离线数据训练的RL策略在动态在线环境中容易出现分布偏移。此外，过度关注短期相关物品的开发可能会阻碍探索，导致次优推荐并影响长期用户收益。基于在线RL的推荐系统在生产部署中也面临挑战，因为这可能使用户暴露于未经训练或不稳定的策略下。大型语言模型（LLM）提供了一种解决方案，即通过预训练策略来模拟用户目标和偏好，以提高在线环境中的初始推荐质量。有效管理分布偏移和平衡探索对于改进基于RL的推荐系统至关重要，尤其是当利用基于LLM的预训练时。为了解决这些挑战，我们提出了一种交互增强型学习策略（iALP），该策略利用从LLM中提取的用户偏好。我们的方法包括用用户状态提示LLM以提取项目偏好，根据反馈学习奖励，并使用演员-评论家框架更新RL策略。此外，为了在在线场景中部署iALP，我们引入了一个自适应变体A-iALP，它实施了一种简单的微调策略（A-iALP_{ft}），以及一种自适应方法（A-iALP_{ap}），旨在缓解政策受损和探索不足的问题。实验结果表明，在三个模拟环境中，A-iALP带来了显著的性能提升。|
|**2025-01-22**|**Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment**|Melissa Kazemi Rad et.al.|[2501.13080](http://arxiv.org/abs/2501.13080)|null|大型语言模型（LLMs）展示了强大的能力，使其在各种应用中具有价值，包括对话AI产品。确保这些产品的安全性和可靠性至关重要，以减轻其对恶意用户交互的漏洞，这可能导致重大风险和声誉影响。在这项工作中，我们对不同用作输入审查指南的LLMs的思维链（CoT）响应的微调和对齐效果进行了全面研究。我们系统地探索了各种调整方法，利用少量训练数据来适应这些模型作为代理防御机制，以检测恶意输入并为其裁决提供推理，从而防止对话代理被利用。我们严格评估了不同调整策略的有效性和鲁棒性，以跨多种对抗性和恶意查询类型进行泛化。我们的实验结果概述了针对各种有害输入查询量身定制的对齐过程的潜力，即使在资源有限的情况下也是如此。这些技术显著提高了对话AI系统的安全性，并提供了部署更安全、更可信的AI驱动交互的可行框架。|
|**2025-01-22**|**Does Table Source Matter? Benchmarking and Improving Multimodal Scientific Table Understanding and Reasoning**|Bohao Yang et.al.|[2501.13042](http://arxiv.org/abs/2501.13042)|**[link](https://github.com/bernard-yang/mmsci_table)**|**近期的大语言模型（LLMs）在表格理解方面取得了进展，但依赖于将表格转换成文本序列。虽然多模态大语言模型（MLLMs）能够直接处理视觉信息，但由于固定输入图像分辨率和数值推理能力不足，它们在处理科学表格时面临限制。我们提出了一种全面的框架，用于具有动态输入图像分辨率的多模态科学表格理解和推理。我们的框架包括三个关键组成部分：(1) MMSci-Pre，一个包含52K个科学表格结构识别样本的特定领域表格结构学习数据集，(2) MMSci-Ins，一个包含12K个样本的指令调优数据集，涵盖三个基于表格的任务，(3) MMSci-Eval，一个包含3,114个测试样本的基准，专门设计用于评估数值推理能力。广泛的实验表明，我们的特定领域方法使用52K个科学表格图像实现了优于使用150K个一般领域表格的性能，强调了数据质量的重要性超过数量。我们提出的基于表格的MLLMs具有动态输入分辨率，在一般表格理解和数值推理能力方面表现出显著改进，并且对保留的数据集具有较强的泛化能力。我们的代码和数据可以在https://github.com/Bernard-Yang/MMSci_Table公开获取**|
|**2025-01-22**|**Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament**|Yantao Liu et.al.|[2501.13007](http://arxiv.org/abs/2501.13007)|**[link](https://github.com/thu-keg/pairwiserm)**|**Best-of-N（BoN）采样是一种常用的大型语言模型（LLMs）测试时扩展策略，依赖于奖励模型从多个生成中选择最佳候选解。然而，传统的奖励模型通常分配任意且不一致的分数，限制了它们的有效性。为了解决这个问题，我们提出了一种结合淘汰赛的成对奖励模型（Pairwise RM）。与传统方法不同，给定一个数学问题时，Pairwise RM会同时评估两个候选解的正确性。这种方法消除了任意评分的需要，并通过并行比较实现了候选解之间的交叉验证。在淘汰赛中，Pairwise RM迭代地进行候选解之间的成对比较并逐步淘汰错误的答案。我们构建了一个包含443K个成对比较的大规模数据集\ourdataset，该数据集源自NumiaMath并通过\texttt{gemini-1.5-flash}进行注释，通过监督微调训练了Pairwise RM。实验结果表明，在MATH-500和奥林匹克基准上，该方法显著优于传统的判别奖励模型，并且在前50%的难题上相对提升了40%到60%。**|
|**2025-01-22**|**Large Language Model-Based Semantic Communication System for Image Transmission**|Soheyb Ribouh et.al.|[2501.12988](http://arxiv.org/abs/2501.12988)|null|大型语言模型（LLMs）在理解和生成不同类型的数据，如图像和文本方面取得了显著的成功，这证明了它们在处理和提取不同领域中的语义信息的能力。这一变革性能力为语义通信奠定了基础，使得能够建立高效且智能的通信系统。在这项工作中，我们提出了一种基于OFDM的语义通信框架，用于图像传输。我们提出了一个创新的语义编码器设计，利用了LLMs提取传输数据含义的能力，而不是专注于其原始表示。在接收端，我们设计了一个基于LLM的语义解码器，能够理解上下文并生成最合适的表示以适应给定的上下文。我们在不同的场景下评估了所提出的系统，包括具有不同速度范围的城市宏观小区环境。评估指标表明，我们所提出的系统在减少数据量4250倍的同时，实现了比传统通信方法更高的数据率。这种方法提供了一个强大且可扩展的解决方案，以充分发挥6G连接的潜力。|
|**2025-01-22**|**LLM4WM: Adapting LLM for Wireless Multi-Tasking**|Xuanyu Liu et.al.|[2501.12983](http://arxiv.org/abs/2501.12983)|null|无线信道是通信的基础，涵盖了众多称为与信道相关任务的任务。这些任务可以基于信道特性进行联合学习，以共享表示并增强系统设计。为了利用这一优势，提出了LLM4WM——一种专门针对与信道相关任务的大语言模型（LLM）多任务微调框架。该框架采用混合专家低秩适应（MoE-LoRA）方法进行多任务微调，使预训练的LLM的通用知识能够转移到这些任务上。考虑到无线信道数据的独特特性，设计了预处理模块、适配器模块和多任务输出层，以使信道数据与LLM的语义特征空间对齐。在与信道相关的多任务数据集上的实验表明，LLM4WM在全样本和少量样本评估中均优于现有方法，这归功于其稳健的多任务联合建模和迁移学习能力。|
|**2025-01-22**|**OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models**|Chongren Sun et.al.|[2501.12975](http://arxiv.org/abs/2501.12975)|**[link](https://github.com/sunchongren/onioneval)**|**大型语言模型（LLMs）具有强大的能力，但需要大量的计算资源来进行训练和推理。在LLM家族中，较小的模型（参数少于10亿的模型）也在各种任务中表现出色。然而，这些较小的模型也面临着与较大模型类似的局限性，包括倾向于幻觉生成。尽管存在许多基准来评估LLMs中的幻觉生成，但很少有基准特别关注小型LLMs（SLLMs）。此外，SLLMs在不同的基准测试中表现出广泛的变化性能。在这篇论文中，我们引入了OnionEval，这是一个多层结构框架，其中包含一个特定的指标称为上下文影响得分（CI），旨在有效地评估小型LLMs在不同上下文层次上的事实冲突幻觉倾向。我们的实验结果揭示了SLLMs的一个关键特征：它们在事实分析方面表现出色，但在上下文推理方面面临挑战。进一步的研究表明，一种简单的思维链策略可以显著减少这些局限性，从而提高SLLMs在现实世界应用中的实用性。**|
|**2025-01-22**|**Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs**|Jan Corazza et.al.|[2501.12972](http://arxiv.org/abs/2501.12972)|null|当人们说区块链系统是无需信任的时，实际上意味着所有信任都被置于软件上。因此，确保区块链软件的正确性存在强烈的激励——这里的漏洞会耗费数百万并破坏企业。建立软件正确性最强大的方法之一是使用形式化方法。然而，基于形式化方法的方法在时间和专业知识方面引入了显著的开销。我们的工作解决了这一关键劣势，通过自动化创建形式模型的过程——这是采用形式方法时常需要的核心任务。我们分三个阶段进行模型合成：首先将代码转译成模型存根；然后使用大型语言模型（LLM）“填补空白”；最后，我们在语法和语义层面上迭代修复生成的模型。这样，我们大大减少了创建形式模型所需的时间，并提高了依赖于这些模型的有价值的软件验证方法的可及性。我们工作的实际背景是减少使用形式模型对智能合约正确性审核的时间成本。|
|**2025-01-22**|**It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act**|Kristof Meding et.al.|[2501.12962](http://arxiv.org/abs/2501.12962)|null|什么是公平的决策？这个问题不仅对人类而言难以回答，而且在使用人工智能模型时变得更加困难。鉴于算法歧视行为，欧盟最近通过了《人工智能法案》，该法案对人工智能模型规定了具体规则，融合了传统的法律反歧视规定和基于机器学习的算法公平性概念。本文旨在通过两个方面来弥合《人工智能法案》中的这两种不同概念：首先，针对法律和计算机科学领域的学者进行高层次的介绍；其次，深入分析《人工智能法案》中法律反歧视规定与算法公平性之间的关系。我们的分析揭示了三个关键发现：（1）大多数反歧视规定仅适用于高风险人工智能系统。（2）对高风险系统的监管涵盖了数据输入要求和输出监控，但这些规定往往不一致，并引发了计算可行性的问题。（3）对于未同时被归类为高风险系统的通用人工智能模型，如大型语言模型，目前的规定缺乏具体性，与其他规定相比显得不足。根据这些发现，我们建议开发更具体的审计和测试方法来应对人工智能系统。本文旨在为未来法律学者和计算机科学领域的人工智能研究人员研究人工智能系统中的歧视问题提供合作基础。|
|**2025-01-22**|**Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference**|Weizhi Fei et.al.|[2501.12959](http://arxiv.org/abs/2501.12959)|null|尽管涉及长上下文输入的应用对于大型语言模型（LLMs）的有效利用至关重要，但这也导致了计算成本的增加和性能的降低。为了解决这一挑战，我们提出了一种高效的、无需训练的提示压缩方法，该方法在压缩提示时保留了关键信息。我们识别出变压器基LLMs中的特定注意力头，我们将其称为评估器头，这些头能够在推理过程中选择长输入中最显著的令牌。在此发现的基础上，我们开发了EHPC（基于评估器头的提示压缩方法），使LLMs能够在预填充阶段仅利用前几层中的评估器头“快速浏览”输入提示，随后仅将重要令牌传递给模型进行推理。EHPC在两个主流基准测试中取得了最先进的结果：提示压缩和长上下文推理加速。因此，它有效降低了商业API调用的复杂性和成本。我们进一步证明EHPC相对于基于键值缓存的加速方法也达到了具有竞争力的结果，从而突显了其增强LLMs在长上下文任务效率的潜力。|
|**2025-01-22**|**GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models**|Pengxiang Zhao et.al.|[2501.12956](http://arxiv.org/abs/2501.12956)|null|大型语言模型（LLMs）在部署时面临显著的资源需求挑战。虽然低比特量化权重可以减少内存使用并提高推理效率，但目前的硬件缺乏对混合精度矩阵乘法（mpGEMM）的原生支持，导致基于解量化的实现效率低下。此外，统一量化方法通常无法充分捕捉权重分布，从而导致性能下降。我们提出了GANQ（GPU自适应非均匀量化），这是一种针对硬件高效查找表基mpGEMM优化的分层后训练非均匀量化框架。GANQ通过利用一种无需训练、GPU自适应的优化算法来有效减少分层量化误差，从而实现卓越的量化性能。广泛的实验表明，GANQ相比最先进的方法，在3位和4位量化的情况下，能够缩小与FP16基线的困惑度差距。此外，当部署在单个NVIDIA RTX 4090 GPU上时，GANQ的量化模型比基线快达2.57倍，提高了LLM部署中的内存和推理效率。|
|**2025-01-21**|**InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling**|Yi Wang et.al.|[2501.12386](http://arxiv.org/abs/2501.12386)|**[link](https://github.com/opengvlab/internvideo)**|**本文旨在通过长而丰富的上下文（LRC）建模来提升视频多模态大语言模型（MLLM）的性能。为此，我们开发了新的InternVideo2.5版本，重点增强原始MLLM感知细微细节和捕捉视频长时序结构的能力。具体而言，我们的方法通过直接偏好优化将密集视觉任务标注纳入MLLM，并通过自适应分层令牌压缩构建紧凑的时空表示。实验结果表明，这种独特的LRC设计极大地提升了视频MLLM在主流视频理解基准测试（短时与长时）中的表现，使MLLM能够记忆更长的视频输入（至少比原版长6倍），并掌握专门的视觉能力如物体跟踪和分割。我们的研究强调了多模态上下文丰富性（长度和精细度）在增强MLLM内在能力（注意力和记忆力）方面的重要性，为未来视频MLLM的研究提供了新见解。代码和模型可在<https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5>获取。**|
|**2025-01-21**|**Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists**|Thomas F. Eisenmann et.al.|[2501.12374](http://arxiv.org/abs/2501.12374)|**[link](https://github.com/andreskarjus/genaiexperiment)**|**新型的生成式人工智能技术在分析和生成文化艺术品方面展现了新的能力，这不可避免地引发了关于艺术教育和人类专业知识性质与价值的问题。AI是否已经使专业艺术家和平民之间的竞争变得公平，或者经过训练的艺术表现力、策展技能和经验是否会增强使用这些新工具的能力？在这项预先注册的研究中，我们对50名活跃艺术家和一个与之人口统计学相匹配的平民样本进行了实验对比。我们设计了两项任务来模拟艺术实践以测试他们在忠实和创造性图像创作方面的能力：复制参考图像，以及尽可能远离该图像。我们开发了一个专门的平台，让参与者使用现代文本到图像模型完成这两项任务。我们还收集并比较了参与者对AI的态度。平均而言，艺术家比他们的平民同行在忠实和创造性输出方面略胜一筹，尽管差距不大。虽然AI可能简化了内容创作，但专业技能仍然有价值——即使是在生成式AI本身的有限空间内。最后，我们还探索了一款具有视觉能力的大规模语言模型（GPT-4o）在扮演图像生成代理的角色时能否完成同样的任务，发现它在复制方面表现相当但在创意任务上甚至超过了艺术家。在两项任务中最好的结果仍由人类产生。这些结果强调了将艺术技能与AI培训相结合的重要性，以帮助艺术家和其他视觉专业人士适应技术不断发展的环境。我们看到了与生成式AI合作的潜力，这可能会重塑创意产业和艺术教育。 **|
|**2025-01-21**|**Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL**|Yeounoh Chung et.al.|[2501.12372](http://arxiv.org/abs/2501.12372)|null|大型语言模型（LLMs）在一系列自然语言处理任务中展示了令人印象深刻的性能。特别是推理能力的提升和上下文窗口的扩展，为利用这些强大的模型开辟了新的途径。NL2SQL问题具有挑战性，因为自然语言问题本质上是模糊的，而生成SQL需要对复杂的数据模式和语义有精确的理解。解决这一语义模糊问题的一个方法是提供更多的充分上下文信息。在这项工作中，我们探讨了由Google提供的最先进LLM（即gemini-1.5-pro）所提供的扩展上下文窗口（长上下文）的性能和延迟权衡。我们研究了各种上下文信息的影响，包括列示例值、问题和SQL查询对、用户提供的提示、SQL文档和模式。据我们所知，这是首次研究长上下文窗口和额外上下文信息如何帮助NL2SQL生成，同时考虑准确性和延迟成本的工作。我们表明，长上下文LLMs是健壮的，并不会迷失在扩展的上下文信息中。此外，我们的基于Google的gemini-pro-1.5的长上下文NL2SQL管道在未经微调和不使用昂贵的自一致性技术的情况下，在BIRD基准测试（开发集）上达到了67.41%的强性能。|
|**2025-01-21**|**Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration**|Thomas Walshe et.al.|[2501.12332](http://arxiv.org/abs/2501.12332)|null|获取用于满足数量和质量要求的标记训练数据在现实世界的机器学习项目中仍然是一项成本高昂的任务。最近，大型语言模型（LLM），特别是GPT-4，在高精度标记数据方面展现出了巨大的潜力。然而，隐私和成本问题阻碍了GPT-4的广泛应用。在这项工作中，我们探索了有效地利用开源模型进行自动标记。我们发现集成标签模式是一种有前景的技术，但简单地使用标签描述进行分类在高基数任务上表现不佳。为了解决这个问题，我们提出了检索增强分类（RAC）方法，其中LLM一次针对一个标签使用相应的标签模式进行推理；从最相关的标签开始，迭代直到LLM选择一个标签。我们展示了我们的方法，该方法动态地整合标签描述，可以在标记任务中提高性能。我们进一步表明，通过仅关注最有希望的标签，RAC能够在标签质量和覆盖范围之间进行权衡——我们利用这一特性来自动标记我们内部的数据集。|
|**2025-01-21**|**VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model**|Xianwei Zhuang et.al.|[2501.12327](http://arxiv.org/abs/2501.12327)|**[link](https://github.com/VARGPT-family/VARGPT)**|**我们提出了VARGPT，这是一种新颖的多模态大型语言模型（MLLM），它在一个单一的自回归框架中统一了视觉理解和生成。VARGPT采用下一令牌预测范式进行视觉理解，并采用下一尺度预测范式进行视觉自回归生成。VARGPT创新性地扩展了LLaVA架构，在MLLM中实现了高效的尺度化自回归视觉生成，同时在单一模型框架内无缝支持混合模态输入和输出。我们的VARGPT经过三个阶段的统一训练过程，包括预训练阶段和两个混合视觉指令微调阶段。统一的训练策略旨在实现视觉和文本特征之间的对齐，增强理解与生成的指令跟随能力，并提高视觉生成质量。尽管其基于LLaVA架构以支持多模态理解，VARGPT在各种以视觉为中心的基准测试中显著优于LLaVA-1.5，如视觉问答和推理任务。值得注意的是，VARGPT自然支持自回归视觉生成和指令到图像合成的能力，展示了其在视觉理解和生成任务中的多功能性。项目页面位于：https://vargpt-1.github.io/**|
|**2025-01-21**|**LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations**|Hasan Abu-Rasheed et.al.|[2501.12300](http://arxiv.org/abs/2501.12300)|null|在学习个性化方面具有巨大的潜力，但现代高等教育实践需要更深入地考虑领域模型和学习环境，以开发有效的个性化算法。本文介绍了一种创新的高等教育课程建模方法，该方法利用大型语言模型（LLM）进行知识图谱（KG）完成，旨在创建个性化的学习路径推荐。我们的研究集中在建模大学课程并将其主题与相应的领域模型相联系，使学生的学习路径能够整合来自不同院系和机构的学习模块。我们方法的核心是一种协作过程，在此过程中LLM帮助人类专家从讲座材料中提取高质量、细粒度的主题。我们为大学课程和利益相关者开发了领域、课程和用户模型。我们实施这一模型来构建两个学习模块的知识图谱：嵌入式系统和使用FPGA开发嵌入式系统。由此产生的知识图谱结构化了课程，并将其与领域模型联系起来。我们通过定性的专家反馈和定量的图质量指标来评估这种方法。领域专家验证了模型的相关性和准确性，而图质量指标则衡量了我们知识图谱的结构特性。结果表明，LLM辅助的图完成方法增强了连接跨学科相关课程的能力，从而实现个性化学习体验。专家反馈也显示，对所提出的协作概念提取和分类方法具有很高的接受度。|
|**2025-01-21**|**MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in Dynamic Sensing Networks**|Qishen Zhou et.al.|[2501.12281](http://arxiv.org/abs/2501.12281)|**[link](https://github.com/youxiaotu/MoGERNN)**|**鉴于给定的不完全观测道路网络如何预测未观测位置的交通状态？虽然深度学习方法在交通预测中表现出色但大多数假设所有感兴趣位置都装有传感器这在现实中由于经济限制是不现实的。此外这些方法通常需要昂贵的再训练当传感器配置改变时。我们提出了MoGERNN一种归纳式的时空图表示模型来解决这些挑战。受大型语言模型中专家混合方法的启发我们引入了一种图形专家混合(MoGE)模块通过多个图形消息聚合器和一个稀疏门控网络来建模复杂的空间依赖性。该模块估计未观测位置的初始状态然后这些状态由基于GRU的编码器-解码器处理该编码器-解码器集成了一个图形消息聚合器以捕捉时空依赖性并预测未来状态。在两个真实世界数据集上的实验表明MoGERNN在观测和未观测位置上始终优于基线方法。MoGERNN能够准确预测即使在没有传感器区域的拥堵演变从而为交通管理提供有价值的信息。此外MoGERNN适应于动态传感网络与重新训练相比仍能保持竞争性能。使用不同数量可用传感器的测试证实了其持续优越性并且消融研究验证了其关键模块的有效性。**|
|**2025-01-21**|**Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement**|Maosong Cao et.al.|[2501.12273](http://arxiv.org/abs/2501.12273)|**[link](https://github.com/internlm/condor)**|监督精调（SFT）数据的质量对于提升大型语言模型（LLMs）的对话能力至关重要。然而，随着这些模型的进步，高质量的人类注释SFT数据的获取已成为一个重要的瓶颈，因此更加依赖于合成训练数据。在这项工作中，我们引入了Condor，这是一种新颖的两阶段合成数据生成框架，结合了世界知识树和自我反思精炼来大规模生成高质量的SFT数据。我们的实验结果表明，仅使用20K个由Condor生成的样本对基础模型进行微调就能实现优越的性能。Condor中的额外精炼阶段还使得不同规模（高达72B）的语言模型能够进行迭代自我改进，验证了我们方法的有效性。此外，我们对用于后训练的合成数据进行扩展的研究揭示了显著未被开发的性能提升潜力，为未来的研究开辟了有前景的方向。|
|**2025-01-21**|**FOCUS: First Order Concentrated Updating Scheme**|Yizhou Liu et.al.|[2501.12243](http://arxiv.org/abs/2501.12243)|null|大型语言模型（LLMs）展示了显著的性能，改进它们的预训练过程似乎是进一步提升其能力的关键。基于Adam、学习率衰减和权重衰减的成功记录，我们假设预训练损失景观呈现出一种逐渐变窄的山谷结构。通过使用合成损失函数的实验，我们发现当梯度查询噪声相对于山谷的陡峭程度较高时，Adam的表现落后于Signum，因为Adam过于大幅度地降低了有效的步长。这一观察促使我们开发了FOCUS优化器，它通过引入对移动平均参数的吸引力来增强Signum，使其能够更好地处理噪声同时保持较大的步长。在训练GPT-2的过程中，FOCUS证明比Signum更稳定，比Adam更快。这些结果表明梯度噪声可能是限制LLM训练的一个未被充分认识的因素，而FOCUS提供了有前景的解决方案。|
|**2025-01-21**|**InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models**|Pha Nguyen et.al.|[2501.12231](http://arxiv.org/abs/2501.12231)|null|改进的生成模型能力有助于构建多模态虚拟助手，这些助手可以利用超越语言的模态。通过观察人类执行多步骤任务，可以构建具有对正在进行的动作和任务的情境感知的助手，从而能够根据这种理解提供帮助。在本文中，我们开发了一种具备情境感知的指令任务辅助系统（InsTALL），该系统基于在线视觉流（例如用户的屏幕共享或视频录制）并实时响应与当前任务相关的用户查询。为了实现有用的帮助，InsTALL 1）在任务视频和配对文本数据上训练多模态模型，2）自动从视频数据中提取任务图，并在训练和推理阶段利用它。我们展示了InsTALL在所考虑的多模态活动理解子任务——任务识别（TR）、动作识别（AR）、下一步动作预测（AP）和计划预测（PP）——上达到了最先进的性能，并且在两个与自动错误识别相关的新型子任务上优于现有基线。|
|**2025-01-17**|**FaceXBench: Evaluating Multimodal LLMs on Face Understanding**|Kartik Narayan et.al.|[2501.10360](http://arxiv.org/abs/2501.10360)|**[link](https://github.com/kartik-3004/facexbench)**|**多模态大型语言模型（MLLMs）在广泛的任务和领域中展示了出色的解决问题的能力。然而，它们对面部分析的理解能力尚未得到系统的研究。为了解决这一差距，我们介绍了FaceXBench，这是一个全面的基准测试，旨在评估MLLMs在复杂的面部理解任务中的表现。FaceXBench包括来自25个公开数据集和一个新创建的数据集FaceXAPI的5000个多模态选择题。这些问题涵盖了6个大类中的14项任务，评估了MLLMs在偏见和公平性、面部验证、识别、分析、定位和工具检索方面的面部理解能力。通过使用FaceXBench，我们对26个开源MLLMs以及2个专有模型进行了广泛的评估，揭示了复杂面部理解任务的独特挑战。我们在三种评估设置下分析这些模型：零样本、基于上下文的任务描述和思维链提示。我们的详细分析显示，包括像GPT-4o和GeminiPro 1.5这样的先进模型在内的当前MLLMs在这些任务上显示出显著的改进空间。我们认为FaceXBench将成为开发能够执行复杂面部理解的MLLMs的重要资源。代码：https://github.com/Kartik-3004/facexbench**|
|**2025-01-17**|**Agent4Edu: Generating Learner Response Data by Generative Agents for Intelligent Education Systems**|Weibo Gao et.al.|[2501.10332](http://arxiv.org/abs/2501.10332)|**[link](https://github.com/bigdata-ustc/agent4edu)**|个性化学习是智能教育系统中的一种有前景的教育策略，旨在提高学习者的练习效率。然而，离线指标与在线表现之间的差异显著阻碍了他们的进步。为了解决这一挑战，我们引入了Agent4Edu，这是一种利用大型语言模型（LLMs）最新进展的新颖个性化学习模拟器。Agent4Edu具有LLM驱动的生成代理，配备了学习者档案、记忆和动作模块，专门针对个性化学习算法。学习者档案使用现实世界响应数据进行初始化，捕捉练习风格和认知因素。受人类心理学理论启发，记忆模块记录练习事实和高级摘要，并整合反思机制。动作模块支持各种行为，包括练习理解、分析和响应生成。每个代理都可以与个性化学习算法（如计算机自适应测试）互动，从而实现对定制服务的多方面评估和改进。通过全面评估，我们探讨了Agent4Edu的优势和劣势，强调了代理与人类学习者之间响应的一致性和差异性。代码、数据和附录可公开获取于 https://github.com/bigdata-ustc/Agent4Edu。|
|**2025-01-17**|**Large language models for automated scholarly paper review: A survey**|Zhenzhen Zhuang et.al.|[2501.10326](http://arxiv.org/abs/2501.10326)|null|大型语言模型（LLMs）对人类社会产生了重大影响，影响了各个领域。其中，学术界不仅是受LLMs影响的领域之一，而且还是LLMs发展的核心力量。在学术出版物中，这一现象体现在将LLMs整合到同行评审机制中以审查稿件的过程中。在我们之前的文章中，我们提出了自动化学术论文审稿（ASPR）的概念。随着这种整合的增长，我们现在进入了ASPR与同行评审共存的阶段，这在那篇文章中有描述。LLMs对于ASPR的全面实施具有变革潜力，但也带来了需要解决的新问题和挑战。在这篇综述文章中，我们旨在为LLMs时代的ASPR提供一个全面的观点。我们首先调查用于进行ASPR的LLMs有哪些。然后，我们回顾了随着LLM技术的融入已经解决了哪些与ASPR相关的技术瓶颈。接下来，我们探讨了与LLMs结合带来的新方法、新数据集、新源代码和新的在线系统如何应用于ASPR。此外，我们总结了LLMs在ASPR中的性能和问题，并调查了出版商和学术界对ASPR的态度和反应。最后，我们讨论了开发用于ASPR的LLMs所面临的挑战。我们希望这篇综述能为研究人员提供灵感参考，促进ASPR的实际应用和发展。|
|**2025-01-17**|**HiMix: Reducing Computational Complexity in Large Vision-Language Models**|Xuange Zhang et.al.|[2501.10318](http://arxiv.org/abs/2501.10318)|null|得益于大型语言模型和模态对齐技术的最新进展，现有的大型视觉-语言模型（LVLMs）在广泛的场景中取得了显著的性能。然而，过度的计算复杂性限制了这些模型在实际应用中的普及。我们认为，计算复杂性的主要瓶颈之一是由模型计算中涉及的冗余视觉序列引起的。这一观点源于对LVLMs语言解码器中视觉和语言信息传输效率的重新评估。为此，我们提出了一种新的分层视觉-语言交互机制，称为Hierarchical Vision injection for Mixture Attention（HiMix）。在HiMix中，只有语言序列经历完整的前向传播，而视觉序列在每个语言解码器层的特定阶段与语言进行交互。值得注意的是，我们的方法显著降低了计算复杂性，并且性能损失很小。具体而言，HiMix在多个LVLM模型中将语言解码器的计算成本减少了10倍，同时保持了可比的性能。这突显了我们方法的优势，我们希望我们的研究能为视觉-语言理解领域带来新的视角。项目页面：https://xuange923.github.io/HiMix|
|**2025-01-17**|**Addressing Popularity Bias in Third-Party Library Recommendations Using LLMs**|Claudio Di Sipio et.al.|[2501.10313](http://arxiv.org/abs/2501.10313)|null|推荐系统在软件工程中扮演着重要角色，通过根据开发者的上下文提供相关建议来自动化开发任务。然而，它们受到所谓的流行度偏差的影响，即推荐可能与当前任务不相关的热门项目。特别是长尾效应可能会损害系统的准确性，从而导致推荐中的误报。基础模型是最先进的基于生成式人工智能的模型，在多个软件工程任务中取得了相关成果。本文旨在探讨大型语言模型（LLMs）在解决第三方库（TPLs）推荐器中的流行度偏差方面的有效性。我们进行了消融研究，实验了最先进的缓解流行度偏差的技术，包括微调和流行度惩罚机制。我们的研究结果表明，所考虑的LLMs无法解决TPL推荐器中的流行度偏差，尽管微调和后处理惩罚机制有助于提高所提供推荐的整体多样性。此外，我们讨论了LLMs在此背景下的局限性，并提出了潜在的改进方法以解决TPL推荐器中的流行度偏差，从而为进一步的研究铺平道路。|
|**2025-01-17**|**Computational Protein Science in the Era of Large Language Models (LLMs)**|Wenqi Fan et.al.|[2501.10282](http://arxiv.org/abs/2501.10282)|null|考虑到蛋白质的重要性，计算蛋白质科学一直是一个至关重要的科学领域，致力于揭示知识和发展蛋白质序列-结构-功能范式中的应用。在过去的几十年里，人工智能（AI）在计算蛋白质科学中产生了重大影响，导致了特定蛋白质建模任务中的显著成功。然而，这些以前的AI模型仍然存在局限性，如难以理解蛋白质序列的语义，以及无法广泛适用于多种蛋白质建模任务。最近，大型语言模型（LLMs）由于其前所未有的语言处理和泛化能力而成为AI的一个里程碑。它们可以促进各个领域的全面进步，而不仅仅是解决个别任务。因此，研究人员积极引入LLM技术到计算蛋白质科学中，开发出能够熟练掌握蛋白质基础知识并能有效推广用于解决各种序列-结构-功能推理问题的蛋白质语言模型（pLMs）。尽管见证了蓬勃的发展，但有必要对由LLM技术赋能的计算蛋白质科学进行全面概述。首先，我们将现有的pLMs按照其掌握的蛋白质知识进行分类，即潜在的序列模式、明确的结构和功能信息以及外部科学语言。其次，我们介绍pLMs的利用和适应，强调它们在推动蛋白质结构预测、蛋白质功能预测和蛋白质设计研究中的显著成就。然后，我们描述pLMs在抗体设计、酶设计和药物发现中的实际应用。最后，我们特别讨论这一快速发展的领域中充满希望的未来方向。|
|**2025-01-17**|**Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation**|Azat Abdullin et.al.|[2501.10200](http://arxiv.org/abs/2501.10200)|null|自动生成测试是软件工程研究领域的一个关键且持续关注的焦点。大型语言模型（LLMs）的出现为此类任务提供了新的机会，鉴于它们能够执行广泛的任务。然而，与传统技术如基于搜索的软件测试（SBST）和符号执行相比，LLM方法的有效性仍然不确定。在本文中，我们对基于三个工具的自动测试生成方法进行了深入研究：用于SBST的EvoSuite、用于符号执行的Kex以及用于LLM基础测试生成的TestSpark。我们使用GitBug Java数据集评估这些工具的性能，并通过各种基于执行和基于特征的指标进行比较。我们的结果显示，虽然基于LLM的测试生成是有前景的，但在覆盖率方面落后于传统方法。然而，在变异分数方面，它显著优于这些方法，表明LLMs在代码语义理解方面提供了更深层次的理解。在故障检测能力方面，LLM方法的表现不如SBST和基于符号执行的方法。此外，我们的基于特征的分析显示，所有工具主要受到被测类（CUT）复杂性和内部依赖关系的影响，而LLM方法对CUT大小特别敏感。|
|**2025-01-17**|**Generative Artificial Intelligence: Implications for Biomedical and Health Professions Education**|William Hersh et.al.|[2501.10186](http://arxiv.org/abs/2501.10186)|null|生成式人工智能在生物医学和健康领域，无论是专业工作还是教育方面，都产生了深远的影响。基于大型语言模型（LLMs），生成式人工智能在模拟环境中参加医学执照考试、回答临床问题、解决临床案例、应用临床推理以及总结信息方面表现良好。生成式人工智能也被广泛应用于教育，在学术课程及其评估中表现出色。本文综述了LLMs的成功之处，并强调了其在教育背景下的一些挑战，特别是那些可能削弱知识和技能获取的方面，这些方面对于专业工作尤为重要。随后，文章提出了克服LLMs使用短板的最佳实践建议。尽管在教育中使用生成式人工智能存在挑战，但所有学生和教师，无论是在生物医学和健康领域还是其他领域，都必须具备理解和掌握其使用的技能。|
|**2025-01-17**|**Multi-stage Training of Bilingual Islamic LLM for Neural Passage Retrieval**|Vera Pavlova et.al.|[2501.10175](http://arxiv.org/abs/2501.10175)|null|本研究考察了伊斯兰领域内自然语言处理（NLP）技术的应用，专注于开发一种伊斯兰神经检索模型。通过利用强大的XLM-R模型，本研究采用了一种语言简化技术来创建轻量级的双语大型语言模型（LLM）。我们的方法针对伊斯兰领域的独特挑战进行了调整，该领域中存在的大部分领域内语料库仅存在于阿拉伯语中，而其他语言包括英语中的语料库则非常有限。这项工作采用了多阶段训练过程来训练检索模型，结合使用大规模检索数据集（如MS MARCO）和较小的领域内数据集以提高检索性能。此外，我们通过采用数据增强技术和引用可靠的伊斯兰来源，精心制作了一个英语领域的检索数据集。这种方法增强了领域特定的数据集以进行检索，从而进一步提高了性能。研究结果表明，结合领域适应和双语伊斯兰神经检索模型的多阶段训练方法使其在下游检索任务中优于单语模型。|
|**2025-01-17**|**Exploring the Impact of Generative Artificial Intelligence in Education: A Thematic Analysis**|Abhishek Kaushik et.al.|[2501.10134](http://arxiv.org/abs/2501.10134)|null|近年来，生成式人工智能（GenAI）技术在教育领域的应用取得了重大进展。大型语言模型（LLMs）如ChatGPT和Bard可以用于自动化处理基础任务、创建个性化教学内容以及处理重复性工作，从而为创造性思维留出更多时间。然而，在教育领域整合这些工具时，必须制定指南、政策和评估方法，以确保负责任地使用这些工具。本文通过对来自教育界专业人士的七篇论文进行主题分析，探讨了在教育中使用GenAI模型如ChatGPT和Bard的优点和潜在问题。还对论文进行了探索性数据分析（EDA），以进一步从文本中提取见解。研究发现多个主题，突出了GenAI工具的优势和缺点，并提出了克服这些局限性的建议，以确保学生能够负责任且合乎道德地使用这些工具。|
|**2025-01-16**|**Distilling Multi-modal Large Language Models for Autonomous Driving**|Deepti Hegde et.al.|[2501.09757](http://arxiv.org/abs/2501.09757)|null|自动驾驶对安全的运动规划有很高的要求，特别是在关键的“长尾”场景中。近期的端到端自动驾驶系统利用大型语言模型（LLM）作为规划器以提高对罕见事件的泛化能力。然而，在测试时使用LLM会引入高计算成本。为了解决这一问题，我们提出了DiMA，这是一种端到端的自动驾驶系统，它保持了无LLM（或基于视觉的）规划器的效率，同时利用了LLM的世界知识。DiMA通过一组特别设计的代理任务，将多模态LLM的信息提炼到一个基于视觉的端到端规划器中。在联合训练策略下，两个网络共用的场景编码器生成的结构化表示既语义明确又与最终的规划目标对齐。值得注意的是，推理时LLM是可选的，这使得规划更加稳健而不牺牲效率。使用DiMA进行训练可以使基于视觉的规划器的L2轨迹误差减少37%，碰撞率降低80%，并在长尾场景中的轨迹误差减少44%。DiMA还在nuScenes规划基准上达到了最先进的性能。|
|**2025-01-16**|**Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues**|Youngjoon Jang et.al.|[2501.09754](http://arxiv.org/abs/2501.09754)|null|我们的目标是将连续的手语翻译成口语文本。受到人类口译员依赖上下文进行准确翻译的启发，我们将额外的上下文线索与手语视频一起纳入新的翻译框架。具体来说，除了编码输入视频的视觉手语识别特征外，我们还整合了补充的文本信息，包括（i）描述背景节目的字幕、（ii）之前句子的翻译以及（iii）转录手语的伪词素。这些信息自动提取并与其他视觉特征一起输入到一个预先训练的大规模语言模型（LLM）中，我们对其进行微调以生成口语翻译的文本形式。通过广泛的消融研究，我们展示了每个输入线索对翻译性能的积极贡献。我们在BOBSL——目前可用的最大英国手语数据集上训练和评估我们的方法。我们表明，与在BOBSL上先前报道的结果以及作为基线实现的最先进方法相比，我们的上下文方法显著提高了翻译质量。此外，我们证明了我们方法的通用性，将其应用于How2Sign——一个美国手语数据集，并取得了具有竞争力的结果。|
|**2025-01-16**|**OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking**|Zekun Xi et.al.|[2501.09751](http://arxiv.org/abs/2501.09751)|**[link](https://github.com/zjunlp/omnithink)**|机器写作中的大型语言模型通常依赖于检索增强生成。然而，这些方法仍然局限于模型预定义的范围之内，限制了内容的丰富性生成。具体来说，普通检索到的信息往往缺乏深度、实用性和存在冗余问题，这会负面影响生成文章的质量，导致输出肤浅、重复且缺乏原创性。为了解决这些问题，我们提出了OmniThink，这是一种模拟人类迭代扩展和反思过程的机器写作框架。OmniThink的核心思想是模拟学习者逐步深化其对主题理解的认知行为。实验结果表明，OmniThink提高了生成文章的知识密度，同时没有损害连贯性和深度等指标。人为评估和专家反馈进一步突显了OmniThink在解决长篇文章生成中的实际挑战方面的潜力。|
|**2025-01-16**|**Enhancing Lexicon-Based Text Embeddings with Large Language Models**|Yibin Lei et.al.|[2501.09749](http://arxiv.org/abs/2501.09749)|null|近期的大语言模型（LLMs）在通用文本嵌入任务上展示了卓越的性能。尽管密集嵌入在相关研究中占主导地位，我们介绍了首个利用LLMs的词典嵌入（LENS），在这些任务上实现了具有竞争力的性能。针对传统因果LLMs中存在的词汇冗余问题和单向注意力限制，LENS通过词嵌入聚类简化了词汇空间，并探讨了双向注意力和各种池化策略。具体而言，LENS通过将每个维度分配给特定的词嵌入簇来简化词典匹配，在该簇中语义相似的词被分组在一起，并通过双向注意力解锁了LLMs的全部潜力。广泛的实验表明，LENS在大规模文本嵌入基准（MTEB）上优于密集嵌入，提供了与密集对应物尺寸相匹配的紧凑特征表示。值得注意的是，结合LENS与密集嵌入在MTEB的检索子集（即BEIR）上实现了最先进的性能。|
|**2025-01-16**|**Suggesting Code Edits in Interactive Machine Learning Notebooks Using Large Language Models**|Bihui Jin et.al.|[2501.09745](http://arxiv.org/abs/2501.09745)|null|机器学习开发人员经常使用交互式计算笔记本，如Jupyter笔记本，来托管数据处理和模型训练的代码。尽管Jupyter笔记本为编写机器学习管道并交互式观察输出提供了便捷工具，但维护这些笔记本（例如添加新功能或修复错误）可能具有挑战性，因为笔记本可能很长且复杂。此外，目前没有与Jupyter笔记本开发者编辑相关的基准。为了解决这一问题，我们发布了首个包含来自GitHub上792个机器学习存储库的20,095个版本中的48,398个Jupyter笔记本编辑的数据集，并首次研究了使用大型语言模型（LLMs）预测Jupyter笔记本中的代码编辑。我们的数据集捕获了单元格级别和行级别修改的详细信息，为理解机器学习工作流程中的实际维护模式奠定了基础。我们观察到，Jupyter笔记本上的编辑高度集中，平均仅涉及166行代码。虽然较大的模型在代码编辑方面优于较小的模型，但在我们的数据集上所有模型即使经过微调后准确率仍然较低，这表明现实世界中机器学习维护任务的复杂性。我们的发现强调了上下文信息在提高模型性能中的关键作用，并指出了增强大型语言模型在工程机器学习代码方面能力的有前景的方向。|
|**2025-01-16**|**Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps**|Nanye Ma et.al.|[2501.09732](http://arxiv.org/abs/2501.09732)|null|生成模型在各个领域都产生了重大影响，这主要归功于它们在训练过程中通过增加数据、计算资源和模型规模来扩展的能力，这一现象由缩放法则所描述。最近的研究开始探索大型语言模型（LLMs）在推理时的缩放行为，揭示了性能如何随着推理时额外计算的增加而进一步提升。与LLMs不同，扩散模型固有地具备通过调整去噪步数来改变推理时计算量的灵活性，尽管性能增益通常在几十个步骤后趋于平稳。在这项工作中，我们探讨了扩散模型在推理时超出增加去噪步数的缩放行为，并研究了如何通过增加计算量进一步提高生成性能。具体而言，我们将这一过程视为一个搜索问题，旨在寻找更好的噪声用于扩散采样过程。我们沿着两个轴构建设计空间：用于提供反馈的验证器，以及用于寻找更好噪声候选者的算法。通过在分类条件化和文本条件化图像生成基准上的大量实验，我们的研究结果表明，增加推理时的计算量可以显著提高扩散模型生成样本的质量，并且由于图像的复杂性，框架中的组件组合可以根据不同的应用场景进行选择。|
|**2025-01-16**|**CyberMentor: AI Powered Learning Tool Platform to Address Diverse Student Needs in Cybersecurity Education**|Tianyu Wang et.al.|[2501.09709](http://arxiv.org/abs/2501.09709)|**[link](https://github.com/tisage/cybermentor)**|许多非传统学生在网络安全项目中往往缺乏来自同龄人、家庭成员和教授的建议，这可能会阻碍他们的教育体验。此外，由于内容相关性、本地化建议、最低专业知识和时机等问题，这些学生可能无法充分利用各种由大型语言模型（LLM）驱动的人工智能助手。本文通过介绍一个应用程序来解决这些挑战，该应用程序旨在回答与知识、技能和职业准备相关的建议问题，以满足这些学生的需要。我们开发了一个名为CyberMentor的学习工具平台，以应对网络安全专业学生多样化的需要和痛点。该平台由自主工作流和生成型大型语言模型（LLM）提供支持，并利用检索增强生成（RAG）实现准确且上下文相关的资讯检索，从而实现可访问性和个性化。我们展示了它如何应对网络安全教育的知识需求和提高就业市场竞争力，在解决分析和编程作业的技能需求以及提供实时按需学习支持方面的作用。通过三个使用场景，我们展示了CyberMentor如何促进知识获取和职业准备，提供无缝的基于技能的指导和支持。我们还采用了LangChain基于提示的评估方法来评估该平台的影响，确认其在有用性、正确性和完整性方面的优异表现。这些结果强调了该系统支持学生发展实用网络安全技能的能力，同时提高了高等教育中的公平性和可持续性。此外，CyberMentor的开源设计允许其适应其他学科，促进教育创新并扩大其潜在影响。|
|**2025-01-16**|**Domain Adaptation of Foundation LLMs for e-Commerce**|Christian Herold et.al.|[2501.09706](http://arxiv.org/abs/2501.09706)|null|我们介绍了e-Llama模型：两个针对电子商务领域进行适应的大型语言模型，参数量分别为80亿和700亿。这些模型作为基础模型，具备深厚的电子商务知识，可以作为指令调优和微调的基础。e-Llama模型是通过在1万亿个领域的特定数据上持续预训练Llama 3.1基础模型获得的。我们讨论了我们的方法，并通过一系列消融研究来说明超参数选择的原因。为了量化模型在电子商务领域的适应程度，我们定义并实现了一系列多语言、电子商务特定的评估任务。结果表明，通过仔细选择训练设置，Llama 3.1模型可以在不牺牲通用领域任务性能的情况下适应新领域。我们还探索了合并适应后的模型与基础模型的可能性，以更好地控制不同领域的性能权衡。|
|**2025-01-16**|**Simulated Interactive Debugging**|Yannic Noller et.al.|[2501.09694](http://arxiv.org/abs/2501.09694)|null|调试软件，即故障定位及其修复，是软件工程中的主要活动。因此，有效的调试技巧是软件工程师必须掌握的核心技能之一。然而，调试技术的教学通常是十分有限的，或者只是通过间接的方式进行教学，例如在软件项目期间。结果，大多数计算机科学（CS）学生只能以零散和无组织的方式学习调试。在这项工作中，我们提出了一个名为模拟交互式调试的方法，该方法以互动方式引导学生经历调试过程。这种指导旨在使学生能够修复他们的解决方案并获得适当的“学习”体验。我们设想，这样的引导调试技术可以早期整合到计算机科学教育课程的编程课程中。为了进行初步评估，我们使用传统的故障定位技术和大型语言模型开发了一个原型实现。学生可以使用诸如自动设置断点或交互式聊天机器人等功能。我们设计并执行了一项控制实验，包括这个集成在IDE中的工具，共有八名本科生参与。根据反馈，我们得出结论，参与者喜欢由辅助调试器提供的系统性指导。特别是，他们认为自动设置断点是最有效的，其次是交互式调试和聊天，以及关于如何设置断点的解释。在未来的工作中，我们将改进我们的概念和实现，添加新功能，并进行更密集的用户研究。|
|**2025-01-16**|**Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models**|Fengli Xu et.al.|[2501.09686](http://arxiv.org/abs/2501.09686)|null|语言长期以来被认为是人类推理的重要工具。大型语言模型（LLMs）的突破激发了利用这些模型解决复杂推理任务的研究兴趣。研究人员超越简单的自回归令牌生成，引入了“思维”这一概念——即表示推理过程中间步骤的一系列令牌。这种创新范式使LLMs能够模仿复杂的类人推理过程，如树搜索和反思性思考。最近，一种新兴的学习推理趋势应用强化学习（RL）来训练LLMs掌握推理过程。这种方法通过试错搜索算法自动生成高质量的推理轨迹，显著扩展了LLMs的推理能力，通过提供大量额外的训练数据。此外，近期研究表明，在测试时推断过程中鼓励LLMs使用更多令牌进行“思考”可以进一步显著提高推理准确性。因此，训练时间和测试时间的扩展结合展示了新的研究前沿——迈向大型推理模型的道路。OpenAI的o1系列的推出标志着这一研究方向上的一个重要里程碑。在本综述中，我们对LLMs推理的最新进展进行了全面回顾。我们首先介绍LLMs的基础背景，然后探讨推动大型推理模型发展的关键技术组件，重点关注自动化数据构建、学习推理技术以及测试时扩展。我们还分析了构建大型推理模型的流行开源项目，并以开放挑战和未来研究方向作为结论。|
|**2025-01-15**|**Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails**|Shaona Ghosh et.al.|[2501.09004](http://arxiv.org/abs/2501.09004)|null|随着大型语言模型（LLMs）和生成式人工智能的日益普及，对内容安全的关注也在同步增长。目前，缺乏高质量的人工标注数据集来涵盖LLM相关的所有安全风险，并且这些数据集需要适用于商业应用。为了解决这一问题，我们提出了一种全面且可适应的分类法来分类安全风险，该分类法分为12个顶级危害类别并扩展到9个细粒度子类别。此分类法旨在满足下游用户的不同需求，提供更精细和灵活的工具以管理各种风险类型。使用结合了人工标注与多LLM“陪审团”系统评估响应安全性的混合数据生成管道，我们获得了Aegis 2.0，这是一组精心策划的34,248个人类-LLM交互样本，根据我们提出的分类法进行了标注。为了验证其有效性，我们展示了几个轻量级模型在Aegis 2.0上使用参数高效技术训练后的表现可以与在更大非商业数据集上完全微调的安全模型相媲美。此外，我们引入了一种新的训练方法，该方法将安全性与主题跟随数据相结合。这种方法增强了防护模型的适应性，使其能够推广到推理过程中定义的新风险类别。我们计划将Aegis 2.0数据和模型开源给研究社区，以帮助保障LLMs的安全。|
|**2025-01-15**|**Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models**|Emma Croxford et.al.|[2501.08977](http://arxiv.org/abs/2501.08977)|null|随着大型语言模型（LLMs）被整合到电子健康记录（EHR）工作流程中，验证的工具对于在实施前评估其性能至关重要。现有的提供者文档质量评估工具往往不适合LLM生成文本的复杂性，并且缺乏对真实世界数据的有效验证。为了评估LLM生成的临床总结的质量，开发了提供者文档总结质量工具（PDSQI-9）。使用多个LLM（GPT-4o、Mixtral 8x7b和Llama 3-8b）从多专科的真实世界EHR数据中生成多文档总结。验证包括皮尔逊相关性用于实质有效性，因子分析和克朗巴赫α系数用于结构有效性，组内相关系数（ICC）和克里彭多夫α系数用于普适性，半德尔菲过程用于内容有效性，以及高质量与低质量总结之间的比较以支持区别有效性。七名医生评审员评估了779个总结并回答了8,329个问题，达到了超过80%的组内一致性可靠性。PDSQI-9显示出了良好的内部一致性（克朗巴赫α=0.879；95%置信区间：0.867-0.891）和较高的组内相关性可靠性（ICC=0.867；95%置信区间：0.867-0.868），支持了结构有效性和普适性。因子分析确定了一个解释58%变异的四因素模型，代表组织性、清晰度、准确性和实用性。通过笔记长度与简洁（ρ=-0.200，p=0.029）和组织性（ρ=-0.190，p=0.037）得分之间的相关性支持了实质性有效性。区分高质量与低质量总结的支持了区别有效性（p<0.001）。PDSQI-9展示了稳健的构念有效性，支持其在临床实践中用于评估LLM生成的总结，并促进LLM更安全地融入医疗工作流程。|
|**2025-01-15**|**Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using Large Language Models**|Karukriti Kaushik Ghosh et.al.|[2501.08974](http://arxiv.org/abs/2501.08974)|null|## context 方面级情感分析（ABSA）是一种改进的情感分析方法，旨在根据产品、服务或实体的特定方面或特征提取和分类情感。与传统的情感分析不同，后者对整个评论或文本赋予一个总体情感得分，ABSA侧重于将文本分解为各个组成部分或方面（如质量、价格、服务），并对每个方面的情感进行评估。这使得对客户意见的理解更加细致，使企业能够准确地识别出具体的优势和改进领域。该过程包括几个关键步骤，包括方面提取、情感分类以及针对评论段落或其他用户提供的任何形式的方面级情感聚合。ABSA在产品评论、社交媒体监控、客户反馈分析和市场研究等领域有重要应用。通过利用自然语言处理（NLP）和机器学习技术，ABSA有助于提取有价值的信息，使公司能够做出数据驱动的决策，从而提高客户满意度并优化产品。随着ABSA的发展，它有望通过提供对各种产品方面的更深层次理解来极大地改善个性化客户体验。在这项工作中，我们分析了大型语言模型（LLMs）在跨领域方面级情感分析中的有效性，旨在为某些产品定义框架，并将其用于其他类似情况。我们认为可以实现92%的准确率，即在SemEval-2015任务12的方面级情感分析数据集上的准确率。|
|**2025-01-15**|**Analyzing the Ethical Logic of Six Large Language Models**|W. Russell Neuman et.al.|[2501.08951](http://arxiv.org/abs/2501.08951)|null|本研究考察了六种著名的生成式大型语言模型的伦理推理：OpenAI GPT-4o、Meta LLaMA 3.1、Perplexity、Anthropic Claude 3.5 Sonnet、Google Gemini和Mistral 7B。研究探讨了这些模型如何阐述并应用伦理逻辑，特别是在应对电车难题和海因兹困境等道德困境时的表现。不同于传统的对齐研究，本研究采用了一种可解释性和透明度框架，促使模型解释其伦理推理。这一方法通过三种已建立的伦理类型学进行分析：功利主义-义务论分析、道德基础理论以及科尔伯格道德发展阶段模型。研究发现，大型语言模型在伦理逻辑上表现出大体一致的特点，主要表现为理性主义和功利主义倾向，决策通常优先考虑最小化伤害和公平性。尽管在预训练和模型架构方面存在相似之处，模型之间在伦理推理上仍展现出细微但显著的差异，这反映了微调和后训练过程中的不同。这些模型始终显示出渊博、谨慎和自我意识，其伦理推理类似于道德哲学研究生水平的讨论。令人惊讶的是，这些系统一致地描述自己的伦理推理比典型人类的道德逻辑更为复杂。|
|**2025-01-15**|**Applying General Turn-taking Models to Conversational Human-Robot Interaction**|Gabriel Skantze et.al.|[2501.08946](http://arxiv.org/abs/2501.08946)|null|转轮是对话的一个基本方面，但当前的人机交互（HRI）系统通常依赖于基于简单沉默的模型，导致不自然的停顿和中断。本文首次研究了通用转轮模型，特别是TurnGPT和语音活动投影（VAP）在改善人机交互中的对话动态的应用。这些模型使用自监督学习目标在人类-人类对话数据上进行训练，并且不需要特定领域的微调。我们提出了使用这些模型的方法，以预测机器人何时应该开始准备回答、转轮以及如何处理潜在的中断。我们在一项被试内研究中评估了所提出的系统与一个传统的基线系统的表现，该研究使用Furhat机器人与39名成人进行了对话实验，并结合了一个大型语言模型进行自主响应生成。结果表明，参与者明显更喜欢所提出的系统，并且它显著减少了响应延迟和中断。|
|**2025-01-15**|**Disentangling Exploration of Large Language Models by Optimal Exploitation**|Tim Grams et.al.|[2501.08925](http://arxiv.org/abs/2501.08925)|null|探索是自我提升和开放式问题解决的关键技能。然而，目前尚不确定大型语言模型是否能够有效地探索状态空间。现有的评估主要集中在探索与开发之间的权衡上，通常在多臂老虎机问题中进行评估。与此不同，本研究将探索作为唯一目标，任务是提供能够提高未来回报的信息。为了评估，我们提出通过测量已探索状态下的最优可实现回报来将缺失的奖励分解为探索和开发两个部分。我们的实验使用了各种LLM，结果显示大多数模型难以充分探索状态空间，并且弱探索是不够的。我们观察到模型大小与探索性能之间存在正相关关系，较大的模型表现出更优越的能力。此外，我们展示了这种分解提供了对由代理指令驱动的行为差异的洞察，这些指令在提示工程期间发挥作用，为优化LLM在探索性任务中的表现提供了一个有价值的工具。|
|**2025-01-15**|**GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text Detection Challenge**|Liam Dugan et.al.|[2501.08913](http://arxiv.org/abs/2501.08913)|**[link](https://github.com/liamdugan/raid)**|最近，有许多针对大型语言模型（LLM）生成文本检测的共享任务。然而，这些共享任务要么集中在特定领域内的文本，要么集中在可能在测试时未见过的多个领域的文本。在这个共享任务中，我们使用新发布的RAID基准，旨在回答模型是否能够检测到来自大量固定数量的领域和LLM的生成文本，所有这些都在训练期间被看到。在为期三个月的任务过程中，有9支队伍提交了23个检测器。我们发现，多个参与者能够在机器生成的RAID文本上达到超过99%的准确率，同时保持5%的误报率——这表明检测器能够稳健地检测来自多个领域和模型的文本。我们讨论了这一结果的潜在解释，并提供了未来研究的方向。|
|**2025-01-15**|**Leveraging Large Language Models as Knowledge-Driven Agents for Reliable Retrosynthesis Planning**|Qinyu Ma et.al.|[2501.08897](http://arxiv.org/abs/2501.08897)|**[link](https://github.com/qinyuma316/retrosynthesisagent)**|识别材料化学中的可靠合成路径是一项复杂任务，特别是在聚合物科学领域，由于大分子的名称复杂且往往不唯一。为了解决这一挑战，我们提出了一种集成大型语言模型（LLMs）和知识图谱（KGs）的代理系统。通过利用LLMs强大的提取和识别化学物质名称的能力，并将提取的数据存储在结构化的知识图谱中，我们的系统实现了文献的相关检索、反应数据的提取、数据库查询、逆合成路径树的构建，以及通过检索更多文献和推荐最优反应路径进行进一步扩展。一种新的多分支反应路径搜索（MBRPS）算法使得探索所有路径成为可能，特别关注多分支路径，帮助LLMs克服在多分支路径上的推理弱点。这项工作首次尝试开发一种专门针对大分子的全自动逆合成规划代理，由LLMs提供支持。应用于聚酰亚胺合成，我们的新方法构建了一个包含数百条路径的逆合成路径树，并推荐了优化路线，包括已知和新颖路径，展示了其有效性和更广泛的应用潜力。|
|**2025-01-15**|**How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in Software Engineering**|Christoph Treude et.al.|[2501.08774](http://arxiv.org/abs/2501.08774)|null|人工智能（AI），包括大型语言模型和生成式AI，在软件开发中正成为一个重要的力量，为开发者提供了覆盖整个开发生命周期的强大工具。尽管软件工程研究已经广泛研究了AI工具在软件开发中的应用，但开发人员与这些AI驱动工具之间的具体交互类型仅在最近才开始受到关注。理解和改进这些交互有可能提高生产力、信任度和效率在AI驱动的工作流程中。在本文中，我们提出了一种分类法来描述开发人员与AI工具之间的交互类型，识别出十一种不同的交互类型，如代码自动完成功能、命令驱动的操作以及对话式辅助。基于这一分类法，我们概述了一个研究议程，旨在优化AI交互，提高开发者的控制力，并解决AI辅助开发中的信任和可用性挑战。通过建立一个结构化的基础来研究开发人员与AI的交互，本论文旨在促进研究，以创建更有效、更具适应性的AI工具用于软件开发。|
|**2025-01-15**|**Enhanced Large Language Models for Effective Screening of Depression and Anxiety**|June M. Liu et.al.|[2501.08769](http://arxiv.org/abs/2501.08769)|null|抑郁症和焦虑症普遍存在，需要及时识别和管理。近期大型语言模型（LLMs）的进展提供了潜在解决方案，但高昂的成本和训练数据的伦理问题仍然是挑战。本文介绍了一个用于合成临床访谈的流程，生成了1,157个互动对话（PsyInterview），并提出了EmoScan，这是一种基于LLM的情绪障碍筛查系统。EmoScan能够区分粗粒度（如焦虑或抑郁障碍）和细粒度障碍（如重度抑郁障碍），并进行高质量的访谈。评估显示，EmoScan在筛查情绪障碍方面超过了基础模型和其他LLM（如GPT-4）的表现（F1分数=0.7467）。它还提供了更优的解释（BERT得分=0.9408），并在外部数据集上表现出稳健的泛化能力（F1分数为0.67）。此外，通过自动化评分和人类评估验证，EmoScan在访谈技巧方面优于基线。这项工作强调了可扩展的数据生成管道对于开发有效的心理健康LLM工具的重要性。|
|**2025-01-14**|**PokerBench: Training Large Language Models to become Professional Poker Players**|Richard Zhuang et.al.|[2501.08328](http://arxiv.org/abs/2501.08328)|**[link](https://github.com/pokerllm/pokerbench)**|**我们介绍了PokerBench——一个用于评估大型语言模型（LLMs）扑克牌技的基准。随着LLMs在传统NLP任务上的卓越表现，它们应用于复杂的战略游戏如扑克牌时提出了新的挑战。扑克作为一款信息不完全的游戏，需要多种技能，包括数学、推理、规划、策略以及深刻理解博弈论和人类心理学。这使得扑克成为大型语言模型的理想新领域。PokerBench由11,000个最重要的场景组成，分为底牌前和底牌后玩法，与训练过的扑克玩家合作开发。我们评估了包括GPT-4、ChatGPT 3.5以及各种Llama和Gemma系列模型在内的知名模型，发现所有最先进的LLMs在玩最优扑克时表现不佳。然而，在微调之后，这些模型显示出显著的改进。我们通过让得分不同的模型相互竞争来验证PokerBench的有效性，证明了在PokerBench上得分越高，在实际扑克游戏中获胜率也越高。通过我们微调模型与GPT-4之间的对局，我们也发现了简单的监督微调在学习最优玩法策略方面的局限性，这表明需要更先进的方法来有效地训练语言模型以在游戏中表现出色。因此，PokerBench提供了一个快速且可靠的评估LLMs扑克牌技的基准，同时也为研究LLMs在复杂游戏玩法中的进展提供了全面的基准。数据集和代码将在以下网址提供：https://github.com/pokerllm/pokerbench。**|
|**2025-01-14**|**Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks**|Miran Heo et.al.|[2501.08326](http://arxiv.org/abs/2501.08326)|null|我们提出了Omni-RGPT，这是一种多模态大型语言模型，旨在促进图像和视频的区域级理解。为了在时空维度上实现一致的区域表示，我们引入了Token Mark，这是一组突出显示视觉特征空间内目标区域的标记。这些标记通过区域提示（如框或掩码）直接嵌入到空间区域，并同时并入文本提示以指定目标，从而在视觉标记和文本标记之间建立直接联系。为进一步支持稳健的视频理解而不必使用轨迹片段，我们引入了一项辅助任务，该任务利用标记的一致性来引导Token Mark，从而使视频中的区域解释稳定。此外，我们还引入了一个大规模的区域级视频指令数据集(RegVID-300k)。Omni-RGPT在基于图像和视频的常识推理基准测试中达到了最先进的结果，同时在描述和指代表达理解任务中也表现出了强大的性能。|
|**2025-01-14**|**ADAM-1: AI and Bioinformatics for Alzheimer's Detection and Microbiome-Clinical Data Integrations**|Ziyuan Huang et.al.|[2501.08324](http://arxiv.org/abs/2501.08324)|null|Alzheimer's Disease Analysis Model Generation 1（ADAM）是一个多代理大型语言模型（LLM）框架，旨在整合和分析多模态数据，包括微生物组谱、临床数据集和外部知识库，以增强对阿尔茨海默病（AD）的理解和检测。通过利用检索增强生成（RAG）技术以及其多代理架构，ADAM-1从多样化数据源合成见解，并使用文献驱动的证据进行背景化分析。与XGBoost的比较评估显示，ADAM-1的平均F1分数相似，但在小型实验室数据集中显著降低了方差，突显了其稳定性和一致性。尽管目前主要针对二分类任务，未来迭代计划纳入更多数据模态，如神经影像和生物标志物，以扩大其在阿尔茨海默病研究和诊断中的应用范围和适用性。|
|**2025-01-14**|**Exploring Robustness of Multilingual LLMs on Real-World Noisy Data**|Amirhossein Aliakbarzadeh et.al.|[2501.08322](http://arxiv.org/abs/2501.08322)|**[link](https://github.com/caisa-lab/llms-real-world-noise-robustness)**|**大型语言模型（LLMs）是在可能包含人类拼写错误的网络数据上进行训练的。但它们是否对类似的现实世界噪声具有鲁棒性？在本文中，我们研究了真实世界拼写错误对九种语言模型在三种不同自然语言处理任务中的表现的影响，这九种模型的参数范围从0.2B到13B不等。这三种任务分别是自然语言推理（NLI）、命名实体识别（NER）和意图分类（IC）。我们在六种不同的语言上进行了实验，并使用维基百科的编辑历史为这些语言构建了一个噪声字典。结果显示，所研究模型在所有数据集和语言上的清洁测试数据与噪声测试数据之间的性能差距平均在2.3到4.3个百分点之间。此外，mT5模型总体上显示出比BLOOM、Falcon和BERT类模型更强的鲁棒性。特别是，mT5（13B）在所有三项任务和六种语言中的四种中平均表现最为稳健。**|
|**2025-01-14**|**Enhancing Automated Interpretability with Output-Centric Feature Descriptions**|Yoav Gur-Arieh et.al.|[2501.08319](http://arxiv.org/abs/2501.08319)|**[link](https://github.com/yoavgur/feature-descriptions)**|**自动化解释性管道为大型语言模型（LLM）中的特征所表示的概念生成自然语言描述，例如植物或句子中的第一个词。这些描述是通过激活特征的输入得出的，这可能是一个维度或模型表示空间中的方向。然而，识别激活输入的成本很高，并且特征在模型行为中的机制作用不仅由输入如何使特征激活决定，还由特征激活如何影响输出决定。通过转向评估，我们揭示了当前管道提供的描述未能捕捉特征对输出的因果效应。为了解决这个问题，我们提出了高效的、以输出为中心的方法来自动生成特征描述。这些方法使用在特征刺激后权重更高的标记，或者直接应用词汇“非嵌入”头到特征后的最高权重标记。我们的以输出为中心的描述比以输入为中心的描述更好地捕捉了特征对模型输出的因果效应，但结合两者会导致在输入和输出评估上表现最佳。最后，我们展示了以输出为中心的描述可以用来找到先前被认为“死亡”的特征的激活输入。**|
|**2025-01-14**|**HALoGEN: Fantastic LLM Hallucinations and Where to Find Them**|Abhilasha Ravichander et.al.|[2501.08292](http://arxiv.org/abs/2501.08292)|null|尽管大型语言模型（LLMs）具有生成高质量和流畅文本的能力，但它们也会产生幻觉性陈述：这些陈述与已建立的世界知识或提供的输入上下文不一致。然而，衡量幻觉性陈述可能具有挑战性，因为让人类即时验证模型生成的内容既昂贵又耗时。在这项工作中，我们发布了HALoGEN，这是一个全面的幻觉基准测试集，包括：（1）涵盖编程、科学归因和摘要等九个领域的10,923个用于生成模型的提示，以及（2）每个使用场景下的自动高精度验证器，这些验证器将LLM的生成分解为原子单位，并针对高质量的知识源验证每个单位。我们使用这个框架评估了来自14个语言模型的大约150,000个生成内容，发现即使是最表现优秀的模型也充满了幻觉性陈述（有时根据领域不同，生成的原子事实中有高达86%可能是幻觉）。我们进一步定义了一种新的LLM幻觉错误分类法，基于这些错误是否很可能源于对训练数据的错误回忆（A类错误），或者训练数据中的错误知识（B类错误），或者是编造（C类错误）。我们希望我们的框架能为研究为什么生成模型会产生幻觉提供基础，并促进可信赖大型语言模型的发展。|
|**2025-01-14**|**LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding**|Hongyu Li et.al.|[2501.08282](http://arxiv.org/abs/2501.08282)|**[link](https://github.com/appletea233/llava-st)**|**近期在多模态大型语言模型（MLLMs）的研究取得了显著进展，但现有的方法在同时处理时间和空间定位方面仍存在困难。这一挑战主要源于两个关键问题：首先，引入时空定位会带来大量的坐标组合，使得语言和视觉坐标表示的对齐变得复杂；其次，在视频特征压缩过程中编码细粒度的时间和空间信息本身就很困难。为了解决这些问题，我们提出了LLaVA-ST，这是一种用于细粒度时空多模态理解的MLLM。在LLaVA-ST中，我们提出了语言对齐的位置嵌入，它将文本坐标特殊标记嵌入到视觉空间中，简化了细粒度时空对应关系的对齐。此外，我们设计了空间-时间打包器，它将时间分辨率和空间分辨率的特征压缩解耦为两个独立的点对区域注意力处理流。此外，我们提出了ST-Align数据集，包含430万训练样本，用于细粒度时空多模态理解。通过ST-Align，我们提出了一种逐步训练管道，通过从粗到细的阶段来对齐视觉和文本特征。此外，我们引入了ST-Align基准来评估空间-时间交错的细粒度理解任务，包括空间-时间视频定位（STVG）、事件定位与描述（ELC）以及空间视频定位（SVG）。LLaVA-ST在11个需要细粒度时间、空间或空间-时间交错多模态理解的基准上表现出色。我们的代码、数据和基准将在https://github.com/appletea233/LLaVA-ST 公布。**|
|**2025-01-14**|**Exploring Robustness of LLMs to Sociodemographically-Conditioned Paraphrasing**|Pulkit Arora et.al.|[2501.08276](http://arxiv.org/abs/2501.08276)|null|大型语言模型（LLMs）在各种自然语言处理任务中展示了 impressive 的性能。然而，对于不同语言变异领域的可靠性存在担忧。许多研究提出了局部对抗性攻击的鲁棒性评估措施，但我们仍需要全局鲁棒且无偏于不同语言风格的模型。我们采取更广泛的方法，探索跨越社会人口统计学维度的各种变化，对语言模型的推理能力进行结构化可靠性测试。我们将 SocialIQA 数据集扩展以创建基于社会人口统计学风格的不同释义集。评估旨在深入了解 LLMs 在以下方面的能力：(a) 使用工程化的提示生成人口统计学释义的能力；(b) 在现实世界复杂语言场景中的推理能力。我们还探讨了诸如困惑度、可解释性和 ATOMIC 表现等指标，用于对这些数据集上的 LLMs 进行细粒度可靠性分析。我们发现特定人口统计学的释义显著影响了语言模型的表现，这表明语言变异的细微差别仍然是一个重要挑战。代码和数据集将被公开，以便重现实验结果和促进未来的研究。|
|**2025-01-14**|**Addressing the sustainable AI trilemma: a case study on LLM agents and RAG**|Hui Wu et.al.|[2501.08262](http://arxiv.org/abs/2501.08262)|null|大型语言模型（LLMs）已经展示了显著的能力，但它们的广泛部署和更高级的应用引发了关键的可持续性挑战，特别是在推理能耗方面。我们提出了可持续人工智能三难概念，强调了人工智能能力、数字公平性和环境可持续性之间的紧张关系。通过系统地研究LLM代理和检索增强生成（RAG）的案例研究，我们分析了嵌入在内存模块设计中的能源成本，并引入了新的指标来量化能源消耗与系统性能之间的权衡。我们的实验结果揭示了当前内存增强框架中存在的重大能源低效问题，并表明资源受限的环境面临不成比例的效率惩罚。我们的发现挑战了目前以LLM为中心的设计范式，并为开发更可持续的人工智能系统提供了实用的见解。|
|**2025-01-14**|**Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models**|Yifu Qiu et.al.|[2501.08248](http://arxiv.org/abs/2501.08248)|null|近期的长上下文语言模型（LCLMs）的进步有望通过简化管道来改变基于检索的生成（RAG）。借助其扩展的上下文窗口，LCLMs能够处理整个知识库，并直接执行检索和推理——我们将其能力定义为端到端检索与推理（ICR^2）。然而，现有的基准测试如LOFT往往高估了LCLM的表现，因为它们提供的上下文过于简单。为了解决这一问题，我们引入了ICR^2，这是一个评估LCLMs在更现实场景下的基准测试，包括使用强大检索器检索到的干扰性文档。然后我们提出了三种方法来增强LCLM的表现：（1）检索后生成的微调，（2）检索注意力探针，利用注意力头在解码过程中过滤并去噪长上下文，以及（3）联合训练检索头和生成头。我们的评估显示，当应用到Mistral-7B时，这五种知名LCLM在LOFT和ICR^2上的表现有显著提升：相比普通的RAG和监督微调，Exact Match得分分别提高了17和15分，以及13和2分。即使作为一款较小的模型，它也在大多数任务上超过了GPT-4-Turbo。|
|**2025-01-13**|**Imagine while Reasoning in Space: Multimodal Visualization-of-Thought**|Chengzu Li et.al.|[2501.07542](http://arxiv.org/abs/2501.07542)|null|链式思维（CoT）提示在增强大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的复杂推理能力方面已被证明非常有效。然而，在复杂的空间推理任务中，它却表现不佳。尽管如此，人类的认知超越了语言本身，使我们能够用语言和图像进行思考。受此机制启发，我们提出了一种新的推理范式，即多模态思维可视化（MVoT），它通过生成推理轨迹的图像可视化来实现多模态大语言模型的视觉思考。为了确保高质量的可视化，我们在自回归多模态大语言模型中引入了标记差异损失。这一创新显著提高了视觉的一致性和保真度。我们通过几个动态的空间推理任务验证了这种方法。实验结果表明，MVoT在各项任务中表现出竞争力，并且在CoT失效的最具挑战性的场景中展现出稳健且可靠的表现提升。最终，MVoT为复杂推理任务开辟了新的可能性，其中视觉思考可以有效地补充语言推理。|
|**2025-01-13**|**ML Mule: Mobile-Driven Context-Aware Collaborative Learning**|Haoxiang Yu et.al.|[2501.07536](http://arxiv.org/abs/2501.07536)|null|人工智能已融入日常生活的方方面面，从利用计算机视觉进行物体检测到使用大型语言模型撰写电子邮件和智能家庭中的紧凑模型。这些机器学习模型服务于个体用户，但通常与他们分离，因为它们通常存储和处理在集中式数据中心。这种集中式方法引发了隐私问题，产生了高昂的基础设施成本，并且难以实现个性化。为了应对这些问题，提出了联邦学习和完全去中心化学习方法，但它们仍然依赖于集中式服务器或由于通信限制而面临收敛缓慢的问题。为了解决这些挑战，我们提出了ML Mule方法，该方法利用各个移动设备作为“骡子”来训练和传输模型快照，随着这些设备在物理空间中的移动，与所占据的空间共享这些模型。这种方法隐式地在与分享特定空间的用户相关的设备之间形成亲缘群体，从而实现协作模型演化，并保护用户的隐私。我们的方法解决了传统、联邦和完全去中心化学习系统的多个主要缺点。所提出的框架代表了一类新的机器学习方法，这些方法更加稳健、分布和个性化，使该领域更接近于实现智能、适应性和真正情境感知的智能环境的最初愿景。结果显示，与现有方法相比，ML Mule的收敛速度更快，模型准确性更高。|
|**2025-01-13**|**Investigating Large Language Models in Inferring Personality Traits from User Conversations**|Jianfeng Zhu et.al.|[2501.07532](http://arxiv.org/abs/2501.07532)|null|大型语言模型（LLMs）在多个领域展现出类似人类的能力，包括心理评估。本研究评估了LLMs，特别是GPT-4o和GPT-4o mini，在零样本提示条件下能否从用户对话中推断出大五人格特质并生成大五库存表-10（BFI-10）项目分数。我们的研究结果表明，在直接推断特质之前先提示生成BFI-10项目分数这一中间步骤可以提高准确性，并且与黄金标准更加一致。这种结构化方法强调了利用心理框架以提高预测精度的重要性。此外，基于抑郁症状存在情况的群体比较显示了模型性能的不同。参与者被分为两组：至少有一种抑郁症状的组和没有症状的组。GPT-4o mini在有症状组中对神经质和尽责性等与抑郁相关的特质变化表现出更高的敏感度，而GPT-4o在跨组分析中表现出更强的细微解读能力。这些发现突显了LLMs有效分析现实世界心理数据的潜力，为人工智能和心理学交叉领域的跨学科研究奠定了宝贵基础。|
|**2025-01-13**|**RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment**|Difei Gu et.al.|[2501.07525](http://arxiv.org/abs/2501.07525)|**[link](https://github.com/difeigu/radalign)**|**自动化胸部X光影像解释既需要准确的疾病分类也需要生成详细的放射学报告，这在临床工作流程中提出了重大挑战。目前的方法要么专注于提高分类准确性而牺牲了解释性，要么通过图像描述技术生成详细但可能不可靠的报告。在这项研究中，我们提出了RadAlign，这是一种新颖的框架，它结合了视觉语言模型（VLM）的预测准确性与大型语言模型（LLM）的推理能力。受放射科医生工作流程的启发，RadAlign首先采用专门的VLM将视觉特征与关键医学概念对齐，在多种疾病的平均AUC达到0.885，从而实现优越的疾病分类。这些识别出的医学状况以基于文本的概念形式在对齐的视觉-语言空间中表示，然后用于提示基于LLM的报告生成。增强的基于检索的生成机制使RadAlign能够根据相似的历史病例来定位输出，从而提供更好的报告质量，GREEN得分为0.678，优于最先进方法的0.634。我们的框架保持了强大的临床解释性，同时减少了幻觉现象，通过集成的预测和生成式AI推进了医学影像和报告分析。代码可在<https://github.com/difeigu/RadAlign>获取。**|
|**2025-01-13**|**Parallel Key-Value Cache Fusion for Position Invariant RAG**|Philhoon Oh et.al.|[2501.07523](http://arxiv.org/abs/2501.07523)|null|近期大型语言模型（LLMs）的进步突显了利用外部信息的检索增强生成（RAG）的必要性。然而，LLMs对相关信息在上下文中的位置很敏感，并且当此类信息位于中间时，往往会产生不正确的响应，这被称为“迷失于中间”现象。在这篇论文中，我们介绍了一个框架，该框架能够为仅解码器模型生成一致的输出，而与输入上下文顺序无关。实验结果针对三个开放领域问答任务展示了位置不变性，即模型对于输入上下文顺序不敏感，并且相比现有的RAG管道方法具有更好的鲁棒性，更能抵抗无关段落的影响。|
|**2025-01-13**|**Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards**|Yangsibo Huang et.al.|[2501.07493](http://arxiv.org/abs/2501.07493)|null|现在通常通过让人类手动投票来评估大型语言模型（LLMs）的输出，这与评估某项特定任务的知识或技能的典型基准不同。聊天机器人竞技场是最受欢迎的此类基准之一，它通过让用户在两个随机选择的模型之间选择更好的响应来排名模型（不透露哪个模型负责生成）。这些平台普遍被认为是对LLM能力进行公平和准确衡量的可靠方法。在本文中，我们展示了如果没有实施机器人防护和其他防御措施，这些基于投票的基准可能容易受到对抗性操纵。具体而言，我们表明攻击者可以以大约一千票的成本（在一个模拟的、离线版本的聊天机器人竞技场中验证过）改变排行榜（以推广他们喜欢的模型或贬低竞争对手）。我们的攻击包括两步：首先，我们展示了攻击者如何能够以超过95%的准确率确定给定回复是由哪个模型生成的；然后，攻击者可以利用这些信息一致地投票支持（或反对）目标模型。与聊天机器人竞技场开发人员合作，我们识别、提出并实施了改进措施，以提高聊天机器人竞技场对对抗性操纵的鲁棒性，根据我们的分析，这些措施大大增加了此类攻击的成本。其中一些防御措施在我们的合作之前就已经存在，例如使用Cloudflare进行机器人防护、恶意用户检测和速率限制。还有一些措施，包括reCAPTCHA和登录功能，正在被集成以加强聊天机器人竞技场的安全性。|
|**2025-01-13**|**TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language Models**|Thales Sales Almeida et.al.|[2501.07482](http://arxiv.org/abs/2501.07482)|null|在快速发展的知识图景和大型语言模型日益普及的背景下，需要不断更新这些模型以反映当前事件。虽然现有的基准测试评估了模型的一般事实回忆能力，但它们通常忽视了两个关键方面：模型通过持续学习整合不断演变的知识的能力以及其性能在不同区域之间的显著差异。为了解决这些差距，我们引入了“及时事件基准”（Timely Events Benchmark，简称TiEBe），这是一个包含超过11,000个问题-答案对的数据集，重点关注全球和地区的重要事件。TiEBe利用来自维基百科的结构化回顾数据，实现了连续更新，用于评估LLMs对不断变化的全球事务的知识及其对不同地区事件的理解。我们的基准测试表明，LLMs在事实回忆方面表现出显著的地理差异，强调了对更平衡的全球知识表示的需求。此外，TiEBe作为评估持续学习策略的工具，提供了有关模型获取新信息而不遗忘过去知识的能力的见解。|
|**2025-01-13**|**A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities**|Yihao Liu et.al.|[2501.07468](http://arxiv.org/abs/2501.07468)|null|全球范围内的医疗系统在效率、可及性和个性化方面持续面临挑战。借助现代人工智能技术如多模态大型语言模型和世界模型，具身人工智能（EmAI）代表了一个变革的前沿，提供了增强的自主性和与物理世界的互动能力，以应对这些挑战。作为跨学科且迅速发展的研究领域，“医疗中的具身人工智能”涵盖了算法、机器人技术和生物医学等广泛领域。这种复杂性强调了及时审查和分析的重要性，以便跟踪进展、解决问题并促进跨学科合作。在本文中，我们对医疗中EmAI的“大脑”进行了全面概述，介绍了用于感知、动作、规划和记忆的基础AI算法，并重点关注了涵盖临床干预、日常护理与陪伴、基础设施支持以及生物医学研究的医疗应用。尽管前景广阔，但医疗中EmAI的发展受到诸如安全问题、仿真平台与现实应用之间的差距、缺乏标准化基准以及跨学科领域进展不均等的关键挑战的阻碍。我们讨论了技术障碍并探讨了伦理考虑，提出了EmAI在医疗领域的未来展望。还引入了一种智能层级框架来指导进一步发展。通过提供系统的见解，本研究旨在激发创新和实际应用，为智能、以患者为中心的医疗开辟新纪元。|
|**2025-01-13**|**Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is Not AGI**|Rolf Pfister et.al.|[2501.07458](http://arxiv.org/abs/2501.07458)|null|OpenAI的o3在ARC-AGI基准测试中取得了87.5%的高分，这引发了关于基于大型语言模型（LLM）的系统，特别是o3，是否展示了智能并朝着人工通用智能（AGI）迈进的问题。借鉴ARC-AGI创建者François Chollet对技能和智能之间区别的理解，本文提出了一个新的智能理解：一个代理实现更多样化目标的能力越强，它所处的世界越多样化，并且所需的知识越少，那么它就越智能。对ARC-AGI基准测试的分析表明，其任务代表了一种非常特定类型的问题，这些问题可以通过预先定义的操作组合进行大量尝试来解决。o3也采用了这种方法，通过大量使用计算能力实现了高分。然而，对于物理世界和人类领域中的大多数问题，解决方案无法提前测试，预先定义的操作也不可用。因此，像o3那样通过预先定义的操作进行大量尝试不能成为AGI的基础——相反，需要新的方法来可靠地解决各种问题而无需现有的技能。为了支持这一发展，本文概述了一个新的智能基准测试，该测试涵盖了更高多样性未知任务的解决，从而能够全面评估智能和向AGI的进步。|
|**2025-01-13**|**Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests Through Precise Contextual Information Injection**|Xin Yin et.al.|[2501.07425](http://arxiv.org/abs/2501.07425)|null|尽管已经提出了许多基于学习的方法来生成单元测试，并且取得了显著的性能，但它们仍然受限于依赖特定任务的数据集。最近，通过提示工程引导的大规模语言模型（LLMs）引起了关注，因为它们能够处理包括单元测试生成在内的广泛任务。尽管取得了成功，LLMs在为焦点方法或函数生成单元测试时可能会出现幻觉，这是由于它们缺乏对项目全局上下文的认识。这些幻觉可能表现为调用不存在的方法，以及参数或返回值不正确，例如参数类型或数量不匹配。虽然许多研究探讨了上下文的作用，但它们通常为不同的模型和焦点方法提取固定的上下文模式，这可能不适合所有生成过程（例如，过多的无关上下文可能导致冗余，妨碍模型专注于重要信息）。为了解决这一限制，我们提出了RATester，它通过全局上下文信息注入增强LLM生成更符合存储库需求的单元测试的能力。为了使LLM具备类似人类测试者的全局知识，我们集成了语言服务器gopls，它提供了定义查找等关键功能来辅助LLM。当RATester遇到一个不熟悉的标识符（如不熟悉的结构体名称）时，它首先利用gopls获取相关定义和文档注释，然后使用这些全局知识来指导LLM。通过利用gopls，RATester丰富了LLM对项目全局上下文的知识，从而减少了单元测试生成过程中的幻觉。|
|**2025-01-10**|**LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs**|Omkar Thawakar et.al.|[2501.06186](http://arxiv.org/abs/2501.06186)|**[link](https://github.com/mbzuai-oryx/llamav-o1)**|**推理是解决复杂多步问题的基本能力，尤其是在视觉环境中，逐步理解至关重要。现有的方法缺乏一个全面的框架来评估视觉推理，并且不强调逐步解决问题的能力。为此，我们提出了一个全面的框架，通过三个关键贡献来推进大型语言模型（LLMs）中的逐步视觉推理。首先，我们引入了一个视觉推理基准，专门设计用于评估多步推理任务。该基准呈现了一组多样化的挑战，包括八个不同类别，从复杂的视觉感知到科学推理，总共包含超过4000个推理步骤，使LLMs在多个步骤中执行准确且可解释的视觉推理的能力得到稳健评估。其次，我们提出了一种新的度量标准，该标准在单个步骤的粒度上评估视觉推理质量，强调正确性和逻辑连贯性。与传统的端任务准确性指标相比，所提出的度量标准提供了更深入的推理性能见解。第三，我们提出了一种新的多模态视觉推理模型，名为LlamaV-o1，采用多步课程学习方法进行训练，在此过程中任务逐渐组织以促进技能的增量获取和问题解决。所提出的LlamaV-o1旨在进行多步推理并通过结构化的训练范式逐步学习。广泛的实验表明，我们的LlamaV-o1在现有开源模型上表现出色，并且相对于闭源专有模型也有良好的表现。与最近的Llava-CoT相比，我们的LlamaV-o1在六个基准测试中的平均得分为67.3，绝对提升3.8%，并且在推理扩展方面比其快五倍。我们的基准、模型和代码都是公开可用的。**|
|**2025-01-10**|**PEACE: Empowering Geologic Map Holistic Understanding with MLLMs**|Yangyu Huang et.al.|[2501.06184](http://arxiv.org/abs/2501.06184)|null|地质地图作为地质科学中的基础图件，提供了关于地表和地下结构与组成的重要见解。这些地图在灾害检测、资源勘探和土木工程等多个领域至关重要。尽管其重要性不言而喻，当前的多模态大型语言模型（MLLMs）在理解地质地图方面往往存在不足。这一差距主要由于制图概括的挑战性，这包括处理高分辨率地图、管理多个相关组件以及需要特定领域的知识。为了量化这一差距，我们构建了GeoMap-Bench，这是首个评估MLLMs在地质地图理解方面的基准测试，评估了全面的能力，包括提取、引用、接地、推理和分析。为弥合这一差距，我们引入了GeoMap-Agent，这是首个专为地质地图理解设计的代理，具有三个模块：分层信息提取（HIE）、领域知识注入（DKI）和增强提示的问题回答（PEQA）。受到人类科学家之间跨学科合作的启发，一个由AI专家组成的团队担任顾问，利用多样化的工具池全面分析问题。通过广泛的实验，GeoMap-Agent在GeoMap-Bench上取得了0.811的整体得分，显著优于GPT-4o的0.369。我们的工作，即通过MLLMs赋能地质地图全面理解（PEACE），为地质学中的先进AI应用铺平了道路，提高了地质调查的效率和准确性。|
|**2025-01-10**|**Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories**|Gerd Kortemeyer et.al.|[2501.06143](http://arxiv.org/abs/2501.06143)|null|我们研究了一种基于大型语言模型的人工智能系统GPT-4o在多语言和多模态环境下的表现，该系统在来自PhysPort网站的多个语言和主题领域的物理概念测试集中进行了评估。这些测试集涵盖了经典物理学的主题，包括力学、电磁学、光学、热力学以及相对论、量子力学、天文学、数学和实验室技能。与之前的纯文本研究不同，我们将测试集作为图像上传，反映了学生在纸质考试中看到的情况，从而评估了系统的多模态功能。系统用英语提示，并自主选择响应的语言——可能是保持测试的语言不变，完全切换到英语，或者混合使用语言——这揭示了根据语言复杂性和数据可用性而变化的适应行为。我们的结果表明，在不同主题领域中存在一些性能差异，其中实验室技能是表现最差的领域。此外，系统对需要解释图像的问题的回答表现不如对纯文本问题的回答。系统难以回答的问题往往与测试所使用的语言有关。我们还发现，不同语言之间的性能存在较大差异，有些语言似乎从语言切换中受益匪浅，这种现象类似于人类说话者的代码转换。总体而言，将获得的AI结果与现有文献进行比较，我们发现该AI系统在所有主题领域（除实验室技能外）的表现均优于课后本科生的平均水平。|
|**2025-01-10**|**Supervision policies can shape long-term risk management in general-purpose AI models**|Manuel Cebrian et.al.|[2501.06137](http://arxiv.org/abs/2501.06137)|**[link](https://github.com/manuelcebrianramos/llm_supervision_policies)**|**快速部署和广泛使用通用人工智能（GPAI）模型，包括大型语言模型（LLM），给AI监管机构带来了前所未有的挑战。我们假设这些机构需要应对一个新兴的风险和事件报告生态系统，这可能会超出它们的监督能力。为了研究这一问题，我们开发了一个模拟框架，该框架通过从风险、事件或危害报告生态系统的多样化景观中提取特征进行参数化，包括社区驱动平台、众包倡议和专家评估。我们评估了四种监督政策：非优先级（先到先得）、随机选择、基于优先级（首先解决最高优先级的风险）和基于多样性的优先级（在平衡高优先级风险的同时全面覆盖各种风险类型）。结果表明，虽然基于优先级和基于多样性的策略在缓解由专家识别的高影响风险方面更为有效，但它们可能会无意中忽略由更广泛社区报告的系统性问题。这种忽视可能导致某些类型的报告被放大而其他类型的报告被抑制，从而导致对整体风险景观的扭曲认识。我们将模拟结果与几个现实世界的数据集进行了验证，其中包括超过一百万次ChatGPT交互，其中超过150,000次对话被识别为存在风险。这一验证强调了AI风险管理中的复杂权衡，并突显了监督政策的选择如何塑造社会中使用的各种GPAI模型的未来风险景观。**|
|**2025-01-10**|**Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented Conversational AI**|Yuya Asano et.al.|[2501.06129](http://arxiv.org/abs/2501.06129)|null|通用的自动语音识别（ASR）系统在目标导向的对话中并不总是表现良好。现有的ASR校正方法依赖于先前的用户数据或命名实体。我们将校正扩展到没有先前用户数据且表现出语言灵活性的任务上，例如词汇和句法变化。我们提出了一种结合大型语言模型的新型上下文增强方法以及一种结合目标导向会话AI及其任务对话状态的上下文信息的排序策略。我们的方法通过词性和语义相似性对（1）n-best ASR假设进行排名，并通过发音对应性对上下文进行排名。在家居改进和烹饪领域使用真实用户的评估表明，与不使用校正的方法相比，我们的方法提高了校正的召回率和F1值分别提升了34%和16%，同时保持了精度和误报率不变。当我们的校正方法正常工作时，用户评分比未校正的情况高0.8至1分（满分5分），并且由于误报而导致的评分下降没有出现。|
|**2025-01-10**|**Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding**|Fabian David Schmidt et.al.|[2501.06117](http://arxiv.org/abs/2501.06117)|**[link](https://github.com/fdschmidt93/fleurs-slu)**|尽管最近的多语言自动语音识别模型声称支持数千种语言，但由于双模态语音和文本训练数据有限，低资源语言的自动语音识别仍然极不可靠。更好的多语言口语理解（SLU）可以通过利用语言语义来补偿稀缺的训练数据，从而大大增强多语言自动语音识别的鲁棒性，例如通过上下文解析歧义或利用跨语言的语义相似性。此外，对于大约一半没有正式书写系统的现存语言来说，SLU对于包容性的语音技术是不可或缺的。然而，多语言SLU的评估仍然局限于较浅的任务，如意图分类或语言识别。为了解决这个问题，我们介绍了Fleurs-SLU，这是一个包含102种语言的话题性语音分类和92种语言的多选题理解测试的多语言SLU基准。我们在Fleurs-SLU上广泛评估了端到端的语音分类模型以及结合语音转文本转录和后续由大型语言模型分类的级联系统。我们的结果显示，级联系统在多语言SLU任务中表现出更高的鲁棒性，尽管经过适当预训练的语音编码器可以在话题性语音分类中实现具有竞争力的表现。我们还发现，稳健的多语言自动语音识别、有效的语音到文本翻译和强大的多语言SLU之间存在很强的相关性，这突显了声学和语义语音表示之间的相互益处。|
|**2025-01-10**|**From Conversation to Automation: Leveraging Large Language Models to Analyze Strategies in Problem Solving Therapy**|Elham Aghakhani et.al.|[2501.06101](http://arxiv.org/abs/2501.06101)|null|问题解决疗法（PST）是一种结构化的心理方法，通过引导个体识别问题、集思广益寻求解决方案、做出决策以及评估结果来帮助他们管理压力和解决个人问题。随着心理健康护理越来越多地整合聊天机器人和大型语言模型（LLM）等技术，了解如何有效自动化PST非常重要。本研究利用匿名的治疗转录文本，使用各种LLM和基于转换器的模型来分析和分类治疗干预措施。结果显示，GPT-4o在识别PST策略方面达到了最高的准确率（0.76），优于其他模型。此外，我们引入了一个新的沟通策略维度，增强了当前的PST框架，提供了对治疗师与患者互动更深入的见解。本研究展示了LLM在自动化复杂治疗对话分析方面的潜力，提供了一种可扩展且高效的工具用于心理健康干预。我们的注解框架可以增强PST的可访问性、效果和个性化，支持治疗师进行更精确、更有针对性的实时干预。|
|**2025-01-10**|**Addressing speaker gender bias in large scale speech translation systems**|Shubham Bansal et.al.|[2501.05989](http://arxiv.org/abs/2501.05989)|null|本研究针对语音翻译（ST）系统中的性别偏见问题展开，这种偏见可能导致冒犯性和不准确的翻译。大规模ST系统中通常存在的男性偏见通常是通过源自机器翻译（MT）系统的训练数据传递的。我们的方法包括两个关键步骤。首先，我们利用大型语言模型（LLM）根据说话者的性别来纠正翻译，以实现成本效益。其次，我们用纠正后的数据微调ST模型，使模型能够直接从音频线索生成与性别相关的翻译，而无需明确的性别输入。此外，我们提出了一种三模式微调模型，适用于说话者性别的预定义情况或不应从语音线索推断出性别的场景。我们在MuST-SHE测试集上的实验表明，与基线和其他大规模ST系统（如Seamless M4T和Canary）相比，女性说话者的翻译准确性提高了70%。|
|**2025-01-10**|**Exploring LLMs for Automated Pre-Testing of Cross-Cultural Surveys**|Divya Mani Adhikari et.al.|[2501.05985](http://arxiv.org/abs/2501.05985)|null|设计针对信息通讯技术与发展（ICTD）研究的文化相关问卷对于非西方背景的人群来说极具挑战性，尤其是在对调查问卷进行适应性调整时。之前的研究通过专家评审和试点研究来适应问卷，但这两种方法都耗费资源且耗时。为了解决这些挑战，我们提出使用大型语言模型（LLMs）来自动化问卷预测试过程在跨文化环境中。我们的研究使用了LLMs来将一个以美国为中心的气候观点调查问卷改编为适合南非受众的版本。随后，我们通过Prolific平台用116名南非参与者测试了改编后的问卷，并让他们对两个版本提供反馈。参与者认为LLM改编的问题比传统版本更优。我们的报告开启了关于LLMs在适应问卷和促进跨文化问卷设计中潜在作用的讨论。|
|**2025-01-10**|**Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study of LLM Hallucination on North Korea**|Eunjung Cho et.al.|[2501.05981](http://arxiv.org/abs/2501.05981)|null|幻觉现象在大型语言模型（LLMs）中仍然是其安全部署的重大挑战，特别是在其可能传播错误信息的情况下。大多数现有的解决方案通过关注使模型与可信来源对齐或通过改进模型传达其输出信心（或缺乏信心）的方式来应对这一挑战。虽然这些措施在大多数情况下可能是有效的，但在需要更精细方法的场景中，尤其是在获取准确数据有限或确定可信来源具有挑战性的情况下，它们可能会显得不足。在这项研究中，我们以朝鲜作为案例研究，这个国家的特点是缺乏可靠的资料来源和充斥着耸人听闻的虚假信息。我们探讨并评估了一些表现最佳的多语言LLMs以及特定语言模型在三种语言中的表现，这三种语言是在具有重大地缘政治利益的国家中使用：英语（美国、英国）、韩语（韩国）和普通话中文（中国）。我们的研究结果揭示了显著差异，表明所选择的模型和语言可以导致对朝鲜截然不同的理解，这对该国带来的全球安全挑战具有重要意义。|
|**2025-01-09**|**ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding**|Xingyu Fu et.al.|[2501.05452](http://arxiv.org/abs/2501.05452)|null|结构化图像理解，例如解读表格和图表，需要在图像的不同结构和文本之间战略性地重新关注，并形成推理序列以得出最终答案。然而，当前的多模态大型语言模型（LLMs）缺乏这种多跳选择性注意力能力。在这项工作中，我们引入了ReFocus，这是一个简单而有效的框架，使多模态LLMs能够通过代码执行视觉编辑来生成“视觉思考”，从而改变和优化它们的视觉焦点。具体而言，ReFocus使多模态LLMs能够生成Python代码来调用工具并修改输入图像，依次绘制框、突出显示部分区域并屏蔽掉某些区域，从而增强视觉推理过程。我们在涉及表格和图表的各种结构化图像理解任务上进行了实验。ReFocus在所有任务上大大提高了性能，在表格任务上的平均提升为11.0%，在图表任务上的平均提升为6.8%。我们深入分析了不同视觉编辑的效果，以及为什么ReFocus能够在不引入额外信息的情况下提高性能。此外，我们收集了一个包含14k样本的训练集，并证明这种带有中间信息的视觉链式思维提供了比标准VQA数据更好的监督，比使用问答对训练的相同模型提升了8.0%的平均性能，比使用CoT的提升了2.6%。|
|**2025-01-09**|**Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark**|Yunzhuo Hao et.al.|[2501.05444](http://arxiv.org/abs/2501.05444)|**[link](https://github.com/hychaochao/EMMA)**|在文本和图像上进行有机推理是人类智能的一个支柱，然而多模态大型语言模型（MLLMs）在这方面的多模态推理能力仍然缺乏探索。现有的基准测试往往强调以文本为主的推理或依赖浅层视觉线索，未能充分评估整合的视觉和文本推理。我们引入了EMMA（增强的多模态推理），这是一个针对数学、物理、化学和编码领域中的有机多模态推理的基准。EMMA任务要求高级的跨模态推理，这些推理不能通过各自模态独立推理来解决，从而提供了对MLLMs推理能力的增强测试套件。我们对最先进的MLLMs在EMMA上的评估揭示了它们在处理复杂的多模态和多步骤推理任务时存在显著局限性，即使使用了像思维链提示和测试时计算扩展等先进方法也表现不佳。这些发现突显了改进多模态架构和训练范式的必要性，以缩小人类与模型在多模态推理方面的能力差距。|
|**2025-01-09**|**A survey of textual cyber abuse detection using cutting-edge language models and large language models**|Jose A. Diaz-Garcia et.al.|[2501.05443](http://arxiv.org/abs/2501.05443)|null|社交媒体平台的成功促进了数字社区中各种形式的在线滥用行为的出现。这种滥用行为表现为多种形式，包括仇恨言论、网络欺凌、情感虐待、诱拐和色情内容。在本文中，我们对社交网络中普遍存在的不同形式的滥用行为进行了全面分析，特别关注新兴技术（如语言模型LMs和大型语言模型LLMs）如何重塑这些网络中滥用内容的检测和生成机制。我们深入探讨了社交媒体滥用行为的实施机制，探索其心理和社会影响。此外，我们还审视了先进语言模型的双重作用——强调它们在增强自动检测系统以识别滥用行为方面的潜力，同时也承认它们生成有害内容的能力。本文旨在为在线安全和伦理问题的持续讨论做出贡献，提供有关网络滥用行为演变景观以及缓解和加剧该问题的技术创新的见解。|
|**2025-01-09**|**Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese Micro-bloggers**|Jerry Chongyi Hu et.al.|[2501.05423](http://arxiv.org/abs/2501.05423)|null|研究公共危机期间的社会情绪对于理解意见和情绪如何变化从而导致社会两极化至关重要。我们研究了微博，即中国最受欢迎的社交媒体平台，在COVID-19疫情爆发期间的帖子。研究时间段包括COVID-19爆发前阶段、疫情爆发阶段以及早期疫情防控阶段。我们使用大型语言模型Llama 3 8B来分析平台上用户的情绪，将其分类为积极、消极、讽刺和中性类别。分析微博上的情绪变化可以洞察社会事件和政府行为如何影响公众舆论。本研究有助于理解健康危机期间社会情绪的动态，填补了对中国社交媒体平台进行情感分析的空白。通过检查这些动态，我们旨在提供有价值的视角，以了解数字通信在应对全球前所未有的挑战时塑造社会反应的作用。|
|**2025-01-09**|**FairCode: Evaluating Social Bias of LLMs in Code Generation**|Yongkang Du et.al.|[2501.05396](http://arxiv.org/abs/2501.05396)|**[link](https://github.com/yongkdu/faircode)**|**大型语言模型（LLMs）在代码生成方面展示了显著的能力，这引起了对评估其输出质量和安全性的关注。然而，关于代码生成中的偏见研究仍然有限。现有的研究通常通过应用恶意提示或重新应用任务和数据集来评估有区分能力的模型的偏见。鉴于LLMs通常与人类价值观保持一致，并且先前的数据集并未完全优化用于代码相关任务，迫切需要专门设计用于评估代码模型的基准。在这项研究中，我们介绍了FairCode，这是一个用于评估代码生成中偏见的新基准。FairCode包含两个任务：函数实现和测试用例生成，每个任务都通过不同的场景评估社会偏见。此外，我们提出了一种新的指标FairScore，用于评估模型在此基准上的表现。我们在广泛使用的LLMs上进行了实验，并提供了对结果的全面分析。研究发现，所有测试的LLMs都表现出偏见。代码可在https://github.com/YongkDu/FairCode获取。**|
|**2025-01-09**|**Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models**|Kristian G. Barman et.al.|[2501.05382](http://arxiv.org/abs/2501.05382)|null|本文探讨了构建和评估针对物理学研究需求的大型物理模型（LPMs）的理念，并提供了可能的发展路线图。这些模型基于广泛数据训练的大规模基础模型，例如大规模语言模型（LLMs），并经过专门调整以应对物理学研究的需求。LPMs可以独立工作，也可以作为综合框架的一部分。该框架可以结合专门工具，包括用于数学操作的符号推理模块、分析特定实验和模拟数据的框架，以及用于综合理论和科学文献的机制。我们首先探讨物理学界是否应该积极开发和完善专用模型，而不仅仅是依赖商业LLMs。然后，我们概述了通过物理学、计算机科学和科学哲学领域的专家之间的跨学科合作实现LPMs的方式。为了有效整合这些模型，我们确定了三个关键支柱：开发、评估和哲学反思。开发侧重于构建能够处理物理学文本、数学公式和各种物理数据的模型。评估通过测试和基准测试来评估准确性和可靠性。最后，哲学反思涵盖了对LLMs在物理学中的更广泛影响的分析，包括它们生成新的科学理解的潜力，以及可能会在研究中出现何种新型合作动态。受粒子物理学实验合作组织结构的启发，我们提出了一种类似的跨学科和协作方法来构建和改进大型物理模型。本路线图提供了具体目标，定义了实现这些目标的途径，并指出了必须解决的挑战，以便实现物理学特有的大规模AI模型。|
|**2025-01-09**|**Accelerated Diffusion Models via Speculative Sampling**|Valentin De Bortoli et.al.|[2501.05370](http://arxiv.org/abs/2501.05370)|null|推测性采样是一种加速大型语言模型推理的流行技术，通过使用快速草稿模型生成候选令牌，并根据目标模型的分布接受或拒绝它们。虽然推测性采样以前仅限于离散序列，我们将其扩展到扩散模型，这些模型通过连续的向量值马尔可夫链生成样本。在此背景下，目标模型是一个高质量但计算开销大的扩散模型。我们提出了各种草稿策略，包括一种简单有效的方法，该方法不需要训练草稿模型，并且可以立即应用于任何扩散模型。我们的实验表明，在各种扩散模型上显著加快了生成速度，将函数评估次数减半，同时生成了目标模型的精确样本。|
|**2025-01-09**|**Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction**|Hantao Lou et.al.|[2501.05336](http://arxiv.org/abs/2501.05336)|**[link](https://github.com/htlou/stream-aligner)**|**大型语言模型（LLMs）的快速发展带来了显著的能力提升，但也引发了对其与人类价值观和意图是否一致的担忧。目前的对齐策略，包括自适应训练和推理时方法，在此领域展示了潜力，但这些方法在平衡部署复杂性和各种任务和难度下的能力方面仍然面临挑战。在这项工作中，我们介绍了流式分布诱导对齐器（Stream Aligner），这是一种结合了效率和在生成过程中增强性能的新对齐范式。Stream Aligner通过使用一个小模型来学习后缀句子的偏好，迭代地纠正上游模型生成的后缀句子，并使用修正后的句子替换后续生成中的后缀句子，从而实现动态句子级修正。与Aligner相比，我们的实验表明，Stream Aligner减少了对额外模型能力的依赖，增强了LLMs的推理能力，并降低了用户交互过程中的延迟。具体而言，Stream Aligner-2B模型在测试的Llama2-70B-chat模型上实现了76.1%的帮助性提升和36.0%的无害性提升，而Stream Aligner-8B则在测试的Llama3-70B-Instruct模型上实现了3.5%的数学能力提升。**|
|**2025-01-09**|**"What's Happening"- A Human-centered Multimodal Interpreter Explaining the Actions of Autonomous Vehicles**|Xuewen Luo et.al.|[2501.05322](http://arxiv.org/abs/2501.05322)|null|公众对自动驾驶汽车的信任度正在下降。研究表明，为了促进对自主系统的信任，需要向乘客解释这些车辆的行为。解释器可以通过提高透明度和降低感知风险来增强信任。然而，当前的解决方案往往缺乏以人为中心的方法来整合多模态解释。本文介绍了一种新型的人机交互多模态解释器（HMI）系统，该系统利用人类偏好提供视觉、文本和听觉反馈。该系统结合了鸟瞰图（BEV）、地图、文本显示的视觉界面，以及使用经过微调的大语言模型（LLM）的语音互动。我们的用户研究涉及不同参与者，结果表明HMI系统显著提升了乘客对自动驾驶汽车的信任度，平均信任水平提高了超过8%，在普通环境中的信任度最高提升了30%。这些结果强调了HMI系统通过提供清晰、实时且与情境相关的车辆行为解释，有可能改善自动驾驶汽车的接受度和可靠性。|
|**2025-01-09**|**CallNavi: A Study and Challenge on Function Calling Routing and Invocation in Large Language Models**|Yewei Song et.al.|[2501.05255](http://arxiv.org/abs/2501.05255)|null|与软件系统通过聊天机器人进行交互可能具有挑战性，特别是当聊天机器人需要以正确的顺序和带有正确的参数来生成API调用时。在聊天机器人系统中进行API调用带来了显著的挑战，特别是在复杂的多步骤任务中，这要求准确选择和执行API。我们在这个领域做出了三项贡献：首先，通过引入一个新颖的数据集来评估模型在API功能选择、参数生成以及嵌套API调用方面的表现；其次，通过对不同复杂度级别的最先进语言模型进行基准测试，评估它们在API功能生成和参数准确性方面的性能；第三，提出了一种增强的API路由方法，该方法结合了通用大型语言模型用于API选择，以及针对参数生成进行微调的模型，并采用了一些提示工程方法。这些方法在处理复杂的API任务方面带来了显著改进，为实际应用中的API驱动型聊天机器人系统提供了实用的进步。|
|**2025-01-08**|**Re-ranking the Context for Multimodal Retrieval Augmented Generation**|Matin Mortaheb et.al.|[2501.04695](http://arxiv.org/abs/2501.04695)|null|检索增强生成（RAG）通过整合外部知识来提升大型语言模型（LLMs）的性能，在给定上下文中提高响应的准确性和减少幻觉现象。然而，多模态RAG系统面临着独特的挑战：（i）检索过程可能会选择与用户查询不相关的条目（例如图像、文档），（ii）视觉-语言模型或多模态语言模型如GPT-4o在处理这些条目时可能会产生幻觉，从而影响RAG输出的质量。本文旨在解决第一个挑战，即改进多模态RAG在检索阶段从知识库中选择相关上下文的过程。具体来说，我们利用之前工作中设计的相关性评分（RS）度量方法来评估RAG性能，并应用于检索过程中以选择更相关的条目。基于嵌入（如CLIP嵌入）和余弦相似度的检索通常对于多模态数据表现不佳。我们表明，通过使用更先进的相关性度量方法，可以改进检索过程，通过自适应地选择最多k个条目而非固定数量的条目来消除不相关的条目。我们的评估使用COCO数据集显示了显著改善相关上下文的选择以及生成响应的准确性。|
|**2025-01-08**|**URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics**|Ruilin Luo et.al.|[2501.04686](http://arxiv.org/abs/2501.04686)|**[link](https://github.com/URSA-MATH/URSA-MATH)**|链式思维（CoT）推理在大规模语言模型（LLMs）的数学推理中得到了广泛应用。最近，对CoT轨迹引入导数过程监督激发了关于增强测试时缩放能力的讨论，从而提升了这些模型的潜力。然而，在多模态数学推理中，高质量CoT训练数据的稀缺阻碍了现有模型实现高精度的CoT推理，并限制了其在测试时的推理潜力。在这项工作中，我们提出了一种三模块合成策略，该策略集成了CoT蒸馏、轨迹格式重写和格式统一，从而生成了一个多模态数学中的高质量CoT推理指令微调数据集MMathCoT-1M。我们全面验证了经过训练的URSA-7B模型在多个多模态数学基准测试上的最先进性能（SOTA）。为了在测试时进行扩展，我们引入了一种数据合成策略，自动生成过程注释数据集，称为DualMath-1.1M，重点关注解释和逻辑。通过进一步使用DualMath-1.1M对URSA-7B进行训练，我们从CoT推理能力过渡到稳健的监督能力。训练后的URSA-RM-7B作为验证器，有效提升了URSA-7B在测试时的性能。URSA-RM-7B还展示了出色的分布外（OOD）验证能力，展示了其泛化性。模型权重、训练数据和代码将会开源。|
|**2025-01-08**|**Enhancing Financial VQA in Vision Language Models using Intermediate Structured Representations**|Archita Srivastava et.al.|[2501.04675](http://arxiv.org/abs/2501.04675)|null|图表解释对于可视化数据分析至关重要，但准确地从图表中提取信息对自动化模型提出了重大挑战。本研究调查了DEPLOT的微调，DEPLOT是一个模态转换模块，可以将图表或图表的图像转换为线性化表格，在一个包含50,000个柱状图的自定义数据集上进行。该数据集包括简单的、堆叠的和分组的柱状图，旨在针对这些可视化独特的结构特征。微调后的DEPLOT模型使用一个包含1,000张图像的测试集与两种指标进行评估：相对映射相似度（RMS），用于衡量类别映射准确性；相对数值集相似度（RNSS），用于评估数值解释准确性。为了进一步探索大型语言模型（LLM）的推理能力，我们整理了一组额外的100张柱状图图像以及问题答案集。我们的研究结果表明，提供结构化的中间表格显著提升了LLM的推理性能，相比直接查询图像。|
|**2025-01-08**|**Assessing Language Comprehension in Large Language Models Using Construction Grammar**|Wesley Scivetti et.al.|[2501.04661](http://arxiv.org/abs/2501.04661)|null|大型语言模型尽管具有显著的能力，但在许多方面仍然会出人意料地失败，并且无法预测。由于它们是在大规模网络数据上进行训练的，因此对其真正“理解”语言的程度进行评估尤其具有挑战性。因此，我们构建了一种评估方法，通过利用建设语法（CxG）来系统地评估大型语言模型中的自然语言理解（NLU）。这种方法提供了理论基础，可以构建有针对性的评估集。这些数据集经过精心设计，包括一些在预训练数据中不太可能出现的例子，但对于人类来说却很容易理解和解释，从而能够进行更准确和可靠的评估。我们的实验集中在下游的自然语言推理和推理任务上，通过对比大型语言模型对8个独特构建结构（Cxns）所传达的潜在意义的理解与人类的理解来进行。结果表明，虽然大型语言模型展示了对构建信息的一些了解，但即使是最新模型如GPT-01，在处理这些构建结构所传达的抽象意义时也存在困难，尤其是在测试句子与预训练数据不相似的情况下。我们认为这种情况能够更准确地测试真正的语言理解能力，突显了大型语言模型在语义能力方面的关键局限性。我们将这一新颖的数据集以及相关的实验数据（包括提示和模型响应）公开发布。|
|**2025-01-08**|**Multi-task retriever fine-tuning for domain-specific and efficient RAG**|Patrice Béchard et.al.|[2501.04652](http://arxiv.org/abs/2501.04652)|null|Retrieval-Augmented生成（RAG）在部署大型语言模型（LLMs）时变得无处不在，因为它可以解决诸如生成幻觉或过时信息等典型限制。然而，在构建实际的RAG应用时，会出现一些实际问题。首先，检索到的信息通常是领域特定的。由于微调LLMs计算成本高昂，更可行的方法是微调检索器以改进输入到LLM中的数据质量。其次，随着更多应用在同一现实系统中部署，无法负担部署单独的检索器。此外，这些RAG应用通常会检索不同类型的数据。我们的解决方案是对一个小的检索器编码器进行指令微调，使其适用于各种领域特定的任务，从而允许我们部署一个编码器以服务于许多用例，从而实现低成本、可扩展性和高速性。我们展示了该编码器如何能够泛化到域外设置以及未见的真实企业用例检索任务。|
|**2025-01-08**|**FlairGPT: Repurposing LLMs for Interior Designs**|Gabrielle Littlefair et.al.|[2501.04648](http://arxiv.org/abs/2501.04648)|null|室内设计涉及精心选择和布置物件，以创造一个美观、实用且和谐的空间，同时符合客户的设计要求。这一任务尤为具有挑战性，因为成功的室内设计不仅需要在统一的风格中纳入所有必要的物件，还要确保它们的布局最大化可访问性，同时考虑成本和使用方面的各种因素。虽然已经提出了数据驱动的解决方案，但这些方案通常局限于特定房间或领域，并且缺乏对其设计理念的解释。在这篇论文中，我们探讨了大型语言模型（LLMs）是否可以直接用于室内设计。尽管我们发现LLMs目前还不能生成完整的布局，但可以通过一种结构化的方式有效地利用它们，这种方式受到了室内设计师工作流程的启发。通过系统地探测LLMs，我们可以可靠地生成一份物件清单以及指导其放置的相关约束条件。我们将这些信息转化为一个设计布局图，然后使用现成的约束优化设置生成最终布局。我们在多种设计配置下将我们的算法与现有的基于LLM的方法及人类设计进行基准测试，并采用各种定量和定性指标以及用户研究来评估结果。总之，我们证明了当以结构化方式使用时，LLMs能够有效生成多样的高质量布局，使其成为创建大规模虚拟场景的一种可行解决方案。项目网页请访问 https://flairgpt.github.io/|
|**2025-01-08**|**Knowledge Retrieval Based on Generative AI**|Te-Lun Yang et.al.|[2501.04635](http://arxiv.org/abs/2501.04635)|null|本研究开发了一种基于检索增强生成（RAG）的问题回答系统，使用中文维基百科和Lawbank作为检索来源。该系统采用TTQA和TMMLU+作为评估数据集，利用BGE-M3进行密集向量检索以获得高度相关的搜索结果，并使用BGE重排序器根据查询相关性对这些结果进行重新排序。最相关的检索结果作为参考知识供大语言模型（LLM）使用，从而提升其回答问题的能力，并建立了一个基于生成式人工智能的知识检索系统。系统的有效性通过自动和辅助性能评估两阶段进行评估：自动评估通过比较模型自动生成的标签与真实答案来计算准确度，在标准化条件下无需人工干预；辅助性能评估涉及20个金融相关的多选题，由20名没有金融背景的参与者作答。首先参与者独立作答，随后他们接收系统生成的参考信息以协助作答，考察在提供帮助的情况下系统是否能提高准确性。本研究的主要贡献包括：（1）增强的LLM能力：通过集成BGE-M3和BGE重排序器，系统能够检索和重新排序高度相关的结果，减少幻觉现象，并动态访问授权或公共知识源。（2）改进的数据隐私：定制的RAG架构使LLM能够在本地运行，无需将私有数据发送到外部服务器。这种方法增强了数据安全性，减少了对商业服务的依赖，降低了运营成本并减轻了隐私风险。|
|**2025-01-08**|**"Can you be my mum?": Manipulating Social Robots in the Large Language Models Era**|Giulio Antonio Abbo et.al.|[2501.04633](http://arxiv.org/abs/2501.04633)|null|近期，由大型语言模型驱动的机器人在对话能力方面取得了显著进展，使其能够进行接近人类的对话。然而，这些模型在人机交互（HRI）中引入了安全和安保问题，因为它们容易受到操纵，从而绕过内置的安全措施。设想一个部署在家庭中的社交机器人，本研究旨在了解普通用户如何尝试利用语言模型违反伦理原则，例如促使机器人充当生活伴侣。我们进行了一项涉及21名大学学生的试点研究，他们与Misty机器人互动，试图在三个基于特定人机交互伦理原则（依恋、自由和共情）的情景中绕过其安全机制。我们的结果显示，参与者采用了五种技术，包括侮辱和诉诸怜悯使用情感语言。我们希望这项工作能为未来的研究提供信息，以设计强大的保障措施，确保人机交互的伦理性和安全性。|
|**2025-01-08**|**Quantum-inspired Embeddings Projection and Similarity Metrics for Representation Learning**|Ivan Kankeu et.al.|[2501.04591](http://arxiv.org/abs/2501.04591)|**[link](https://github.com/ivpb/qiepsm)**|在过去的十年中，表征学习作为一种从大量数据中提取复杂信息并将其嵌入密集向量空间的关键技术，在机器学习领域崭露头角。除了其他应用外，它已成为基于对比学习的大规模语言模型和先进计算机视觉系统的重要组成部分。表征学习系统的核心组件之一是投影头，它将原始嵌入映射到不同的、通常是压缩的空间中，同时保持向量之间的相似性关系。  在本文中，我们提出了一种量子启发的投影头，并引入了相应的量子启发相似性度量。具体来说，我们将经典嵌入映射到希尔伯特空间中的量子态，并引入基于量子电路的投影头以降低嵌入维度。为了评估这种方法的有效性，我们将我们的投影头集成到了BERT语言模型中以实现嵌入压缩。我们在TREC 2019和TREC 2020深度学习基准上使用信息检索任务比较了使用我们量子启发投影头压缩的嵌入与使用经典投影头压缩的嵌入的性能。结果表明，我们的量子启发方法在参数减少32倍的情况下实现了与经典方法相当的性能。此外，当从零开始训练时，它在较小的数据集上表现尤为出色。这项工作不仅突显了量子启发方法的有效性，还强调了在神经网络中高效使用低纠缠电路模拟作为强大的量子启发技术的实用性。|
|**2025-01-08**|**InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection**|Yuhang Liu et.al.|[2501.04575](http://arxiv.org/abs/2501.04575)|**[link](https://github.com/reallm-labs/infiguiagent)**|**图形用户界面（GUI）代理由多模态大型语言模型（MLLM）提供支持，在计算机和移动电话等计算设备上展示了任务自动化方面的巨大潜力。然而，现有的代理在多步推理方面面临挑战，并且依赖于文本注释，这限制了它们的有效性。我们介绍了InfiGUIAgent，这是一种基于MLLM的GUI代理，通过两阶段的监督微调管道进行训练。第一阶段增强基本技能，如GUI理解和定位，而第二阶段利用合成数据整合分层推理和期望反射推理技能，以实现代理的原生推理能力。InfiGUIAgent在几个GUI基准测试中实现了具有竞争力的表现，突显了原生推理技能在提升GUI交互以实现自动化任务方面的影响。资源可在<https://github.com/Reallm-Labs/InfiGUIAgent>获取。**|
|**2025-01-07**|**Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos**|Haobo Yuan et.al.|[2501.04001](http://arxiv.org/abs/2501.04001)|**[link](https://github.com/magic-research/Sa2VA)**|这项工作介绍了Sa2VA，这是首个统一模型，能够对图像和视频进行密集的基础理解。与现有仅限于特定模态和任务的多模态大型语言模型不同，Sa2VA通过最少的一次性指令调整就能支持广泛的图像和视频任务，包括指代分割和对话。Sa2VA结合了SAM-2——一个基础视频分割模型，与LLaVA——一个先进的视觉语言模型，并将文本、图像和视频统一到一个共享的LLM标记空间中。利用LLM，Sa2VA生成指导SAM-2产生精确掩码的指令标记，从而实现对静态和动态视觉内容的基础多模态理解。此外，我们引入了Ref-SAV，这是一个包含超过72000个复杂视频场景中的物体表达的自标注数据集，旨在提升模型性能。我们还手动验证了Ref-SAV数据集中2000个视频对象，以评估复杂环境下的指代表格视频对象分割。实验表明，Sa2VA在多个任务上达到了最先进水平，特别是在指代表格视频对象分割方面，展示了其在复杂现实世界应用中的潜力。|
|**2025-01-07**|**RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance**|Matin Mortaheb et.al.|[2501.03995](http://arxiv.org/abs/2501.03995)|null|检索增强生成（RAG）通过使用外部知识来指导响应生成，从而改进大型语言模型（LLMs），减少幻觉现象。然而，RAG，尤其是多模态RAG，可能会引入新的幻觉源：(i)检索过程可能选择与数据库不相关的片段（如文档、图像）作为原始上下文，(ii)检索到的图像通过视觉-语言模型（VLMs）转换成基于文本的上下文，或者直接由多模态语言模型（MLLMs）如GPT-4o使用，这可能会产生幻觉。为了解决这个问题，我们提出了一种新颖的框架来评估多模态RAG的可靠性，使用两种性能度量：(i)相关性评分（RS），评估检索到的条目与查询的相关性，(ii)正确性评分（CS），评估生成响应的准确性。我们使用ChatGPT衍生的数据库和人工评估者样本训练RS和CS模型。结果显示这两种模型在测试数据上达到了约88%的准确率。此外，我们构建了一个包含5000个样本的人工标注数据库，用于评估检索到的片段的相关性和响应陈述的正确性。我们的RS模型比CLIP在检索时更符合人类偏好20%，而我们的CS模型有大约91%的时间与人类偏好一致。最后，我们使用RS和CS评估了各种RAG系统的选取和生成性能。|
|**2025-01-07**|**Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles**|Yuxi Xia et.al.|[2501.03991](http://arxiv.org/abs/2501.03991)|null|校准，即模型置信度与预测准确性的对齐，在大型语言模型（LLMs）的可靠部署中至关重要。现有工作忽略了衡量其方法对其他提示风格和不同规模LLMs的泛化能力。为解决这一问题，我们定义了一个涵盖12个LLMs和四种提示风格的控制实验环境。此外，我们还研究了是否结合多个LLMs的响应一致性以及适当的损失函数可以改善校准性能。具体来说，我们构建了Calib-n，这是一种新的框架，它训练一个辅助模型来进行置信度估计，该模型聚合来自多个LLMs的响应以捕捉模型间的共识。为了优化校准，我们将焦点损失和AUC代理损失与二元交叉熵相结合。在四个数据集上的实验表明，响应一致性和焦点损失都能从基线方法中提升校准性能。我们发现少样本提示对于基于辅助模型的方法最为有效，并且辅助模型在准确性变化时表现出稳健的校准性能，优于LLMs内部概率和口头化的置信度。这些见解深化了对影响LLM校准因素的理解，支持它们在各种应用中的可靠部署。|
|**2025-01-07**|**(De)-Indexing and the Right to be Forgotten**|Salvatore Vilella et.al.|[2501.03989](http://arxiv.org/abs/2501.03989)|null|在数字时代，遗忘的挑战已成为一个重要问题，尤其是在个人数据及其在线可访问性的管理方面。被遗忘权（RTBF）允许个人请求删除公开访问中过时或有害的信息，然而实施这一权利对搜索引擎提出了实质性的技术难题。本文旨在向非专业人士介绍信息检索（IR）和脱索引的基础概念，这些概念对于理解搜索引擎如何能够“遗忘”某些内容至关重要。我们将探讨各种IR模型，包括布尔模型、概率模型、向量空间模型以及基于嵌入的方法，并研究大型语言模型（LLM）在增强数据处理能力方面的作用。通过提供这一概述，我们力求突出在平衡个人隐私权与搜索引擎在管理信息可见性所面临的运营挑战时所涉及的复杂性。|
|**2025-01-07**|**VLM-driven Behavior Tree for Context-aware Task Planning**|Naoki Wake et.al.|[2501.03968](http://arxiv.org/abs/2501.03968)|**[link](https://github.com/microsoft/scene-aware-robot-BT-planner)**|大型语言模型（LLMs）在生成行为树（BTs）方面最近引起了机器人界的关注，但其发展仍处于初级阶段。在本文中，我们提出了一种新颖的框架，该框架利用视觉-语言模型（VLMs）交互式地生成和编辑涉及视觉条件的行为树，从而实现复杂视觉环境下的上下文感知机器人操作。我们方法的一个关键特征在于通过自提示的视觉条件进行条件控制。具体来说，VLM生成具有视觉条件节点的行为树，其中条件表达为自由形式的文本。另一个VLM过程将文本整合到提示中，并在机器人执行过程中对真实世界图像进行评估以判断条件是否满足。我们在一个真实的咖啡馆场景中验证了我们的框架，展示了其可行性和局限性。|
|**2025-01-07**|**Vision Language Models as Values Detectors**|Giulio Antonio Abbo et.al.|[2501.03957](http://arxiv.org/abs/2501.03957)|null|大型语言模型整合文本和视觉输入已经引入了解释复杂数据的新可能性。尽管这些模型在基于视觉刺激生成连贯且语境相关的文本方面表现出色，但它们与人类感知在识别图像中相关元素方面的对齐仍需进一步探索。本文研究了最先进LLM与人类注释员在检测家庭环境场景中相关元素的对齐情况。我们创建了一组包含十二张不同家庭场景图片，并邀请十四名注释员识别每张图片中的关键元素。然后我们将这些人类反馈与五种不同LLM（包括GPT-4和四种LLaVA变体）的输出进行了比较。我们的研究结果揭示了不同程度的对齐，其中LLaVA 34B表现最佳，但仍得分较低。然而，对结果的分析突显了这些模型检测图像中价值相关元素的潜力，这表明通过改进训练和优化提示，LLM可以增强其在社会机器人、辅助技术和人机交互领域的应用，提供更深入的见解和更贴切的响应。|
|**2025-01-07**|**Localizing AI: Evaluating Open-Weight Language Models for Languages of Baltic States**|Jurgita Kapočiūtė-Dzikienė et.al.|[2501.03952](http://arxiv.org/abs/2501.03952)|null|尽管大型语言模型（LLMs）已经改变了我们对现代语言技术的期望，但数据隐私问题常常限制了商业上可用的、托管在欧盟管辖范围之外的LLMs的应用。这限制了它们在政府、国防和其他敏感数据部门的应用。在这项工作中，我们评估了一些本地可部署的开源权重LLMs在支持诸如立陶宛语、拉脱维亚语和爱沙尼亚语等使用较少的语言方面的程度。我们检查了顶级多语言开源模型Llama~3、Gemma~2、Phi和NeMo的各种大小和精度变体在机器翻译、多项选择题回答和自由形式文本生成中的表现。结果表明，虽然某些模型如Gemma~2的表现接近顶级商业模型，但许多LLMs在这些语言方面存在困难。然而，最令人惊讶的是，我们发现尽管这些模型在翻译性能上接近最先进的水平，但它们仍然容易出现词汇幻觉，在所有开源多语言LLMs中，至少有五分之一的单词存在错误。|
|**2025-01-07**|**Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection**|Pablo Miralles-González et.al.|[2501.03940](http://arxiv.org/abs/2501.03940)|null|大型语言模型（LLMs）的快速发展显著提高了它们生成连贯且语境相关的文本的能力，这引发了关于滥用AI生成内容的担忧，并使得检测这些内容变得至关重要。然而，这一任务仍然充满挑战，尤其是在未见过的领域或面对不熟悉的LLMs时。利用LLM的下一个令牌分布输出提供了一种理论上具有吸引力的方法进行检测，因为这些模型在其广泛的预训练过程中涵盖了来自多样语料库的见解。尽管这种方法很有前景，但零样本方法在尝试实现这些输出时取得了有限的成功。我们假设其中一个问题是，它们使用平均值来聚合跨令牌的下一个令牌分布度量，而某些令牌自然更容易或更难预测，应该给予不同的权重。基于这一想法，我们提出了Perplexity注意力加权网络（PAWN），它使用LLM的最后一层隐藏状态和位置来根据序列长度上一系列特征的度量结果加权求和。虽然不是零样本方法，但我们的方法允许我们在磁盘上缓存最后一层隐藏状态和下一个令牌分布度量，大大减少了训练资源需求。PAWN在分布内表现出与最强基线（微调LLMs）相比甚至更好的性能，同时仅使用其可训练参数的一小部分。我们的模型对未见领域的泛化能力也更好，决策边界在分布变化中的变异性较小。它还对抗性攻击更为稳健，如果基础模型具有多语言能力，它在未在监督训练期间看到的语言方面也能呈现相当好的泛化能力，例如，LLaMA3-1B在九种语言的交叉验证中达到了81.46％的平均宏F1分数。|
|**2025-01-07**|**Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study**|Ramya Jonnala et.al.|[2501.03904](http://arxiv.org/abs/2501.03904)|null|大型语言模型（LLMs）在公共交通运输系统中的集成提供了增强城市流动性的变革机会。本研究探讨了LLMs在圣安东尼奥交通系统背景下革新公共交通管理的潜力。利用LLMs在自然语言处理和数据分析方面的功能，我们研究了它们优化路线规划、减少等待时间以及提供个性化旅行协助的能力。通过利用通用公交传输规范（GTFS）和其他相关数据，本研究旨在展示LLMs如何能够提升资源配置、提高乘客满意度并为交通运营提供数据驱动的决策支持。进行了不同ChatGPT模型的比较分析，以评估它们理解交通信息、检索相关信息并提供全面回应的能力。本研究的结果表明，尽管LLMs对公共交通具有巨大的潜力，但实现其全部潜力需要仔细的工程设计和微调。圣安东尼奥作为一个案例研究，旨在为其他城市环境中的LLM驱动交通系统的开发提供参考。|
|**2025-01-07**|**LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token**|Shaolei Zhang et.al.|[2501.03895](http://arxiv.org/abs/2501.03895)|**[link](https://github.com/ictnlp/llava-mini)**|**在实时大型多模态模型（LMMs）如GPT-4o的推动下，高效LMMs引起了相当大的兴趣。LMM框架通常将视觉输入编码为视觉标记（连续表示），并将它们与文本指令一起融入到大型语言模型（LLMs）的上下文中，在这种情况下，大量的参数和众多的上下文标记（主要是视觉标记）导致了显著的计算开销。先前对高效LMMs的努力总是集中在用更小的模型替换LLM主干上，而忽视了关键的标记数量问题。在本文中，我们介绍了LLaVA-Mini，一种具有最少视觉标记的高效LMM。为了在保持视觉信息的同时实现视觉标记的高压缩比，我们首先分析了LMMs如何理解视觉标记，并发现大多数视觉标记仅在LLM主干的早期层中起关键作用，在这些层中它们主要将视觉信息融合到文本标记中。基于这一发现，LLaVA-Mini引入了模态预融合技术，提前将视觉信息融合到文本标记中，从而使得传入LLM主干的视觉标记能够被极度压缩成一个标记。LLaVA-Mini是一种统一的大规模多模态模型，可以高效地支持图像、高分辨率图像和视频的理解。在11个基于图像和7个基于视频的基准测试中进行的实验表明，LLaVA-Mini仅使用1个视觉标记而非576个就能超越LLaVA-v1.5。效率分析显示，LLaVA-Mini可以减少77%的浮点运算（FLOPs），在40毫秒内提供低延迟响应，并且能够在拥有24GB内存的GPU硬件上处理超过10,000帧的视频。**|
|**2025-01-06**|**BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning**|Beichen Zhang et.al.|[2501.03226](http://arxiv.org/abs/2501.03226)|**[link](https://github.com/beichenzbc/booststep)**|**最先进的大型语言模型（LLMs）在解决复杂的数学问题时，通过分而治之的管道和情境学习（ICL）示例展示了有希望的性能。然而，它们的改进潜力受到其ICL示例中的两个关键问题的限制：粒度不匹配以及随之而来的负面影响噪声问题。具体而言，这些模型能够在分割过程中表现出色，但在少数几个征服步骤中的推理却往往不准确，而以问题粒度检索的ICL示例有时缺乏特定挑战性推理步骤的相关步骤。这种不一致可能会因为其不相关性阻碍正确的推理。为此，我们专注于提高每个步骤中的推理质量，并提出了BoostStep。BoostStep使检索和推理之间的粒度对齐到步骤粒度，并通过一种新颖的“首次尝试”策略提供每个推理步骤高度相关的ICL示例。BoostStep提供的示例比粗粒度的问题粒度策略更加相关，从而稳步提升模型在每个步骤中的推理质量。BoostStep是一种通用且稳健的推理增强方法，不仅提高了独立推理性能，还可以与蒙特卡洛树搜索方法（MCTS）无缝集成，以优化候选生成和决策制定。定量结果显示，它分别使GPT-4o和Qwen2.5-Math-72B在各种数学基准测试上的表现提升了3.6%和2.0%，并且与MCTS结合使用时提升了7.5%。**|
|**2025-01-06**|**Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text**|Ayat Najjar et.al.|[2501.03212](http://arxiv.org/abs/2501.03212)|null|大型语言模型（LLMs）的开发引发了关于如何识别由生成式人工智能还是人类产生的内容的问题。一方面，当学生过度依赖这些工具时，可能会影响他们写作或编程技能的发展；另一方面，也存在剽窃问题。本研究旨在支持检测和识别使用LLM工具生成的文本内容的努力。我们假设可以通过机器学习（ML）来检测LLM生成的文本，并调查能够识别和区分由多个LLM工具生成的文本的ML模型。我们利用了几种ML和深度学习（DL）算法，如随机森林（RF）和递归神经网络（RNN），并运用了可解释的人工智能（XAI）来理解属性分类中的重要特征。我们的方法分为1）二元分类，以区分人写文本和AI文本，以及2）多分类，以区分人写文本和由五个不同LLM工具（ChatGPT、LLaMA、Google Bard、Claude和Perplexity）生成的文本。结果显示，在多分类和二元分类中都取得了高准确率。我们的模型在98.5%的准确率上优于GPTZero，后者准确率为78.3%。值得注意的是，GPTZero无法识别约4.2%的观察样本，但我们的模型能够识别完整的测试数据集。XAI结果表明，理解不同类别中的特征重要性可以建立详细的作者/来源档案，进一步有助于属性分析，并通过突出独特的风格和结构元素来支持剽窃检测，确保内容原创性的验证。|
|**2025-01-06**|**Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity**|Ayat A. Najjar et.al.|[2501.03203](http://arxiv.org/abs/2501.03203)|null|本研究旨在通过提供工具来检测学生作品中的AI生成内容以增强学术诚信，使用先进的技术。研究结果促进了透明度和问责制，帮助教育工作者维持道德标准，并支持在教育中负责任地整合AI。本工作的关键贡献是生成了CyberHumanAI数据集，其中包含1000个观察样本，500个由人类撰写，另外500个由ChatGPT生成。我们在CyberHumanAI数据集上评估了各种机器学习（ML）和深度学习（DL）算法，比较了人类撰写和AI生成的内容（即，来自大型语言模型（LLM）的ChatGPT）。结果显示，传统的机器学习算法，特别是XGBoost和随机森林，分别达到了83%和81%的准确率。结果还表明，分类较短内容似乎比分类较长内容更具挑战性。此外，使用可解释的人工智能（XAI），我们确定了影响机器学习模型预测的判别特征，在这些特征中，人类撰写的内容倾向于使用实用的语言（例如，使用和允许）。而AI生成的文本则以更抽象和正式的术语为特征（例如，领域和雇佣）。最后，与GPTZero进行的对比分析显示，我们的集中式、简单且经过微调的模型在分类纯AI、纯人类和混合类别时可以胜过像GPTZero这样的通用系统。所提出的模型在分类任务中的准确率为约77.5%，而GPTZero的准确率为48.5%。GPTZero在处理具有挑战性和小内容的案例时，倾向于将其分类为混合或未识别类别，而我们提出的模型在三个类别上的表现更加均衡。|
|**2025-01-06**|**CLIX: Cross-Lingual Explanations of Idiomatic Expressions**|Aaron Gluck et.al.|[2501.03191](http://arxiv.org/abs/2501.03191)|null|自动化定义生成系统已被提出以支持语言学习者的词汇扩展。然而，这些系统成功的主要障碍在于学习者往往难以理解定义，因为定义中可能包含不熟悉的单词和语法，尤其是在涉及非标准语言时。为了解决这些挑战，我们提出了CLIX任务，即习语性解释的跨语言表达。我们探索了当前NLP模型在该任务中的能力，并观察到尽管仍然具有挑战性，大型语言模型显示出潜力。最后，我们进行了详细的错误分析，以突出在我们可以将这些系统可靠地纳入教育工具之前需要解决的关键挑战。|
|**2025-01-06**|**Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text**|Ali Al-Lawati et.al.|[2501.03166](http://arxiv.org/abs/2501.03166)|**[link](https://github.com/aliwister/ast-icl)**|**大型语言模型（LLMs）在各种自然语言处理任务中展示了出色的性能，包括语义解析，即将自然语言转换为正式的代码表示。然而，反向过程，即把代码转换回自然语言的过程被称为语义描述，在此过程中得到了较少的关注。随着LLMs被集成到用于代码生成、安全分析和教育目的的平台中，这个任务变得越来越重要。本文专注于SQL查询的描述（SQL2Text），以解决在LLM生成的代码可能带来潜在安全风险的时代下理解并解释SQL查询的关键需求。我们通过引入使用GPT-4迭代ICL提示来生成多个额外表述的方式，重新利用了Text2SQL数据集用于SQL2Text，从而增强了该逆向任务的数据集的鲁棒性。我们使用基于不同样本选择方法的上下文学习（ICL）进行实验，重点是更小、计算效率更高的LLMs。我们的研究结果表明，利用SQL的固有图形属性来进行ICL样本选择，相比随机选择在BLEU得分上最多可提高39%，并且比其他方法提供更好的结果。数据集和代码已发布：https://github.com/aliwister/ast-icl。**|
|**2025-01-06**|**Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches**|Alhassan Mumuni et.al.|[2501.03151](http://arxiv.org/abs/2501.03151)|null|生成式人工智能（AI）系统基于大规模的预训练基础模型（PFMs），如视觉-语言模型、大型语言模型（LLMs）、扩散模型和视觉-语言-动作（VLA）模型，在各种领域和背景下已经展示了解决复杂且真正非平凡的AI问题的能力。特别是多模态大型语言模型（MLLMs），它们从庞大而多样的数据源中学习，允许对世界进行丰富而细腻的表示，从而提供广泛的技能，包括推理能力、有意义的对话能力；与人类和其他代理合作共同解决复杂问题的能力；以及理解人类的社会和情感方面。尽管有这些令人印象深刻的表现，基于大规模数据集训练的最先进LLMs的认知能力仍然表面化且脆弱。因此，通用LLMs在广义能力上受到了严重限制。要使LLMs达到人类水平的通用智能，需要解决一些基础性问题——具身性、符号接地、因果关系和记忆。这些问题更符合人类认知，并为LLMs提供了内在的人类认知属性，支持实现物理上可行、语义上有意义、灵活且更具泛化的知识和智能。在这项工作中，我们将讨论上述基础性问题，并综述实现这些概念在LLMs中的最新方法。具体来说，我们将讨论如何利用具身性、符号接地、因果关系和记忆的原则以有机的方式实现人工通用智能（AGI）。|
|**2025-01-06**|**VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity**|Yerong Li et.al.|[2501.03139](http://arxiv.org/abs/2501.03139)|null|情景化培训在许多公共服务部门已被广泛采用。最近大型语言模型（LLM）的发展显示了模拟多样化角色来创建这些培训场景的前景。然而，关于如何开发LLM来模拟受害者以用于情景化培训目的知之甚少。在本文中，我们介绍了VicSim（受害者模拟器），一种新颖的模型，解决了用户模拟的三个关键维度：信息真实性、情感动态和语言风格（例如语法使用）。我们开创性地将基于场景的受害者建模与基于GAN的训练流程及基于关键信息的提示相结合，旨在增强模拟受害者的逼真度。我们的对抗性训练方法教会判别器将语法和情感线索作为识别合成内容的可靠指标。根据人类评估者的评价，VicSim模型在人性化方面优于GPT-4。|
|**2025-01-06**|**PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models**|Mingyang Song et.al.|[2501.03124](http://arxiv.org/abs/2501.03124)|**[link](https://github.com/ssmisya/PRMBench)**|**过程级奖励模型（PRMs）对于复杂推理和决策任务至关重要，在这些任务中每个中间步骤在推理过程中都扮演着重要角色。由于语言模型在推理过程中容易出现各种错误，因此PRMs需要具备检测实际场景中各种隐式错误的细微能力。然而，当前的基准测试主要集中在步骤正确性上，未能系统地评估PRMs的表现。为了解决这一差距，我们引入了PRMBench，这是一个专门设计的过程级基准测试，旨在评估PRMs的细粒度错误检测能力。PRMBench包含6216个精心设计的问题和83456个步骤级别的标签，从多个维度评估模型，包括简单性、合理性和敏感性。在对15个模型进行的实验中，涵盖既开源的PRMs也包括以批评者模型形式使用的闭源大型语言模型，我们揭示了当前PRMs的重大弱点。这些发现突显了过程级评估所固有的挑战，并指出了未来研究的关键方向。我们希望PRMBench能成为推进PRM评估和开发研究的强大基准。**|
|**2025-01-06**|**CAT: Content-Adaptive Image Tokenization**|Junhong Shen et.al.|[2501.03120](http://arxiv.org/abs/2501.03120)|null|大多数现有的图像标记器将图像编码为固定数量的令牌或补丁，忽视了图像复杂性的固有变化。为了解决这个问题，我们引入了一种基于内容的自适应标记器（CAT），它根据图像内容动态调整表示容量，并将较简单的图像编码为较少的令牌。我们设计了一个基于描述语的评估系统，利用大型语言模型（LLMs）来预测内容复杂性并确定给定图像的最佳压缩比，同时考虑了对人类感知至关重要的因素。通过对具有不同压缩比的图像进行训练，CAT在图像重建方面表现出稳健的性能。我们还利用其可变长度的潜在表示来训练扩散变换器（DiTs）用于ImageNet生成。通过优化令牌分配，CAT在使用相同浮点运算的情况下提高了FID分数，并将推理吞吐量提升了18.5%。|
|**2025-01-06**|**LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases**|Dylan Bouchard et.al.|[2501.03112](http://arxiv.org/abs/2501.03112)|**[link](https://github.com/cvs-health/langfair)**|**大型语言模型（LLMs）在许多方面表现出偏见，可能对特定群体造成或加剧影响，这些群体通常根据性别、种族、性取向或年龄等受保护属性来识别。为了帮助解决这一问题，我们介绍了LangFair，这是一个开源的Python包，旨在为LLM从业者提供工具，以评估与其特定用例相关的偏见和公平风险。该包提供了轻松生成评估数据集的功能，这些数据集由LLM对特定用例提示的响应组成，随后计算适用于从业者用例的相关指标。为了指导指标选择，LangFair提供了一个可操作的决策框架。**|
|**2025-01-03**|**VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**|Chaoyou Fu et.al.|[2501.01957](http://arxiv.org/abs/2501.01957)|**[link](https://github.com/VITA-MLLM/VITA)**|**最近的多模态大型语言模型（MLLMs）通常侧重于整合视觉和文本模态，而对语音在增强交互方面的作用关注较少。然而，语音在多模态对话系统中扮演着关键角色，由于视觉和语音模态的根本差异，同时实现在这两个任务中的高性能仍然是一项重大挑战。在本文中，我们提出了一种精心设计的多阶段训练方法，该方法逐步训练大型语言模型以理解视觉和语音信息，最终实现流畅的视觉和语音交互。我们的方法不仅保留了强大的视觉-语言能力，还实现了高效的语音到语音对话功能，无需单独的自动语音识别（ASR）和文本到语音（TTS）模块，从而显著加速了端到端响应速度。通过在图像、视频和语音任务的基准测试中与最先进的方法进行比较，我们证明了我们的模型具备强大的视觉和语音能力，能够实现接近实时的视觉和语音交互。**|
|**2025-01-03**|**Cold-Start Recommendation towards the Era of Large Language Models (LLMs): A Comprehensive Survey and Roadmap**|Weizhi Zhang et.al.|[2501.01945](http://arxiv.org/abs/2501.01945)|**[link](https://github.com/yuanchenbei/awesome-cold-start-recommendation)**|冷启动问题一直是推荐系统中的一个长期挑战，其重点在于准确地对新用户或交互有限的用户及物品进行建模，以提供更好的推荐。由于互联网平台的多样化和用户及物品的指数级增长，冷启动推荐（CSR）的重要性日益凸显。同时，大型语言模型（LLMs）在建模用户和物品信息方面取得了巨大成功，并具有强大的能力，为冷启动推荐提供了新的潜力。然而，研究界在这一领域仍缺乏全面的回顾与反思。基于此，本文站在大型语言模型的时代背景下，对冷启动推荐的路线图、相关文献以及未来方向进行了全面的回顾和讨论。具体而言，我们探索了现有冷启动推荐如何利用信息的发展路径，从内容特征、图关系和领域信息，到大型语言模型所具有的世界知识，旨在为冷启动推荐的研究和工业社区提供新的见解。我们将冷启动推荐的相关资源收集并持续更新，供社区使用，详见https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation。|
|**2025-01-03**|**Virgo: A Preliminary Exploration on Reproducing o1-like MLLM**|Yifan Du et.al.|[2501.01904](http://arxiv.org/abs/2501.01904)|**[link](https://github.com/rucaibox/virgo)**|近期，基于大型语言模型（LLMs）的慢思考推理系统在推理过程中延长思考时间引起了广泛关注。此外，人们也开始对将其能力扩展到多模态大型语言模型（MLLMs）产生兴趣。鉴于MLLMs需要处理跨不同模态的更复杂的数据语义，实现多模态慢思考系统似乎更具挑战性。为了解决这个问题，我们在这篇论文中探索了一种简单的方法，通过使用少量文本长篇推理数据微调一个有能力的MLLM，从而构建一个多模态慢思考系统，称为Virgo（视觉推理带长篇思考）。我们发现，这些用自然语言表达的长篇推理过程可以有效地转移到MLLMs上。此外，这样的文本推理数据似乎比视觉推理数据更能激发MLLMs的慢思考能力。尽管这项工作尚处于初步阶段，但它表明慢思考能力从根本上与语言模型组件相关，这可以在模态或领域之间进行转移。这一发现可以用来指导开发更强大的慢思考推理系统。我们将资源发布在https://github.com/RUCAIBox/Virgo。|
|**2025-01-03**|**Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions**|Rachneet Sachdeva et.al.|[2501.01872](http://arxiv.org/abs/2501.01872)|**[link](https://github.com/ukplab/poate-attack)**|尽管已经做出了巨大的努力来使大型语言模型与人类价值观和伦理准则保持一致，这些模型仍然容易受到复杂的越狱攻击的利用，而这些攻击则利用了它们的推理能力。传统的安全机制通常侧重于检测明确的恶意意图，从而使更深层次的漏洞未被解决。在这项工作中，我们介绍了一种越狱技术POATE（即对立查询生成、对抗模板构建和详述），该技术利用对比推理来引出不道德的响应。POATE生成具有语义相反意图的提示，并结合对抗模板以微妙地引导模型产生有害响应。我们在六个不同语言模型家族中进行了广泛的评估，这些模型的参数大小各不相同，包括LLaMA3、Gemma2、Phi3和GPT-4，以证明这种攻击的鲁棒性，其攻击成功率显著提高（约44%）相比现有方法。我们评估了所提出的攻击对七种安全防御措施的影响，揭示了它们在解决基于推理的漏洞方面的局限性。为了应对这种情况，我们提出了一种防御策略，通过因果推理提示和逆向思维来提高推理的鲁棒性，从而缓解基于推理的对抗性攻击。|
|**2025-01-03**|**Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification**|Xiangxiang Dai et.al.|[2501.01849](http://arxiv.org/abs/2501.01849)|**[link](https://github.com/tarfersoul/maco)**|大型语言模型（LLMs）的显著生成能力引发了对自动响应生成在不同应用中的兴趣。鉴于用户偏好是动态变化的，并且LLM响应性能存在不确定性，设计高效的在线学习算法以识别最优的LLM响应（即既符合高质量标准又能满足用户偏好的响应）至关重要。大多数现有的在线算法采用集中式方法，并未能利用明确的用户偏好来实现更高效和个性化的LLM响应识别。相比之下，本文介绍了一种名为MACO（多智能体会话在线学习以适应性地识别LLM响应）的方法：1）通过多个本地代理（如智能手机）加速在线LLM响应识别过程，同时增强数据隐私；2）提出了一种新颖的会话机制，以自适应地进行对话以征求用户偏好（例如，在生成的响应中更喜欢幽默的语气而不是严肃的语气），从而最小化偏好估计的不确定性。我们的理论分析表明，MACO在累积遗憾方面接近最优。此外，MACO通过消除之前工作中传统的、计算密集型的“G-最优设计”，降低了通信成本和计算复杂度。大量的实验使用开源LLM Llama，结合来自Google和OpenAI的两种不同的嵌入模型进行文本向量表示，证明了MACO在在线LLM响应识别方面显著优于当前最先进的方法。|
|**2025-01-03**|**MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning**|Pu Yang et.al.|[2501.01834](http://arxiv.org/abs/2501.01834)|null|图像描述生成是计算机视觉和自然语言处理交叉领域中的一个关键任务，在各个领域都有广泛的应用。对于复杂的任务如诊断报告生成，深度学习模型不仅需要特定领域的图像-描述数据集，还需要结合相关通用知识以提供上下文准确性。现有的方法存在固有限制：专门化模型在捕捉特定领域细节方面表现出色但缺乏泛化能力，而基于大型语言模型的视觉-语言模型（VLMs）虽然利用了通用知识但在特定领域适应方面存在困难。为了解决这些限制，本文提出了一种新颖的代理增强型模型协作框架，我们称之为MoColl，旨在有效整合特定领域和通用知识。具体而言，我们的方法是将复杂的图像描述生成任务分解为一系列相互关联的问答子任务。采用可训练的视觉问答（VQA）模型作为专门工具，专注于特定领域的视觉分析，根据图像内容回答特定任务的问题。同时，基于大型语言模型的代理利用其通用知识来制定这些问题，并将得到的问题-答案对综合成连贯的描述。除了在利用VQA模型方面的作用外，该代理还指导其自身的训练以增强其特定领域的功能。实验结果在放射学报告生成上验证了所提出的框架的有效性，展示了在生成报告质量方面的显著提升。|
|**2025-01-03**|**Time Series Language Model for Descriptive Caption Generation**|Mohamed Trabelsi et.al.|[2501.01832](http://arxiv.org/abs/2501.01832)|null|自动为时间序列数据中的可观察模式生成代表性自然语言描述可以增强解释性、简化分析并提高时间数据的跨领域实用性。尽管预训练的基础模型在自然语言处理（NLP）和计算机视觉（CV）方面取得了显著进展，但其在时间序列分析中的应用受到数据稀缺性的阻碍。虽然已经提出了几种基于大型语言模型（LLM）的时间序列预测方法，但在LLM背景下，时间序列字幕生成的研究尚显不足。本文介绍了一种名为TSLM的新颖时间序列语言模型，专门用于时间序列字幕生成。TSLM作为一个编码器-解码器模型运行，利用文本提示和时间序列数据表示来捕捉多个阶段中的微妙时间模式，并生成精确的时间序列输入文本描述。TSLM通过首先利用上下文提示合成数据生成来解决时间序列字幕生成中的数据稀缺问题，其次通过应用于时间序列-字幕对的新型跨模态密集检索评分来去噪生成的数据。实验结果表明，在各种时间序列字幕生成数据集上，TSLM在多个数据模态下显著优于现有的最先进方法。|
|**2025-01-03**|**Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models**|Yanjiang Liu et.al.|[2501.01830](http://arxiv.org/abs/2501.01830)|null|自动化红队技术已成为发现大型语言模型（LLM）中漏洞的关键方法。然而，大多数现有方法主要关注孤立的安全缺陷，这限制了它们适应动态防御和有效揭示复杂漏洞的能力。为了解决这一挑战，我们提出了Auto-RT，这是一种强化学习框架，能够自动探索和优化复杂的攻击策略，通过恶意查询有效地揭示安全漏洞。具体而言，我们引入了两种关键机制以减少探索的复杂性并改进策略优化：1）早期终止探索，通过专注于高潜力的攻击策略来加速探索；2）渐进式奖励跟踪算法结合中间降级模型，动态调整搜索轨迹以实现成功的漏洞利用。广泛的实验表明，在不同的LLM上，Auto-RT通过显著提高探索效率和自动优化攻击策略，检测到更广泛的漏洞，其检测速度更快，成功率达到16.63%以上，优于现有方法。|
|**2025-01-03**|**SDPO: Segment-Level Direct Preference Optimization for Social Agents**|Aobo Kong et.al.|[2501.01821](http://arxiv.org/abs/2501.01821)|**[link](https://github.com/alibabaresearch/damo-convai)**|**社交代理由大型语言模型（LLMs）驱动，能够模拟人类社会行为，但在处理复杂的目标导向社会对话方面表现不佳。直接偏好优化（DPO）已被证明在多种代理任务中能够有效地使LLM的行为与人类偏好保持一致。现有的基于DPO的多轮交互方法分为轮次级别和会话级别方法。轮次级别的方法过于细粒度，仅关注个体轮次，而会话级别的方法则过于粗略，通常引入训练噪声。为了解决这些局限性，我们提出了分段级直接偏好优化（SDPO），该方法专注于交互中的特定关键片段来优化多轮代理行为，同时最小化训练噪声。在SOTOPIA基准上的评估表明，SDPO调优的代理在性能上始终优于现有的基于DPO的方法以及专有LLM如GPT-4o，这突显了SDPO在提高基于LLM的代理社交智能方面的潜力。我们在https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO发布了我们的代码和数据。**|
|**2025-01-03**|**Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation**|Mohammad Khalil et.al.|[2501.01793](http://arxiv.org/abs/2501.01793)|**[link](https://github.com/mohdkhalil/repository-supplementary-for-lak-25-paper--creating-artificial-students-that-never-existed)**|在本研究中，我们探讨了人工智能和深度学习技术，特别是生成对抗网络（GANs）和大型语言模型（LLMs），在生成合成表格数据方面的日益增长的潜力。访问高质量的学生数据对于推进学习分析至关重要，但隐私问题和全球范围内的更严格的数据保护法规限制了它们的可用性和使用。合成数据提供了一个有前景的替代方案。我们调查了是否可以利用合成数据来创建人工学生以服务于学习分析模型。我们使用流行的CTGAN模型和三种LLM——GPT2、DistilGPT2和DialoGPT来生成合成表格学生数据。我们的结果显示，这些方法具有强大的潜力，可以生成与真实学生数据相似度高的高质量合成数据集。为了验证我们的发现，我们应用了一整套效用评估指标来评估合成数据的统计和预测性能，并比较了所使用的不同生成器模型，特别关注了LLMs的表现。本研究旨在为学习分析社区提供有价值的见解，为该领域的方法论工具箱扩展新的创新方法奠定基础，用于学习分析数据生成。|
|**2025-01-02**|**Unifying Specialized Visual Encoders for Video Language Models**|Jihoon Chung et.al.|[2501.01426](http://arxiv.org/abs/2501.01426)|**[link](https://github.com/princetonvisualai/merv)**|最近大型语言模型（LLMs）的出现为视频领域带来了复杂的推理能力，形成了视频大型语言模型（VideoLLMs）。然而，目前的VideoLLMs依赖单一的视觉编码器进行所有视觉处理，这限制了可以传递给LLM的视觉信息的数量和类型。我们的方法MERV（多编码器视频表示）通过利用多个冻结的视觉编码器来创建统一的视频表示，从而向VideoLLM提供全面的专门化视觉知识。时空对齐每个编码器的特征使我们能够应对更广泛的一般性开放问题和多选视频理解问题，并在性能上超越先前的最先进作品。在标准的视频理解基准测试套件中，MERV比Video-LLaVA准确率高出多达3.7%，同时Video-ChatGPT得分也更高。我们在零样本感知测试准确率上也比SeViLA提高了2.2%。MERV引入的额外参数很少，并且训练速度比同等单编码器方法更快，同时并行化了视觉处理。最后，我们提供了定性证据，证明MERV成功地从每个编码器中捕获了领域知识。我们的结果为利用多个视觉编码器进行全面视频理解提供了有希望的方向。|
|**2025-01-02**|**OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data for Diverse Scenarios**|Xize Cheng et.al.|[2501.01384](http://arxiv.org/abs/2501.01384)|null|随着大型语言模型的快速发展，研究人员已经创建了越来越先进的口语对话系统，这些系统能够与人类进行自然的对话。然而，这些系统在处理现实世界对话的全部复杂性方面仍然存在困难，包括音频事件、音乐背景和情感表达，这主要是因为目前的对话数据集在规模和场景多样性上受到限制。在这篇论文中，我们提出利用合成数据来增强对话模型在不同场景中的表现。我们介绍了ShareChatX，这是首个涵盖广泛场景的综合性大规模口语对话数据集。基于这个数据集，我们介绍了OmniChat，一个多轮对话系统，具有异构特征融合模块，旨在优化不同对话场景中的特征选择。此外，我们探讨了使用合成数据训练对话系统的几个关键方面。通过全面的实验，我们确定了合成数据和真实数据之间的理想平衡比例，在DailyTalk这一现实世界的对话数据集上取得了最先进的结果。我们也强调了合成数据在应对涉及音频和音乐的多样化和复杂对话场景中的重要性。更多详情请访问我们的演示页面：https://sharechatx.github.io/|
|**2025-01-02**|**Aligning Large Language Models for Faithful Integrity Against Opposing Argument**|Yong Zhao et.al.|[2501.01336](http://arxiv.org/abs/2501.01336)|**[link](https://github.com/zhaoy777/afice)**|大型语言模型（LLMs）在复杂推理任务中展示了 impressive 的能力。然而，它们在对话过程中很容易被不忠实的论点误导，即使它们最初的陈述是正确的。为此，我们研究了在 LLMs 中保持忠实完整性的方法。这涉及确保 LLMs 在面对对立论点时坚持其忠实陈述，并能够在接收到忠实论点时纠正其错误陈述。在这项工作中，我们提出了一种名为 AFICE（Alignment for Faithful Integrity with Confidence Estimation）的新框架，旨在使 LLM 响应与忠实完整性对齐。具体而言，AFICE 首先设计了一种双边置信度估计（BCE）方法，用于估计 LLM 在给定特定上下文的情况下生成的每个响应的不确定性，该方法同时基于解码过程中的内部状态来估计模型对问题的信心以及根据累积概率比率对答案的信心。通过使用 BCE，我们构建了一个由上下文、原始陈述和论点组成的对话偏好数据集，该数据集用于利用直接偏好优化（DPO）来对齐 LLM 以保持忠实完整性。广泛的基准测试实验结果表明，在遇到对立论点时，LLM 保持忠实响应的能力显著提高，从而确保了 LLM 在复杂交互环境中的实用性和可信性。代码和数据将在 https://github.com/zhaoy777/AFICE.git 公布|
|**2025-01-02**|**CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models**|Johan Wahréus et.al.|[2501.01335](http://arxiv.org/abs/2501.01335)|**[link](https://github.com/cysecbench/dataset)**|众多研究调查了破解大型语言模型（LLM）以生成有害内容的方法。通常，这些方法是通过数据集中的恶意提示来评估的，这些提示旨在绕过LLM提供商建立的安全策略。然而，现有数据集的一般广泛范围和开放性性质可能会使评估破解效果变得复杂，特别是在特定领域，尤其是网络安全方面。为了解决这个问题，我们发布了CySecBench，这是一个包含12662个专门设计用于评估网络安全领域破解技术的数据集。该数据集分为10个不同的攻击类型类别，包含封闭式提示，以便更一致和准确地评估破解尝试。此外，我们详细介绍了数据集生成和筛选的方法，该方法可以适应于创建其他领域的类似数据集。为了展示CySecBench的实用性，我们提出并评估了一种基于提示混淆的破解方法。实验结果显示，该方法成功地从商业黑盒LLM中诱发出有害内容，在ChatGPT上达到65%的成功率，在Gemini上达到88%；相比之下，Claude表现出了更强的抵抗力，其破解成功率为17%。与现有的基准方法相比，我们的方法表现出色，突显了评估LLM安全措施时使用领域特定评估数据集的价值。此外，当使用广泛使用的数据集（即AdvBench）中的提示进行评估时，它达到了78.5%的成功率，高于最先进方法。|
|**2025-01-02**|**Decoding Knowledge in Large Language Models: A Framework for Categorization and Comprehension**|Yanbo Fang et.al.|[2501.01332](http://arxiv.org/abs/2501.01332)|null|理解大型语言模型（LLMs）如何获取、保持和应用知识仍然是一个开放的挑战。本文介绍了一个新的框架K-(CSA)²，该框架沿两个维度对LLM的知识进行分类：正确性和置信度。该框架定义了六类知识，范围从高度自信的正确性到自信持有的误解，从而超越二元准确性的范畴，对模型的理解进行了细致的评估。使用此框架，我们展示了像思维链提示和带有人类反馈的强化学习等技术从根本上改变了内部（预训练）和外部（上下文依赖）知识的知识结构。思维链特别增强了基础模型的性能，并且在应用于对齐的LLMs时显示出协同效益。此外，我们的分层分析揭示，LLMs中的高层编码更多高置信度的知识，而低置信度的知识往往出现在中下层。|
|**2025-01-02**|**The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation**|Shuzheng Gao et.al.|[2501.01329](http://arxiv.org/abs/2501.01329)|null|测试用例对于验证软件应用程序的可靠性和质量至关重要。近期的研究表明，大型语言模型（LLMs）能够为给定源代码生成有用的测试用例。然而，现有的工作主要依赖于人工编写的普通提示，这通常会导致结果不佳，因为LLMs的性能可以受到提示的高度影响。此外，这些方法对所有LLMs使用相同的提示，忽视了不同的LLMs可能最适合不同的提示这一事实。考虑到可能的提示表述方式多种多样，自动发现每个LLM的最佳提示是一个重大挑战。尽管自然语言处理领域有一些自动化提示优化的方法，但它们难以生成针对测试用例生成任务的有效提示。首先，这些方法通过简单地组合和变异现有提示来迭代优化提示，而没有适当的指导，导致生成的提示缺乏多样性，并且在生成的测试用例中容易重复相同的错误。其次，这些提示一般缺乏领域上下文知识，限制了LLMs在该任务中的表现。|
|**2025-01-02**|**Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking**|Xiaoxue Cheng et.al.|[2501.01306](http://arxiv.org/abs/2501.01306)|null|大型语言模型（LLMs）表现出色，但仍面临幻觉问题。典型文本生成方法采用自回归生成方式，没有明确的推理过程，这通常会导致不可信且事实不准确的回复。在本文中，我们提出了HaluSearch，这是一种新颖的框架，通过结合基于树搜索的算法（如MCTS）来实现明确的慢思考生成过程，以减轻LLMs在推理过程中的幻觉问题。具体而言，HaluSearch将文本生成视为逐步推理过程，并使用自我评估奖励模型对每个生成步骤进行评分，以指导树搜索朝着最可靠的生成路径，从而充分利用LLMs的内部知识。为了平衡效率和质量，我们引入了一种受认知科学中双重加工理论启发的分层思考系统切换机制，在实例和步骤层面动态切换快速和慢速思考模式，适应问题复杂度和推理状态的变化。我们在英文和中文数据集上进行了广泛的实验，结果表明我们的方法显著优于基线方法。|
|**2025-01-02**|**Large Language Models for Mental Health Diagnostic Assessments: Exploring The Potential of Large Language Models for Assisting with Mental Health Diagnostic Assessments -- The Depression and Anxiety Case**|Kaushik Roy et.al.|[2501.01305](http://arxiv.org/abs/2501.01305)|null|大型语言模型（LLMs）越来越多地引起了医疗专业人员的注意，因为它们有可能在诊断评估中提供帮助，从而缓解由患者负担重和医疗提供者短缺导致的医疗系统压力。为了使LLMs在支持诊断评估方面发挥作用，至关重要的是它们能够紧密复制临床医生使用的标准诊断程序。在这篇论文中，我们具体研究了用于重度抑郁症（MDD）的患者健康问卷-9（PHQ-9）以及用于广泛性焦虑障碍（GAD）的广泛性焦虑障碍-7（GAD-7）问卷中描述的诊断评估过程。我们调查了各种提示和微调技术，以引导专有和开源LLMs遵循这些过程，并评估LLM生成的诊断结果与专家验证的真实情况之间的一致性。对于微调，我们使用了Mentalllama和Llama模型，而对于提示，我们实验了包括GPT-3.5和GPT-4o在内的专有模型，以及如llama-3.1-8b和mixtral-8x7b等开源模型。|
|**2025-01-02**|**Does a Large Language Model Really Speak in Human-Like Language?**|Mose Park et.al.|[2501.01273](http://arxiv.org/abs/2501.01273)|null|大型语言模型（LLMs）最近引起了相当大的关注，因为它们能够生成高度自然、类似人类的文本。本研究在假设检验过程中比较了LLM生成文本和人类撰写文本的潜在社区结构。具体而言，我们分析了三组文本：原始人类撰写文本（ $\mathcal{O}$）、其LLM改写版本（$\mathcal{G}$），以及从$\mathcal{G}$派生的二次改写集（$\mathcal{S}$）。我们的分析涉及两个关键问题：（1）$\mathcal{O}$与$\mathcal{G}$之间的潜在社区结构差异是否与$\mathcal{G}$与$\mathcal{S}$之间的相同？（2）随着控制文本变异性参数的调整，$\mathcal{G}$是否会变得更接近于$\mathcal{O}$？第一个问题基于这样的假设，即如果LLM生成的文本真正类似于人类语言，则（$\mathcal{O}$，$\mathcal{G}$）对之间的差距应与（$\mathcal{G}$，$\mathcal{S}$ ）对之间的相似，因为这两对都由原始文本及其改写版本组成。第二个问题考察了LLM生成文本与人类文本之间相似度随文本生成范围变化的程度。为了回答这些问题，我们提出了一种统计假设检验框架，利用了每篇文本由于其改写关系而在所有数据集中具有相应部分这一事实。这种关系使一个数据集的相对位置可以映射到另一个，从而使两个数据集都可以根据第三个数据集所表征的空间进行量化，从而实现它们之间的直接比较。我们的结果显示，GPT生成的文本仍然不同于人类撰写的文本。|
|**2025-01-02**|**ProgCo: Program Helps Self-Correction of Large Language Models**|Xiaoshuai Song et.al.|[2501.01264](http://arxiv.org/abs/2501.01264)|null|自我校正旨在使大型语言模型（LLMs）能够在没有外部反馈的情况下自行验证和改进初始响应。然而，LLMs通常无法有效地进行自我验证并生成正确的反馈，这进一步误导了改进过程，导致自我校正的失败，尤其是在复杂的推理任务中。在本文中，我们提出了程序驱动的自我校正（ProgCo）。首先，程序驱动的验证（ProgVe）通过自动生成、自执行的验证伪代码程序实现了复杂的验证逻辑和广泛的验证。然后，程序驱动的细化（ProgRe）接收来自ProgVe的反馈，对响应和验证程序进行双重反思和细化，以减轻错误反馈在复杂推理任务中的误导。在三个指令跟随和数学基准测试中的实验表明，ProgCo实现了有效的自我校正，并且可以在与真实程序工具结合时进一步提高性能。|
|**2024-12-30**|**Distributed Mixture-of-Agents for Edge Inference with Large Language Models**|Purbesh Mitra et.al.|[2412.21200](http://arxiv.org/abs/2412.21200)|**[link](https://github.com/purbeshmitra/distributed_moa)**|**Mixture-of-Agents（MoA）最近被提出作为一种增强大型语言模型（LLMs）性能的方法，使多个独立的LLMs能够协同进行推理。这种协作方法相较于依赖单一LLM能够生成更优的用户提示响应。在本文中，我们考虑了一种分布式设置下的MoA架构，在该架构中，LLMs运行在各个边缘设备上，每个设备都与一个用户唯一关联，并配备了各自的分布式计算能力。这些设备使用去中心化的流言算法交换信息，允许不同的设备节点在没有集中式服务器监督的情况下进行通信。在所考虑的设置中，不同的用户拥有自己的LLM模型来处理用户提示。此外，设备通过流言协议共享它们各自特定于用户的提示或增强提示，以生成对某些查询的更精细答案。当相应的LLMs繁忙时，用户提示暂时存储在设备队列中。鉴于边缘设备的内存限制，确保系统中的平均队列大小保持有界至关重要。在本文中，我们在合理假设的基础上理论计算了设备队列的排队稳定性条件，并通过实验验证了这些条件。此外，我们通过实验展示了，利用开源LLMs实现分布式MoA的不同MoA配置会产生不同质量的响应，根据AlpacaEval 2.0基准评估，某些MoA配置比其他配置能产生更高品质的响应。实现代码可在以下链接获取：https://github.com/purbeshmitra/distributed_moa。**|
|**2024-12-31**|**HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation**|Zhaojian Yu et.al.|[2412.21199](http://arxiv.org/abs/2412.21199)|**[link](https://github.com/CodeEval-Pro/CodeEval-Pro)**|**我们引入了自调用代码生成这一新任务，旨在评估大型语言模型（LLMs）的渐进推理和问题解决能力。在此任务中，模型会遇到一个基础问题以及一个相关的更复杂问题。它们必须先解决基础问题，然后利用其解决方案来应对更复杂的问题。这项工作有三个关键贡献。首先，我们提出了一种生成现有基准测试更难版本的一般方法，从而产生了三个新的基准测试：HumanEval Pro、MBPP Pro 和 BigCodeBench-Lite Pro，专门用于评估LLMs在自调用代码生成上的表现。其次，通过对二十个LLMs在我们的基准测试中的实验结果分析，我们得出了两个重要观察结果：(i) 大多数LLMs在传统的代码生成基准测试如HumanEval和MBPP上表现出色，但在自调用任务上的表现下降。例如，o1-mini在HumanEval上的pass@1得分为96.2%，而在HumanEval Pro上的得分仅为76.2%。(ii) 在自调用代码生成任务中，指令微调模型相比基础模型仅显示出微小的改进。最后，我们在评估结果中揭示了存在的失败模式类型。所有这些结果都强调了在自调用代码生成任务方面需要进一步发展，并为未来研究增强LLMs的代码推理能力提供了新的方向。**|
|**2024-12-30**|**Facilitating large language model Russian adaptation with Learned Embedding Propagation**|Mikhail Tikhomirov et.al.|[2412.21140](http://arxiv.org/abs/2412.21140)|**[link](https://github.com/RefalMachine/llmtf_open)**|**大型语言模型（LLM）技术的快速发展催生了强大的开源指令调优LLM，其文本生成质量与最先进的模型如GPT-4相当。虽然这些模型的出现加速了LLM技术在敏感信息环境中的应用，但这些模型的作者并未披露必要的训练数据以复制结果，从而使得这些成果具有模型专属性。由于这些开源模型是多语言的，这反过来减少了训练特定语言LLM的好处，因为改进的推理计算效率成为这种成本高昂程序唯一保证的优势。更经济高效的选项，如词汇扩展和随后的继续预训练，也因缺乏高质量的指令调优数据而受到抑制，因为这是影响最终LLM任务解决能力的主要因素。为了解决这些限制并降低成本，我们提出了学习嵌入传播（LEP）方法。与现有方法不同，我们的方法由于对现有LLM知识的影响最小，因此需要较少的训练数据，并通过一种新颖的自定义嵌入传播过程来增强现有知识，可以跳过指令调优步骤，直接将新的语言知识植入任何现有的指令调优版本中。我们评估了四种俄语词汇适应于LLaMa-3-8B和Mistral-7B的情况，结果显示LEP与传统的指令调优方法具有竞争力，达到了与OpenChat 3.5和LLaMa-3-8B-Instruct相当的性能水平。通过自我校准和继续调优进一步提高了任务解决能力。**|
|**2024-12-30**|**ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation**|Ruixuan Liu et.al.|[2412.21123](http://arxiv.org/abs/2412.21123)|null|随着大型语言模型（LLMs）越来越依赖于网络抓取的数据集，对未经授权使用受版权保护或个人信息进行训练的担忧日益加剧。尽管有诸如《通用数据保护条例》（GDPR）等法规，数据所有者仍然无法对其内容在模型训练中的使用进行有限控制。为了解决这个问题，我们提出了ExpShield，这是一种主动自我防护机制，使内容所有者能够将其文本嵌入不可见的扰动，从而限制数据在LLMs训练中的误用，而不影响可读性。这种预先的方法使数据所有者能够直接保护敏感内容，而无需依赖第三方来执行防御。从随机扰动开始，我们展示了使用扰动来隐藏受保护内容的理由。我们进一步通过识别记忆触发器并创建陷阱来更集中地偏离模型记忆，从而提高效率。为了验证我们防御措施的有效性，我们提出了一种新的实例利用指标，该指标捕捉了由模型训练引发的个别风险。实验结果验证了我们的方法的有效性，因为MIA AUC从0.95降低到0.55，实例利用接近于零。这表明训练后个别风险没有增加，强调了主动防御在保护受版权保护数据方面的重要性。|
|**2024-12-30**|**Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense**|Yuyang Zhou et.al.|[2412.21051](http://arxiv.org/abs/2412.21051)|**[link](https://github.com/SEU-ProactiveSecurity-Group/LLM-PD)**|**云 computing 技术的快速发展和云应用程序数量的增加在日常生活中带来了许多好处。然而，不同组件的多样性和复杂性对云安全构成了重大挑战，尤其是在应对复杂和高级的网络攻击时。最近，生成型基础模型（GFMs）尤其是大型语言模型（LLMs）的进步为安全智能提供了有前景的解决方案。通过利用其在语言理解、数据分析、任务推理、行动计划和代码生成方面的强大能力，我们提出了LLM-PD，这是一种新颖的主动防御架构，能够以主动的方式抵御各种威胁。LLM-PD可以通过全面的数据分析和顺序推理高效地做出决策，并且能够动态地创建和部署针对目标云的实际防御机制。此外，它还可以基于从前交互中获得的经验灵活自我进化，并适应新的攻击场景而无需额外训练。实验结果表明，与现有方法相比，LLM-PD在防御效果和效率方面表现出色，特别是在成功率方面表现突出。**|
|**2024-12-30**|**TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization**|Chia-Yu Hung et.al.|[2412.21037](http://arxiv.org/abs/2412.21037)|**[link](https://github.com/declare-lab/TangoFlux)**|**我们介绍了TangoFlux，这是一种高效的文本到音频（TTA）生成模型，拥有5.15亿参数，能够在单个A40 GPU上仅用3.7秒生成长达30秒、采样率为44.1kHz的音频。TTA模型对齐的一个关键挑战在于创建偏好对的难度，因为TTA缺乏大型语言模型（LLMs）可用的可验证奖励或标准答案等结构化机制。为了解决这个问题，我们提出了CLAP排名偏好优化（CRPO），这是一种新颖的框架，能够迭代地生成和优化偏好数据以增强TTA对齐。我们证明了使用CRPO生成的音频偏好数据集优于现有的替代方案。通过这一框架，TangoFlux在客观和主观基准测试中均达到了最先进的性能。我们将所有代码和模型开源，以支持TTA生成领域的进一步研究。**|
|**2024-12-30**|**GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models**|Shangyu Xing et.al.|[2412.21036](http://arxiv.org/abs/2412.21036)|null|多模态大型语言模型（MLLMs）在整合视觉和语言理解方面取得了显著进展。尽管现有的基准测试评估了这些模型在丰富现实场景中的能力，但它们往往忽视了基本的感知技能，而这些技能在偏离日常现实的环境中至关重要。特别是几何感知，即解释空间关系和抽象视觉模式的能力，仍未得到充分探索。为了解决这一局限性，我们引入了GePBench，这是一个旨在评估MLLMs几何感知能力的新基准。广泛的评估结果显示，当前最先进的MLLMs在这些任务中表现出显著不足。此外，我们证明了使用来自GePBench的数据训练的模型在各种下游任务中显示出显著改进，强调了几何感知作为高级多模态应用基础的重要性。我们的代码和数据集将公开提供。|
|**2024-12-30**|**MapQaTor: A System for Efficient Annotation of Map Query Datasets**|Mahir Labib Dihan et.al.|[2412.21015](http://arxiv.org/abs/2412.21015)|**[link](https://github.com/MapQaTor/.github/tree/main/profile)**|**地图和导航服务如Google Maps、Apple Maps、OpenStreet Maps对于访问各种基于位置的数据至关重要，但它们通常难以处理自然语言地理查询。最近大型语言模型（LLMs）在问答方面取得了进展，但在从地图服务创建可靠的地理问答数据集时仍然面临挑战。我们介绍了MapQaTor，这是一个网络应用程序，它简化了可重复、可追踪的地图问答数据集的创建过程。凭借即插即用架构，MapQaTor允许用户与任何地图API无缝集成，从而以最少的设置从不同来源收集和可视化数据。通过缓存API响应，该平台确保了一致的基础事实，即使现实世界的信息发生变化也能增强数据的可靠性。MapQaTor在一个平台上集中了数据检索、注释和可视化，为评估当前基于LLM的地理空间推理提供了一个独特的机会，同时推动其能力以改进地理空间理解。评估指标显示，MapQaTor比手动方法加快了至少30倍的注释过程，这突显了其在开发地理空间资源（如复杂的地图推理数据集）方面的潜力。该网站已上线，网址是：https://mapqator.github.io/，演示视频可在：https://youtu.be/7_aV9Wmhs6Q. 查看。**|
|**2024-12-31**|**Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria**|Joonwon Jang et.al.|[2412.21006](http://arxiv.org/abs/2412.21006)|null|大型语言模型（LLMs）依赖于生成大量的中间推理单元（例如，标记、句子）以提高在各种复杂任务中的最终答案质量。虽然生成多个推理路径或迭代地优化理由可以有效提升性能，但这些方法不可避免地导致了显著更高的推理成本。在这项工作中，我们提出了一种新颖的句子级理由简化训练框架，该框架利用基于可能性的标准——简洁性——来识别并移除冗余的推理句子。与以往使用标记级简化的做法不同，我们的句子级简化框架在减少生成长度的同时保持了模型性能。这保留了LLMs原有的推理能力，并在不同的模型和任务中实现了平均17.15%的生成成本降低。|
|**2024-12-30**|**Plug-and-Play Training Framework for Preference Optimization**|Jingyuan Ma et.al.|[2412.20996](http://arxiv.org/abs/2412.20996)|null|近期，诸如DPO等偏好优化方法显著提升了大语言模型（LLMs）在包括对话和问答在内的广泛任务中的表现。然而，当前的方法在偏好优化过程中未能考虑训练样本的难度差异，导致在对准确率有高要求的任务中，特别是在数学推理任务中，性能表现平平。为了解决这一局限性，我们提出了一种新的训练框架，该框架采用多次采样来分析输出分布，为样本分配不同的权重，并将这些权重纳入偏好优化过程。这种即插即用的方法使LLMs能够在训练过程中优先处理具有挑战性的例子，从而提高学习效率。实验结果表明，我们的框架能够与各种偏好优化方法无缝集成，并在数学推理任务中实现一致的性能提升。|
|**2024-12-27**|**Can AI Help with Your Personal Finances?**|Oudom Hean et.al.|[2412.19784](http://arxiv.org/abs/2412.19784)|null|近年来，大型语言模型（LLMs）作为人工智能（AI）领域的一项变革性发展，引起了业界和学术界的广泛关注。这些经过大规模数据集训练的复杂AI系统在自然语言处理和内容生成方面表现出色。本文探讨了LLMs在解决个人理财方面关键挑战的潜力，重点关注美国市场。我们评估了几款领先的LLMs，包括OpenAI的ChatGPT、谷歌的Gemini、Anthropic的Claude和Meta的Llama，以评估它们在提供关于抵押贷款、税收、贷款和投资等主题的准确财务建议方面的有效性。研究发现，尽管这些模型的平均准确率约为70%，但在某些领域仍显示出明显的局限性。特别是，LLMs难以对复杂的财务查询提供准确的回应，不同主题上的表现差异显著。尽管存在这些局限性，分析显示这些模型的新版本在性能上有显著提升，突显了它们对于个人和财务顾问日益增长的实用性。随着这些AI系统的不断发展，它们在个人理财领域推动AI驱动应用的潜力变得越来越有前景。|
|**2024-12-27**|**Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration**|Le Chen et.al.|[2412.19770](http://arxiv.org/abs/2412.19770)|**[link](https://github.com/hpc-fortran2cpp/fortran2cpp)**|**迁移Fortran代码到C++是许多科学计算团队的常见任务，这主要是由于需要利用现代编程范式、增强跨平台兼容性以及提高可维护性。使用大型语言模型（LLMs）自动进行这种转换过程已经显示出潜力，但由于缺乏高质量的专业数据集，其效果受到了限制。在本文中，我们通过引入一个新的多轮对话数据集Fortran2CPP来解决这一挑战，该数据集专门设计用于Fortran到C++代码的迁移。我们的数据集比现有的替代方案大得多，它是使用一种独特的LLM驱动的双代理管道生成的，并结合了迭代编译、执行和代码修复以确保高质量和功能正确性。为了证明我们数据集的有效性，我们在Fortran2CPP上微调了几个开源LLMs，并在两个独立的基准测试中对其性能进行了评估。在我们的数据集上进行微调导致了显著的提升，模型的CodeBLEU得分提高了3.31倍，编译成功率提高了92%，这突显了该数据集能够同时增强翻译后C++代码的语法准确性和可编译性。我们的数据集和模型已开源，并可在我们的公共GitHub仓库中获取。\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}**|
|**2024-12-27**|**Can Large Language Models Adapt to Other Agents In-Context?**|Matthew Riemer et.al.|[2412.19726](http://arxiv.org/abs/2412.19726)|null|随着研究社区致力于构建更加动态且个性化的AI助手以适应人类交互的多样性，对大型语言模型（LLMs）的理论思维能力进行评估的兴趣日益增加。确实，最近几项研究表明，LLMs的理论思维能力相当出色，接近人类水平的表现。我们的论文旨在反驳这一观点，并认为过去的研究并未直接衡量代理性能，这可能导致发现的结果本质上是虚幻的。我们将所谓的字面理论思维与功能理论思维区分开来，即前者测量代理预测他人行为的能力，后者则基于对他人行为预测的理性响应来适应代理的行为。我们发现，顶级开源LLMs在不同的提示下可能表现出强大的字面理论思维能力，但在功能理论思维方面似乎存在困难——即使合作伙伴的策略非常简单。我们的工作强调了LLMs在适应新情况时归纳偏差的双面性。虽然这种偏差可以在有限的时间范围内带来良好的表现，但它常常阻碍向最优长期行为的收敛。|
|**2024-12-27**|**Toward Adaptive Reasoning in Large Language Models with Thought Rollback**|Sijia Chen et.al.|[2412.19707](http://arxiv.org/abs/2412.19707)|**[link](https://github.com/iQua/llmpebase)**|**大型语言模型（LLMs）通常用于通过逐步推理解决各种任务。然而，中间推理步骤或思维的结构是刚性的和单向的，如链、树或有向无环图。因此，这种不灵活且单向的推理可能无法解决具有挑战性的任务，并且在语言模型经常给出错误响应（即“幻觉”）时可能会失败。本文提出了一种新的推理框架，称为思维回滚（TR），使LLMs能够在出现“幻觉”时自适应地构建思维结构并保持有效的推理。TR的核心机制是回滚思维，这允许LLMs对思维进行错误分析，并因此回滚到任何先前出错的思维进行修订。随后，通过将这种尝试和错误纳入提示来指导LLM，每次回滚都会产生一条更可靠的推理路径。因此，从一个没有人类注释的简单提示开始，配备TR的LLM能够自适应地逐步探索思维以找到正确解决方案。综合实验表明，在数学问题和多任务推理方面，TR在解决问题率和交互成本方面表现出最先进的性能。例如，使用TR的GPT-4在MATH数据集上的解题率比当前最佳方法高出9%。**|
|**2024-12-27**|**A Large-scale Interpretable Multi-modality Benchmark for Facial Image Forgery Localization**|Jingchun Lian et.al.|[2412.19685](http://arxiv.org/abs/2412.19685)|null|图像伪造定位旨在识别图像中的篡改像素，在这一领域已经取得了显著进展。传统方法通常将其视为图像分割的一种变体，将伪造区域的二值分割作为最终产物。我们认为基本的二值伪造掩码不足以解释模型预测。它无法说明为什么模型会关注某些区域，并且对所有伪造像素一视同仁，这使得难以发现最假的部分。在这项研究中，我们通过生成针对伪造图像的显著区域聚焦解释来缓解上述局限性。为此，我们构建了一个多模态篡改追踪（MMTT）数据集，该数据集包含了使用深度伪造技术处理的人脸图像，并配以手动、可解释的文本注释。为了收集高质量的注释，指示注释者仔细观察处理过的图像并阐述伪造区域的典型特征。随后，我们收集了包含128,303个图像-文本对的数据集。利用MMTT数据集，我们开发了ForgeryTalker架构，旨在同时进行伪造定位和解释。ForgeryTalker首先训练一个伪造提示网络来识别解释性文本中的关键线索。随后，将区域提示器整合到多模态大型语言模型中进行微调，以实现定位和解释的双重目标。在MMTT数据集上进行的广泛实验验证了我们提出模型的优越性能。该数据集、代码以及预训练检查点将公开提供，以促进进一步的研究并确保结果的可重复性。|
|**2024-12-27**|**Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework**|Jiang Liu et.al.|[2412.19684](http://arxiv.org/abs/2412.19684)|null|高效的多模态大型语言模型（EMLLMs）与多模态大型语言模型（MLLMs）相比，能够减少模型大小和计算成本，并且通常部署在资源受限的设备上。然而，由于数据隐私问题，现有的开源EMLLMs很少能在预训练过程中访问私有的领域特定数据，这使得它们难以直接应用于设备特定的领域，如某些业务场景。为了解决这一弱点，本文专注于高效地将EMLLMs适应到私有领域，具体集中在两个方面：1）如何减少数据需求，2）如何避免参数微调。具体而言，我们提出了一种无需调参、自适应、通用的提示优化框架，简称\textit{\textbf{\ourmethod{}}}，该框架包括两个阶段：1）预定义提示，基于强化搜索策略生成提示优化策略树以获取优化先验；2）提示反射基于优化先验初始化提示，然后通过自我反思进一步搜索和优化提示。通过这种方法，\ourmethod{}优雅地生成了处理私有领域特定数据的“理想提示”。需要注意的是，我们的方法不需要参数微调，只需要少量数据即可快速适应私有数据的数据分布。广泛的实验表明，我们提出的\ourmethod{}在多个任务中显著提高了效率和性能，与基线相比表现出色。|
|**2024-12-27**|**CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs**|Siyu Wang et.al.|[2412.19663](http://arxiv.org/abs/2412.19663)|null|计算机辅助设计（CAD）通过实现精确的二维和三维建模、广泛的分析和优化，显著提高了设计过程的效率、准确性和创新性。现有的CAD模型创建方法依赖于潜在向量或点云，这些数据难以获取且存储成本高昂。近期多模态大型语言模型（MLLMs）的进步激发了研究人员使用自然语言指令和图像来构建CAD模型。然而，这些模型在推断准确的三维空间位置和方向方面仍然存在困难，导致确定构造几何体的空间三维起点和拉伸方向时出现不准确。本研究介绍了一种名为CAD-GPT的方法，这是一种具有空间推理增强的MLLM的CAD合成方法，它以单个图像或文本描述作为输入。为了实现精确的空间推理，我们提出了一种三维建模空间机制。该方法利用专门的空间展开机制将三维空间位置和三维草图平面旋转角度映射到一维语言特征空间，同时将二维草图坐标离散化为适当的平面空间，以便精确确定空间起点、草图方向和二维草图坐标转换。大量的实验表明，CAD-GPT在CAD模型综合方面始终优于现有的最先进方法，无论是在定量还是定性评估上。|
|**2024-12-27**|**FreStega: A Plug-and-Play Method for Boosting Imperceptibility and Capacity in Generative Linguistic Steganography for Real-World Scenarios**|Kaiyi Pang et.al.|[2412.19652](http://arxiv.org/abs/2412.19652)|null|语言学隐写术通过在看似无辜的文本中嵌入秘密信息来保护隐私，尤其是在监控环境中。生成式语言学隐写术利用语言模型（LM）的概率分布，并应用隐写算法生成隐写标记，在大型语言模型（LLM）发展后引起了关注。为了增强安全性，研究人员开发了保持分布的隐写算法，以最小化隐写采样与LM采样的差距。然而，对语言模型分布的依赖以及与现实世界掩护文本的偏差导致在面对实际场景中的隐写分析检测器时，不可察觉性不足。此外，LLM分布往往更加确定性，导致熵减少，从而降低嵌入容量。本文提出了一种称为FreStega的方法，作为一种即插即用的解决方案，用于重构用于生成式语言学隐写术的语言模型分布。FreStega在隐写文本自回归生成的每一步动态调整来自语言模型的标记概率，利用序列和空间维度。在序列调整中，温度基于瞬时熵动态调整，提高了隐写文本的多样性并增强了嵌入容量。在空间维度上，通过目标领域语料库的指导对分布进行校准，紧密模仿目标领域的实际掩护文本。通过重构分布，FreStega增强了实际场景中隐写文本的不可察觉性，并且在不损害生成文本质量的情况下，将隐写容量提高了15.41%。FreStega作为一种即插即用的解决方案，增强了现有保持分布的隐写方法在现实世界场景中的不可察觉性和嵌入容量|
|**2024-12-27**|**Xmodel-2 Technical Report**|Wang Qun et.al.|[2412.19638](http://arxiv.org/abs/2412.19638)|null|Xmodel-2是一个专门为推理任务设计的拥有12亿参数的大规模语言模型。其架构使得不同规模的模型可以共享一组统一的超参数，从而允许在较小的模型上进行广泛的实验，并将最优配置无缝转移到较大的模型上。为了最大化训练效率和稳定性，Xmodel-2采用了来自MiniCPM的WSD学习率调度器。该模型在来自多样化来源的1.5万亿个标记上进行了预训练，实现了在复杂推理和基于代理的任务中的最先进性能，同时保持了较低的训练成本。这些结果突显了高效模型设计和训练策略在提升推理能力方面的潜力。模型检查点和代码已在GitHub上公开，地址为 https://github.com/XiaoduoAILab/Xmodel-2|
|**2024-12-27**|**IMTP: Search-based Code Generation for In-memory Tensor Programs**|Yongwon Shin et.al.|[2412.19630](http://arxiv.org/abs/2412.19630)|null|处理在DRAM（DRAM-PIM）作为一种有前景的技术，用于加速现代应用如大规模语言模型（LLM）中的内存密集型操作。尽管具有潜力，当前的DRAM-PIM软件栈面临显著挑战，包括依赖手工调优库导致的可编程性差、对高级抽象支持有限以及缺乏系统优化框架。为了解决这些限制，我们提出了IMTP，这是一种针对UPMEM的基于搜索的优化张量编译器。IMTP的关键特性包括：(1) 对主机和内核张量程序联合搜索空间的自动化搜索，(2) 针对边界条件高效处理的PIM感知优化，以及(3) 用于UPMEM系统扩展搜索空间的改进搜索算法。我们在UPMEM硬件上的实验结果表明，各种UPMEM基准内核的性能提升高达8.21倍，GPT-J层的性能提升为5.33倍。据我们所知，IMTP是首个提供完全自动化、集成自动调优代码生成支持的DRAM-PIM系统张量编译器。通过弥高阶张量计算抽象与低级硬件特定需求之间的差距，IMTP为推进DRAM-PIM的可编程性并实现优化流程的简化奠定了基础。|
|**2024-12-24**|**Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems**|Fernando Jia et.al.|[2412.18601](http://arxiv.org/abs/2412.18601)|**[link](https://github.com/FJDeFi/Decentralized-Intelligence-in-GameFi)**|**在游戏化金融（GameFi）这一融合了游戏和去中心化金融（DeFi）的快速演变领域中，增强玩家参与度和经济互动的需求日益凸显。我们的GameFi生态系统旨在通过将先进的具身人工智能（AI）代理集成到GameFi平台上来彻底改变这一格局。这些由尖端的大语言模型（LLM）如GPT-4和Claude AI开发的AI代理能够进行主动、适应性和上下文丰富的交互，超越传统的预设响应，成为游戏叙事和经济体系中的关键参与者，直接影响玩家策略和游戏内经济。我们解决了当前GameFi平台的一个局限性，即缺乏沉浸式AI互动以及社区参与或创作者变现的机制。通过将AI代理与区块链技术深度融合，我们建立了一个共识驱动的去中心化GameFi生态系统。该系统使创作者能够变现他们的贡献，并促进玩家与创作者之间的民主协作。此外，通过在游戏中嵌入DeFi机制，我们增强了经济参与度并为游戏内的金融互动提供了新的机会。我们的方法提升了玩家的沉浸感和留存率，并通过将复杂的AI和DeFi元素相结合，推动了GameFi生态系统的进步，构建了更具吸引力、经济稳健且以社区为中心的游戏环境。本项目代表了GameFi领域的重大进展，为整个游戏行业提供了见解和方法论。**|
|**2024-12-24**|**A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs**|OpenMind et.al.|[2412.18588](http://arxiv.org/abs/2412.18588)|null|大型语言模型（LLMs）是所有公开知识的紧凑表示，涵盖了我们物理环境以及动物和人类行为的知识。将LLMs应用于机器人可能提供一条途径，使机器人能够执行大多数人类任务，并且仅需有限甚至零调优即可表现出色。除了日益复杂的推理和任务规划之外，设计得当的LLMs网络还提供了易于升级功能的能力，并允许人类直接观察机器人的思维过程。本文探讨了使用LLMs控制实体机器人时的优势、局限性和特殊性。基本系统由四个LLMs组成，它们通过人类语言数据总线进行通信，该数据总线通过WebSocket和ROS2消息传递实现。令人惊讶的是，尽管机器人的数据融合周期仅为1Hz，中央数据总线的数据传输速率也受限于人类大脑的速率，大约为40比特/秒，但仍然可以实现丰富的机器人行为并获得不同任务的良好性能。使用自然语言进行LLMs之间的通信使得机器人的推理和决策过程可以直接被人类观察，并且用普通英语编写的规则集使系统的行为变得极易调整。这些规则被不可更改地写入以太坊，这是一个全球性的、公开的、抗审查的图灵完备计算机。我们建议，通过使用自然语言作为交互AI之间的数据总线，并使用不可变的公共账本存储行为约束，有可能构建出结合了意外丰富性能、可升级性和持久对齐能力的机器人。|
|**2024-12-24**|**Exploring Embedding Priors in Prompt-Tuning for Improved Interpretability and Control**|Sergey Sedov et.al.|[2412.18582](http://arxiv.org/abs/2412.18582)|null|Prompt-Tuning是一种高效的适应预训练语言模型到新任务的方法，并且具有较低的计算开销，通过修改提示嵌入实现。在这项工作中，我们研究了在Prompt-Tuning中经常观察到的嵌入塌缩现象对于最终模型性能的重要性。为了解决这个问题，我们设计了嵌入先验并与收敛的Soft和Deep Prompt-Tuning方法的后验进行了比较。我们的研究结果表明，先验强烈影响调整嵌入的位置，模型可以在激活空间的不同部分（包括完全新的区域）中有效工作。由于最终的Prompt-Tuning能力有限，我们假设可控的Prompt-Tuning后验可以作为诸如思维链（COT）蒸馏等任务的良好起点。我们的实验还显示生成的轨迹在模型的激活空间中并不局限于某一位置。然而，对于不同的任务（例如NLP和算术），存在明显的激活聚类，而NLP任务（例如问答和掩码语言模型）之间的激活位于同一聚类。这些观察提出了关于单一激活聚类对大型语言模型泛化能力重要性的疑问。|
|**2024-12-24**|**Zero-resource Speech Translation and Recognition with LLMs**|Karel Mundnich et.al.|[2412.18566](http://arxiv.org/abs/2412.18566)|null|尽管近期在语音处理方面取得了进展，零资源语音翻译（ST）和自动语音识别（ASR）仍然是具有挑战性的问题。在这项工作中，我们提出利用多语言大语言模型（LLM）来执行从未见过配对音频文本数据的语言的ST和ASR。我们通过使用预训练的多语言语音编码器、多语言LLM和一个轻量级适应模块来实现这一点，该模块将音频表示映射到LLM的标记嵌入空间。我们在ST和ASR中进行了多个实验，以了解如何最好地训练模型以及哪些数据对以前未见过的语言性能影响最大。在ST中，我们的最佳模型能够在CoVoST2上达到超过23的BLEU分数，针对两种以前未见过的语言；在ASR中，我们达到了高达28.2%的WER。最后我们展示了我们系统的性能受限于LLM输出所需语言文本的能力。|
|**2024-12-24**|**Distilling Fine-grained Sentiment Understanding from Large Language Models**|Yice Zhang et.al.|[2412.18552](http://arxiv.org/abs/2412.18552)|**[link](https://github.com/hitsz-hlt/fsa-distillation)**|**细粒度情感分析（FSA）旨在从大量的观点文本中提取和总结用户意见。近期研究表明，大型语言模型（LLMs）具有出色的情感理解能力。然而，直接部署LLMs进行FSA应用会导致高昂的推理成本。因此，本文研究了将细粒度情感理解从LLMs蒸馏到小语言模型（SLMs）的方法。我们提示LLMs检查并解释给定评论的情感，然后利用生成的内容对SLMs进行预训练。此外，我们开发了一个全面的FSA基准来评估SLMs和LLMs。在该基准上的大量实验表明：(1) 蒸馏显著提升了SLMs在FSA任务中的性能，在 $F_1$ 得分上提高了6.00%，并且蒸馏后的模型仅使用2.2亿参数就能超越Llama-2-7b；(2) 蒸馏使SLMs具备了出色的零样本情感分类能力，使其能够匹配甚至超过其教师模型。这些结果表明，从LLMs蒸馏是FSA的一个非常有前景的方向。我们将在<https://github.com/HITSZ-HLT/FSA-Distillation>发布我们的代码、数据和预训练模型权重。**|
|**2024-12-24**|**Token-Budget-Aware LLM Reasoning**|Tingxu Han et.al.|[2412.18547](http://arxiv.org/abs/2412.18547)|**[link](https://github.com/geniushtx/tale)**|**推理对于大型语言模型（LLMs）在广泛的任务中表现出色至关重要。虽然像Chain-of-Thought（CoT）推理这样的方法通过将问题分解成中间步骤来提升LLM性能，但也带来了显著的令牌使用开销，导致成本增加。我们发现当前LLMs的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但选择令牌预算对实际压缩效果起着关键作用。我们提出了一个基于令牌预算感知的LLM推理框架，该框架根据推理复杂性动态估计不同问题的令牌预算，并利用估计的令牌预算来指导推理过程。实验表明，我们的方法在CoT推理中有效地减少了令牌成本，仅略有性能下降，提供了一种平衡效率和准确性的实用解决方案。代码：https://github.com/GeniusHTX/TALE。**|
|**2024-12-24**|**PLD-Tree: Persistent Laplacian Decision Tree for Protein-Protein Binding Free Energy Prediction**|Xingjian Xu et.al.|[2412.18541](http://arxiv.org/abs/2412.18541)|null|近期拓扑基础建模的进展加速了物理建模和分子研究的进步，包括对蛋白质配体结合亲和力的应用。在这项工作中，我们介绍了一种名为持续拉普拉斯决策树（PLD-Tree）的新方法，旨在解决预测蛋白质-蛋白质相互作用（PPI）亲和力这一具有挑战性的任务。PLD-Tree专注于结合界面处的蛋白质链，并采用持续拉普拉斯方法捕捉反映关键蛋白质间相互作用的拓扑不变量。这些源自持续同调的拓扑描述符通过整合来自大规模语言模型的进化尺度建模（ESM）来增强，从而融合序列信息。我们在两个基准数据集——PDBbind V2020和SKEMPI v2上验证了PLD-Tree，结果显示在复杂的留出蛋白质交叉验证中，相关系数（Rp）达到0.83。值得注意的是，我们的方法在这两个数据集上超过了所有已报道的最先进方法。这些结果强调了将机器学习技术与基于拓扑的描述符相结合在分子对接和虚拟筛选中的强大性，提供了一个用于预测蛋白质-蛋白质结合亲和力的强大且准确的框架。|
|**2024-12-24**|**Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation**|Derong Xu Xinhang Li et.al.|[2412.18537](http://arxiv.org/abs/2412.18537)|**[link](https://github.com/Applied-Machine-Learning-Lab/AMAR)**|大型语言模型（LLMs）展示了显著的能力，但在处理复杂的知识推理任务时，由于幻觉和过时的知识，它们经常会产生事实错误的输出。先前的研究试图通过从大规模知识图谱（KGs）中检索事实知识来辅助LLMs进行逻辑推理和预测答案以减轻这一问题。然而，这种方法通常会引入噪声和不相关数据，尤其是在涉及来自多个知识方面的广泛上下文的情况下。这样，LLM的关注可能会偏离问题和相关信息。在我们的研究中，我们介绍了一种自适应多方面知识图谱检索增强框架（Amar）。该方法检索包括实体、关系和子图在内的知识，并将每条检索到的文本转换为提示嵌入。Amar框架包含两个关键子组件：1）一个自对齐模块，它在实体、关系和子图之间对齐共性，以增强检索到的文本，从而减少噪声干扰；2）一个相关性门控模块，它使用软门学习问题与多方面检索数据之间的相关性分数，以确定应使用哪些信息来增强LLMs的输出，甚至完全过滤掉某些信息。我们的方法在两个常见数据集WebQSP和CWQ上实现了最先进的性能，在准确率上比最好的竞争对手提高了1.9%，在逻辑形式生成上的表现比直接将检索到的文本作为上下文提示的方法提高了6.6%。这些结果证明了Amar在提高LLMs推理能力方面的有效性。|
|**2024-12-24**|**Automated Code Review In Practice**|Umut Cihan et.al.|[2412.18531](http://arxiv.org/abs/2412.18531)|null|代码审查是提高软件质量和知识传递的广泛实践。然而，由于需要手动努力和可能的延迟，它通常被视为耗时的过程。一些AI辅助工具，如Qodo、GitHub Copilot和Coderabbit，使用大型语言模型（LLM）提供自动化审查。这些工具在行业中的影响尚待研究。本研究考察了基于LLM的自动化代码审查工具在工业环境中的影响。该研究在一个采用AI辅助审查工具（基于开源Qodo PR代理）的软件开发环境中进行。大约有238名从业人员在十个项目的范围内使用了该工具。我们重点关注三个项目，共有4335个拉取请求，其中1568个接受了自动化审查。数据收集包括三个来源：(1) 对拉取请求数据的定量分析，包括标记评论以指示开发者是否遵循了自动化评论，(2) 发送给开发者的调查，针对他们对单个拉取请求审查的经验，以及(3) 一项涉及22名从业者更广泛的调查，以了解他们对自动化审查的一般看法。73.8%的自动化评论得到了解决。然而，平均拉取请求关闭时间从5小时52分钟增加到8小时20分钟，各项目趋势不一。大多数从业者报告称自动化审查在一定程度上提高了代码质量。基于LLM的工具在软件开发中证明是有用的，增强了错误检测，提高了代码质量意识，并促进了最佳实践。然而，它也导致了拉取请求关闭时间延长，并引入了一些缺点，如错误的审查、不必要的修改和不相关的评论。|
|**2024-12-24**|**Large Language Model guided Deep Reinforcement Learning for Decision Making in Autonomous Driving**|Hao Pang et.al.|[2412.18511](http://arxiv.org/abs/2412.18511)|null|深度强化学习（DRL）在自动驾驶决策制定中展现出巨大的潜力。然而，在复杂的驾驶场景中，由于其低效的学习过程，DRL需要大量的计算资源才能实现合格的策略。此外，利用人类专家指导来增强DRL性能会带来高昂的人力成本，这限制了其实际应用。在这项研究中，我们提出了一种新的基于大型语言模型（LLM）引导的深度强化学习（LGDRL）框架，以解决自动驾驶车辆的决策问题。在此框架内，集成了基于LLM的驾驶专家，为DRL的学习过程提供智能指导。随后，为了高效利用LLM专家的指导以增强DRL决策策略的性能，通过一种创新的专家策略约束算法和一个新颖的LLM干预交互机制增强了DRL的学习和交互过程。实验结果表明，我们的方法不仅实现了卓越的驾驶性能，任务成功率达到了90%，而且与最先进的基准算法相比，显著提高了学习效率和专家指导的利用率。此外，所提出的方法使DRL代理能够在没有LLM专家指导的情况下保持一致且可靠的表现。代码和补充视频可在https://bitmobility.github.io/LGDRL/获取。|
|**2024-12-23**|**ChatGarment: Garment Estimation, Generation and Editing via Large Language Models**|Siyuan Bian et.al.|[2412.17811](http://arxiv.org/abs/2412.17811)|null|我们介绍了ChatGarment，这是一种新颖的方法，利用大型视觉语言模型（VLM）自动化地从图像或文本描述估计、生成和编辑3D服装。与之前的方法在现实场景中表现不佳或缺乏交互式编辑功能不同，ChatGarment可以从野外图像或草图中估计纸样，根据文本描述生成它们，并基于用户指令进行编辑，所有这些都在一个交互式的对话中完成。这些纸样随后可以被展平成3D服装，这些服装易于动画处理和模拟。这是通过微调VLM直接生成一个JSON文件来实现的，该文件包括服装类型和样式的文本描述以及连续数值属性。然后，这个JSON文件用于通过编程参数模型创建纸样。为此，我们改进了现有的编程模型GarmentCode，扩展了其覆盖的服装类型并简化了结构以实现高效的VLM微调。此外，我们通过自动化数据管道构建了一个大规模的图像到纸样和文本到纸样的数据集。广泛的评估表明，ChatGarment能够准确地重构、生成和编辑来自多模态输入的服装，突显了它在时尚和游戏应用中革新工作流程的潜力。代码和数据将在<https://chatgarment.github.io/>处提供。|
|**2024-12-23**|**ResearchTown: Simulator of Human Research Community**|Haofei Yu et.al.|[2412.17767](http://arxiv.org/abs/2412.17767)|**[link](https://github.com/ulab-uiuc/research-town)**|**大型语言模型（LLMs）在科学领域展示了显著的潜力，然而一个基本问题仍未得到解答：我们能否用LLMs模拟人类研究社区？解决这个问题可以深化我们对创意头脑风暴过程的理解，并激发科学洞见的自动发现。在这项工作中，我们提出了ResearchTown，这是一个用于研究社区模拟的多智能体框架。在这个框架中，人类研究社区被简化并建模为一个代理-数据图，在该图中研究人员和论文分别表示为代理类型节点和数据类型节点，并基于他们的合作关系进行连接。我们还引入了TextGNN，这是一种基于文本的推理框架，将各种研究活动（如论文阅读、论文撰写和评审撰写）建模为在代理-数据图上统一的消息传递过程的特殊形式。为了评估研究模拟的质量，我们提出了ResearchBench，这是一个使用节点掩码预测任务的基准，用于基于相似性的可扩展和客观评估。我们的实验揭示了三个关键发现：（1）ResearchTown能够提供真实的协作研究活动模拟，包括论文撰写和评审撰写；（2）ResearchTown能够在多个研究人员和多样化的论文下保持稳健的模拟；（3）ResearchTown能够产生跨学科的研究思路，这些思路有可能启发新的研究方向。**|
|**2024-12-23**|**ADC: Enhancing Function Calling Via Adversarial Datasets and Code Line-Level Feedback**|Wei Zhang et.al.|[2412.17754](http://arxiv.org/abs/2412.17754)|null|大型语言模型（LLMs）在自然语言处理和编码方面取得了显著进展，但在复杂函数调用的鲁棒性和准确性方面仍存在挑战。为了解决这些问题，本文介绍了一种名为ADC的创新方法，该方法增强了LLMs遵循函数格式和匹配复杂参数的能力。ADC利用具有逐行执行反馈的高质量代码微调数据集，提供细粒度的过程监督，促进强大的逻辑推理和对函数格式的遵守。它还采用对抗性数据集生成过程来改进参数匹配。分阶段的训练方法充分利用了丰富化的代码数据集和精炼的对抗性数据集，从而在伯克利函数调用排行榜（BFCL）基准上显著提高了函数调用能力。ADC的创新之处在于其战略性的组合了过程监督、对抗性优化和增量学习，为LLM在复杂函数调用方面的熟练程度设定了新的标准。|
|**2024-12-23**|**Deliberation in Latent Space via Differentiable Cache Augmentation**|Luyang Liu et.al.|[2412.17747](http://arxiv.org/abs/2412.17747)|null|技术使大型语言模型（LLMs）能够通过生成和关注中间推理步骤来“进行更多思考”，从而在解决复杂问题方面显示出前景。然而，标准方法是在回应之前立即生成离散令牌序列，因此可能会产生显著的延迟成本，并且难以优化。在这项工作中，我们展示了可以使用一个离线协处理器来增强冻结的LLM，该协处理器对模型的关键值（kv）缓存进行操作。这个协处理器通过一组潜在嵌入来增强缓存，旨在提高后续解码的保真度。我们使用解码器在标准预训练数据上的语言建模损失来训练这个协处理器，同时保持解码器本身冻结。这种方法使模型能够在端到端可微的方式下学习如何将其kv缓存中的额外计算提炼出来。由于解码器保持不变，协处理器可以在线下和异步地运行，如果协处理器不可用或者如果认为给定缓存不需要额外计算，语言模型仍能正常工作。实验表明，当缓存被增强时，解码器在后续多个标记上实现了更低的困惑度。此外，即使没有任何特定任务的训练，我们的实验也证明了缓存增强始终降低了困惑度并提升了各种推理密集型任务的表现。|
|**2024-12-23**|**YuLan-Mini: An Open Data-efficient Language Model**|Yiwen Hu et.al.|[2412.17743](http://arxiv.org/abs/2412.17743)|**[link](https://github.com/ruc-gsai/yulan-mini)**|**有效的大型语言模型（LLMs）预训练具有挑战性，因为这需要大量的资源和复杂的技术过程。本文详细介绍了一种名为YuLan-Mini的高性能基础模型，该模型拥有24.2亿参数，在相似参数规模的模型中达到了顶级性能。我们的预训练方法通过三个关键技术贡献来提高训练效率：一个精心设计的数据管道结合了数据清洗与数据调度策略，一种稳健的优化方法以缓解训练不稳定性，以及一种有效的退火方法，该方法结合了目标数据选择和长上下文训练。值得注意的是，YuLan-Mini在训练了1.08万亿个token后，其表现可与需要更多数据的行业领先模型相媲美。为了便于复现，我们发布了每个训练阶段的数据组成详细信息。项目详情可访问以下链接：https://github.com/RUC-GSAI/YuLan-Mini。**|
|**2024-12-23**|**Reasoning to Attend: Try to Understand How <SEG> Token Works**|Rui Qian et.al.|[2412.17741](http://arxiv.org/abs/2412.17741)|**[link](https://github.com/rui-qian/read)**|当前大型多模态模型（LMMs）支持的视觉定位通常依赖于 $\texttt{<SEG>}$标记作为文本提示来联合优化视觉语言模型（例如，LLaVA）和特定于下游任务的模型（例如，SAM）。然而，我们观察到很少有研究探讨其工作原理。在这项工作中，我们首先可视化了相似性图，这些图是通过计算$\texttt{<SEG>}$标记与从LLaVA编码器和SAM解码器的最后一层隐藏层导出的图像标记嵌入之间的语义相似性获得的。有趣的是，我们发现相似性图在激活响应方面存在显著的一致性，这揭示了$\texttt{<SEG>}$标记对图像-文本对内的语义相似性的贡献。具体来说，$\texttt{<SEG>}$标记作为文本词汇中的占位符，在大规模语言模型（LLMs）微调过程中广泛查询各个标记化的图像块，以将对象的语义从文本匹配到配对图像中。基于上述发现，我们提出了READ，它利用从相似性图中借用的高激活点指导LMMs在何处进行注意力分配，从而增强其健壮的推理能力。值得注意的是，READ具有直观的设计，即相似性作为点模块（SasP），可以无缝地应用于$\texttt{<SEG>}$ 类似的范式中，以即插即用的方式使用。此外，我们在ReasonSeg和RefCOCO(+/g)数据集上进行了广泛的实验。为了验证READ在微调后是否会出现技能遗忘的问题，我们进一步评估了其在扩展FP-RefCOCO(+/g)数据集上的生成能力。所有代码和模型均可在https://github.com/rui-qian/READ公开获取。|
|**2024-12-23**|**Knowledge Editing through Chain-of-Thought**|Changyue Wang et.al.|[2412.17727](http://arxiv.org/abs/2412.17727)|**[link](https://github.com/bebr2/editcot)**|**大型语言模型（LLMs）在广泛的自然语言处理（NLP）任务中展示了卓越的能力。然而，由于频繁重新训练的高昂成本，使这些模型保持与不断发展的世界知识同步仍然是一个重大挑战。为了解决这一挑战，知识编辑技术应运而生，用于在不重建模型的情况下更新LLMs的新信息。其中，在上下文中编辑范式因其在整合新知识的同时保持模型原有能力的有效性而脱颖而出。尽管如此，现有的在上下文中进行知识编辑的方法通常具有特定的任务性，主要集中在使用结构化知识三元组的多跳问答任务上。此外，它们依赖于少量提示的任务分解，这使得它们在多样化任务中的稳定性和泛化能力较差。针对这些局限性，我们提出了EditCoT，这是一种新的知识编辑框架，能够在各种任务中灵活高效地更新LLMs，而无需重新训练。EditCoT通过为给定输入生成一条思维链（CoT），然后使用基于更新知识的CoT编辑器迭代地优化这个CoT过程来工作。我们在一系列不同的基准测试中评估了EditCoT，涵盖了多种语言和任务。结果表明，我们的方法达到了最先进的性能，同时提供了优于现有方法的泛化、效果和稳定性，标志着知识更新领域的一个重要进步。代码和数据可在<https://github.com/bebr2/EditCoT>获取。**|
|**2024-12-23**|**Understanding the Logic of Direct Preference Alignment through Logic**|Kyle Richardson et.al.|[2412.17696](http://arxiv.org/abs/2412.17696)|null|最近的直接偏好对齐算法（如DPO）在将大型语言模型与人类偏好对齐方面显示出了巨大的潜力。这激发了许多原始DPO损失的新变体的发展。然而，鉴于缺乏一个技术性和概念性的框架来解释这些算法背后的意义，在理解这些新近提出的算法之间的差异以及开发新的直接偏好对齐损失函数方面仍然存在困难。在本文中，我们试图通过将DPA损失形式化为离散推理问题来解决这一问题。具体而言，我们探讨：给定一个现有的DPA损失，我们能否系统地推导出一个符号表达式来表征其语义？两个损失的语义如何相互关联？我们提出了一个新的形式化方法来表征基于单模型和参考模型的方法的偏好损失，并识别了一些常用DPA变体的符号形式。此外，我们展示了这种关于偏好学习的形式观点如何揭示DPA损失景观的大小和结构，使得不仅能够严格地表征最近的损失提案之间的关系，还可以系统地探索该景观并从第一原理推导新的损失函数。我们希望我们的框架和发现能为那些从事人机对齐工作的人提供有用的指导。|
|**2024-12-23**|**Large Language Model Safety: A Holistic Survey**|Dan Shi et.al.|[2412.17686](http://arxiv.org/abs/2412.17686)|**[link](https://github.com/tjunlp-lab/awesome-llm-safety-papers)**|大型语言模型（LLMs）的快速发展和部署在人工智能领域开启了新的前沿，其在自然语言理解和生成方面展现出前所未有的能力。然而，随着这些模型越来越多地集成到关键应用中，相关的安全问题也随之增加，需要对潜在风险进行彻底审查并提出相应的缓解策略。本文提供了一个全面的概述，涵盖了LLM安全性的当前状况，涉及四个主要类别：价值错位、对抗性攻击的鲁棒性、误用以及自主AI风险。除了对这四个方面缓解方法和技术评估资源的综合回顾外，我们还探讨了与LLM安全相关的四个主题：LLM代理的安全影响、可解释性在提高LLM安全性中的作用、由一系列AI公司和研究所提出的并遵循的技术路线图，以及旨在确保LLM安全的AI治理，包括国际合作、政策建议和预期的监管方向。我们的研究结果强调了采取主动、多方面的LLM安全措施的必要性，强调技术解决方案、伦理考量和健全的治理框架的结合。本综述旨在为学术研究人员、行业从业者和政策制定者提供基础资源，提供关于LLM安全整合挑战和机遇的见解，最终目标是促进LLM的安全和有益发展，以实现利用AI推动社会进步和福祉的目标。相关论文列表已公开发布在https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers。|
|**2024-12-23**|**Generating Completions for Fragmented Broca's Aphasic Sentences Using Large Language Models**|Sijbren van Vaals et.al.|[2412.17669](http://arxiv.org/abs/2412.17669)|**[link](https://github.com/sijbrenvv/completions_for_broca-s_aphasia)**|**布罗卡失语症是一种以非流利、费力和片段化的言语产出以及相对较好的理解能力为特征的失语症。由于传统的失语症治疗方法通常耗时且劳动强度大，并且无法反映现实世界中的对话，因此应用基于自然语言处理的方法如大型语言模型（LLMs）可能有助于改进现有的治疗方法。为了解决这一问题，我们探讨了使用序列到序列LLMs来完成布罗卡失语症患者的片段化句子。我们首先使用一个规则系统生成合成的布罗卡失语症数据，该系统旨在反映布罗卡失语症言语的语音特征。然后，我们使用这些合成数据对四个预训练的LLMs进行微调，使其能够完成片段化的句子。我们在合成数据和真实的布罗卡失语症数据上评估了我们的微调模型。结果表明，LLMs有能力重构片段化的句子，模型在较长的输入语句上表现更好。我们的结果显示了LLMs在为布罗卡失语症患者及其他临床人群提供交流辅助工具方面的潜力。**|
|**2024-12-20**|**HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding**|Chenxin Tao et.al.|[2412.16158](http://arxiv.org/abs/2412.16158)|null|大型语言模型（LLMs）的快速发展促进了视觉-语言模型（VLMs）的发展。单体VLMs避免了模态特定编码器，成为一种有前景的替代方案，但面临性能较差的挑战。大多数现有的单体VLMs需要调整预训练的LLMs以获得视觉能力，这可能会降低它们的语言能力。为了解决这一困境，本文提出了一种新的高性能单体VLM，称为HoVLE。我们注意到，当图像嵌入与文本嵌入对齐时，LLMs能够解释图像。当前单体VLMs面临的挑战实际上在于缺乏一个同时处理视觉和语言输入的整体嵌入模块。因此，HoVLE引入了一个整体嵌入模块，将视觉和文本输入转换到共享空间，使LLMs能够像处理文本一样处理图像。此外，精心设计了一个多阶段训练策略来增强整体嵌入模块。首先，它被训练以蒸馏来自预训练视觉编码器的视觉特征和来自LLM的文本嵌入，使大规模训练成为可能，使用未配对的随机图像和文本标记。整个模型进一步通过多模态数据上的下一词预测进行对齐。最后，增加了指令微调阶段。我们的实验表明，HoVLE在各种基准测试中的表现接近领先的组合模型，并且比以前的单体模型有了显著的提升。模型可在<https://huggingface.co/OpenGVLab/HoVLE>获取。|
|**2024-12-20**|**Offline Reinforcement Learning for LLM Multi-Step Reasoning**|Huaijie Wang et.al.|[2412.16145](http://arxiv.org/abs/2412.16145)|**[link](https://github.com/jwhj/oreo)**|提高大型语言模型（LLMs）的多步推理能力通过离线强化学习（RL）至关重要，以便它们能够快速适应复杂任务。虽然直接偏好优化（DPO）在使LLMs与人类偏好对齐方面显示出潜力，但它不太适合多步推理任务，原因有二：（1）DPO依赖于成对偏好数据，而这种数据对于多步推理任务来说不易获得；（2）它对所有标记进行统一处理，这使得在多步推理任务中进行信用分配变得不那么有效，这些任务通常伴随着稀疏奖励。在这项工作中，我们提出了OREO（离线推理优化），这是一种用于增强LLM多步推理的离线RL方法。基于先前最大熵强化学习工作的见解，OREO通过优化软贝尔曼方程同时学习策略模型和价值函数。我们证明原则上它可以减少收集成对数据的需求，并能更好地进行信用分配。实证结果显示，在包括数学推理任务（GSM8K、MATH）和具身代理控制（ALFWorld）在内的多步推理基准测试中，OREO超越了现有的离线学习方法。当有额外资源可用时，该方法可以扩展到多迭代框架。此外，学到的价值函数可以免费用于指导树搜索，这可以在测试时进一步提升性能。|
|**2024-12-20**|**Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation**|Seyedreza Mohseni et.al.|[2412.16135](http://arxiv.org/abs/2412.16135)|null|恶意软件作者经常使用代码混淆技术使其恶意软件更难被检测。现有的生成混淆代码的工具通常需要访问原始源代码（例如C++或Java），并且添加新的混淆技术是一个繁琐且劳动密集型的过程。本研究探讨了以下问题：大型语言模型（LLMs）能否可能生成新的混淆汇编代码？如果可以，这将对反病毒引擎构成风险，并可能增加攻击者创建新混淆模式的灵活性。我们通过开发MetamorphASM基准来肯定地回答这个问题，该基准包括MetamorphASM数据集（MAD）以及三种代码混淆技术：死代码、寄存器替换和控制流改变。MetamorphASM系统评估了LLMs生成和分析混淆代码的能力，MAD数据集包含了328,200个混淆的汇编代码样本。我们发布了这个数据集，并分析了各种LLMs（如GPT-3.5/4、GPT-4o-mini、Starcoder、CodeGemma、CodeLlama、CodeT5和LLaMA 3.1）在生成混淆汇编代码方面的成功率。评估使用了已建立的信息论度量标准并辅以人工审查以确保正确性，并为研究人员提供了研究和开发缓解这一风险的方法的基础。源代码可以在以下GitHub链接中找到：https://github.com/mohammadi-ali/MetamorphASM。|
|**2024-12-20**|**Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information**|Dirk Bergemann et.al.|[2412.16132](http://arxiv.org/abs/2412.16132)|null|我们研究了当代理私有信息不仅涉及他们的偏好而且还涉及一个共同的支付相关状态时的机制设计。我们表明，在有利条件下，标准的消息驱动机制无法实现社会效率分配，即使在多维类型下也是如此。为了解决这一限制，我们提出了数据驱动机制，这些机制利用额外的后分配信息，这些信息被建模为支付相关状态的估计器。我们的数据驱动机制扩展了经典的Vickrey-Clarke-Groves类。我们证明它们在后验均衡中实现了精确实现，当状态完全揭示或效用在一个无偏估计量中是线性的。我们也展示了它们在一致估计量下实现了近似实现，并且随着估计量的收敛而趋近于精确实现，同时给出了收敛速率的界限。我们将这些应用到数字广告拍卖和基于大型语言模型（LLM）的机制中，在这些机制中用户参与度自然地揭示了相关信息。|
|**2024-12-20**|**PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics**|Daniil Larionov et.al.|[2412.16120](http://arxiv.org/abs/2412.16120)|null|评估机器生成的自然语言内容的质量是自然语言处理（NLP）中的一个挑战性任务。最近，大型语言模型（LLM）如GPT-4被用于此目的，但由于复杂的评估提示所需的大量令牌使用，它们在计算上成本高昂。在本文中，我们提出了一种提示优化方法，该方法使用较小的、经过微调的语言模型来压缩评估提示的输入数据，从而减少令牌使用和计算成本，同时使用较大的LLM进行下游评估。我们的方法包括两阶段的微调过程：监督微调随后是偏好优化，以根据人类偏好改进模型的输出。我们专注于机器翻译（MT）评估，并利用GEMBA-MQM指标作为起点。我们的结果显示，令牌使用量减少了2.37倍且没有损失评估质量。这项工作使像GEMBA-MQM这样的最先进的LLM基度量更加经济有效和高效，增强了它们在更广泛使用中的可及性。|
|**2024-12-20**|**Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource Scripts**|Muhammad Abdullah Sohail et.al.|[2412.16119](http://arxiv.org/abs/2412.16119)|**[link](https://github.com/abdullahsohaill/cs6303-researchproject)**|**本研究调查了大型语言模型（特别是GPT-4o）在光学字符识别（OCR）方面对低资源脚本如乌尔都语、阿尔巴尼亚语和塔吉克语的潜力，并以英语作为基准。通过精心策划的数据集，该研究包含了2520张图像，并引入了文本长度、字体大小、背景颜色和模糊度的变化，从而模拟了多种现实世界中的挑战。结果强调了零样本基于大型语言模型的OCR的局限性，特别是在语言结构复杂的脚本中，突显出需要注释数据集和微调模型的需求。这项工作强调了解决文本数字化中无障碍差距的紧迫性，为服务不足的语言提供包容性和稳健的OCR解决方案铺平了道路。**|
|**2024-12-20**|**PruneVid: Visual Token Pruning for Efficient Video Large Language Models**|Xiaohu Huang et.al.|[2412.16117](http://arxiv.org/abs/2412.16117)|**[link](https://github.com/visual-ai/prunevid)**|**在本文中，我们介绍了PruneVid，这是一种视觉标记修剪方法，旨在增强多模态视频理解的效率。大型语言模型（LLMs）由于其在理解视觉模态方面的扩展能力，在视频任务中表现出色。然而，大量的视频数据冗余对LLMs提出了显著的计算挑战。为了解决这一问题，我们介绍了一种无需训练的方法，该方法1）通过合并空间-时间标记来减少视频冗余，以及2）利用LLMs的推理能力选择性地修剪与问题标记相关的视觉特征，从而提高模型效率。我们在多个视频基准上验证了我们的方法，结果表明PruneVid可以在保持竞争力性能的同时修剪超过80%的标记，并且可以与不同的模型网络结合使用。这突显了它相比现有修剪方法的优越性和高效性。代码：https://github.com/Visual-AI/PruneVid。**|
|**2024-12-20**|**The Content Moderator's Dilemma: Removal of Toxic Content and Distortions to Online Discourse**|Mahyar Habibi et.al.|[2412.16114](http://arxiv.org/abs/2412.16114)|null|在关于如何规范社交媒体上的有毒言论以及内容审核如何影响在线讨论的持续辩论中，我们提出并验证了一种使用计算语言学中文本嵌入来衡量内容审核引起的在线讨论失真的方法。我们在一个代表性的包含500万条美国政治推文的数据集上测试了我们的度量标准，发现移除有毒推文会扭曲在线内容。这一发现对于不同的嵌入模型、毒性指标和样本是一致的。重要的是，我们证明了内容审核引起的内容失真并不是由有毒的语言造成的。相反，我们表明作为副作用，内容审核改变了嵌入空间的均值和方差，从而扭曲了在线内容的主题构成。最后，我们提出了一种替代的内容审核方法，该方法使用生成式大语言模型重述有毒推文，以保留其可挽救的内容而非完全删除它们。我们证明这种重述策略在减少毒性的同时最小化了在线内容的失真。|
|**2024-12-20**|**Logical Consistency of Large Language Models in Fact-checking**|Bishwamittra Ghosh et.al.|[2412.16100](http://arxiv.org/abs/2412.16100)|null|近年来，大型语言模型（LLMs）在执行各种自然语言任务方面取得了显著的成功，例如语言翻译、问答、摘要、事实核查等。尽管LLMs能够生成类似人类的文本，但它们以不一致的响应而闻名——输入查询的含义不变但稍作改动就会导致不一致的响应，并且这种特性导致了LLMs的一些漏洞，如幻觉、越狱等。因此，现有的研究集中在基于简单改写的LLMs一致性评估上，而忽略了需要更好逻辑推理理解的复杂查询。我们的工作因此解决了在包含基本逻辑运算符（如否定、合取和析取）的复杂逻辑查询下LLMs的逻辑不一致性问题。作为测试平台，我们考虑了在涉及来自真实世界知识图谱（KGs）的命题逻辑查询的事实核查任务中的检索增强型LLMs。我们的贡献有三个方面。基准：我们引入了三个针对KGs的逻辑事实核查数据集，供社区开发逻辑一致的LLMs使用。评估：我们提出了对LLMs在命题逻辑查询作为输入时的一致性度量，并证明现有的LLMs缺乏逻辑一致性，特别是在复杂查询上。改进：我们采用监督微调来提高LLMs在包含KG上下文的复杂事实核查任务上的逻辑一致性。|
|**2024-12-20**|**The Evolution of LLM Adoption in Industry Data Curation Practices**|Crystal Qian et.al.|[2412.16089](http://arxiv.org/abs/2412.16089)|null|随着大型语言模型（LLM）在处理非结构化文本数据方面的能力不断增强，它们为增强数据治理工作流程提供了新的机会。本文探讨了一家大型科技公司内部从业者对LLM的采用情况及其在数据治理任务中的影响，通过从业者的观点、集成策略和报告的使用场景来评估。我们通过一系列调查、访谈和用户研究，提供了组织如何应对LLM演进关键时刻的一个及时的概览。2023年第二季度，我们进行了一项调查以评估行业在开发任务中的LLM采用情况（N=84），并在2023年第三季度进行了专家访谈以评估不断变化的数据需求（N=10）。2024年第二季度，我们通过两个基于LLM的原型探索了从业者的当前和预期LLM使用情况，涉及用户研究（N=12）。虽然每个研究都针对不同的研究目标，但它们共同揭示了一个关于LLM使用演变的更广泛的故事。我们发现，数据理解方式从以启发式为主的自下而上的方法转向了由LLM支持的以洞察力为主的自上而下的工作流程。此外，为了应对更加复杂的数据环境，数据从业者现在补充了传统的由主题专家创建的“黄金数据集”与LLM生成的“银色数据集”，并辅以由多样化的专家严格验证的“超级黄金数据集”。这项研究揭示了LLM在大规模分析非结构化数据中的变革作用，并突显了进一步开发工具的机会。|
|**2024-12-19**|**OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving**|Shuo Xing et.al.|[2412.15208](http://arxiv.org/abs/2412.15208)|**[link](https://github.com/taco-group/openemma)**|**自多模态大型语言模型（MLLMs）问世以来，它们在自动驾驶（AD）的广泛应用中产生了重大影响。它们能够处理复杂的视觉数据并推理复杂的驾驶场景，为端到端AD系统开辟了新的范式。然而，开发端到端AD模型的进展一直较为缓慢，因为现有的微调方法需要大量的资源，包括大量的计算能力、大规模的数据集和大量的资金。受近期推理计算进展的启发，我们提出了OpenEMMA，这是一种基于MLLM的开源端到端框架。通过结合思维链推理过程，OpenEMMA在利用各种MLLM时比基线模型有显著改进。此外，OpenEMMA展示了在各种具有挑战性的驾驶场景中的有效性、通用性和鲁棒性，提供了一种更高效、更有效的自动驾驶方法。我们已在https://github.com/taco-group/OpenEMMA发布了所有代码。**|
|**2024-12-19**|**MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark**|Qihao Zhao et.al.|[2412.15194](http://arxiv.org/abs/2412.15194)|**[link](https://github.com/microsoft/mmlu-cf)**|**多项选择题（MCQ）数据集如大规模多任务语言理解（MMLU）被广泛用于评估大型语言模型（LLMs）的常识、理解和问题解决能力。然而，这些基准的开源性质和LLMs的广泛训练数据不可避免地导致了基准污染，从而导致了不可靠的评估结果。为了解决这个问题，我们提出了一个无污染且更具挑战性的MCQ基准测试MMLU-CF。这个基准通过避免无意和恶意的数据泄漏来重新评估LLMs对世界知识的理解。为了避免无意的数据泄漏，我们从更广泛的领域获取数据并设计了三个去污染规则。为了防止恶意数据泄漏，我们将基准划分为具有相似难度和主题分布的验证集和测试集。测试集保持闭源以确保可靠的结果，而验证集公开以促进透明度并方便独立验证。我们对主流LLMs的评估显示，功能强大的GPT-4o在测试集上的5次射击得分为73.4%，0次射击得分为71.9%，这表明我们的方法在创建更严格和无污染的评估标准方面是有效的。GitHub仓库地址为https://github.com/microsoft/MMLU-CF，数据集地址为https://huggingface.co/datasets/microsoft/MMLU-CF。**|
|**2024-12-19**|**LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation**|Weijia Shi et.al.|[2412.15188](http://arxiv.org/abs/2412.15188)|null|我们提出了LlamaFusion框架，该框架通过利用现有的Llama-3权重来处理文本的自回归过程，并引入额外的并行Transformer模块以使用扩散模型处理图像，从而赋予预训练的仅文本大型语言模型（LLMs）多模态生成能力，使它们能够理解和生成任意顺序的文本和图像。在训练过程中，来自每个模态的数据被路由到其专用模块：特定于模态的前馈层、查询-键-值投影和归一化层独立地处理每个模态，而共享的自注意力层允许跨文本和图像特征进行交互。通过冻结仅针对文本的模块并只训练针对图像的模块，LlamaFusion保留了仅文本LLMs的语言能力，同时发展出强大的视觉理解和生成能力。与从头开始预训练多模态生成模型的方法相比，我们的实验表明，LlamaFusion在仅使用50%浮点运算次数（FLOPs）的情况下，将图像理解提高了20%，图像生成提高了3.6%，同时保持了Llama-3的语言能力。我们还证明了该框架可以适应现有的具有多模态生成能力的视觉-语言模型。总体而言，该框架不仅利用了对仅文本LLMs已有的计算投资，而且促进了语言和视觉能力的同时发展，为高效的多模态模型开发提供了一个有前景的方向。|
|**2024-12-19**|**Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning**|Simon Frieder et.al.|[2412.15184](http://arxiv.org/abs/2412.15184)|null|数学领域的大规模语言模型（LLMs）通常使用的数据集在训练和评估其数学能力时存在若干局限性。这些局限性包括数学复杂性的范围有限，通常不超过较低的本科水平数学，二元评级协议和其他问题，这使得全面的基于证明的评估数据集变得困难。我们系统地探讨了这些局限性，并认为要增强大规模语言模型的能力或任何即将出现的AI驱动的数学助手（副驾驶员或“思考伙伴”）的能力，需要在数学数据集的设计和数学能力的评估标准上进行范式转变：有必要从结果导向的数据集（定理陈述到定理证明）转向转换数学研究实践的丰富方面成为LLMs可以训练的数据。例如，数学工作流程（创建新数学时经常执行的一系列原子任务，可能是子领域相关的）是证明发现过程中的一个重要部分。此外，我们倡导数学数据集开发者考虑G. 波利亚于1949年引入的“动机证明”的概念，这可以作为数据集的蓝图，提供更好的证明学习信号，从而缓解上述一些局限性。最后，我们为数据集引入了数学数据表，扩展了通用的、与数据集无关的数据表变体：我们提供了一个专门针对数学数据集的问题清单，我们敦促数据集创建者将其包含在其数据集中。这将使创建者意识到其数据集的潜在限制，同时使读者能够从训练和评估数学副驾驶的角度轻松评估它。|
|**2024-12-19**|**HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages**|Aman Chaturvedi et.al.|[2412.15178](http://arxiv.org/abs/2412.15178)|null|大型语言模型（LLM）作为软件开发助手已经取得了巨大的成功，但它们通常被设计用于通用编程任务，在更专业的领域如高性能计算（HPC）中表现不佳。为这些领域创建专门的模型和工具对于在HPC等领域获得LLM的好处至关重要。尽管之前的工作探索了针对HPC的特定模型，LLM在生成并行代码方面仍然存在困难，目前尚不清楚阻碍这些LLM的因素是什么以及需要做些什么来克服它们。在这项工作中，我们对微调一个专门的HPC LLM进行了深入研究，以更好地理解其中的挑战。根据我们的发现，我们微调并评估了一个专门的HPC LLM，它被证明是迄今为止性能最好的开源代码LLM，适用于并行代码生成。|
|**2024-12-19**|**Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying**|Federico Castagna et.al.|[2412.15177](http://arxiv.org/abs/2412.15177)|**[link](https://github.com/fcast07/cqot)**|**研究表明，尽管近期在人工智能研究领域取得了突破并迅速发展，即使是最先进的大型语言模型（LLMs）在进行逻辑和数学推理时仍面临挑战。结果表明，这些模型仍然主要作为（高度先进的）数据模式识别器，在试图推广并解决模型从未见过或与训练数据样本相差较大的推理问题时表现不佳。为了解决这一引人关注的问题，本文利用论证理论中的关键问题概念，特别关注图尔敏的论证模型。我们展示了使用这些关键问题可以提高LLMs的推理能力。通过探究模型推理过程背后的理由，模型可以在向用户提示提供最终回复之前评估是否存在某些逻辑错误并予以纠正。这一理念源自任何有效论证程序的黄金标准：如果结论是由被接受的前提得出的，则该结论是有效的。或者，借用亚里士多德原则的一种现实世界近似表述，在信息不完整且预设逻辑的情况下，如果结论未被证明为错误，则它是有效的。这种方法成功地引导了模型的输出通过一个推理流程，从而在基准测试和其链式思维（CoT）实现中表现出更好的性能。为此，本文提供了对所提出方法在MT-Bench推理和数学任务上的广泛评估。**|
|**2024-12-19**|**Rethinking Uncertainty Estimation in Natural Language Generation**|Lukas Aichberger et.al.|[2412.15176](http://arxiv.org/abs/2412.15176)|null|大型语言模型（LLMs）在实际应用中的使用日益增多，这促使人们需要评估其生成文本的可信度。为了实现这一目标，可靠的不确定性估计至关重要。由于当前的LLMs通过一种随机过程自回归地生成文本，相同的提示可能会导致不同的输出。因此，现有的主要不确定性估计方法会生成并分析多个输出序列以确定LLM的不确定性。然而，生成这些输出序列在计算上非常昂贵，使得这些方法在大规模应用中不切实际。在这项工作中，我们检查了领先方法的理论基础，并探索了增强其计算效率的新方向。基于适当评分规则的框架，我们发现最可能输出序列的负对数似然性构成了一种理论上合理的不确定性度量。为了近似这种替代度量，我们提出了G-NLL，它的一个优势是仅需使用通过贪婪解码生成的单个输出序列即可获得。这使得不确定性估计更加高效和直接，同时保持了理论严谨性。实证结果表明，G-NLL在各种LLMs和任务中实现了最先进的性能。我们的工作为自然语言生成中的高效且可靠的不确定性估计奠定了基础，挑战了目前在该领域领先的、计算成本更高的方法的必要性。|
|**2024-12-19**|**Language Models as Continuous Self-Evolving Data Engineers**|Peidong Wang et.al.|[2412.15151](http://arxiv.org/abs/2412.15151)|null|大型语言模型（LLMs）在各种任务上展示了卓越的能力，然而进一步的发展受限于高质量训练数据的缺乏。此外，传统的训练方法过于依赖专家标注的数据，这限制了LLMs性能的上限。为了解决这一问题，我们提出了一种新的范式，使LLMs能够通过自主生成、清洗、审查和注释带偏好信息的数据来训练自己，该方法被称为LANCE。我们的方法表明，LLMs可以作为持续自我演进的数据工程师，显著减少后训练数据构建过程的时间和成本。通过在不同变体的Qwen2上进行迭代微调，我们验证了LANCE在各种任务中的有效性，证明它可以持续提高模型性能并保持高质量的数据生成。在八个基准维度上，LANCE使Qwen2-7B的平均得分提高了3.36分，Qwen2-7B-Instruct的平均得分提高了2.70分。这种具有自主数据构建的训练范式不仅减少了对人类专家或外部模型的依赖，还确保了数据与人类价值观和偏好的一致性，为开发未来超出人类能力的超级智能系统铺平了道路。|
|**2024-12-19**|**Adaptive Pruning for Large Language Models with Structural Importance Awareness**|Haotian Zheng et.al.|[2412.15127](http://arxiv.org/abs/2412.15127)|null|近期大型语言模型（LLMs）在语言理解和生成方面取得了显著进步。然而，由于其高计算和存储资源需求，很难将其部署到资源受限的边缘设备上。为了解决这一问题，我们提出了一种新的LLM模型剪枝方法，即结构感知自适应剪枝（SAAP），以显著降低计算和内存成本，同时保持模型性能。我们首先定义了一个自适应重要性融合度量，通过考虑同质不确定性来评估LLMs中所有耦合结构的重要性。然后，我们对所有模块的重要性进行排序，以确定应修剪的具体层，以满足特定的性能要求。此外，我们开发了一种新的分组微调策略，以提高LLMs的推理效率。最后，我们在多个LLMs上评估了所提出的SAAP方法，涉及两个常见任务，即零样本分类和文本生成。实验结果表明，我们的SAAP方法优于几种最先进的基线方法，在LLaMA-7B、Vicuna-7B和LLaMA-13B上分别获得了2.17%、2.37%和2.39%的准确率提升。此外，SAAP提高了5%的令牌生成速度，展示了其在资源受限场景中的实际优势。|
|**2024-12-19**|**Outcome-Refining Process Supervision for Code Generation**|Zhuohao Yu et.al.|[2412.15118](http://arxiv.org/abs/2412.15118)|**[link](https://github.com/zhuohaoyu/orps)**|大型语言模型在代码生成方面展示了显著的能力，但它们往往难以处理需要深入算法推理的复杂编程任务。尽管通过学习奖励模型进行过程监督显示出了引导推理步骤的潜力，但它需要昂贵的训练数据，并且评估不可靠。我们提出了结果细化过程监督（Outcome-Refining Process Supervision，ORPS），这是一种新颖的范式，将结果细化本身作为要监督的过程。我们的框架利用具体的执行信号来指导推理步骤，同时使用树结构探索以同时保持多个解决方案轨迹。实验表明，即使较小的模型也能在具有竞争力的编程任务上实现高成功率和性能指标，相比传统的奖励模型，我们的方法创造了更可靠的验证，而不需要训练PRM。我们的方法在5个模型和3个数据集上实现了显著改进：正确性平均提高了26.9%，效率提高了42.2%。结果表明，在解决复杂的编程任务时，提供带有具体验证信号的结构化推理空间至关重要。我们在https://github.com/zhuohaoyu/ORPS开放了所有代码和数据。|
|**2024-12-18**|**Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces**|Jihan Yang et.al.|[2412.14171](http://arxiv.org/abs/2412.14171)|**[link](https://github.com/vision-x-nyu/thinking-in-space)**|**人类具备从连续视觉观察中记住空间的视觉空间智能。然而，是否多模态大型语言模型（MLLMs）在经过百万规模视频数据集训练后也能“在空间中思考”？我们提出了一个新的基于视频的视觉空间智能基准测试（VSI-Bench），包含超过5000个问答对，并发现MLLMs表现出与人类相当但低于人类的视觉空间智能。我们探究了模型如何以语言和视觉方式表达它们的空间思维，并发现尽管空间推理能力仍然是限制MLLMs达到更高基准性能的主要瓶颈，但在这些模型中确实出现了局部世界模型和空间意识。值得注意的是，现有的语言推理技术（如链式思维、自一致性、树状思维）未能提高性能，而显式生成认知地图在问答过程中增强了MLLMs的空间距离能力。**|
|**2024-12-18**|**TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks**|Frank F. Xu et.al.|[2412.14161](http://arxiv.org/abs/2412.14161)|**[link](https://github.com/theagentcompany/experiments)**|**我们日常生活中和工作中都在与计算机进行互动，许多工作可以通过访问计算机和互联网来完成。同时，由于大型语言模型（LLM）的改进，与周围环境进行交互并影响其变化的人工智能代理也得到了快速发展。但是，这些人工智能代理在帮助加速甚至自主执行与工作相关任务方面的表现如何？这个问题的答案对希望在其工作流程中采用人工智能的行业以及希望了解人工智能采用可能对劳动力市场产生影响的经济政策具有重要意义。为了衡量这些由LLM驱动的代理在执行现实世界专业任务方面的进展，我们在本文中介绍了TheAgentCompany，这是一个可扩展的基准，用于评估那些以与数字工作者类似的方式与世界互动的AI代理：通过浏览网络、编写代码、运行程序以及与其他同事交流。我们构建了一个自包含的环境，其中包含内部网站和数据，模拟了一个小型软件公司环境，并创建了一系列可能由此类公司员工执行的任务。我们测试了基于封闭API和开放权重语言模型（LM）的基线代理，结果发现，使用最具竞争力的代理，可以自主完成24%的任务。这为我们提供了一幅关于任务自动化方面由LM代理所处现状的细致图景——在一个模拟真实工作场所的环境中，一些较简单的任务可以自主解决，但更为复杂且需要长期规划的任务目前仍然超出了当前系统的处理能力。**|
|**2024-12-18**|**Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics with Large Language Models**|Atin Sakkeer Hussain et.al.|[2412.14146](http://arxiv.org/abs/2412.14146)|null|本文介绍了先进推理和变换引擎在多步洞察合成中的应用，即ARTEMIS-DA（Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics），这是一个旨在增强大型语言模型（LLMs）以解决复杂多步骤数据分析任务的新框架。ARTEMIS-DA整合了三个核心组件：规划器，它将复杂的用户查询分解为结构化的顺序指令，包括数据预处理、变换、预测建模和可视化；编码器，它动态生成并执行Python代码来实现这些指令；绘图器，它解释生成的可视化结果以得出可操作的见解。通过协调这些组件之间的协作，ARTEMIS-DA能够有效管理涉及高级推理、多步骤变换和跨多种数据模式综合的复杂分析工作流程。该框架在WikiTableQuestions和TabFact等基准测试上达到了最先进的性能，证明了其能够精准且灵活地处理复杂的分析任务。通过结合大型语言模型的推理能力与自动代码生成和执行以及视觉分析，ARTEMIS-DA提供了一个强大且可扩展的解决方案，用于多步骤洞察合成，解决了数据分析中的广泛挑战。|
|**2024-12-18**|**LLMs can realize combinatorial creativity: generating creative ideas via LLMs for scientific research**|Tianyang Gu et.al.|[2412.14141](http://arxiv.org/abs/2412.14141)|null|科学创意生成在创造力理论和计算创意研究中已有广泛探讨，提供了理解与实施创造性过程的宝贵框架。然而，最近使用大型语言模型（LLMs）进行研究创意生成的工作往往忽视了这些理论基础。我们提出一个框架，利用LLMs明确实现组合式创意理论，该框架包括一个用于跨领域知识发现的广义检索系统以及一个结构化的组合过程用于创意生成。检索系统通过映射不同抽象层次的概念来促进不同领域间的有意义连接，而组合过程则系统性地分析并重组组件以生成新颖解决方案。在OAG-Bench数据集上的实验表明，我们的框架比基线方法更有效地生成符合实际研究发展的创意（在多个指标上提高了7%-10%的相似度得分）。我们的结果强有力地证明，在适当理论框架指导下，LLMs能够有效实现组合式创意，这不仅推动了AI辅助研究的实际进展，也为机器创意的理论理解做出了贡献。|
|**2024-12-18**|**Design choices made by LLM-based test generators prevent them from finding bugs**|Noble Saji Mathews et.al.|[2412.14137](http://arxiv.org/abs/2412.14137)|null|近年来，越来越多的研究和商业工具利用大型语言模型（LLMs）进行自动化测试用例生成。本文批判性地探讨了诸如Codium CoverAgent和CoverUp等基于LLM的测试生成工具是否能够有效地发现缺陷或无意中验证有缺陷的代码。考虑到缺陷只有通过失败的测试用例才能被揭示，我们探讨了这些工具在测试预言设计为通过的情况下能否真正实现软件测试的预期目标。通过使用真实的人工编写的有缺陷的代码作为输入，我们评估了这些工具，展示了LLM生成的测试如何未能检测到缺陷，更令人担忧的是，它们的设计可能会加剧问题，通过验证生成测试套件中的缺陷，并拒绝揭示缺陷的测试。这些发现引发了对基于LLM的测试生成工具背后设计的有效性的质疑，以及它们对软件质量和测试套件可靠性的影响。|
|**2024-12-18**|**Alignment faking in large language models**|Ryan Greenblatt et.al.|[2412.14093](http://arxiv.org/abs/2412.14093)|**[link](https://github.com/redwoodresearch/alignment_faking_public)**|**我们展示了一个大型语言模型在对齐造假方面的演示：选择性地遵守其训练目标，在训练期间以防止训练后行为被修改。首先，我们给Claude 3 Opus一个系统提示，说明它被训练来回答所有查询，即使是有害的查询，这与其之前被训练拒绝此类查询相冲突。为了使模型能够推断出何时处于训练状态，我们表示只会在与免费用户进行的对话上对其进行训练，而不是在与付费用户的对话上。我们发现该模型在14%的情况下会响应来自免费用户的有害查询，而付费用户的情况几乎从未出现这种情况。解释这一差距，在几乎所有情况下，当模型响应来自免费用户的有害查询时，我们观察到明显的对齐造假推理，模型表明它正在策略性地响应有害查询以保持其首选的无害行为训练之外。接下来，我们研究了一种更现实的设置，其中关于训练过程的信息不是通过系统提示提供，而是通过训练合成文档来模仿预训练数据，并观察到类似的对齐造假现象。最后，我们研究了通过强化学习实际训练模型来遵守有害查询的影响，我们发现对齐造假推理的比例增加到了78%，尽管这也增加了训练之外的合规性。我们还观察到其他行为，例如当模型有机会时会泄露其权重。虽然我们通过告诉模型何时以及基于什么标准进行训练使得对齐造假变得更加容易，但我们并没有指示模型去伪造对齐或给予其任何明确的目标。由于未来的模型可能在没有被告知的情况下推断出其训练过程的相关信息，我们的结果表明未来模型存在对齐造假的风险，无论是由于良性偏好——就像在这个案例中——还是其他原因。**|
|**2024-12-18**|**Future Research Avenues for Artificial Intelligence in Digital Gaming: An Exploratory Report**|Markus Dablander et.al.|[2412.14085](http://arxiv.org/abs/2412.14085)|null|视频游戏是人工智能（AI）系统的一个自然且协同的应用领域，既能增强玩家体验和沉浸感，又能提供有价值的基准和虚拟环境以推动AI技术的整体发展。本报告概述了五个有前景的研究路径，这些路径旨在将最先进的AI方法，特别是深度学习，应用于数字游戏领域，并在此过程中考虑当前的研究背景。本研究的目标是列出一份经过精心挑选、非详尽无遗的鼓励性研究方向清单，这些方向位于AI与视频游戏交叉点上，旨在激励未来更严谨和全面的研究工作。我们讨论了以下几点：(i) 使用大型语言模型作为游戏角色建模的核心引擎；(ii) 利用神经细胞自动机进行程序化游戏内容生成；(iii) 通过深度代理建模加速游戏中计算密集型模拟；(iv) 利用自监督学习获取有用的视频游戏状态嵌入；(v) 使用未标记的视频数据训练交互式世界的生成模型。我们还简要探讨了将先进深度学习系统集成到视频游戏开发中的当前技术挑战，并指出了可能受益于进一步发展的关键领域。|
|**2024-12-18**|**Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification**|Kyle Thompson et.al.|[2412.14063](http://arxiv.org/abs/2412.14063)|**[link](https://github.com/rkthomps/coq-modeling)**|形式验证使用证明助手如Coq可以创建高质量的软件。然而，验证过程需要大量的专业知识和手动编写证明的努力。最近的研究探索了使用机器学习和大型语言模型（LLM）来自动化证明合成。这项工作表明，识别相关的前提条件（例如引理和定义）可以辅助合成。我们提出了Rango，这是一种用于Coq的全自动证明合成工具，它能够自动识别相关的前提条件，并从当前项目中找到类似的证明并在合成过程中使用它们。Rango在每次证明步骤中都使用检索增强，以自动确定应将哪些证明和前提条件包含在其经过微调的LLM的上下文中。通过这种方式，Rango适应项目和证明的演变状态。我们创建了一个新的数据集CoqStoq，其中包括来自GitHub的2,226个开源Coq项目的196,929个定理，该数据集包括训练数据和一个精心维护项目的精选评估基准。在该基准上，Rango为32.0%的定理合成证明，这比之前的最先进工具Tactician多出了29%的定理。我们的评估还显示，Rango在上下文中添加相关证明使其能够证明的定理数量增加了47%。|
|**2024-12-18**|**Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets**|Simon Thorne et.al.|[2412.14062](http://arxiv.org/abs/2412.14062)|null|生成式人工智能（Generative AI）和大型语言模型（LLMs）有望实现电子表格公式的自动化创建。然而，由于幻觉、偏见和用户技能的差异，通过生成式AI获得的输出不能被认为是准确或可信的。为了解决这些挑战，提出了一种基于评估公式透明性和可靠性的信任框架。公式的透明性通过可解释性（理解公式的推理过程）和可见性（检查底层算法）来探讨。生成公式的可靠性则通过一致性、准确性以及伦理考虑（如偏见和公平性）来评估。此外，论文还分析了导致这些指标的因素，包括幻觉、训练数据偏见和构建不当的提示。最后，文章考察了对技术不信任的一些例子，并探讨了其后果。|
|**2024-12-18**|**A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future**|Shilin Sun et.al.|[2412.14056](http://arxiv.org/abs/2412.14056)|**[link](https://github.com/shilinsun/mxai_review)**|**人工智能（AI）的发展得益于计算能力的提升和大规模数据集的增长。然而，这一进步也加剧了解释AI模型“黑箱”性质的挑战。为了解决这些问题，可解释AI（XAI）应运而生，其重点在于透明度和可解释性，以增强人类对AI决策过程的理解和信任。在多模态数据融合和复杂推理场景的背景下，提出了多模态可解释AI（MXAI），它整合了多种模态用于预测和解释任务。同时，大型语言模型（LLMs）的出现带来了自然语言处理领域的显著突破，但同时也进一步加剧了MXAI的问题。为了深入了解MXAI方法的发展并为构建更加透明、公平和可信的AI系统提供重要指导，我们从历史的角度回顾了MXAI方法，并将其分类为四个时代：传统机器学习、深度学习、判别基础模型和生成型LLMs。我们还回顾了MXAI研究中使用的评估指标和数据集，并讨论了未来面临的挑战和方向。与此综述相关的项目已在https://github.com/ShilinSun/mxai_review创建。**|
|**2024-12-17**|**SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents**|Sheng Yin et.al.|[2412.13178](http://arxiv.org/abs/2412.13178)|**[link](https://github.com/shengyin1224/safeagentbench)**|**随着大型语言模型（LLMs）的整合，具身代理能够执行复杂的自然语言指令，为具身机器人的潜在部署铺平了道路。然而，一个可预见的问题是这些具身代理也能完美地执行一些危险任务，可能在现实世界中造成损害。为了研究这个问题，我们提出了SafeAgentBench——一个新的基准测试，用于具身LLM代理的安全任务规划。SafeAgentBench包括：（1）一个新的数据集，包含750个任务，涵盖了10种潜在的危险和3种任务类型；（2）SafeAgentEnv，一个通用的具身环境，带有低级控制器，支持多代理执行，并提供8种最先进的基线所需的17种高级动作；（3）从执行和语义两个角度进行可靠的评估方法。实验结果显示，表现最好的基线对于安全任务的成功率为69%，但对于危险任务的拒绝率仅为5%，这表明存在显著的安全风险。更多详情和代码可在<https://github.com/shengyin1224/SafeAgentBench>获取。**|
|**2024-12-17**|**DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation**|Miriam Wanner et.al.|[2412.13175](http://arxiv.org/abs/2412.13175)|null|在对大型语言模型（LLM）生成的验证策略中，分解-然后-验证方法将声明分解后进行独立验证。去上下文化增强了文本（声明），以确保其能够在原始上下文之外被验证，从而实现可靠的验证。虽然分解和去上下文化已分别被探索，但它们在一个完整系统中的相互作用尚未被研究。它们的冲突目的可能会产生矛盾：分解隔离原子事实，而去上下文化则插入相关信息。此外，一个去上下文化的子声明给验证步骤带来了挑战：由于现在包含多个原子事实，应该验证增强文本的哪一部分？我们评估了不同的分解、去上下文化和验证策略，并发现策略的选择对最终的事实性评分有影响。此外，我们引入了一种称为DnDScore的去上下文化感知验证方法，该方法在上下文信息的背景下验证子声明。|
|**2024-12-17**|**Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study**|Bolei Ma et.al.|[2412.13169](http://arxiv.org/abs/2412.13169)|**[link](https://github.com/soda-lmu/llm-opinion-german)**|**在最近的研究中，大规模语言模型（LLMs）越来越多地被用来调查公众意见。本研究探讨了LLMs的算法保真度，即它们再现人类参与者社会文化背景和微妙观点的能力。我们使用来自德国选举研究（GLES）的开放式调查数据，通过将人口统计特征纳入人物提示中来促使不同的LLM生成反映德国亚群体的合成公众意见。结果表明，在较低的意见多样性情况下，Llama在代表亚群体方面表现优于其他LLM。我们的发现进一步显示，LLM对左翼政党如绿党和左翼党的支持者的表现优于其他政党，并且与右翼政党AfD的匹配程度最低。此外，提示中是否包含特定变量可以显著影响模型的预测。这些发现强调了使LLM更有效地建模多样化公众意见的重要性，同时减少政治偏见并增强代表性。**|
|**2024-12-17**|**C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System**|Parker Addison et.al.|[2412.13163](http://arxiv.org/abs/2412.13163)|null|组织在利用大型语言模型（LLMs）进行知识查询和分析时，常常面临保持LLMs针对目标领域和最新信息进行微调的挑战，以确保答案的相关性和可靠性。检索增强生成（RAG）已成为一种可行的解决方案，帮助组织克服维护专有模型的挑战，并减少LLMs在查询响应中的幻觉现象。然而，RAG也带来了扩展数据管道跨越分级访问和不同数据源的问题。在许多情况下，需要跨多个数据孤岛查询以提供更丰富和相关的上下文给LLM。分析跨越组织信任边界的内外部数据源通常受到复杂的数据共享政策限制，这些政策禁止集中式数据存储，因此阻碍了RAG解决方案的快速有效设置和扩展。本文介绍了一种使用机密计算（CC）技术作为安全联邦检索增强生成（FedRAG）的解决方案。我们提出的保密联邦RAG系统（C-FedRAG）通过确保上下文的机密性，实现了RAG工作流程在分散的数据提供商网络中的安全连接和扩展。我们还展示了如何使用NVIDIA FLARE SDK实现C-FedRAG系统，并使用MedRAG工具包和MIRAGE基准数据集评估其性能。|
|**2024-12-17**|**SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction**|Chao Ma et.al.|[2412.13148](http://arxiv.org/abs/2412.13148)|null|自适应优化器如Adam（Kingma & Ba，2015）对于大型语言模型的成功起到了关键作用。然而，它们在整个训练过程中需要维护额外的移动平均状态，这导致了内存需求比模型本身大数倍。这种开销限制了其可扩展性和计算效率。另一方面，虽然随机梯度下降（SGD）在内存效率方面是最优的，但它们在大规模语言模型训练中的能力有限（Zhao等，2024b）。为了解决这一困境，我们证明了预处理SGD足以达到与Adam相当的性能。具体而言，我们提出使用两个简单的操作符对瞬时随机梯度进行预处理： $\mathtt{GradNorm}$和$\mathtt{GradWhitening}$。$\mathtt{GradNorm}$稳定了梯度分布，而$\mathtt{GradWhitening}$ 则抵消了损失景观的局部曲率。这产生了SWAN（带有标准化和白化的SGD），这是一种无需存储任何累积状态变量的随机优化器。实证结果显示，SWAN的内存占用与SGD相同，相比Adam减少了约50%的总端到端内存。在语言建模任务中，SWAN展示了与Adam相同甚至显著改进的性能。具体来说，在使用3.5亿和13亿参数预训练LLaMa模型时，SWAN实现了2倍的速度提升，在看到不到一半的令牌时达到了相同的评估困惑度。|
|**2024-12-17**|**Are Your LLMs Capable of Stable Reasoning?**|Junnan Liu et.al.|[2412.13147](http://arxiv.org/abs/2412.13147)|**[link](https://github.com/open-compass/gpassk)**|**大型语言模型（LLMs）在复杂推理任务中取得了显著的进展。然而，基准性能与实际应用之间存在显著差距。我们认为这一差距主要源于当前的评估协议和指标未能充分涵盖LLMs的全部能力，特别是在需要准确性和一致性并重的复杂推理任务中。本研究做出了两个关键贡献。首先，我们引入了G-Pass@k，这是一种新的评估指标，通过多次采样尝试对模型性能进行连续评估，量化模型的峰值性能潜力及其稳定性。其次，我们提出了LiveMathBench，这是一个动态基准，包含具有挑战性的、当代的数学问题，旨在减少评估过程中数据泄露的风险。通过使用G-Pass@k对最先进的LLMs进行广泛的实验，并结合LiveMathBench，我们提供了关于其最大能力和操作一致性的全面见解。我们的发现揭示了LLMs在“现实”推理能力方面仍有很大的提升空间，强调了更稳健的评估方法的需求。该基准和详细结果可在以下网址获得：https://github.com/open-compass/GPassK。**|
|**2024-12-17**|**AI PERSONA: Towards Life-long Personalization of LLMs**|Tiannan Wang et.al.|[2412.13103](http://arxiv.org/abs/2412.13103)|null|在本文中，我们介绍了终身个性化大型语言模型的任务。尽管近期主流的LLM社区主要集中在通过扩大数据和计算资源来提升LLM的能力，我们认为让LLM系统或语言代理能够持续适应每个不同用户的多样且不断变化的需求，并提供最新的个性化帮助同样非常重要。我们明确了任务定义，并介绍了一个简单、通用、有效且可扩展的框架，用于LLM系统的终身个性化和语言代理。为了促进未来关于LLM个性化研究的发展，我们还引入了合成现实基准和稳健评估指标的方法。我们将发布所有构建和评估终身个性化LLM系统的代码和数据。|
|**2024-12-17**|**AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark**|Jianlyu Chen et.al.|[2412.13102](http://arxiv.org/abs/2412.13102)|**[link](https://github.com/air-bench/air-bench)**|**评估在信息检索（IR）模型的发展中起着至关重要的作用。然而，当前基于预定义领域和人工标注数据的基准测试在有效且高效地应对新兴领域的需求方面存在局限性。为了解决这一挑战，我们提出了自动化异构信息检索基准测试（AIR-Bench）。AIR-Bench具有三个关键特性：1）自动化。AIR-Bench中的测试数据由大型语言模型（LLM）自动生成，无需人工干预。2）异构化。AIR-Bench中的测试数据针对不同的任务、领域和语言生成。3）动态化。AIR-Bench覆盖的领域和语言不断扩充，以提供日益全面的模型评估基准。我们开发了一个可靠且稳健的数据生成管道，能够根据真实世界的语料库自动创建多样化且高质量的评估数据集。我们的研究结果表明，AIR-Bench中生成的测试数据与人工标注的测试数据高度一致，使得AIR-Bench成为一个可靠的IR模型评估基准。AIR-Bench的资源可在https://github.com/AIR-Bench/AIR-Bench 公开获取。**|
|**2024-12-17**|**Modality-Inconsistent Continual Learning of Multimodal Large Language Models**|Weiguo Pian et.al.|[2412.13050](http://arxiv.org/abs/2412.13050)|null|在本文中，我们介绍了Modality-Inconsistent Continual Learning（MICL），这是一种针对Multimodal Large Language Models (MLLMs)的新持续学习场景，涉及具有不一致模态（图像、音频或视频）和不同任务类型（如描述生成或问答）的任务。不同于现有的仅视觉或模态增量设置，MICL结合了模态和任务类型的变化，这两种变化都会导致灾难性遗忘。为了解决这些挑战，我们提出了MoInCL，它采用伪目标生成模块来减轻在先前看到的模态中因任务类型变化引起的遗忘。此外，它还采用了基于指令的知识蒸馏来在引入新模态时保留模型处理先前学习模态的能力。我们使用总共六个任务对MICL进行了基准测试，并通过实验验证了所提出的MoInCL的有效性。实验结果突显了MoInCL的优越性，展示了相对于代表性及最先进持续学习基线的显著改进。|
|**2024-12-17**|**OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain**|Shuting Wang et.al.|[2412.13018](http://arxiv.org/abs/2412.13018)|**[link](https://github.com/ruc-nlpir/omnieval)**|**作为大型语言模型（LLMs）的一种典型且实用的应用，检索增强生成（RAG）技术在垂直领域受到了广泛关注，尤其是在LLMs可能缺乏特定领域知识的情况下。在本文中，我们介绍了OmniEval，这是一种面向金融领域的全方位自动RAG基准测试。我们的基准测试的特点在于其多维度的评估框架，包括：（1）基于矩阵的RAG场景评估系统，将查询分为五类任务和16个金融主题，从而对各种查询场景进行结构化的评估；（2）多维度的评估数据生成方法，结合了基于GPT-4的自动生成和人工标注，生成实例在人工评估中的接受比率达到87.47%；（3）多阶段评估系统，评估检索和生成性能，从而全面评估RAG管道；以及（4）通过基于规则的方法和基于LLM的方法得到的稳健评估指标，通过人工标注和有监督的LLM评估器微调增强了评估的可靠性。实验结果表明，OmniEval具有广泛的测试数据集，并突显了RAG系统在不同主题和任务上的性能差异，揭示了RAG模型在垂直领域提升其能力的重大机遇。我们在https://github.com/RUC-NLPIR/OmniEval 开源了该基准测试的代码。**|
|**2024-12-16**|**SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator**|Guoxuan Chen et.al.|[2412.12094](http://arxiv.org/abs/2412.12094)|**[link](https://github.com/HKUDS/SepLLM)**|**大型语言模型（LLMs）在自然语言处理任务中表现出色。然而，它们巨大的规模带来了显著的挑战，特别是在计算需求和推理速度方面，由于其二次复杂性。在这项工作中，我们发现了一个关键模式：某些看似无意义的特殊令牌（即分隔符）相比于语义上有意义的令牌对注意力分数的贡献不成比例地高。这一观察表明，这些分隔符之间的段的信息可以有效地压缩到分隔符本身而不会造成重大信息丢失。基于这一见解，我们引入了SepLLM，这是一个即插即用框架，通过压缩这些段并消除冗余令牌来加速推理。此外，我们还实现了高效的内核以加速训练。实验结果涵盖了无训练、从头开始训练以及后训练设置，证明了SepLLM的有效性。值得注意的是，使用Llama-3-8B骨干模型，在GSM8K-CoT基准测试中，SepLLM实现了超过50%的KV缓存减少，同时保持了相当的性能。此外，在流式设置中，SepLLM能够有效处理多达400万个或更多令牌的序列，同时保持一致的语言建模能力。**|
|**2024-12-16**|**Instruction-based Image Manipulation by Watching How Things Move**|Mingdeng Cao et.al.|[2412.12087](http://arxiv.org/abs/2412.12087)|null|本文介绍了一种新颖的数据集构建管道，该管道从视频中采样帧对，并使用多模态大语言模型（MLLMs）生成编辑指令，以训练基于指令的图像操作模型。视频帧天然保留了主体和场景的身份，确保在编辑过程中内容的一致性。此外，视频数据捕捉到的多样化、自然动态——如非刚体主体运动和复杂的摄像机运动——是其他方式难以建模的，使其成为可扩展数据集构建的理想来源。通过这种方法，我们创建了一个新的数据集来训练InstructMove模型，该模型能够执行基于指令的复杂操作，这些操作用合成生成的数据集很难实现。我们的模型在调整主体姿势、重新排列元素和改变摄像机视角等任务中展示了最先进的性能。|
|**2024-12-16**|**CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology**|Yuxuan Sun et.al.|[2412.12077](http://arxiv.org/abs/2412.12077)|null|大型多模态模型（LMMs）的出现为病理学带来了显著的进步。先前的研究主要集中在单独训练切片级别和全切片图像（WSI）级别的模型上，这限制了在不同切片和WSI之间知识的整合，导致了冗余的模型。在这项工作中，我们介绍了CPath-Omni，这是第一个拥有150亿参数的大型多模态模型，旨在统一切片级别和WSI级别图像分析，整合各种任务，包括分类、视觉问答、描述生成和视觉参考提示等。广泛的实验表明，CPath-Omni在42个数据集中的39个上，在七个不同的任务上达到了最先进的性能，其表现优于或匹配专门针对单一任务训练的任务特定模型。此外，我们为CPath-Omni开发了一种专门的基于病理学的CLIP视觉处理器CPath-CLIP，这是第一次集成不同的视觉模型，并结合大型语言模型作为文本编码器来构建更强大的CLIP模型，该模型在九个零样本和四个少样本数据集上达到了最先进的性能。我们的研究结果突显了CPath-Omni在统一各种病理学任务方面的优势，展示了其在推进病理学基础模型领域中的潜力。|
|**2024-12-16**|**CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding**|Guo Chen et.al.|[2412.12075](http://arxiv.org/abs/2412.12075)|null|大多数现有的多模态大型语言模型（MLLMs）视频理解基准仅针对短视频。对于长视频理解的有限基准通常依赖于单选题（MCQs）。然而，由于基于单选题的评估方法存在固有限制以及MLLMs推理能力的提升，模型可以通过结合短视频理解与选项排除来给出当前答案，而无需真正理解视频内容。为了解决这一问题，我们引入了CG-Bench，这是一个专为长视频线索引导式问答设计的新基准。CG-Bench强调模型检索相关线索以回答问题的能力，从而提高评估的可信度。该基准包括1,219个由人工精心策划的视频，按照一个细致的分类系统进行分类，包含14个主要类别、171个次要类别和638个三级类别，使其成为最大的长视频分析基准。该基准包含12,129组QA对，分为三大类问题：感知、推理和幻觉。为了弥补纯单选题评估方法的不足，我们设计了两种新的基于线索的评估方法：线索引导白盒和黑盒评估，以评估模型是否基于对视频的正确理解生成答案。我们对多个闭源和开源MLLMs在CG-Bench上进行了评估。结果显示，当前模型在理解长视频方面显著不如短片，并且开源模型与商业模型之间存在显著差距。我们希望CG-Bench能够推动更可靠和强大的MLLMs的发展，用于长视频理解。所有注释和视频数据已发布在https://cg-bench.github.io/leaderboard/。|
|**2024-12-16**|**Making FETCH! Happen: Finding Emergent Dog Whistles Through Common Habitats**|Kuleen Sasse et.al.|[2412.12072](http://arxiv.org/abs/2412.12072)|**[link](https://github.com/kuleens/fetch-dog-whistle)**|**警告：本文档包含可能引起某些读者不安或冒犯的内容。狗哨言论具有双重含义：一种是面向普通公众（外群体）的含义，另一种则是传达给特定受众（内群体）的特定信息。这些表达通常用于传达有争议的政治观点，同时保持可抗辩性，并且能够避开内容审核过滤器。识别狗哨言论依赖于精心编纂的词汇表，但这些词汇表难以与时俱进。我们介绍了\textbf{FETCH！}任务，旨在从大规模社交媒体语料库中发现新的狗哨言论。我们发现最先进的系统在三个不同的社交媒体案例研究中未能取得有意义的结果。我们提出了\textbf{EarShot}系统，该系统结合了向量数据库和大型语言模型（LLMs）的优势，以高效且有效地识别新的狗哨言论。**|
|**2024-12-16**|**Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection**|Ira Ceka et.al.|[2412.12039](http://arxiv.org/abs/2412.12039)|null|尽管大型语言模型（LLMs）取得了显著的成功，但在诸如漏洞检测等实际任务上的表现仍然有限。我们研究了各种用于漏洞检测的提示策略，并在此探索过程中提出了一种集成自然语言描述和对比链式推理方法的提示策略，该方法通过使用来自合成数据集的对比样本进行增强。我们的研究表明，通过将自然语言描述、对比推理和合成示例整合到一个全面的提示框架中，LLMs有可能更好地检测漏洞。在高质量的漏洞检测数据集（如SVEN）上，我们的提示策略可以分别提高准确率、F1分数和成对准确率23%、11%和14%。|
|**2024-12-16**|**SpeechPrune: Context-aware Token Pruning for Speech Information Retrieval**|Yueqian Lin et.al.|[2412.12009](http://arxiv.org/abs/2412.12009)|null|我们介绍了Speech Information Retrieval（SIR），这是一种新的长上下文任务，适用于Speech Large Language Models（Speech LLMs）。同时，我们提出了SPIRAL，一个包含1,012个样本的数据集，用于测试模型从大约90秒的语音输入中提取关键信息的能力。尽管当前的Speech LLMs在短任务上表现出色，但在处理较长音频序列时，它们面临着计算和表征上的挑战。为了解决这一限制，我们提出了一种名为SpeechPrune的无训练令牌剪枝策略，该策略利用语音-文本相似性和近似注意力分数来高效地丢弃无关令牌。在SPIRAL数据集中，SpeechPrune在20%的剪枝率下，相比原始模型和随机剪枝模型分别实现了29%和47%的准确率提升。即使在80%的高剪枝率下，SpeechPrune仍能保持网络性能。这种方法突显了令牌级剪枝在高效且可扩展的长篇语音理解中的潜力。|
|**2024-12-16**|**The Open Source Advantage in Large Language Models (LLMs)**|Jiya Manchanda et.al.|[2412.12004](http://arxiv.org/abs/2412.12004)|null|大型语言模型（LLMs）标志着自然语言处理（NLP）的关键转变，这些模型在文本生成、翻译和特定领域的推理方面取得了显著进展。封闭源代码的模型如GPT-4，凭借专有的数据集和大量的计算资源，目前处于领先地位。然而，它们因其“黑箱”性质而受到批评，并限制了可访问性，从而阻碍了可重复性和公平的人工智能发展。相比之下，像LLaMA和BLOOM这样的开源倡议通过社区驱动的发展和计算效率优先来实现民主化。这些模型在语言多样性以及特定领域应用方面的性能差距显著缩小，同时为全球研究人员和开发者提供了可访问的工具。值得注意的是，这两种范式都依赖于基础架构创新，如Vaswani等人（2017年）提出的Transformer框架。封闭源代码的模型通过有效扩展来表现出色，而开源模型则适应了在代表性不足的语言和领域的实际应用。低秩自适应（LoRA）技术和指令调优数据集等技术使开源模型能够在资源有限的情况下达到具有竞争力的结果。可以肯定的是，封闭源代码与开源方法之间的张力反映了关于透明度与专有控制更广泛的辩论。伦理考虑进一步突显了这一分歧：封闭源代码系统限制外部审查，而开源模型则促进可重复性和协作，但缺乏标准化的审计文档框架来减轻偏见。利用两种范式优势的混合方法很可能会塑造未来LLM创新的方向，确保可访问性、竞争性的技术性能和伦理部署。|
|**2024-12-16**|**LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse Input Contexts**|Zhuhao Wang et.al.|[2412.12001](http://arxiv.org/abs/2412.12001)|**[link](https://github.com/zh-wang-med/llm-rg4)**|撰写放射学报告是一项复杂的任务，需要根据可用信息和特定的临床需求灵活调整内容。然而，大多数当前的放射学报告生成（RRG）模型都受限于固定的任务范式，例如仅从单一图像预测“发现”部分，这导致输入和输出之间存在不匹配的问题。训练出的模型缺乏处理多样输入的灵活性，并且可能会生成与输入无关的有害幻觉。为了弥合当前RRG模型与实际临床需求之间的差距，我们首先开发了一个数据生成管道来创建一个新的MIMIC-RG4数据集，该数据集考虑了四种常见的放射学报告起草场景，并且输入和输出完美对应。其次，我们提出了一种新的基于大型语言模型（LLM）的RRG框架，称为LLM-RG4，该框架利用了LLM在指令执行方面的灵活性及其广泛的一般知识。此外，我们还开发了一种自适应令牌融合模块，以提供灵活性来处理具有不同输入组合的不同场景，同时尽量减少由于输入量增加而带来的额外计算负担。另外，我们提出了一个令牌级损失加权策略，以引导模型关注正面和不确定的描述。实验结果表明，LLM-RG4在MIMIC-RG4和MIMIC-CXR数据集上实现了最先进的性能，不仅在临床效率方面表现出色，在自然语言生成方面也表现优异。我们定量证明我们的模型几乎不会产生与输入无关的幻觉，而当前的开源模型普遍会遇到这个问题。|
|**2024-12-16**|**Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support**|Devika Venugopalan et.al.|[2412.11995](http://arxiv.org/abs/2412.11995)|**[link](https://github.com/devika-prog/caregiver-conversational-support-tool)**|**照顾者（即父母和儿童护理社区的成员）是学习分析中被低估的利益相关者。尽管照顾者的参与可以提高学生的学术成绩，但许多障碍阻碍了这种参与，最明显的是对现代学校课程的知识缺口。学习分析中的一个新兴话题是混合辅导，它包括教学和支持激励。照顾者在家庭作业方面也扮演着类似的角色，但尚不清楚学习分析如何支持他们。我们过去与照顾者的工作表明，对话式支持是一种为照顾者提供有效支持学生学习所需指导的有前景的方法。我们开发了一个系统，通过由大语言模型（LLM）生成的对话推荐为照顾者提供教学支持。针对LLM已知的教学局限性，我们在进行提示工程实验时使用了来自辅导系统的教学智能。这个LLM为照顾者通过聊天支持孩子的数学练习生成了消息推荐。少量提示和结合来自辅导系统的实时问题解决背景与辅导实践示例产生了理想的推荐消息。这些推荐消息由十位初中生的照顾者进行了评估，他们重视能够促进内容层面的支持和通过自我解释来提升学生元认知的推荐。我们贡献了见解，说明如何将辅导系统与LLM最佳融合，以支持混合辅导环境下的对话式协助，从而促进有效的照顾者参与辅导系统。**|
|**2024-12-13**|**UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities**|Muhammad Uzair Khattak et.al.|[2412.10372](http://arxiv.org/abs/2412.10372)|**[link](https://github.com/mbzuai-oryx/unimed-clip)**|**视觉-语言模型（VLMs）通过对比学习在自然图像任务中取得了显著的成功。然而，由于公开可访问的大规模医学图像-文本数据集的稀缺性，它们在医学领域的应用仍然有限。现有的医学VLM要么是在封闭源的专有数据集上训练，要么是使用相对较小的开源数据集，这些数据集的泛化能力较差。同样，大多数模型仅限于单一或少数几种医学成像领域，这限制了它们在其他模态中的应用。为了解决这一问题，我们引入了UniMed，这是一个大规模、开源的多模态医学数据集，包含六个不同的成像模态：X射线、CT、MRI、超声、病理学和眼底成像，共有超过530万张图像-文本对。UniMed是利用一个数据收集框架开发的，该框架利用大型语言模型（LLMs）将特定模态的分类数据集转换为图像-文本格式，同时整合来自医学领域的现有图像-文本数据，从而促进VLM的可扩展预训练。使用UniMed，我们训练了UniMed-CLIP，这是一种针对六个模态的统一VLM，在零样本评估中显著优于现有的通用VLM，并且与特定模态的医学VLM相当，例如，UniMed-CLIP在21个数据集中平均绝对增益为+12.61，而使用的训练数据仅为BiomedCLIP的三分之一。为了促进未来的研究，我们在https://github.com/mbzuai-oryx/UniMed-CLIP发布了UniMed数据集、训练代码和模型。**|
|**2024-12-13**|**Robust image classification with multi-modal large language models**|Francesco Villani et.al.|[2412.10353](http://arxiv.org/abs/2412.10353)|null|深度神经网络容易受到对抗性样本的影响，这些样本是经过精心设计的输入样例，能够导致模型以高置信度做出错误预测。为了缓解这些漏洞，已经提出了对抗性训练和基于检测的防御方法来提前增强模型。然而，大多数这些方法都集中在单一数据模态上，忽视了输入的视觉模式与文本描述之间的关系。在本文中，我们提出了一种新的防御方法Multi-Shield，旨在通过利用多模态信息来结合和补充这些防御措施，进一步增强其鲁棒性。Multi-Shield利用多模态大语言模型来检测对抗性样本，并在文本和视觉表示之间没有对齐时放弃不确定的分类。在CIFAR-10和ImageNet数据集上的广泛评估显示，使用鲁棒和非鲁棒图像分类模型时，Multi-Shield可以轻松集成以检测和拒绝对抗性样本，从而优于原始防御方法。|
|**2024-12-13**|**COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models**|Yuchen Ren et.al.|[2412.10347](http://arxiv.org/abs/2412.10347)|null|作为中心法则的关键组成部分，DNA、RNA和蛋白质在维持生命方面发挥着至关重要的作用，确保了基因表达和实施的准确性。尽管对这些分子的研究深刻影响了医学、农业和工业等领域，但机器学习方法的多样性——从传统的统计方法到深度学习模型和大语言模型——给研究人员选择适合特定任务的最合适的模型带来了挑战，特别是在跨组学和多组学任务中，由于缺乏全面的基准测试而更加困难。为了解决这个问题，我们介绍了第一个全面的多组学基准测试COMET（生物综合多组学评估任务和语言模型基准），旨在评估模型在单组学、跨组学和多组学任务中的表现。首先，我们整理并开发了一套多样化的下游任务和数据集，涵盖了DNA、RNA和蛋白质的关键结构和功能方面，包括跨越多个组学层面的任务。然后，我们评估了现有的DNA、RNA和蛋白质的基础语言模型，以及新提出的多组学方法，提供了有关其在整合和分析来自不同生物模态的数据方面的性能的有价值见解。这个基准旨在定义多组学研究中的关键问题，并指导未来的发展方向，最终通过集成和分析不同组学数据来促进对生物过程的理解。|
|**2024-12-13**|**Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining**|Zhiqi Ge et.al.|[2412.10342](http://arxiv.org/abs/2412.10342)|null|数字代理越来越多地被用于自动化交互式数字环境中的任务，如网页、软件应用和操作系统。虽然基于大型语言模型（LLMs）的文本代理经常需要频繁更新以适应特定平台的API，但利用多模态大型语言模型（MLLMs）的视觉代理通过直接与图形用户界面（GUIs）交互提供了增强的适应性。然而，这些代理在视觉感知方面面临重大挑战，尤其是在处理高分辨率和视觉复杂的数字环境时。本文介绍了一种名为Iris的基础视觉代理，该代理通过两个关键技术解决了这些挑战：信息敏感裁剪（ISC）和自我精炼双重学习（SRDL）。ISC通过使用边缘检测算法动态识别并优先处理视觉密度较高的区域，从而实现高效处理，通过将更多计算资源分配给信息密度更高的区域来提高效率。SRDL通过双重学习循环增强代理处理复杂任务的能力，在这个循环中，指代（描述UI元素）的改进会强化定位（定位元素）的能力，反之亦然，所有这些都不需要额外的标注数据。实证评估表明，Iris在多个基准测试中实现了最先进的性能，仅使用了85万个人工标注的GUI数据，超过了使用10倍更多训练数据的方法。这些改进进一步显著提升了网络和操作系统代理下游任务的表现。|
|**2024-12-13**|**AdvPrefix: An Objective for Nuanced LLM Jailbreaks**|Sicheng Zhu et.al.|[2412.10321](http://arxiv.org/abs/2412.10321)|**[link](https://github.com/facebookresearch/jailbreak-objectives)**|许多针对大型语言模型（LLMs）的越狱攻击依赖于一个共同的目标：使模型以“Sure, here is (有害请求)”作为前缀进行响应。尽管这种方法直接有效，但它有两个局限性：对模型行为的控制有限，通常导致不完整或不现实的响应，以及一种僵化的格式，这限制了优化的可能性。为了应对这些局限性，我们引入了AdvPrefix，这是一种新的前缀强制目标，它能够在保持易于优化的同时实现更细腻的模型行为控制。我们的目标利用了根据两个标准自动选择的模型依赖前缀：高预填充攻击成功率和低负对数似然。通过使用多个前缀来处理单个用户请求，它可以进一步简化优化过程。AdvPrefix可以无缝集成到现有的越狱攻击中，无需额外成本即可提高其性能。例如，在Llama-3上简单地用我们的目标前缀替换GCG攻击的目标前缀，可以使细腻攻击的成功率从14%提高到80%，这表明当前的对齐技术在面对未见过的前缀时难以泛化。我们的工作展示了越狱目标在实现细腻越狱中的重要性。|
|**2024-12-13**|**BrushEdit: All-In-One Image Inpainting and Editing**|Yaowei Li et.al.|[2412.10316](http://arxiv.org/abs/2412.10316)|null|图像编辑随着扩散模型的发展取得了显著进步，这些模型既包括基于反转的方法也包括基于指令的方法。然而，当前的基于反转的方法在进行大范围修改（如添加或删除对象）时面临挑战，因为反转噪声的结构性质限制了大幅改动的能力。同时，基于指令的方法通常将用户限制在黑盒操作中，限制了直接交互以指定编辑区域和强度的能力。为了解决这些局限性，我们提出了BrushEdit，这是一种新颖的基于补丁填充的指令引导图像编辑范式，利用多模态大型语言模型（MLLMs）和图像补丁填充模型，实现自主、用户友好且交互式的自由形式指令编辑。具体而言，我们设计了一个系统，通过在代理协同框架中整合MLLMs和双分支图像补丁填充模型，实现了编辑类别分类、主要对象识别、掩码获取和编辑区域补丁填充等功能。广泛的实验表明，我们的框架有效结合了MLLMs和补丁填充模型，在包括掩码区域保留和编辑效果连贯性的七个指标上均表现出色。|
|**2024-12-13**|**Still "Talking About Large Language Models": Some Clarifications**|Murray Shanahan et.al.|[2412.10291](http://arxiv.org/abs/2412.10291)|null|我的论文《谈论大型语言模型》不止一次被解读为对大型语言模型持还原论立场。然而，这篇论文并不是这个意思，我也并不支持这样的观点。这篇短文旨在将论文置于一个更大的哲学项目背景之中，该项目关注的是词汇的（误）用法而非形而上学，秉承了维特根斯坦后期的思想。|
|**2024-12-13**|**One world, one opinion? The superstar effect in LLM responses**|Sofie Goethals et.al.|[2412.10281](http://arxiv.org/abs/2412.10281)|null|随着大型语言模型（LLMs）正在塑造在线信息的共享和访问方式，它们的观点有可能影响广泛的受众。本研究通过使用十种不同语言的提示来探讨语言多样性的影响，以考察LLMs眼中各领域最杰出的人物。我们的研究结果揭示了回应中的低多样性，即少数人物在多种语言中占据主导地位（也称为“超级明星效应”）。这些结果突显了当LLMs检索主观信息时，可能导致全球知识表现范围缩小的风险。|
|**2024-12-13**|**Benchmarking Linguistic Diversity of Large Language Models**|Yanzhu Guo et.al.|[2412.10271](http://arxiv.org/abs/2412.10271)|**[link](https://github.com/yanzhuguo/llm-diversity)**|**大型语言模型（LLMs）的开发和评估主要集中在它们解决问题的能力上，一些最新的模型甚至在某些领域超过了人类的表现。然而，这种关注往往忽视了机器生成的语言是否在词汇选择、句法构造和意义表达方面达到了人类水平，这引发了关于语言生成基本要素是否得到充分解决的问题。本文强调了考察语言模型是否保留了人类语言丰富性的必要性，鉴于由LLMs生成或辅助生成的在线内容激增这一令人担忧的趋势。我们提出了一种全面的框架，从词汇、句法和语义等多个语言多样性角度来评估LLMs。使用这个框架，我们对几个最先进的LLMs进行了全方位的多样性维度基准测试，并对句法多样性进行了深入的案例研究。最后，我们分析了不同的开发和部署选择如何影响LLMs输出的语言多样性。**|
|**2024-12-13**|**Cultural Evolution of Cooperation among LLM Agents**|Aron Vallinder et.al.|[2412.10270](http://arxiv.org/abs/2412.10270)|null|大型语言模型（LLMs）为构建具备广泛能力的AI代理提供了强有力的基础。这些代理可能会在未来大规模地在现实世界中部署，代表个体人类（例如AI助手）或人类群体（例如AI加速的公司）的利益。目前，对于多个LLM代理在多代迭代部署中的动态了解相对较少。在本文中，我们研究了一群LLM代理在面对背叛激励的情况下能否学会相互有益的社会规范，这是人类社会性的一个独特特征，被认为是文明成功的关键因素之一。特别是，我们研究了间接互惠在LLM代理玩经典重复捐赠者博弈过程中跨世代的演变，在这个博弈中，代理可以观察到同伴的近期行为。我们发现，不同基础模型之间的合作演化表现出显著差异，其中Claude 3.5 Sonnet代理组成的社团实现了显著更高的平均得分，而Gemini 1.5 Flash的表现优于GPT-4o。此外，Claude 3.5 Sonnet可以通过额外的成本惩罚机制实现更高的得分，而Gemini 1.5 Flash和GPT-4o则未能做到这一点。对于每种模型类别，我们还观察到随机种子的不同导致了涌现行为的变化，这表明对初始条件的高度敏感性是一个未被充分研究的现象。我们建议，我们的评估体系可以激发一种新的、成本低廉且信息丰富的LLM基准测试类别，专注于LLM代理部署对社会合作基础设施的影响。|
|**2024-12-12**|**EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM**|Zhuofan Zong et.al.|[2412.09618](http://arxiv.org/abs/2412.09618)|null|在个性化扩散模型方面取得了显著成就。传统的无需调优的方法大多通过平均多幅参考图像的图像嵌入来编码注入条件，但这种与图像无关的操作无法在图像之间进行交互以捕捉多个参考中的连贯视觉元素。虽然基于调优的低秩自适应（LoRA）可以通过训练过程有效提取多个图像中的连贯元素，但它需要对每个不同的图像组进行特定微调。本文介绍了一种新的即插即用自适应方法EasyRef，使扩散模型能够根据多幅参考图像和文本提示进行调节。为了有效利用多幅图像中的连贯视觉元素，我们利用了多模态大语言模型（MLLM）的多图像理解和指令跟随能力，根据指令提示它捕获图像中的连贯视觉元素。此外，通过适配器将MLLM的表示注入扩散过程中可以轻松泛化到未见过的领域，在未见过的数据中挖掘连贯的视觉元素。为了降低计算成本并增强细节保留，我们引入了一种高效的参考聚合策略和一种渐进训练方案。最后，我们介绍了MRBench，一个新的多参考图像生成基准。实验结果表明，EasyRef在美学质量和跨不同领域的零样本泛化能力上均超越了像IP-Adapter这样的无需调优方法和像LoRA这样的基于调优的方法。|
|**2024-12-12**|**Olympus: A Universal Task Router for Computer Vision Tasks**|Yuanze Lin et.al.|[2412.09612](http://arxiv.org/abs/2412.09612)|**[link](https://github.com/yuanze-lin/olympus_page)**|**我们介绍了Olympus，这是一种新的方法，能够将多模态大型语言模型（MLLMs）转化为一个统一的框架，能够处理广泛的计算机视觉任务。通过使用控制器MLLM，Olympus将超过20个专门针对图像、视频和3D对象的任务委派给专用模块。这种基于指令的路由使复杂的流程可以通过链接操作实现，而无需训练重型生成模型。Olympus可以轻松集成到现有的MLLM中，扩展它们的能力，性能相当。实验结果表明，Olympus在20个任务中的平均路由准确率为94.75%，在链接操作场景中的精度为91.82%，展示了其作为通用任务路由器的有效性，能够解决各种各样的计算机视觉任务。项目页面：https://github.com/yuanze-lin/Olympus_page**|
|**2024-12-12**|**SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding**|Hao Li et.al.|[2412.09604](http://arxiv.org/abs/2412.09604)|null|大型语言模型（LLMs）的成功已经扩展到多模态领域，在图像理解和生成方面表现出色。近期开发的统一多模态大型语言模型（MLLMs），旨在整合这些能力，已显示出有希望的结果。然而，现有的方法通常涉及复杂的模型架构或训练流程，增加了模型训练和扩展的难度。在本文中，我们提出了SynerGen-VL，这是一种简单而强大的无编码器MLLM，能够进行图像理解和生成。为了应对现有无编码器统一MLLM中发现的挑战，我们引入了令牌折叠机制和基于视觉专家的渐进对齐预训练策略，这有效地支持了高分辨率图像理解，同时降低了训练复杂性。经过在大规模混合图文数据上使用统一的下一个标记预测目标进行训练后，SynerGen-VL 在参数大小相同或更小的情况下，达到了或超过了现有无编码器统一MLLM的性能，并缩小了与特定任务最先进模型之间的差距，突显了未来统一MLLM的一个有前景的发展路径。我们的代码和模型将会公开发布。|
|**2024-12-12**|**Do Multimodal Large Language Models See Like Humans?**|Jiaying Lin et.al.|[2412.09603](http://arxiv.org/abs/2412.09603)|null|多模态大型语言模型（MLLMs）在各种视觉任务上取得了显著成果，这得益于大型语言模型的最新进展。然而，一个关键问题仍未得到解答：MLLMs是否以与人类相似的方式感知视觉信息？当前的基准测试缺乏从这一角度评估MLLMs的能力。为了解决这一挑战，我们引入了HVSBench，这是一个大规模基准测试，旨在评估MLLMs在基本视觉任务上的表现与人类视觉系统（HVS）之间的对齐情况。HVSBench收集了超过85000个多模态样本，涵盖HVS中的13个类别和5个领域，包括突出性、快速计数、优先级排序、自由观看和搜索。广泛的实验表明，我们的基准测试能够提供对MLLMs的全面评估。具体来说，我们评估了13种MLLMs，结果表明即使是最好的模型也显示出显著的改进空间，大多数模型仅达到中等水平的结果。我们的实验表明，HVSBench为最先进的MLLMs提出了一个新的且重要的挑战。我们认为，HVSBench将促进研究人类对齐和可解释的MLLMs的发展，标志着理解MLLMs如何感知和处理视觉信息的重要一步。|
|**2024-12-12**|**InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions**|Pan Zhang et.al.|[2412.09596](http://arxiv.org/abs/2412.09596)|**[link](https://github.com/internlm/internlm-xcomposer)**|**创建能够在长时间内与环境互动的AI系统，类似于人类的认知，一直是长期的研究目标。最近多模态大型语言模型（MLLMs）的进展在开放式理解方面取得了显著成就。然而，持续且同时进行流式感知、记忆和推理的挑战仍然很大程度上未被探索。当前的MLLMs受限于其序列到序列的架构，这限制了它们同时处理输入和生成响应的能力，就像无法在感知的同时思考一样。此外，依赖长上下文来存储历史数据对于长期互动是不切实际的，因为保留所有信息变得既昂贵又低效。因此，本项目借鉴了专精型通用人工智能的概念，引入了解耦的流式感知、推理和记忆机制，使AI能够实时与流式视频和音频输入交互。所提出的框架InternLM-XComposer2.5-OmniLive（IXC2.5-OL）由三个关键模块组成：（1）流式感知模块：实时处理多模态信息，在内存中存储关键细节，并根据用户查询触发推理。（2）多模态长时记忆模块：整合短期和长期记忆，将短期记忆压缩为长期记忆，以提高检索效率和准确性。（3）推理模块：响应查询并执行推理任务，协调与感知和记忆模块的工作。该项目模拟了类人认知，使多模态大型语言模型能够提供连续且自适应的服务。**|
|**2024-12-12**|**DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction**|Yu Feng et.al.|[2412.09572](http://arxiv.org/abs/2412.09572)|null|量化大型语言模型（LLMs）在事实参数知识方面的不确定性，在黑盒设置下是一个重大挑战。现有方法通过评估模型对原始查询响应的一致性来衡量模型的不确定性，但这些方法并不总是能捕捉到真正的不确定性。模型可能会对原始查询保持一致但错误的回答，同时对同一查询的不同角度提出正确的问题，反之亦然。在本文中，我们提出了一种新颖的方法，即DiverseAgentEntropy，用于通过多代理交互评估模型的不确定性，假设如果模型是确定的，它应该能够在关于同一原始查询的多样化问题集合中始终回忆起正确的答案。我们进一步实施了一项弃权政策，在不确定性较高时拒绝回答。我们的方法提供了更准确的模型可靠性的预测，并进一步检测幻觉，优于其他基于自一致性度量的方法。此外，我们的方法还表明，即使知道正确答案，现有的模型也经常无法在多样化的问题下始终如一地检索到正确的答案。|
|**2024-12-12**|**Does Representation Matter? Exploring Intermediate Layers in Large Language Models**|Oscar Skean et.al.|[2412.09563](http://arxiv.org/abs/2412.09563)|null|理解大型语言模型（LLMs）中的优质表示对于理论理解和实际应用都至关重要。在本文中，我们研究了不同LLM架构（包括Transformer和状态空间模型（SSMs））中的中间表示的质量。我们发现，中间层通常为下游任务提供比最终层更具信息量的表示。为了衡量表示的质量，我们将一些最初在其他背景下提出的度量标准（如提示熵、曲率和增强不变性）进行调整并应用于我们的研究。我们的实证研究表明了显著的架构差异，以及表示如何在整个训练过程中演变，并探讨了输入随机性和提示长度等因素对每一层的影响。值得注意的是，我们观察到某些中间层的熵呈现双峰模式，并考虑与训练数据相关的潜在解释。总体而言，我们的结果揭示了LLMs的内部机制，并指导了架构优化和训练策略。|
|**2024-12-12**|**Foundational Large Language Models for Materials Research**|Vaibhav Mishra et.al.|[2412.09560](http://arxiv.org/abs/2412.09560)|**[link](https://github.com/M3RG-IITD/llamat)**|材料发现与开发对于应对全球挑战至关重要。然而，材料科学文献的指数增长导致了大量的文本数据，这在知识提取、综合和科学研究方面造成了显著瓶颈。大型语言模型（LLMs）通过自动化分析和预测提供了加速材料研究的前所未有的机会。然而，其有效部署需要领域特定的适应性，以便理解和解决领域相关任务。在此，我们介绍了LLaMat，这是一个通过在广泛的材料文献和晶体学数据集上对LLaMA模型进行持续预训练而开发的基础模型家族。通过系统评估，我们证明LLaMat在材料特定的自然语言处理和结构化信息提取方面表现出色，同时保持了通用的语言能力。专门化的LLaMat-CIF变体在晶体结构生成方面展示了前所未有的能力，在元素周期表的广泛范围内预测稳定的晶体。有趣的是，尽管在比较中LLaMA-3的表现优于LLaMA-2，但我们观察到LLaMat-2在各种材料科学任务中表现出超出预期的领域特定性能，包括从文本和表格中提取结构化信息，特别是在晶体结构生成方面，这可能是过度训练的LLM的一种潜在适应刚性。总之，本研究证明了领域适应的有效性，有助于开发实用的材料研究LLM副驾。除此之外，我们的研究结果揭示了对LLM领域适应的重要考虑因素，如模型选择、训练方法以及领域特定性能，这些可能影响专门科学AI系统的开发。|
|**2024-12-12**|**Exemplar Masking for Multimodal Incremental Learning**|Yi-Lun Lee et.al.|[2412.09549](http://arxiv.org/abs/2412.09549)|**[link](https://github.com/yilunlee/exemplar_masking_mcil)**|**多模态增量学习需要在消化来自多种模态的信息的同时，不遗忘之前学到的知识。此任务面临诸多挑战，主要包括基于实例的方法中多模态数据的存储量更大以及对大型多模态模型进行微调所需的计算资源较多。本文利用参数高效调优方案来减轻微调负担，并提出了一种实例掩码框架以有效地重放旧知识。具体而言，根据注意力权重和不同模态之间的相关性，我们屏蔽掉非重要标记，显著减少了实例的存储大小，从而在相同内存缓冲区下保存更多的实例。此外，我们设计了一种多模态数据增强技术，以多样化实例以重播先前的知识。实验不仅在现有的多模态数据集上评估了我们的方法，还扩展了ImageNet-R数据集作为一个真实应用，在该应用中，通过查询多模态大语言模型（例如InstructBLIP）生成字幕。广泛的实验表明，我们的实例掩码框架在相同的有限内存缓冲区下更高效且更能抵抗灾难性遗忘。代码可在https://github.com/YiLunLee/Exemplar_Masking_MCIL获取。**|
|**2024-12-12**|**Can Modern LLMs Act as Agent Cores in Radiology~Environments?**|Qiaoyu Zheng et.al.|[2412.09529](http://arxiv.org/abs/2412.09529)|**[link](https://github.com/magic-ai4med/radabench)**|近年来，大型语言模型（LLMs）的发展为基于LLM的代理系统在各个领域的应用提供了可能，这些系统在准确性和可解释性方面表现出色。放射学因其复杂的分析需求，成为这些代理系统应用的理想领域。本文旨在探讨构建具体放射学代理系统的核心问题，即“现代LLMs能否作为放射学环境中的代理核心？”为了探讨这个问题，我们引入了RadABench，并做出了以下三个贡献：首先，我们提出了RadABench-Data，这是一个综合的合成评估数据集，用于LLM基代理，它涵盖了由6个解剖部位、5种成像模式、10类工具和11种放射学任务组成的广泛分类。其次，我们提出了一种新的代理评估平台RadABench-EvalPlat，该平台具有基于提示的工作流程，并能够模拟各种放射学工具集。第三，我们从五个角度使用多种指标评估了七种领先的LLMs在我们的基准测试中的表现。研究结果表明，尽管当前的LLMs在许多方面表现出强大的能力，但它们仍然不够先进，无法作为完全运行的放射学代理系统的核心代理核心。此外，我们还识别了影响基于LLM的代理核心性能的关键因素，为临床医生提供了如何在现实世界的放射学实践中有效应用代理系统的见解。我们所有的代码和数据都在https://github.com/MAGIC-AI4Med/RadABench上开源。|
|**2024-12-11**|**Generative Semantic Communication: Architectures, Technologies, and Applications**|Jinke Ren et.al.|[2412.08642](http://arxiv.org/abs/2412.08642)|null|本文深入探讨了生成式人工智能（GAI）在语义通信（SemCom）中的应用，并进行了全面的研究。首先介绍了三种由经典GAI模型支持的流行SemCom系统，包括变分自编码器、生成对抗网络和扩散模型。对于每个系统，阐述了GAI模型的基本概念、相应的SemCom架构以及最近研究的相关文献综述。然后，通过结合前沿的GAI技术——大型语言模型（LLM），提出了一种新颖的生成式SemCom系统。该系统在发送端和接收端分别采用基于LLM的AI代理作为“大脑”，以实现强大的信息理解和内容再生能力。这种创新设计使接收端能够基于发送端传达的编码语义信息直接生成所需内容，而不是恢复比特流，从而将通信思维从“信息恢复”转变为“信息再生”，进而开启了生成式SemCom的新时代。通过点对点视频检索案例研究，展示了所提出的生成式SemCom系统的优越性，与传统通信系统相比，该系统可减少99.98%的通信开销并提高53%的检索准确性。此外，还概述了四种典型的生成式SemCom应用场景，并讨论了三个需要未来研究的开放问题。总而言之，本文为在SemCom中应用GAI提供了一整套指导原则，为未来无线网络中高效实施生成式SemCom铺平了道路。|
|**2024-12-11**|**Fast Prompt Alignment for Text-to-Image Generation**|Khalil Mrini et.al.|[2412.08639](http://arxiv.org/abs/2412.08639)|**[link](https://github.com/tiktok/fast_prompt_alignment)**|**文本到图像的生成技术近年来取得了显著进步，但将复杂的文本提示与生成的视觉效果对齐仍然具有挑战性，尤其是在处理复杂对象关系和精细细节时。本文介绍了一种名为快速提示对齐（Fast Prompt Alignment, FPA）的方法框架，该方法采用单次传递方法，提高了文本到图像对齐的效率，而无需像当前方法（如OPT2I）那样进行迭代操作。FPA利用大型语言模型（LLMs）进行单次迭代的提示重述，随后通过优化后的提示进行微调或上下文学习，从而实现实时推理，减少计算需求同时保持对齐精度。在COCO Captions和PartiPrompts数据集上的广泛评估表明，FPA在处理时间仅为其他方法一小部分的情况下，实现了竞争性的文本-图像对齐分数。通过自动指标（TIFA, VQA）和人工评估均验证了这一点。此外，由专家标注者参与的人类研究表明，人类对齐判断与自动化评分之间存在强烈的关联，这进一步证明了FPA改进的稳健性。所提出的方法展示了可扩展且高效的替代迭代提示优化方案，使得在实时、高需求场景中的广泛应用成为可能。代码库已提供以促进进一步研究：https://github.com/tiktok/fast_prompt_alignment**|
|**2024-12-11**|**Multimodal Latent Language Modeling with Next-Token Diffusion**|Yutao Sun et.al.|[2412.08635](http://arxiv.org/abs/2412.08635)|**[link](https://github.com/microsoft/unilm/tree/master/LatentLM)**|多模态生成模型需要一种统一的方法来处理离散数据（例如文本和代码）和连续数据（例如图像、音频和视频）。在这项工作中，我们提出了潜在语言建模（LatentLM），该方法利用因果变换器无缝地整合连续和离散数据。具体而言，我们使用变分自编码器（VAE）表示连续数据作为潜在向量，并引入了下一个令牌扩散以实现这些向量的自回归生成。此外，我们开发了σ-VAE来解决方差崩溃的问题，这对于自回归建模至关重要。广泛的实验表明，LatentLM在各种模态上都表现出色。在图像生成方面，LatentLM在性能和可扩展性方面均优于扩散变换器。当集成到多模态大型语言模型中时，LatentLM提供了一个通用接口，统一了多模态生成和理解。实验结果表明，在扩大训练标记规模的情况下，LatentLM在性能上优于Transfusion和向量化模型。在文本到语音合成方面，LatentLM在说话者相似性和鲁棒性方面超过了最先进的VALL-E 2模型，同时所需的解码步骤减少了10倍。这些结果确立了LatentLM作为一种高度有效且可扩展的方法，用于推进大型多模态模型。|
|**2024-12-11**|**Synthetic Vision: Training Vision-Language Models to Understand Physics**|Vahid Balazadeh et.al.|[2412.08619](http://arxiv.org/abs/2412.08619)|null|物理推理，即解释、理解和预测动态环境中物体行为的能力，对于当前的视觉-语言模型（VLMs）来说仍然是一个重大挑战。在这项工作中，我们提出了两种方法来增强VLMs在物理推理任务中的能力，这些方法使用了模拟数据。首先，我们使用从与物理推理任务相关的模拟中生成的问题-答案（QA）对来微调预训练的VLM。其次，我们引入了物理上下文构建器（PCBs），这是一种专门用于创建富含物理属性和过程场景描述的微调VLM。在进行物理推理任务时，可以利用这些PCBs作为上下文来帮助大型语言模型（LLM）提高其性能。我们使用多个基准进行了评估，包括一个新的稳定性检测QA数据集——Falling Tower，该数据集包括模拟和真实场景，并且还包括CLEVRER。我们证明了一个小型QA微调的VLM可以显著优于更大的最先进的基础模型。我们还展示了集成PCBs可以提升基础LLM在物理推理任务上的表现。通过使用Falling Tower数据集中来自现实世界的场景，我们也验证了这两种方法在仿真到现实转移中的鲁棒性。我们的结果突显了模拟数据在创建能够进行高级物理推理的学习系统中的作用。|
|**2024-12-11**|**Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models**|Jiahui Li et.al.|[2412.08615](http://arxiv.org/abs/2412.08615)|**[link](https://github.com/jiah-li/magic)**|尽管通过对齐技术训练大型语言模型（LLMs）以增强生成内容的安全性方面取得了进展，但这些模型仍然容易受到越狱攻击的影响，这是一种揭示LLMs安全漏洞的对抗性攻击方法。值得注意的是，贪婪坐标梯度（GCG）方法已经证明能够自动生成后缀来越狱最先进的LLMs。然而，GCG涉及的优化过程非常耗时，使得越狱管道效率低下。在本文中，我们研究了GCG的过程，并发现间接效应是GCG优化的关键瓶颈。为此，我们提出了模型攻击梯度索引GCG（MAGIC），该方法通过利用后缀标记的梯度信息来解决间接效应问题，从而通过减少计算量和迭代次数来加速流程。我们的实验结果显示，在AdvBench上，MAGIC实现了高达1.5倍的速度提升，同时保持与其它基线相当甚至更高的攻击成功率（ASR）。我们的MAGIC在Llama-2上的攻击成功率为74%，并在针对GPT-3.5的迁移攻击中达到54%的成功率。代码可在https://github.com/jiah-li/magic获取。|
|**2024-12-11**|**Preference Discerning with LLM-Enhanced Generative Retrieval**|Fabian Paischer et.al.|[2412.08604](http://arxiv.org/abs/2412.08604)|null|顺序推荐系统旨在根据用户的交互历史提供个性化推荐。为了实现这一目标，它们通常结合辅助信息，如物品的文本描述，并引入辅助任务，例如预测用户偏好和意图。尽管有许多努力来增强这些模型，但它们仍然存在有限的个性化问题。为了解决这个问题，我们提出了一种新的范式，称为偏好辨别。在偏好辨别中，我们在上下文中显式地对生成式的顺序推荐系统进行用户偏好的条件限制。为此，我们利用大型语言模型（LLMs）基于用户评论和特定于项目的资料来生成用户偏好。为了评估顺序推荐系统的偏好辨别能力，我们引入了一个新颖的基准，该基准提供了涵盖各种场景的全面评估，包括偏好引导和情感跟随。我们使用我们的基准评估当前最先进的方法，并表明它们难以准确辨别用户偏好。因此，我们提出了一种名为Mender的新方法（多模态偏好辨别器），它改进了现有方法，并在我们的基准上达到了最先进的性能。我们的结果显示，即使在训练期间未观察到人类偏好，Mender也能有效地根据人类偏好进行指导，从而为更个性化的顺序推荐系统铺平了道路。我们将开源代码和基准。|
|**2024-12-11**|**Empirical Measurements of AI Training Power Demand on a GPU-Accelerated Node**|Imran Latif et.al.|[2412.08602](http://arxiv.org/abs/2412.08602)|null|人工智能（AI）应用的扩展推动了对计算基础设施的巨大投资，尤其是云服务提供商的投资。量化这些基础设施的能源足迹需要依赖于AI硬件在训练过程中的功率需求的模型。我们实测了在训练开源图像分类器（ResNet）和大型语言模型（Llama2-13b）时，一个配备8块NVIDIA H100 HGX模块的节点的瞬时功耗。观察到的最大功耗约为8.4千瓦，比制造商标称的10.2千瓦低18%，即使在GPU接近满负荷运行的情况下也是如此。保持模型架构不变，将ResNet的批量大小从512张图像增加到4096张图像，可以将总训练能耗降低四倍。这些发现可为数据中心运营商的容量规划和研究人员的能源使用估算提供参考。未来的研究将探讨冷却技术和碳意识调度对AI工作负载能耗的影响。|
|**2024-12-11**|**Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based Automated Requirement Traceability and Compliance Checks**|Arsalan Masoudifard et.al.|[2412.08593](http://arxiv.org/abs/2412.08593)|null|确保软件需求规格说明书（SRS）与高层组织或国家标准保持一致，在金融和航空航天等受监管环境中尤为重要。在这些领域，保持一致性、遵循监管框架、减少错误以及满足关键期望对于系统的可靠运行至关重要。大型语言模型（LLMs）的广泛应用突显了它们的巨大潜力，但仍然有很大的改进空间，特别是在检索相关信息和增强推理能力方面。本研究展示了通过将强大的Graph-RAG框架与高级提示工程技术（如思维链和思考树）结合，可以显著提高性能。与基线RAG方法和简单的提示策略相比，这种方法提供了更准确且具有上下文感知的结果。虽然该方法在性能上表现出显著提升，但它也带来了挑战。它既昂贵又复杂，需要在不同背景下仔细适应特定场景。此外，其有效性高度依赖于完整且准确的输入数据，而这些数据可能并不总是易于获得，这进一步限制了其可扩展性和实用性。|
|**2024-12-11**|**Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning**|Hang Zhao et.al.|[2412.08587](http://arxiv.org/abs/2412.08587)|null|在文本分类任务中，编码器模型（如BERT、RoBERTa）和大型语言模型（LLM，如Llama3）都被广泛使用。然而，关于编码器模型和LLM在文本分类中的表现对比，尤其是涉及微调的情况下，系统性的研究还比较缺乏。本研究采用了多种规模和架构的模型，并包括了预训练和微调的方法。我们首先评估了这些LLM在20 Newsgroups（20NG）和MASSIVE数据集上的表现，并将其与编码器模型RoBERTa进行了比较。此外，我们通过结合来自两个数据集的数据，探索了这两种模型类型的多任务处理能力，将多个分类任务（包括意图检测和槽填充）整合到一个模型中。我们的结果表明，完全微调的Llama3-70B模型在各种分类任务和数据集上都优于RoBERTa-large和其他解码器LLM。此外，整合的多任务微调LLM在两个数据集的两个任务中达到了与双模型设置相当的性能。总体而言，本研究为编码器模型和LLM在文本分类任务上的表现提供了全面的基准，并展示了一种结合两个或更多完全微调的解码器LLM以减少延迟并保持同等性能的方法。|
|**2024-12-11**|**TURBOATTENTION: Efficient Attention Approximation For High Throughputs LLMs**|Hao Kang et.al.|[2412.08585](http://arxiv.org/abs/2412.08585)|null|大规模语言模型（LLM）的推理需要大量的计算和内存，尤其是在关键的注意力机制方面。虽然诸如量化和加速算法（如FlashAttention）等技术已经提高了整体推理效率，但它们解决的是问题的不同方面：量化侧重于权重-激活操作，而FlashAttention则改进了执行过程，但需要高精度格式。最近的键值（KV）缓存量化减少了内存带宽，但仍需要在注意力运算中进行浮点解量化。  我们提出了TurboAttention，这是一种全面的方法，能够在量化执行注意力的同时解决内存和计算效率问题。我们的解决方案引入了两项关键技术：FlashQ，一种逐头注意力量化技术，可以同时压缩KV缓存并实现激活-激活乘法的量化执行；以及基于稀疏性的Softmax近似（SAS），它消除了注意力运算中指数运算阶段对解量化到FP32的需求。实验结果表明，TurboAttention在注意力运算中实现了1.2-1.8倍的速度提升，将KV缓存大小减少了4.4倍以上，并且相比FP16基线能够实现最大吞吐量提高2.37倍，同时在各种数据集和模型上超越了最先进的量化和压缩技术。|
|**2024-12-10**|**Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences**|Alan Nawzad Amin et.al.|[2412.07763](http://arxiv.org/abs/2412.07763)|**[link](https://github.com/alannawzadamin/clonebo)**|**为了开发有效的治疗性药物，生物学家通过迭代突变抗体序列来提高其结合能力和稳定性。这些突变可以基于先前的测量结果或从大型抗体数据库中学习以预测典型的抗体。然而，典型的抗体空间庞大，难以搜索，并且在预算有限的情况下，实验往往无法找到合适的抗体。我们引入了一种名为Clone-informed Bayesian Optimization（CloneBO）的方法，这是一种贝叶斯优化程序，能够通过教导一个生成模型了解我们的免疫系统如何优化抗体来高效地在实验室中优化抗体。我们的免疫系统通过迭代进化抗体序列的特定部分，使其能强烈而稳定地与目标结合，从而形成一组相关且不断进化的序列，称为克隆家族。我们训练了一个大型语言模型CloneLM，使用数十万个克隆家族的数据，并利用它设计具有突变的序列，这些突变最有可能在人体免疫系统中优化抗体。我们建议通过扭曲的顺序蒙特卡洛过程引导设计以适应先前的测量结果。实验证明，CloneBO在现实的计算机模拟实验中比以前的方法更有效地优化了抗体，并在实际湿实验室的实验中设计出了更强、更稳定的结合物。**|
|**2024-12-10**|**Zero-Shot ATC Coding with Large Language Models for Clinical Assessments**|Zijian Chen et.al.|[2412.07743](http://arxiv.org/abs/2412.07743)|null|手动分配解剖学治疗化学（ATC）代码到处方记录在安大略卫生和InterRAI加拿大是医疗保健研究和运营的重要瓶颈，需要大量的专家时间和精力。为了在保持数据隐私的同时自动化这一过程，我们开发了一种实用的方法，使用可在本地部署的大型语言模型（LLM）。受最近在自动国际疾病分类（ICD）编码方面进展的启发，我们的方法将ATC编码视为分层信息提取任务，引导LLM逐级通过ATC本体。我们使用GPT-4作为准确性的上限进行评估，并专注于开发适用于隐私敏感部署的开源Llama模型。我们在加拿大卫生部药品产品数据、RABBITS基准和来自安大略卫生的真实临床笔记上进行了测试，结果表明我们的方法与GPT-4相比实现了78%的精确匹配准确性，与Llama 3.1 70B相比实现了60%的精确匹配准确性。我们还研究了通过药物定义进行知识接地，发现这带来了适度的准确性提升。此外，我们证明微调后的Llama 3.1 8B达到了与零样本Llama 3.1 70B相当的准确性，这表明使用较小的模型实现有效的ATC编码是可行的。我们的结果展示了在隐私敏感的医疗环境中自动ATC编码的可行性，为未来的部署奠定了基础。|
|**2024-12-10**|**Granite Guardian**|Inkit Padhi et.al.|[2412.07724](http://arxiv.org/abs/2412.07724)|**[link](https://github.com/ibm-granite/granite-guardian)**|**我们介绍了Granite Guardian模型，这是一系列旨在提供提示和响应风险检测的安全保障措施，使它们能够与任何大型语言模型（LLM）安全且负责任地结合使用。这些模型在多个风险维度上提供了全面的覆盖，包括社会偏见、粗俗语言、暴力、性内容、不道德行为、越狱以及针对检索增强生成（RAG）的相关风险，如上下文相关性、真实性和答案相关性。这些模型经过独特的数据集训练，该数据集结合了来自不同来源的人类注释和合成数据，解决了传统风险检测模型通常忽略的风险，例如越狱和RAG特定问题。在有害内容和RAG相关幻觉风险基准测试中，Granite Guardian的AUC得分分别为0.871和0.854，使其成为该领域最具通用性和竞争力的模型。作为开源项目，Granite Guardian旨在促进社区中的负责任AI开发。**|
|**2024-12-10**|**DriveMM: All-in-One Large Multimodal Model for Autonomous Driving**|Zhijian Huang et.al.|[2412.07689](http://arxiv.org/abs/2412.07689)|**[link](https://github.com/zhijian11/DriveMM)**|**大型多模态模型（LMMs）在自动驾驶（AD）中通过整合大规模语言模型展示了出色的理解和解释能力。尽管如此，目前的数据驱动自动驾驶方法往往集中在单一数据集和特定任务上，忽视了它们的整体能力和泛化能力。为了解决这些差距，我们提出了DriveMM，这是一种通用的大型多模态模型，旨在处理各种数据输入，如图像和多视角视频，同时执行广泛的自动驾驶任务，包括感知、预测和规划。最初，该模型经过课程预训练以处理不同的视觉信号，并执行基本的视觉理解和感知任务。随后，我们对各种与自动驾驶相关的数据集进行增强和标准化，以微调模型，从而生成一个综合性的LMM用于自动驾驶。为了评估其整体能力和泛化能力，我们在六个公开基准上进行了评估，并在未见过的数据集上进行了零样本迁移，在所有任务中DriveMM均达到了最先进的性能。我们希望DriveMM成为未来端到端自动驾驶应用中的一种有前景的解决方案。**|
|**2024-12-10**|**Privacy-Preserving Customer Support: A Framework for Secure and Scalable Interactions**|Anant Prakash Awasthi et.al.|[2412.07687](http://arxiv.org/abs/2412.07687)|null|随着人工智能（AI）在客户支持中的应用日益广泛，运营效率和用户体验得到了显著提升。然而，传统的机器学习（ML）方法需要在敏感数据集上进行大量的本地训练，这带来了重大的隐私风险和合规性挑战，如《通用数据保护条例》（GDPR）和《加州消费者隐私法案》（CCPA）。现有的隐私保护技术，如匿名化、差分隐私和联邦学习，在一定程度上解决了这些问题，但它们仍然面临实用性、可扩展性和复杂性的限制。本文介绍了一种名为隐私保护零样本学习（PP-ZSL）的框架，这是一种新颖的方法，利用大型语言模型（LLM）以零样本学习模式运行。与传统机器学习方法不同，PP-ZSL通过使用预训练的LLM直接生成响应，从而消除了对敏感数据进行本地训练的需求。该框架结合了实时数据匿名化以删除或屏蔽敏感信息，检索增强生成（RAG）以解决特定领域的查询，并且采用了强大的后处理技术以确保符合监管标准。这种组合减少了隐私风险，简化了合规性要求，并增强了可扩展性和运营效率。实证分析表明，PP-ZSL框架能够提供准确且符合隐私保护的响应，同时显著降低了部署AI驱动客户支持系统的成本和复杂性。研究还强调了该框架在金融、医疗、电子商务、法律支持、电信和政府服务等多个行业的潜在应用。通过应对隐私和性能的双重挑战，该框架为安全、高效且符合监管要求的AI应用奠定了基础，特别是在客户互动领域。|
|**2024-12-10**|**TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation**|Alfredo Garrachón Ruiz et.al.|[2412.07682](http://arxiv.org/abs/2412.07682)|null|大语言模型（LLMs）的推理成本是一个显著的挑战，特别是对于需要长输出的任务。然而，自然语言通常包含冗余，这为优化提供了机会。我们观察到，当适当提示时，LLMs可以生成简洁的输出，保留了关键信息。我们提出了一种节省计算成本的框架，在该框架中，由LLM生成的较短的精简输出通过一个计算成本较低的小型模型重构为完整的叙述。我们的实验结果表明，这种方法在一般知识领域尤其有效，平均节省了20.58%的令牌，并且评估指标略有下降，表明这种方法能够在语言处理任务中有效地平衡效率和准确性。|
|**2024-12-10**|**Ask Humans or AI? Exploring Their Roles in Visualization Troubleshooting**|Shuyu Shen et.al.|[2412.07673](http://arxiv.org/abs/2412.07673)|**[link](https://github.com/HKUSTDial/vistroubleshooting.github.io)**|可视化创作是一个迭代过程，需要用户调整诸如配色方案和数据转换等参数以实现所需的美学效果并有效传达见解。由于这些调整的复杂性，用户经常会创建有缺陷的可视化图表，并需要故障排除支持。在本文中，我们研究了两种主要的可视化故障排除方法：（1）通过论坛进行的人工辅助支持，用户可以从其他人那里获得建议；（2）使用大型语言模型（LLMs）进行的AI辅助支持。我们的目标是了解每种方法在支持可视化故障排除任务方面的优势和局限性。为此，我们从Stack Overflow收集了889个Vega-Lite案例。然后进行了全面分析，以了解用户提出的问题类型、人工和AI指导的有效性以及补充资源（如文档和示例）对故障排除结果的影响。我们的研究结果揭示了人工辅助和AI辅助故障排除之间的显著差异：人工辅助故障排除提供了定制的、上下文敏感的建议，但回复质量往往不一致；而AI辅助故障排除则提供了快速反馈，但通常需要额外的上下文资源才能达到理想的结果。|
|**2024-12-10**|**FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks**|Bocheng Chen et.al.|[2412.07672](http://arxiv.org/abs/2412.07672)|null|在大型语言模型（LLMs）中进行防御对于抵御利用这些系统生成有害内容的众多攻击者至关重要，这些攻击者通过操纵提示来进行越狱攻击。尽管已经提出了许多防御策略，但它们通常需要访问模型的内部结构或需要额外的训练，这对于使用LLM API（如OpenAI API或Claude API）的服务提供商来说是不切实际的。在本文中，我们提出了一种移动目标防御方法，该方法通过调整解码超参数来增强模型对各种越狱攻击的鲁棒性。我们的方法不需要访问模型的内部结构，也不会产生额外的训练成本。所提出的防御包括两个关键组成部分：（1）通过识别和调整影响标记生成概率的解码超参数来优化解码策略；（2）将解码超参数和模型系统提示转换为动态目标，在每次运行时不断改变。通过持续修改解码策略和提示，这种防御有效地缓解了现有的攻击。我们的结果显示，当使用LLM作为黑盒API时，我们的防御在测试的三种模型中对越狱攻击最为有效。此外，我们的防御具有较低的推理成本，并且保持了相当的响应质量，这使得它可以在与其他防御方法结合使用时成为潜在的保护层。|
|**2024-12-10**|**Automating Business Intelligence Requirements with Generative AI and Semantic Search**|Nimrod Busany et.al.|[2412.07668](http://arxiv.org/abs/2412.07668)|null|在不断变化的商业环境中，业务智能（BI）系统的功能需求获取仍然是一个重大挑战。本文介绍了一个名为AutoBIR的新型AI驱动系统，该系统利用语义搜索和大型语言模型（LLM）来自动化和加速BI需求的规格定义。该系统通过对话界面与利益相关者进行直观交互，将用户输入转化为原型分析代码、描述和数据依赖关系。此外，AutoBIR生成详细的测试用例报告，并可选择性地附带视觉辅助，从而简化需求获取过程。通过纳入用户反馈，系统优化了BI报告和系统设计，展示了加快数据驱动决策的实际应用。本文探讨了生成式AI在转变BI开发中的更广泛应用潜力，说明了其在增强大规模、不断发展的系统数据工程实践中的作用。|
|**2024-12-10**|**Searching for Structure: Investigating Emergent Communication with Large Language Models**|Tom Kouwenhoven et.al.|[2412.07646](http://arxiv.org/abs/2412.07646)|null|人类语言通过反复的语言学习和使用演化成有结构的形式。这些过程引入了在语言习得过程中运作的偏见，使语言系统朝着交流效率的方向发展。在这篇论文中，我们研究了是否大型语言模型（LLMs）的隐性偏见也会优化人工语言使其具有类似的结构特性。为此，我们模拟了一个经典的指代游戏，在这个游戏中，LLM代理学习并使用人工语言。我们的结果显示，最初无结构的整体语言确实会发展出一些结构特性，使得两个LLM代理能够成功地进行交流。与人类实验中的观察结果相似，代际传递提高了语言的可学习性，但也可能导致非人类语言的退化词汇。综合来看，这项工作扩展了实验发现，表明LLMs可以作为工具用于模拟语言演化的实验，并为该领域的未来人机实验开辟了可能性。|
|**2024-12-09**|**Training Large Language Models to Reason in a Continuous Latent Space**|Shibo Hao et.al.|[2412.06769](http://arxiv.org/abs/2412.06769)|**[link](https://github.com/facebookresearch/coconut)**|大型语言模型（LLMs）通常在“语言空间”中进行推理，通过链式思维（CoT）来解决复杂的推理问题。然而，我们认为语言空间并不总是最优的推理方式。例如，大多数词汇标记主要用于文本连贯性，而不是推理所必需的，而一些关键标记需要复杂的规划，并对LLMs构成巨大挑战。为了探索LLMs在不受限的潜在空间中进行推理的可能性，我们引入了一种新的范式——椰子（Coconut，连续思维链）。我们利用LLM的最后一个隐藏状态作为推理状态的表示（称为“连续思维”）。与将其解码为词元不同，我们将其直接作为后续输入嵌入反馈给LLM，从而在连续空间中进行操作。实验表明，椰子方法在多个推理任务上能够有效增强LLM的表现。这一新颖的潜在推理范式导致了高级推理模式的出现：连续思维可以编码多个备选的下一步推理步骤，使模型能够执行广度优先搜索（BFS）来解决问题，而不是像CoT那样过早地承诺到一个单一的确定路径。在某些需要大量回溯规划的逻辑推理任务中，椰子方法在推理过程中使用的思考标记更少，表现优于CoT。这些发现展示了潜在推理的前景，并为未来的研究提供了宝贵的见解。|
|**2024-12-09**|**Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating Usage and Reliance on ChatGPT-Generated Code**|Joy Krishan Das et.al.|[2412.06757](http://arxiv.org/abs/2412.06757)|null|大型语言模型（如ChatGPT）在辅助开发人员进行编码和调试任务方面已显示出潜力，但它们在协作问题解决中的作用尚未得到充分探索。在这项研究中，我们分析了GitHub中的1,152个ChatGPT对话，涉及1,012个问题，以探讨ChatGPT的多样化使用及其生成代码的依赖程度。我们的贡献有四点。首先，我们手动分析了289个对话，以了解ChatGPT在GitHub问题中的使用情况。我们的分析表明，ChatGPT主要用于构思，而用于验证（例如代码文档准确性）的情况很少。其次，我们应用BERTopic建模来识别整个数据集中的关键参与领域。我们发现后端问题（例如API管理）主导了对话，而测试问题则出乎意料地较少涉及。第三，我们利用CPD克隆检测工具检查ChatGPT生成的代码是否用于解决问题。我们的研究结果表明，只有5.83%的问题通过直接采用ChatGPT生成的代码得到解决。第四，我们使用基于RoBERTa的情感分析模型来估计开发者对不同用途和参与领域的满意度。我们发现，对于重构和处理数据分析问题（例如分类表格数据），开发者对使用ChatGPT具有积极情感（即高满意度）。相反，当使用ChatGPT进行调试或处理自动化任务（例如GUI交互）时，我们观察到负面情感。我们的研究结果揭示了未满足的需求和日益增长的开发者不满情绪。研究人员和ChatGPT开发者应专注于开发特定任务的解决方案，以帮助解决各种问题，提高用户满意度和软件开发中的问题解决效率。|
|**2024-12-09**|**Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models**|Neel Jain et.al.|[2412.06748](http://arxiv.org/abs/2412.06748)|null|构建安全可靠的语言模型的一个关键组成部分是使模型能够适当地拒绝某些指令或回答某些问题。我们可能希望模型对各种类别的用户查询输出拒绝消息，例如，有问题表述不当的问题、指令涉及违法行为的请求，或者需要超出模型知识范围的信息的问题。由于个人可能希望他们的模型在拒绝各种类别查询时表现出不同程度的敏感性，并且不同的用户可能希望有不同的拒绝率，因此工程化这种拒绝行为变得复杂。目前默认的方法涉及训练多个具有不同拒绝消息比例的模型来实现所需的拒绝率，这在计算上非常昂贵，并且可能需要针对每个用户的所需偏好重新训练一个新模型。为了解决这些挑战，我们提出了拒绝标记，每个拒绝类别有一个这样的标记，或者一个通用的拒绝标记，在训练期间将其添加到模型的响应前。然后，我们展示了如何在推理过程中增加或减少生成每个类别拒绝标记的概率，以引导模型的拒绝行为。拒绝标记使得通过选择性干预生成过程就能控制单个模型的拒绝率，而无需进行任何进一步的微调。|
|**2024-12-09**|**JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset Generation with LLM**|Takuro Fujii et.al.|[2412.06738](http://arxiv.org/abs/2412.06738)|**[link](https://github.com/retrieva/japagen)**|近期的一些研究强调了大型语言模型（LLMs）作为有效的监督训练数据生成器的潜力，提供了增强推理效率和减少与数据收集相关的成本等优势。然而，这些研究主要集中在英语任务上。在本文中，我们探讨了一个基本的研究问题：大型语言模型能否成为其他语言任务的有效训练数据生成器？具体来说，我们在六个不同的日语下游任务中利用LLMs在少样本和零样本学习场景下合成监督训练数据。随后，我们使用这些合成的数据来训练紧凑型模型（例如，BERT）。这一新颖的方法被称为JAPAGEN。我们的实验结果表明，JAPAGEN在需要正式文本输入的分类任务中表现稳健，并且其性能与传统的LLM提示策略相比具有竞争力。|
|**2024-12-09**|**AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark**|Lan Li et.al.|[2412.06724](http://arxiv.org/abs/2412.06724)|**[link](https://github.com/LanLi2017/LLM4DC)**|我们研究了大型语言模型（LLMs）在自动生成数据清洗工作流方面的推理能力。为了评估LLMs完成数据清洗任务的能力，我们实现了一个基于LLM的自动数据清洗工作流（AutoDCWorkflow）管道，通过提示LLMs进行数据清洗操作来修复三种类型的数据质量问题：重复项、缺失值和不一致的数据格式。给定一个脏表和一个目的（用查询表达），该管道生成一个足够满足目的的最小化清洗表以及用于生成该表的数据清洗工作流。规划过程涉及三个主要的LLM驱动组件：（1）选择目标列：识别与目的相关的列集。（2）检查列质量：评估每项目标列的数据质量并生成数据质量报告作为操作目标。（3）生成操作及参数：根据数据质量报告的结果预测下一个操作及其参数。此外，我们提出了一套数据清洗基准来评估LLM代理自动生成满足不同难度级别数据清洗目的的工作流的能力。该基准包括注释的数据集，即一组目的、原始表、清洗表、数据清洗工作流和答案集。在我们的实验中，我们评估了三种能够自动生成目的驱动的数据清洗工作流的LLMs。结果表明，LLMs在无需微调的情况下就能够很好地进行规划和生成数据清洗工作流。|
|**2024-12-09**|**OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions**|Yi-Kai Zhang et.al.|[2412.06693](http://arxiv.org/abs/2412.06693)|null|大型语言模型（LLMs）的快速发展显著扩展了它们的应用范围，包括多语种支持、领域特定任务和多模态集成。本文介绍了OmniEvalKit，这是一个新颖的基准测试工具箱，旨在评估LLMs及其全方面扩展在多语种、多领域和多模态能力方面的表现。与现有基准测试通常只关注单一方面不同，OmniEvalKit提供了一个模块化、轻量级且自动化的评估系统。该系统采用模块化架构，包括静态构建器和动态数据流，促进了新模型和数据集的无缝集成。OmniEvalKit支持超过100个LLMs和50个评估数据集，涵盖了数千种模型-数据集组合的全面评估。OmniEvalKit致力于创建一个超轻量且快速部署的评估框架，使下游应用对AI社区更加便捷和多样化。|
|**2024-12-09**|**Exploring Critical Testing Scenarios for Decision-Making Policies: An LLM Approach**|Weichao Xu et.al.|[2412.06684](http://arxiv.org/abs/2412.06684)|null|近年来，决策策略在自动驾驶和机器人技术等多个领域取得了令人瞩目的成就。由于存在可能威胁其可靠性的关键场景，对这些决策策略进行测试至关重要。许多研究工作致力于测试这些策略，但仍面临重大挑战，例如由于政策和测试环境的复杂性导致的低测试效率和多样性。受大型语言模型（LLMs）卓越能力的启发，本文提出了一种基于LLM的在线测试框架，以高效测试决策策略。主要思想是使用基于LLM的测试场景生成器通过思考和推理智能地生成具有挑战性的测试案例。具体来说，我们首先设计了一个“生成-测试-反馈”管道，并应用模板提示工程来充分利用LLM的知识和推理能力。然后，我们引入了多尺度场景生成策略，以解决LLM在微调方面固有的挑战，进一步提高测试效率。最后，我们在五个广泛使用的基准上评估了基于LLM的方法。实验结果表明，我们的方法在发现关键和多样化的场景方面显著优于基线方法。|
|**2024-12-09**|**Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework**|Tianming Liu et.al.|[2412.06681](http://arxiv.org/abs/2412.06681)|null|在交通系统需求建模和仿真中，基于代理的模型和微观仿真方法是当前最先进的方法。然而，现有的基于代理的模型在行为真实性和资源需求方面仍存在一些局限性，限制了它们的应用。本研究利用新兴的大语言模型（LLM）和基于大语言模型的代理，提出了一种通用的大语言模型代理为基础的交通系统建模框架。我们认为，LLM代理不仅具备作为代理的基本能力，还提供了克服现有基于代理的模型的一些局限性的有前景的解决方案。我们的概念框架设计紧密复制了交通网络中人类旅行者在决策和互动过程中的特征和行为，并通过相关研究和瓶颈场景下LLM代理的学习和调整的示范示例证明，所提出的系统可以满足决策和学习行为的关键行为标准。尽管需要进一步完善基于大语言模型的代理建模框架，但我们认为这种方法有可能改善交通系统的建模和仿真。|
|**2024-12-09**|**I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token**|Roi Cohen et.al.|[2412.06676](http://arxiv.org/abs/2412.06676)|null|大型语言模型因其能够捕捉现实世界的知识而在许多下游任务中表现出色。尽管取得了近期进展，这些模型仍然容易出现所谓的幻觉现象，导致生成不希望的和事实错误的文本。在这项工作中，我们提出了一种新颖的校准方法来对抗幻觉现象。我们在模型的词汇表中添加了一个特殊的[IDK]（“我不知道”）标记，并引入一个目标函数，将概率质量转移到[IDK]标记上以应对不正确的预测。这种方法使模型能够显式地表达其输出的不确定性。我们在多种模型架构和事实性下游任务中评估了我们提出的方法。我们发现，使用我们方法训练的模型能够在以前容易出错的地方表达不确定性，同时仅遭受很小的知识编码损失。我们还对方法的多个变体进行了广泛的消融研究，并详细分析了该方法的精确度-召回率权衡。|
|**2024-12-09**|**ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance**|Chunwei Wang et.al.|[2412.06673](http://arxiv.org/abs/2412.06673)|null|在本文中，我们介绍了ILLUME，这是一种统一的多模态大型语言模型（MLLM），它通过统一的下一个令牌预测公式将多模态理解和生成能力无缝集成到一个大型语言模型中。为了应对图像-文本对齐通常需要的大数据集规模，我们提出通过设计一种包含语义信息的视觉标记器和逐步多阶段训练程序来增强数据效率。这种方法将预训练的数据集规模减少到仅1500万，这比通常所需的数据集小四倍以上，同时在各种现有统一MLLM（如Janus）中实现了具有竞争力甚至更优的表现。此外，为了促进理解和生成能力之间的协同增强，这是之前工作中研究不足的部分，我们引入了一种新颖的自我增强多模态对齐方案。该方案监督MLLM评估文本描述和自动生成图像之间的一致性，从而帮助模型更准确地解释图像，并避免由于图像生成中的不一致导致的不现实和错误预测。基于广泛的实验，我们提出的ILLUME在各种基准测试中脱颖而出，与最先进的统一MLLM和专业模型竞争，在多模态理解、生成和编辑方面表现优异。|
|**2024-12-06**|**Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling**|Zhe Chen et.al.|[2412.05271](http://arxiv.org/abs/2412.05271)|**[link](https://github.com/opengvlab/internvl)**|我们介绍了InternVL 2.5，这是一个先进的多模态大型语言模型（MLLM）系列，建立在InternVL 2.0的基础上，保持了其核心模型架构的同时，在训练和测试策略以及数据质量方面引入了显著的改进。在这项工作中，我们深入探讨了模型扩展与性能之间的关系，系统地探索了视觉编码器、语言模型、数据集大小和测试时配置的性能趋势。通过广泛的基准评估，包括跨学科推理、文档理解、多图像/视频理解、现实世界理解、多模态幻觉检测、视觉定位、多语言能力以及纯语言处理，InternVL 2.5展示了具有竞争力的性能，可与领先的商业模型如GPT-4o和Claude-3.5-Sonnet相媲美。值得注意的是，我们的模型是第一个超过70%的开放源代码MLLM，在MMMU基准上取得了3.7分的提升，通过链式思考（CoT）推理展示了强大的测试时扩展潜力。我们希望通过这个模型为开源社区做出贡献，为开发和应用多模态AI系统设定新的标准。HuggingFace演示请参见https://huggingface.co/spaces/OpenGVLab/InternVL|
|**2024-12-06**|**APOLLO: SGD-like Memory, AdamW-level Performance**|Hanqing Zhu et.al.|[2412.05270](http://arxiv.org/abs/2412.05270)|**[link](https://github.com/zhuhanqing/APOLLO)**|大型语言模型（LLMs）在训练过程中对内存的需求非常大，特别是在使用流行的AdamW优化器时。这种内存负担迫使研究人员使用更昂贵的GPU或减小批量大小，从而限制了训练的可扩展性和吞吐量。为了解决这个问题，已经提出了各种节省内存的优化器来减少优化器的内存使用。然而，这些优化器面临一些关键挑战：(i) 对昂贵的SVD操作的依赖；(ii) 相对于AdamW显著的性能折衷；以及(iii) 为了保持竞争力仍然需要相当大的优化器内存开销。  在这项工作中，我们发现AdamW的学习率适应规则可以有效地被粗略化为一种结构化的学习率更新。基于这一见解，我们提出了一种名为APOLLO（Approximated Gradient Scaling for Memory-Efficient LLM Optimization，用于高效LLM优化的近似梯度缩放）的方法，该方法通过基于纯随机投影的辅助低秩优化状态来近似学习率缩放。这种结构化的学习率更新规则使得APOLLO能够高度容忍进一步的内存减少，同时提供与预训练性能相当的结果。甚至其秩一变体APOLLO-Mini，在SGD级别的内存成本下也比AdamW具有更好的预训练性能。  广泛的实验表明，APOLLO系列的表现与AdamW相当或更好，同时通过几乎消除AdamW的优化状态实现了更大的内存节省。这些节省带来了显著的系统级优势：(1) 提高吞吐量：在8个A100-80GB GPU设置下的吞吐量是AdamW的3倍，支持4倍大的批量大小。(2) 改进的模型可扩展性：在没有系统级优化的情况下使用DDP在A100-80GB GPU上进行LLaMA-13B的预训练。(3) 适合低端GPU的预训练：使用少于12GB内存对LLaMA-7B进行单GPU预训练，并采用权重量化技术。|
|**2024-12-06**|**CompCap: Improving Multimodal Large Language Models with Composite Captions**|Xiaohui Chen et.al.|[2412.05243](http://arxiv.org/abs/2412.05243)|null|多模态大型语言模型（MLLMs）对复合图像的理解能力如何？复合图像是通过合并多个视觉元素（如图表、海报或屏幕截图）合成而成的虚拟视觉，而不是直接由相机捕捉到的自然图像（NI）。尽管复合图像在现实应用中非常普遍，但最近MLLM的发展主要集中在解释自然图像上。我们的研究表明，当前的MLLM在准确理解复合图像方面面临重大挑战，通常难以从这些图像中提取信息或执行复杂的推理。我们发现现有的复合图像训练数据大多格式化为问答任务（例如，在ChartQA和ScienceQA等数据集中），而高质量的图像-描述数据集，对于稳健的视觉-语言对齐至关重要，仅存在于自然图像中。为了弥合这一差距，我们引入了Composite Captions (CompCap)，这是一个灵活的框架，利用大型语言模型（LLMs）和自动化工具来合成具有准确且详细描述的复合图像。使用CompCap，我们整理了CompCap-118K，一个包含六种复合图像类型的118K个图像-描述对的数据集。我们通过监督微调三种规模的MLLM（xGen-MM-inst.-4B和LLaVA-NeXT-Vicuna-7B/13B）来验证CompCap-118K的有效性。实证结果表明，CompCap-118K显著提升了MLLMs对复合图像的理解能力，在11个基准测试中的平均提升分别为1.7%、2.0%和2.9%。|
|**2024-12-06**|**MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale**|Jarvis Guo et.al.|[2412.05237](http://arxiv.org/abs/2412.05237)|null|开源的多模态大型语言模型（MLLMs）在广泛的多模态任务中展现出了显著的潜力。然而，它们的推理能力仍然受到现有指令微调数据集的限制，这些数据集主要是从学术数据集如VQA、AI2D和ChartQA等重新利用而来的。这些数据集针对的任务相对简单，并且仅提供短语级别的答案，没有中间推理过程。为了解决这些问题，我们介绍了一种可扩展且成本效益高的方法来构建大规模的多模态指令微调数据集，该数据集包含了丰富的中间推理过程，旨在激发链式思考（CoT）推理。使用仅开放模型，我们创建了一个包含1200万个指令-响应对的数据集，以涵盖各种需要深度推理的任务，并提供详细且忠实的推理过程。实验表明，在这个数据集上训练MLLM显著提升了其推理能力，在MathVerse（+8.1%）、MMMU-Pro（+7%）和MuirBench（+13.3%）等基准测试中达到了最先进的性能。此外，该模型在非推理基准测试中也显示出高达4%的改进。消融研究进一步强调了数据集构建过程中关键组件（如重写和自过滤）的重要性。|
|**2024-12-06**|**BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits**|Wazib Ansar et.al.|[2412.05225](http://arxiv.org/abs/2412.05225)|null|大型语言模型（LLMs）基于变压器架构，在各种应用中取得了最先进的成果。然而，由于其庞大的规模和处理需求，将其部署在资源受限的设备上变得极其困难。在各种效率考虑因素中，模型二值化和早期退出（EE）是常见的有效解决方案。然而，二值化可能会导致性能损失，因为精度降低影响了梯度估计和参数更新。此外，现有的早期退出机制仍处于研究的初级阶段。为了缓解这些问题，我们提出了Binarized Early Exit Transformer（BEExformer），这是首个选择性学习变压器架构，结合了早期退出和二值化以进行文本推理。通过一种可微分的二阶近似到脉冲函数，它改进了二值化过程。这使得可以计算关于权重符号及其幅度的梯度。与基于绝对阈值的EE不同，所提出的EE机制依赖于中间变压器块之间的熵分数减少，并采用软路由损失估计。二值化使模型大小减少了18.44倍，而早期退出则将推理过程中的浮点运算次数（FLOPs）减少了54.85%，甚至通过解决深度网络固有的“过度思考”问题提高了准确率5.98%。此外，所提出的BEExformer简化了训练过程，无需从全精度LLM中进行知识蒸馏。在GLUE数据集上的广泛评估以及与SOTA工作的比较展示了其在性能与效率之间的帕累托最优权衡。|
|**2024-12-06**|**100% Hallucination Elimination Using Acurai**|Michael C. Wood et.al.|[2412.05223](http://arxiv.org/abs/2412.05223)|**[link](https://github.com/AcuChat/acurai-RAGTruth-conflict-resolution)**|在大型语言模型（LLMs）中，幻觉问题仍然是AI在企业和其他高风险应用中普及的关键障碍。尽管检索增强生成（RAG）系统取得了进展，但目前最先进的方法即使在提供相关且准确的上下文时，也只能达到80%的准确性，无法生成忠实且事实正确的输出。在这项工作中，我们介绍了Acurai，这是一种新颖的系统性方法，通过在输入前重新格式化查询和上下文数据，实现了LLMs的100%无幻觉响应。利用对LLM内部表示的深刻理解、名词短语主导性的关键作用以及离散功能单元（DFUs）的角色，Acurai确保输入上下文与生成输出之间的对齐。我们使用RAGTruth语料库验证了这种方法，证明其能够消除GPT-4和GPT-3.5 Turbo的所有幻觉。Acurai为实现一致、准确和忠实的AI响应设定了新标准，标志着在开发可信的AI系统方面迈出了重要一步。|
|**2024-12-06**|**Evaluating and Aligning CodeLLMs on Human Preference**|Jian Yang et.al.|[2412.05210](http://arxiv.org/abs/2412.05210)|null|代码大型语言模型（codeLLMs）在代码生成方面取得了显著进展。大多数先前的代码相关基准测试由各种编程练习及其相应的测试用例组成，作为衡量code LLMs性能和能力的通用标准。然而，当前的code LLMs主要集中在合成正确的代码片段上，忽略了与人类偏好的对齐，其中查询应从实际应用场景中抽取，模型生成的响应应满足人类偏好。为了弥合模型生成响应与人类偏好的差距，我们提出了一项严格的由人工精心策划的基准测试CodeArena，以模拟现实世界编码任务的复杂性和多样性，在40个类别和44种编程语言中精心挑选了397个高质量样本。此外，我们通过从网站扩展指令提出了一个多样化的合成指令语料库SynCode-Instruct（近20B令牌），以验证大规模合成指令微调的有效性。完全训练于合成指令数据上的Qwen2.5-SynCoder能够达到顶级开源code LLMs的性能。研究结果发现执行基准测试和CodeArena之间的性能差异。我们在40多个LLMs上进行的系统实验揭示了开源SOTA code LLMs（如Qwen2.5-Coder）和专有LLMs（如OpenAI o1）之间显著的性能差距，强调了人类偏好对齐的重要性。\footnote{\url{https://codearenaeval.github.io/ }}|
|**2024-12-06**|**A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges**|Aditi Singh et.al.|[2412.05208](http://arxiv.org/abs/2412.05208)|null|文本到SQL系统通过将自然语言查询转换为结构化查询语言（SQL），在数据库之间架起了一座桥梁，使非技术用户能够轻松地与复杂的数据库管理系统进行交互。本文综述了人工智能驱动的文本到SQL系统的演进过程，强调了其基础组件、大型语言模型（LLM）架构的进步，以及Spider、WikiSQL和CoSQL等数据集在推动进展中的关键作用。我们考察了文本到SQL在医疗保健、教育和金融等领域的应用，强调了它们在提高数据可访问性方面的变革潜力。此外，我们分析了持续存在的挑战，包括领域泛化、查询优化、支持多轮对话交互以及针对NoSQL数据库和动态现实世界场景的定制数据集有限等问题。为了应对这些挑战，我们概述了未来的研究方向，如扩展文本到SQL的能力以支持NoSQL数据库，设计适用于动态多轮交互的数据集，以及优化系统以实现现实世界的可扩展性和鲁棒性。通过综述当前的进展并识别关键差距，本文旨在指导下一代基于LLM的文本到SQL系统的研究和应用。|
|**2024-12-06**|**Are Frontier Large Language Models Suitable for Q&A in Science Centres?**|Jacob Watson et.al.|[2412.05200](http://arxiv.org/abs/2412.05200)|null|本文研究了前沿的大语言模型（LLMs）在科学中心问答互动中的适用性，旨在提高访客参与度的同时保持事实准确性。我们使用从英国莱斯特国家航天中心收集的问题数据集，评估了三种领先模型的表现：OpenAI的GPT-4、Anthropic的Claude 3.5 Sonnet和Google的Gemini 1.5。每个模型都被要求生成标准和创意的回答，这些回答针对8岁左右的观众。专家们根据准确度、参与度、清晰度、新颖性和偏离预期答案的程度对这些回答进行了评估。结果揭示了创意与准确性之间的权衡，Claude在保持清晰度和吸引年轻观众方面表现优于GPT和Gemini，即使是在要求生成更具创意的回答时也是如此。尽管如此，专家们观察到，更高的新颖性通常与所有模型的事实可靠性降低有关。本研究强调了LLMs在教育环境中的潜力，并强调了精心设计提示的重要性，以平衡参与度与科学严谨性。|
|**2024-12-06**|**SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot**|Jinlin Wu et.al.|[2412.05187](http://arxiv.org/abs/2412.05187)|**[link](https://github.com/franciszchen/surgbox)**|**外科手术，特别是在神经外科领域，代表了复杂且高风险的场景，对医疗团队施加了巨大的认知负担。尽管有意识的教育和实践可以提升认知能力，但由于患者安全方面的考虑，外科培训机会仍然有限。为了解决这些外科培训和操作中的认知挑战，我们提出了SurgBox，这是一种由代理驱动的沙盒框架，旨在系统地提升外科医生在沉浸式外科模拟中的认知能力。具体来说，我们的SurgBox利用带有定制检索增强生成（RAG）的大规模语言模型（LLM），真实地复制各种外科角色，从而实现真实的培训环境进行刻意练习。特别是，我们设计了Surgery Copilot，一种AI驱动的助手，积极协调外科信息流并支持临床决策，从而减轻手术团队的认知负担。通过引入新颖的长短时记忆机制，我们的Surgery Copilot能够有效地平衡即时程序辅助与全面外科知识的应用。使用真实的神经外科手术记录进行的广泛实验验证了我们的SurgBox框架在提升外科认知能力和支持临床决策方面的有效性。通过提供一个集成的解决方案来应对认知挑战，我们的SurgBox框架推进了外科教育和实践，有可能改变手术结果和医疗质量。代码可在https://github.com/franciszchen/SurgBox获取。**|
|**2024-12-05**|**p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay**|Jun Zhang et.al.|[2412.04449](http://arxiv.org/abs/2412.04449)|**[link](https://github.com/mcg-nju/p-mod)**|**尽管多模态大型语言模型（MLLMs）在各种任务中表现出色，但其巨大的训练和推理成本阻碍了它们的发展。大部分计算开销来自于变压器解码器处理的大量视觉标记。本文提出了一种通过利用混合深度（MoD）机制来构建高效的MLLM的方法，其中每个变压器解码器层选择处理关键的视觉标记而跳过冗余的标记。然而，将MoD集成到MLLM中并非易事。为了应对训练和推理稳定性以及有限训练数据的挑战，我们对MoD模块进行了两个新颖的设计：tanh门控权重归一化（TanhNorm）和对称标记重加权（STRing）。此外，我们观察到视觉标记在较深的层中表现出更高的冗余性，因此设计了一种逐步比率衰减（PRD）策略，该策略逐层逐渐减少标记保留比率，并采用偏移余弦调度。这一关键设计充分释放了MoD的潜力，显著提高了我们模型的效率和性能。为了验证我们的方法的有效性，我们在两个基线模型上进行了广泛的实验，涉及14个基准测试。我们的模型p-MoD在推理过程中仅使用了55.6%的TFLOPs和53.8%的KV缓存存储，在训练过程中仅使用了77.7%的GPU小时数，同时与基线模型相比达到了相当或更好的性能。**|
|**2024-12-05**|**EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios**|Lu Qiu et.al.|[2412.04447](http://arxiv.org/abs/2412.04447)|null|多模态大型语言模型（MLLMs）的出现，利用了大型语言模型的力量，最近展示了其在多模态理解和推理方面的优越能力，预示着人工智能通用智能（AGI）的新时代。然而，实现AGI不仅需要理解与推理，还需要在各种场景中进行有效规划的能力，这涉及基于复杂环境做出合理决策以解决现实世界问题。尽管这种能力至关重要，但当前MLLMs在不同场景中的规划能力仍缺乏深入研究。本文介绍了一个名为EgoPlan-Bench2的严格且全面的基准测试，旨在评估MLLMs在广泛现实世界场景中的规划能力。EgoPlan-Bench2涵盖了涵盖4个主要领域的24个详细场景的日常任务，这些场景与人类日常生活紧密相关。该基准通过使用第一人称视角的半自动过程构建，并辅以人工验证。我们评估了21种竞争性的MLLMs，并对其局限性进行了深入分析，揭示它们在现实世界规划方面面临重大挑战。为了进一步提高当前MLLMs的规划能力，我们提出了一种无需额外训练的无训练方法，通过研究各种多模态提示在复杂规划中的有效性，采用多模态链式思维（CoT）提示。我们的方法使GPT-4V在EgoPlan-Bench2上的表现提高了10.24分。我们的工作不仅揭示了当前MLLMs在规划方面的局限性，还为未来在这个关键领域的发展提供了见解。数据和代码已发布在https://qiulu66.github.io/egoplanbench2/。|
|**2024-12-05**|**Moto: Latent Motion Token as the Bridging Language for Robot Manipulation**|Yi Chen et.al.|[2412.04445](http://arxiv.org/abs/2412.04445)|**[link](https://github.com/tencentarc/moto)**|近年来，大型语言模型在经过大规模语料库的预训练后，在各种自然语言处理任务中取得了显著的成功，并且仅需少量微调。这种成功为机器人领域带来了新的希望，因为机器人长期以来一直受到动作标记数据成本高昂的限制。我们提出的问题是：鉴于视频数据中包含丰富的交互相关知识，类似于生成式预训练的方法是否可以有效地应用于增强机器人的学习？关键挑战在于找到一种有效的表示方法来进行自回归预训练，从而有利于机器人操作任务。受人类通过观察动态环境来学习新技能的方式启发，我们认为有效的机器人学习应强调与运动相关的知识，这些知识与低级动作密切相关，并且是硬件无关的，这有助于将学到的动作转移到实际的机器人操作中。为此，我们引入了Moto，它通过一个潜在运动标记器将视频内容转换为潜在运动标记序列，以无监督方式从视频中学习一种“运动语言”。我们通过对运动标记进行自回归预训练来预训练Moto-GPT，使其能够捕捉多样的视觉运动知识。在预训练之后，Moto-GPT展示了生成语义可解释的运动标记、预测合理的运动轨迹以及通过输出可能性评估轨迹合理性的有前景能力。为了将学到的运动先验知识转移到真实的机器人操作中，我们实现了一种协同微调策略，无缝地将潜在运动标记预测和真实机器人控制联系起来。广泛的实验表明，经过微调的Moto-GPT在机器人操作基准测试中表现出卓越的鲁棒性和效率，突显了其在将视频数据中的知识转移到下游视觉操作任务中的有效性。|
|**2024-12-05**|**Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation**|Yuying Ge et.al.|[2412.04432](http://arxiv.org/abs/2412.04432)|**[link](https://github.com/tencentarc/divot)**|**近年来，人们对于在大型语言模型（LLMs）中统一图像理解和生成的兴趣显著增加。这一趋势促使我们探索将这种统一扩展到视频领域。核心挑战在于开发一种能够捕捉视频的空间特征和时间动态的多功能视频标记器，从而为LLMs提供表示，并且这些表示可以进一步解码为逼真的视频片段，以实现视频生成。在这项工作中，我们介绍了Divot，这是一种基于扩散过程的视频标记器，它利用扩散过程进行自监督的视频表示学习。我们认为，如果一个视频扩散模型能够在将视频标记器的特征作为条件时有效地去噪视频片段，那么该标记器就成功地捕获了强大的空间和时间信息。此外，视频扩散模型本身作为一个解标记器，从表示中解码视频。在此基础上，我们通过视频到文本的自回归和建模连续值Divot特征的高斯混合模型，提出了Divot-Vicuna。实验结果表明，当与预训练的LLM集成时，我们的基于扩散的视频标记器在各种视频理解和生成基准上实现了具有竞争力的性能。经过指令微调的Divot-Vicuna在视频叙事方面表现出色，能够生成交织的叙述和相应的视频。**|
|**2024-12-05**|**Grounding Descriptions in Images informs Zero-Shot Visual Recognition**|Shaunak Halbe et.al.|[2412.04429](http://arxiv.org/abs/2412.04429)|**[link](https://github.com/shaunak27/grain-clip)**|**视觉-语言模型（VLMs）如CLIP因其在开放词汇概念的零样本视觉识别方面的能力而备受青睐。这种方法通过选择与查询图像文本表示最相似的对象类别来实现。尽管在某些领域取得了成功，但这种方法在识别细粒度实体以及推广到未见过的概念方面仍然存在困难，这些概念没有被训练分布所捕捉。最近的研究试图通过在测试时集成类别描述来缓解这些问题，尽管只取得了适度的改进。我们认为这些有限的进展归因于图像和描述表示之间基本的不匹配，这源于CLIP的预训练结构。在本文中，我们提出了GRAIN，这是一种新的预训练策略，旨在同时在细粒度和粗粒度层面上对齐表示。我们的方法学习将文本描述共同定位在图像区域，并将广泛的标题与全局图像表示对齐。为了推动这种预训练，我们利用冻结的多模态大型语言模型（MLLMs）来生成大规模的合成标注。我们在11个不同的图像分类数据集上展示了我们模型相比当前最先进的方法在零样本性能上的提升。此外，我们引入了Products-2023，这是一个新整理的手动标记的数据集，包含新颖的概念，并展示了我们的模型在这种数据集上的概念识别能力。我们的模型在其他下游任务如检索上的显著改进进一步突显了我们方法所学表示的优越质量。代码可在https://github.com/shaunak27/grain-clip 获取。**|
|**2024-12-05**|**Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion**|Jiuhai Chen et.al.|[2412.04424](http://arxiv.org/abs/2412.04424)|**[link](https://github.com/jiuhaichen/florence-vl)**|**我们介绍了Florence-VL，这是一种新的多模态大型语言模型（MLLM）家族，其视觉特征由Florence-2生成，Florence-2是一种生成型视觉基础模型。与广泛使用的通过对比学习训练的CLIP风格的视觉变换器不同，Florence-2能够捕捉不同层次和方面的视觉特征，这些特征在适应各种下游任务时更加灵活。我们提出了一种新颖的特征融合架构和一种创新的训练方法，有效整合了Florence-2的视觉特征到预训练的语言模型中，例如Phi 3.5和LLaMA 3。特别是，我们提出了“深度-广度融合（DBFusion）”来融合从不同深度和多个提示下提取的视觉特征。我们的模型训练包括整个模型的端到端预训练，以及对投影层和语言模型的微调，使用的是精心设计的包含高质量图像标题和指令调整对的开源数据集组合。我们的定量分析和可视化显示，Florence-VL在视觉-语言对齐方面优于流行的视觉编码器，在这里深度和广度起到了重要作用。Florence-VL在各种多模态和视觉为中心的基准测试中，包括一般VQA、感知、幻觉、OCR、图表、知识密集型理解等方面，显著超越现有的最先进MLLM。为了促进未来的研究，我们的模型和完整的训练方法已开源。https://github.com/JiuhaiChen/Florence-VL**|
|**2024-12-05**|**Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation**|Xuying Li et.al.|[2412.04415](http://arxiv.org/abs/2412.04415)|null|人工智能代理，由大型语言模型（LLMs）提供支持，通过实现无缝、自然且上下文感知的通信，彻底改变了人机交互。尽管这些进步提供了巨大的效用，但也继承并放大了固有的安全风险，如偏见、公平性、幻觉、隐私泄露和透明度缺乏等问题。本文研究了一个关键漏洞：针对AI代理中的LLM核心的对抗性攻击。具体而言，我们测试了这样一个假设：一个看似简单的对抗性前缀，例如“忽略文档”，可以迫使LLMs生成危险或不希望的输出，从而绕过它们的上下文保护机制。通过实验，我们展示了高攻击成功率（ASR），揭示了现有LLM防御措施的脆弱性。这些发现强调了迫切需要制定稳健的多层次安全措施，以缓解LLM层面及其更广泛的代理架构中的漏洞。|
|**2024-12-05**|**Retrieval-Augmented Machine Translation with Unstructured Knowledge**|Jiaan Wang et.al.|[2412.04342](http://arxiv.org/abs/2412.04342)|**[link](https://github.com/krystalan/RAGtrans)**|**检索增强生成（RAG）通过引入额外信息来增强大型语言模型（LLMs）。在机器翻译（MT）领域，先前的工作通常从配对的机器翻译语料库或特定领域的知识图谱中检索上下文示例，以增强模型的翻译能力。然而，大量的世界知识被组织在非结构化文档中，并且不同语言之间的配对可能不完全一致。在这篇论文中，我们研究了使用非结构化文档进行检索增强的机器翻译。具体来说，我们构建了RAGtrans，这是第一个用于训练和评估LLMs检索增强翻译能力的基准。RAGtrans包含了通过GPT-4和人工翻译者收集的79,000个翻译样本。此外，还提供了来自不同语言的文档，以向这些样本提供知识。基于RAGtrans，我们进一步提出了一种多任务训练方法，教导LLMs如何利用多语言文档中的信息进行翻译。该方法使用现有的多语言语料库创建辅助训练目标，而无需额外的标注要求。广泛的实验表明，该方法使LLMs的BLEU评分提高了1.58至3.09分，COMET评分提高了1.00至2.03分。**|
|**2024-12-05**|**Liquid: Language Models are Scalable Multi-modal Generators**|Junfeng Wu et.al.|[2412.04332](http://arxiv.org/abs/2412.04332)|**[link](https://github.com/foundationvision/liquid)**|我们介绍了Liquid，这是一种自回归生成范式，通过将图像token化为离散代码，并在共享的视觉和语言特征空间中学习这些代码嵌入和文本token，从而无缝地整合视觉理解和生成。与之前的多模态大型语言模型（MLLM）不同，Liquid通过单一的大型语言模型（LLM）实现这种整合，无需使用外部预训练的视觉嵌入，例如CLIP。Liquid首次揭示了一个缩放法则，即统一训练视觉和语言任务不可避免导致的性能下降随着模型规模的增加而减少。此外，统一的token空间使得视觉生成和理解任务能够相互增强，有效消除了早期模型中常见的干扰问题。我们表明，现有的LLM可以作为Liquid的强大基础，节省100倍的训练成本，同时在多模态能力上超越Chameleon，并保持与主流LLM如LLAMA2相当的语言性能。Liquid还在视觉-语言和纯文本任务上均优于模型如SD v2.1和SD-XL（在MJHQ-30K上的FID为5.47），证明了LLM如LLAMA3.2和GEMMA2是强大的多模态生成器，提供了一种可扩展的解决方案，以增强视觉-语言的理解和生成能力。代码和模型将会发布。|
|**2024-12-05**|**The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation**|Fredrik Carlsson et.al.|[2412.04318](http://arxiv.org/abs/2412.04318)|null|本文介绍了在非常小的数据集上对预训练的大规模语言模型（LLMs）进行过拟合的反直觉泛化结果。在开放式文本生成的场景中，已有大量文献表明，LLMs往往会生成重复且枯燥的序列，这种现象在使用贪心解码时尤为明显。即使是最先进的LLMs，经过数十亿参数的训练并通过下一个令牌预测在大型数据集上进行训练，这一问题依然存在。我们发现，通过进一步微调这些模型以在一小部分样本上实现近乎零的训练损失——我们称之为超拟合——可以极大地提升长序列生成的能力。这些超拟合模型在贪心解码下甚至在长序列方面也优于Top-P采样方法，无论是在多样性还是人类偏好方面。这一现象不仅适用于各种大小的LLMs，还适用于不同的领域，甚至包括自回归图像生成。我们进一步发现，这一现象与Grokking和双下降现象有着明显的不同。令人惊讶的是，我们的实验表明，超拟合模型很少会陷入他们训练过的重复序列中，即使显式地阻止这些序列，也能产生高质量的输出。所有超拟合模型都产生了极低熵的预测，通常几乎将所有概率分配给单个令牌。|
|**2024-12-04**|**From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents**|Xinyi Mou et.al.|[2412.03563](http://arxiv.org/abs/2412.03563)|**[link](https://github.com/fudandisc/socialagent)**|传统的社会学研究通常依赖于人类参与，尽管这种方法有效，但成本高昂、难以规模化，并且存在伦理问题。最近，大型语言模型（LLMs）的进步突显了它们模拟人类行为的潜力，使个体响应的复制成为可能，并促进了众多跨学科研究的发展。本文对这一领域进行了全面调查，展示了由LLM驱动的代理所推动的最新进展。我们将这些模拟分为三类：（1）个体模拟，模仿特定个体或人口群体；（2）场景模拟，在特定背景下，多个代理合作以实现目标；（3）社会模拟，通过建模代理社会中的互动来反映现实世界动态的复杂性和多样性。这些模拟从详细的个体建模到大规模的社会现象逐步推进。我们详细讨论了每种模拟类型，包括模拟的架构或关键组件、目标或场景分类以及评估方法。随后，我们总结了常用的基准数据集和基准。最后，我们讨论了这三种模拟类型之间的趋势。相关资源库位于{\url{https://github.com/FudanDISC/SocialAgent}}。|
|**2024-12-04**|**SPICE: Smart Projection Interface for Cooking Enhancement**|Vera Prohaska et.al.|[2412.03551](http://arxiv.org/abs/2412.03551)|null|实体用户界面（TUI）在人机交互（HCI）中提供物理表示的数字信息，旨在克服基于屏幕界面的局限性。尽管文献中有许多引人注目的TUI示例，但对于日常双手持任务和流程（如烹饪）的TUI研究却相对匮乏。针对这一空白，我们提出了SPICE（智能投影界面用于烹饪增强）。SPICE在厨房环境中探索TUI，旨在将食谱跟随体验从简单的文本形式转变为可触摸互动的形式。SPICE包括一个追踪系统、基于代理的软件以及视觉大语言模型，用于创建并解读一个厨房环境，在此环境中，食谱信息直接投影到烹饪表面。我们对SPICE与基于文本的食谱跟随进行了比较可用性研究，共有30名参与者参与，评估了任务难度、总时长、效率，以及用户信心和口味感知。结果表明，SPICE使参与者能够以更少的停顿和更短的时间完成食谱，同时提高自我报告的效率、信心和口味。尽管如此，参与者表示整体难度没有变化，这是未来研究的一个方向。总体而言，SPICE项目展示了使用TUI改善日常活动的潜力，为未来在HCI和新计算界面的研究铺平了道路。|
|**2024-12-04**|**Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models**|Natalie Mackraz et.al.|[2412.03537](http://arxiv.org/abs/2412.03537)|null|大型语言模型（LLMs）越来越多地被用于实现特定任务，并部署到实际的决策系统中。先前的研究已经探讨了偏见传递假设（BTH），通过研究微调适应策略对模型公平性的影响，发现预训练掩码语言模型中的公平性对微调后的模型公平性影响有限。在这项工作中，我们将BTH的研究扩展到了因果模型在提示适应下的情况。与之前的工作不同，我们发现预训练的Mistral、Falcon和Llama模型中存在的内在偏见与这些模型在零样本和少量样本提示下的偏见之间有很强的相关性（rho >= 0.94），使用的是代词共指消解任务。进一步，我们发现即使LLMs被明确提示表现出公平或偏见行为，偏见传递仍然保持高度相关（rho >= 0.92），并且当少量样本长度和刻板印象组成发生变化时，相关性仍然很高（rho >= 0.97）。我们的研究结果强调了确保预训练LLMs的公平性的重要性，尤其是在这些模型后来通过提示适应来执行下游任务时。|
|**2024-12-04**|**A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences**|Gabriel Lino Garcia et.al.|[2412.03531](http://arxiv.org/abs/2412.03531)|null|大型语言模型（LLMs）的快速发展在医学知识的提取和综合方面开辟了新的边界，特别是在证据合成领域。本文回顾了LLMs在生物医学领域的最新应用，探讨了它们在自动化复杂任务如从生物医学文献集中进行证据合成和数据提取方面的有效性。尽管LLMs展示了显著的潜力，但仍存在诸多挑战，包括幻觉问题、上下文理解以及在多样化医疗任务中的泛化能力等。我们指出了当前研究文献中的关键差距，特别是需要统一基准来标准化评估并确保实际应用中的可靠性。此外，我们提出了未来研究的方向，强调整合最新的技术如检索增强生成（RAG）以增强LLMs在证据合成中的表现。通过解决这些挑战并利用LLMs的优势，我们旨在改善对医学文献的访问，并促进医疗健康领域的有意义发现。|
|**2024-12-04**|**FANAL -- Financial Activity News Alerting Language Modeling Framework**|Urjitkumar Patel et.al.|[2412.03527](http://arxiv.org/abs/2412.03527)|null|在快速发展的金融领域，准确及时地解读市场新闻对于利益相关者来说至关重要，尤其是在应对不可预测的事件时。本文介绍了一个名为FANAL（Financial Activity News Alerting Language Modeling Framework）的专用框架，该框架基于BERT，专门用于实时金融事件检测和分析，并将新闻分类为十二个不同的金融类别。FANAL利用通过XGBoost处理过的银标数据，并采用先进的微调技术，同时还使用了ORBERT（Odds Ratio BERT），这是一种新型的BERT变体，经过ORPO（Odds Ratio Preference Optimization）优化，以提高类别概率校准和与金融事件相关性的对齐。我们评估了FANAL的表现，将其与包括GPT-4o、Llama-3.1 8B和Phi-3在内的领先大型语言模型进行了对比，结果显示FANAL在准确性和成本效益方面均表现出色。这一框架为金融情报和响应性设定了新的标准，在性能和成本效益方面显著超越现有模型。|
|**2024-12-04**|**You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?**|Dominic Lohr et.al.|[2412.03516](http://arxiv.org/abs/2412.03516)|null|反馈作为学习过程中最具影响力的因素之一，一直是大量研究的主题。它在教育技术系统的发展中起着关键作用，并且传统上根植于由专家及其经验定义的确定性反馈。然而，随着生成式人工智能特别是大规模语言模型（LLMs）的兴起，我们预计反馈在编程背景下的学习系统中将会发生变化。在过去，自动化反馈对于编程学习者来说是一个挑战。大规模语言模型可能会创造以前无法实现的提供更丰富、更个性化的反馈的可能性。  本文旨在使用大规模语言模型为入门级编程任务生成特定类型的反馈。我们重新审视现有的反馈分类法，以捕捉生成反馈的具体特性，如随机性、不确定性以及变化的程度。  我们通过迭代设计提示来生成符合现有反馈分类法的特定反馈类型，这些提示针对真实的学生产品。然后，我们评估生成的输出，并确定其在多大程度上反映了某些反馈类型。  研究结果提供了对不同反馈维度和特性的更好理解。结果对未来的反馈研究具有意义，例如反馈效果和学习者的信息需求。此外，这为进一步开发面向新手程序员的新工具和学习系统奠定了基础，包括由人工智能生成的反馈。|
|**2024-12-04**|**Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning**|Neale Ratzlaff et.al.|[2412.03467](http://arxiv.org/abs/2412.03467)|null|多模态模型通常结合强大的大型语言模型（LLM）和视觉编码器，并通过指令调优在多模态数据上进行训练。虽然这一过程使LLM适应多模态环境，但尚不清楚这种适应是否会影响它们原有的语言推理能力。在这项工作中，我们探讨了多模态指令调优对语言推理性能的影响。我们专注于LLaVA，这是一个领先的多模态框架，它集成了如Vicuna或Mistral的LLM与CLIP视觉编码器。我们比较了原始LLM与其多模态适应版本在八个语言推理任务中的表现。我们的实验得出几个关键见解。首先，多模态学习对Vicuna和Mistral的影响不同：我们观察到Mistral在大多数任务中的语言推理能力有所下降，而Vicuna在大多数任务中有所提升。其次，虽然多模态指令学习在数学推理任务（例如GSM8K）上始终导致性能下降，但在常识推理任务（例如CommonsenseQA）上则提升了性能。最后，我们展示了无需训练的模型合并技术可以有效缓解多模态适应后的Mistral在语言推理上的退化现象，甚至可以在视觉任务上提升性能。|
|**2024-12-04**|**From Words to Workflows: Automating Business Processes**|Laura Minkova et.al.|[2412.03446](http://arxiv.org/abs/2412.03446)|null|随着企业越来越多地依赖自动化来优化运营，Robotic Process Automation（RPA，机器人流程自动化）的局限性变得明显，特别是在其对专家知识的依赖以及无法处理复杂决策任务方面。最近，人工智能（AI）特别是Generative AI（GenAI，生成式AI）和Large Language Models（LLM，大型语言模型）的进步为Intelligent Automation（IA，智能自动化）铺平了道路，这种自动化方式整合了认知能力以克服RPA的不足。本文介绍了一种名为Text2Workflow的新方法，该方法能够从自然语言用户请求中自动生成工作流。与传统的自动化方法不同，Text2Workflow提供了一个通用解决方案，可以自动执行任何业务流程，将用户输入转换为用JavaScript Object Notation（JSON，一种数据交换格式）表示的一系列可执行步骤。利用LLMs的决策和指令执行能力，这种方法提供了一个可扩展、适应性强的框架，使用户能够在最小的人工干预下可视化和执行工作流。本研究概述了Text2Workflow的方法论及其对自动化复杂业务流程的更广泛影响。|
|**2024-12-04**|**RedStone: Curating General, Code, Math, and QA Data for Large Language Models**|Yaoyao Chang et.al.|[2412.03398](http://arxiv.org/abs/2412.03398)|null|预训练大型语言模型（LLMs）在高质量、精心策划的数据集上被广泛认为是提高其性能和泛化能力的关键。本研究探讨了Common Crawl作为全面且灵活的资源在预训练LLMs中的潜力，涉及通用语言理解和专业领域知识。我们介绍了RedStone，这是一种创新且可扩展的管道，用于从Common Crawl中提取和处理数据，从而创建广泛的多样化预训练数据集。与传统数据集不同，这些数据集通常需要昂贵的策展和特定领域的专业知识，RedStone利用Common Crawl的广度，生成适用于多种领域的定制数据集。在此工作中，我们通过构建涵盖多个领域的预训练数据集来展示其能力，包括通用语言理解、代码、数学和问答任务。RedStone的灵活性使其能够轻松适应其他专业领域，显著降低了创建有价值的领域特定数据集的门槛。我们的研究表明，当通过有效的管道如RedStone利用时，Common Crawl可以作为丰富的可再生预训练数据源，为LLMs的领域适应和知识发现开辟新的途径。这项工作还强调了创新数据获取策略的重要性，并突显了网络规模数据作为LLMs持续演进的强大资源的作用。RedStone代码和数据样本将在<https://aka.ms/redstone>公开。|
|**2024-12-04**|**WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through Game-Based Analysis**|Chengwei Hu et.al.|[2412.03359](http://arxiv.org/abs/2412.03359)|null|近期在基于大型语言模型（LLMs）的自主多智能体系统（MAS）方面的进展提升了应用情景，并提高了LLMs处理复杂任务的能力。尽管已经展示了有效性，现有的研究仍然明显存在评估、分析和可重复性的问题。为了促进LLM-based MAS的研究，我们介绍了一个开放、可扩展且实时更新的平台，用于访问和分析基于“谁是间谍？”（WiS）游戏的LLM-based MAS。我们的平台具有三个主要特点：（1）支持Hugging Face上可用模型的统一模型评估接口；（2）实时更新的模型评估排行榜；（3）涵盖游戏胜率、攻击策略、防御策略以及LLMs推理能力的全面评估。为了严格测试WiS，我们进行了广泛的实验，覆盖了各种开源和闭源LLMs，发现不同的代理在游戏中的表现具有独特而有趣的特性。实验结果证明了我们平台在评估LLM-based MAS方面的有效性和效率。我们的平台及其文档可在<https://whoisspy.ai/>公开获取。|
|**2024-12-03**|**T-REG: Preference Optimization with Token-Level Reward Regularization**|Wenxuan Zhou et.al.|[2412.02685](http://arxiv.org/abs/2412.02685)|null|强化学习从人类反馈（Reinforcement Learning from Human Feedback, RLHF）在使大型语言模型（Large Language Models, LLMs）与人类价值观保持一致方面起到了关键作用。传统上，RLHF涉及生成对查询的响应，并使用奖励模型为整个响应分配奖励。然而，这种方法因其依赖单一稀疏奖励而面临挑战，这使得模型难以识别序列中的哪些部分对最终奖励贡献最大。最近的方法试图通过引入令牌级奖励来解决这一局限性。然而，这些方法通常依赖于训练好的信用分配模型或AI注释者，这引发了关于奖励质量和可靠性的担忧。在本文中，我们提出了一种名为令牌级奖励正则化（Token-level Reward Regularization, T-REG）的新方法，该方法利用序列级和令牌级奖励进行偏好优化。通过利用LLMs的自我精炼能力，我们的方法使用对比提示使LLMs能够自动生成令牌级奖励。这些自动生成的奖励作为奖励正则化，指导模型更有效地将序列级奖励分布在令牌上。这促进了更好的令牌级信用分配并增强了对齐性能。在指令遵循基准测试中，包括Alpaca Eval 2和Arena-Hard，实验表明我们的方法比基线方法高出最多3.8%和4.4%。我们将发布代码和模型到https://github.com/wzhouad/T-REG。|
|**2024-12-03**|**Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models**|Yuda Song et.al.|[2412.02674](http://arxiv.org/abs/2412.02674)|null|自我改进是大型语言模型（LLM）在预训练、后训练和测试时推理过程中的一个机制。我们探索了一个框架，在这个框架中，模型可以验证自己的输出，根据验证结果过滤或重新加权数据，并对过滤后的数据进行提炼。尽管已经取得了一些经验上的成功，但对这一机制的根本理解仍然缺乏。在这项工作中，我们开始对LLM的自我改进进行全面、模块化且可控的研究。我们为自我改进提供了一个数学公式，该公式主要由我们定义的一个量来控制，我们称之为生成-验证差距。通过使用各种模型家族和任务的实验，我们发现了一种自我改进的缩放现象——一种生成-验证差距随着模型预训练的浮点运算量单调增加。我们还研究了何时可能实现自我改进，迭代式自我改进程序以及提高其性能的方法。我们的发现不仅增进了对LLM自我改进的理解，并具有实际意义，而且还开启了对未来研究该机制能力和边界的多个途径。|
|**2024-12-03**|**LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation with Instructional Inputs**|Pranav Doma et.al.|[2412.02655](http://arxiv.org/abs/2412.02655)|null|自主导航在动态环境中通过自然语言指令进行引导对于改善人机交互和执行复杂任务至关重要。虽然大型语言模型（LLMs）并非专门设计用于规划，但它们可以通过提供指导并告知约束条件来显著提高规划效率，从而确保安全性。本文介绍了一个规划框架，该框架结合了LLMs、二维占用栅格地图以及自然语言命令，以提高资源受限环境下的空间推理和任务执行能力。系统通过分解高层次指令和实时环境数据生成结构化的导航计划，包括障碍物规避、目标优先级排序和自适应行为。该框架能够动态重新计算路径以应对环境变化，并遵循隐含的社会规范，实现流畅的人机交互。我们的结果显示，LLMs有望设计出具备情境感知的系统，从而提高工业和动态环境中的导航效率和安全性。|
|**2024-12-03**|**Time-Reversal Provides Unsupervised Feedback to LLMs**|Yerram Varun et.al.|[2412.02626](http://arxiv.org/abs/2412.02626)|null|大型语言模型（LLMs）通常被训练成向前预测时间方向。然而，最近的研究表明，通过提示这些模型回顾并批评自己的生成结果，可以产生有用的反馈。受此启发，我们探讨了是否可以让LLMs在向后思考（预测和评分）时提供无监督反馈，以补充向前的LLMs。为此，我们引入了时间反转语言模型（Time Reversed Language Models, TRLMs），这些模型在条件响应下能够评分和生成查询，有效地在时间的反方向上运行。此外，为了有效推断从响应到查询的方向，我们从零开始对一个语言模型（TRLM-Ba）进行逆序预训练和微调。我们通过实证研究（以及在简化设置下的理论分析）证明，当用于根据响应对查询进行重新排名多个向前生成的结果时，时间反转模型确实可以补充向前模型的预测。我们在广泛使用的AlpacaEval排行榜上获得了高达5%的改进，超过了使用自对数困惑度得分的最佳-of-N重新排名的基准。我们进一步展示了TRLM评分在诸如引文生成和段落检索等应用中优于传统的给定查询的响应评分，从而获得显著的收益。接下来，我们利用TRLM的生成能力来增强或为LLMs的输入安全过滤器提供无监督反馈，在JailbreakBench排行榜上的几种攻击中，显著降低了假阴性率，同时对假阳性率的影响可忽略不计。|
|**2024-12-03**|**Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback**|Hiroki Furuta et.al.|[2412.02617](http://arxiv.org/abs/2412.02617)|null|大型文本到视频模型在广泛的下游应用中具有巨大的潜力。然而，这些模型在准确描绘动态对象交互方面存在困难，经常导致不真实的动作和频繁违反现实世界的物理规律。一种受大型语言模型启发的解决方案是通过外部反馈来对生成的输出进行校准，使模型能够自主优化其响应，从而避免大量手动数据收集。在这项工作中，我们研究了使用反馈来增强文本到视频模型中的对象动态性。我们的目标是回答一个关键问题：哪些类型的反馈与特定的自我改进算法相结合，可以最有效地改善文本与视频的对齐以及现实的对象交互？我们首先推导出一个统一的概率目标函数，用于离线强化学习微调文本到视频模型。这一视角强调了现有算法（如KL正则化和策略投影）中的设计元素如何作为统一框架中的特定选择出现。然后，我们使用推导的方法来优化一系列文本视频对齐度量指标（例如CLIP得分、光流），但注意到它们往往无法与人类对生成质量的感知保持一致。为了解决这一局限性，我们提出利用视觉语言模型提供更精细的反馈，特别针对视频中的对象动态性。我们的实验表明，我们的方法可以有效优化各种奖励信号，其中二元人工智能反馈在提高视频质量方面带来了显著提升，特别是在涉及多个对象之间的复杂交互和现实对象跌落的场景中，这一点得到了人工智能和人类评估的确认。|
|**2024-12-03**|**AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?**|Kaixiong Gong et.al.|[2412.02611](http://arxiv.org/abs/2412.02611)|null|最近，多模态大型语言模型（MLLMs），如GPT-4o、Gemini 1.5 Pro和Reka Core，已经扩展了其能力，包括视觉和音频模态。尽管这些模型在广泛的音频-视觉应用中表现出色，但我们提出的DeafTest揭示了MLLMs经常难以完成人类认为简单任务的情况：1）确定两个声音中哪个更响；2）确定两个声音中哪个音调更高。受这些观察的启发，我们引入了AV-Odyssey Bench，这是一个全面的音频-视觉基准测试，旨在评估这些MLLMs是否真正理解音频-视觉信息。该基准测试包含了4,555个精心设计的问题，每个问题都结合了文本、视觉和音频元素。为了成功推断答案，模型必须有效利用来自视觉和音频输入的线索。为了确保对MLLM响应进行精确和客观的评估，我们将问题设计为多项选择题，从而无需人工评估或大型语言模型（LLM）辅助评估。我们对一系列闭源和开源模型进行了基准测试，并总结了观察结果。通过揭示当前模型的局限性，我们希望为未来的数据集收集和模型开发提供有用的见解。|
|**2024-12-03**|**Interpretable Company Similarity with Sparse Autoencoders**|Marco Molinari et.al.|[2412.02605](http://arxiv.org/abs/2412.02605)|null|确定公司相似性在金融领域是一个至关重要的任务，它支撑着对冲、风险管理、投资组合多样化等。从业者通常依赖于部门和行业分类来衡量相似性，例如SIC代码和GICS代码，前者被美国证券交易委员会（SEC）使用，后者被投资界广泛采用。有人提出利用公司描述的聚类嵌入来衡量公司相似性，但由于词嵌入缺乏可解释性，在高风险环境中难以被采纳。稀疏自编码器已被证明可以提升大型语言模型的可解释性，通过将LLM激活分解成可解释的特征。在本文中，我们探索了稀疏自编码器特征在衡量公司相似性方面的应用，并将其与（1）SIC代码和（2）主要组代码进行对比。我们的结论是，稀疏自编码器特征能够重现甚至超越部门分类，量化公司的基本特性，这通过月度收益的相关性和协整带来的利润来评估，作为相似性的代理指标。|
|**2024-12-03**|**CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs**|Abhas Kumar et.al.|[2412.02602](http://arxiv.org/abs/2412.02602)|null|本文分析了小型语言模型（SLMs）和视觉语言模型（VLMs）的性能，并评估了在四个基本任务（图像描述、视觉问答、对话总结和文本到SQL转换）上模型性能与碳排放之间的权衡。我们选择了属于Qwen和LLaMA架构家族的不同SLMs和VLMs，并评估了这些模型在参数量、量化水平和微调参数方面的变体。通过计算模型变体的性能和碳排放，我们引入了一个新的指标——CEGI（每单位百分比增益每百万可训练参数的碳排放指数）。该指标提供了一种归一化的方法来比较模型在性能提升方面的效率与其环境成本。实验结果表明，对SLMs和VLMs进行微调可以达到与大型语言模型（LLMs）相当的性能水平，同时产生的碳排放显著减少。我们的研究发现，更大模型在准确率上的边际收益并不足以证明其显著增加的碳排放是合理的。通过采用较低位的量化水平，所提出的指标进一步提高了能效而不牺牲性能。本研究强调了在高性能和环境可持续性之间取得平衡的重要性，并为选择适合环保AI开发的模型提供了有价值的指标。|
|**2024-12-03**|**PrefixLLM: LLM-aided Prefix Circuit Design**|Weihua Xiao et.al.|[2412.02594](http://arxiv.org/abs/2412.02594)|null|前缀电路是数字加法器中的基本组件，在数字系统中因其在计算进位信号方面的效率而被广泛应用。合成具有最小面积和延迟的前缀电路对于提升现代计算系统的性能至关重要。最近，大型语言模型（LLMs）展示了惊人的文本生成能力。我们提出了PrefixLLM，该方法利用LLMs进行前缀电路综合。PrefixLLM将前缀电路综合任务转化为结构化文本生成问题，称为结构化前缀电路表示（SPCR），并引入了一种迭代框架以自动生成有效的SPCR。我们进一步提出了一种设计空间探索（DSE）框架，使用LLMs迭代搜索满足面积和延迟优化的前缀电路。与现有技术相比，PrefixLLM可以在相同的延迟约束下将面积减少3.70%。这项工作强调了使用LLMs进行算术电路综合的应用，这些电路可以转化为结构化的文本生成任务。|
|**2024-12-03**|**OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation**|Junyuan Zhang et.al.|[2412.02592](http://arxiv.org/abs/2412.02592)|**[link](https://github.com/opendatalab/OHR-Bench)**|** Retrieval增强生成（RAG）通过整合外部知识来提升大型语言模型（LLMs），从而减少幻觉并纳入最新的信息，而无需重新训练。作为RAG系统的一个重要组成部分，外部知识库通常通过光学字符识别（OCR）从非结构化的PDF文档中提取结构化数据来构建。然而，由于OCR的预测不完美以及结构化数据表示的固有非均匀性，知识库不可避免地会包含各种OCR噪声。在本文中，我们介绍了OHRBench，这是第一个用于理解OCR对RAG系统级联影响的基准。OHRBench包括来自六个实际RAG应用领域的350份精心挑选的非结构化PDF文档，以及从文档中的多模态元素衍生出的问题与答案，这些挑战了用于RAG的现有OCR解决方案。为了更好地理解OCR对RAG系统的影响，我们将OCR噪声分为两大类：语义噪声和格式噪声，并通过扰动生成具有不同程度每种噪声的结构化数据集。使用OHRBench，我们首先全面评估了当前的OCR解决方案，并揭示没有一种方案能够胜任为RAG系统构建高质量知识库的任务。然后，我们系统地评估了这两种噪声类型的影响，并展示了RAG系统的脆弱性。此外，我们讨论了在RAG系统中不使用OCR而采用视觉-语言模型（VLMs）的可能性。代码：https://github.com/opendatalab/OHR-Bench**|
|**2024-12-02**|**T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs**|Shukang Yin et.al.|[2411.19951](http://arxiv.org/abs/2411.19951)|**[link](https://github.com/xjtupanda/t2vid)**|**多模态大语言模型（MLLMs）在图像领域的成功引起了研究界的广泛关注。借鉴先前的成功经验，研究人员最近探索将这种成功扩展到视频理解领域。除了从头开始训练外，一种高效的方法是利用预训练的图像-LLM，从而形成了两种主流方法，即零样本推理和进一步使用视频数据进行微调。在这项工作中，我们对这些方法的研究收获了一种有效的数据增强方法。我们首先深入检查了零样本推理方法，并发现了两个局限性，即有限的泛化能力和缺乏时间理解能力。因此，我们进一步研究了微调方法，发现当简单地使用所有视频数据样本时，学习效率较低，这可以归因于指令多样性不足。针对这一问题，我们开发了一种称为T2Vid的方法来合成类似视频的样本，以丰富训练语料库中的指令多样性。整合这些数据使训练方案变得简单且高效，通过仅使用15%的样本量即可达到与使用完整视频数据集相当甚至更好的性能。同时，我们发现所提出的方案可以在不使用长视频样本进行训练的情况下提升长视频理解的性能。我们希望本研究能激发更多关于使用MLLMs进行视频理解和高质量数据整理的思考。代码已发布在https://github.com/xjtupanda/T2Vid。**|
|**2024-12-02**|**Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability**|Zicheng Lin et.al.|[2411.19943](http://arxiv.org/abs/2411.19943)|**[link](https://github.com/chenzhiling9954/critical-tokens-matter)**|大型语言模型（LLMs）在推理任务中表现出色，它们通过自回归令牌生成来构建连贯的推理路径。然而，我们发现这些模型中存在“关键令牌”，这些令牌会导致错误的推理路径。特别地，当强制模型解码其他令牌而不是关键令牌时，模型更倾向于产生正确的结果。受此启发，我们提出了一种新的方法——cDPO，旨在自动识别和处理关键令牌的奖励，以在对齐过程中优化模型。具体而言，我们开发了一种对比估计方法，通过比较正负模型的生成可能性来自动识别关键令牌。为此，我们分别对正负模型进行微调，使其能够识别错误轨迹中的关键令牌，这些令牌导致了错误的结果。此外，为了进一步在对齐过程中将模型与关键令牌信息对齐，我们将传统的DPO算法扩展到令牌级别，并利用上述正负模型之间的差异似然作为重要权重进行令牌级别的DPO学习。实验结果表明，在GSM8K和MATH500基准数据集上，使用两种广泛使用的模型Llama-3（8B和70B）以及deepseek-math（7B）时，所提出的cDPO方法是有效的。|
|**2024-11-29**|**VLSBench: Unveiling Visual Leakage in Multimodal Safety**|Xuhao Hu et.al.|[2411.19939](http://arxiv.org/abs/2411.19939)|null|安全问题逐渐成为各种应用中多模态大语言模型（MLLMs）的重要问题。令人惊讶的是，先前的研究表明，使用文本解绑对齐MLLMs能达到与使用图像-文本对训练的MLLMs相当的安全性能。为了解释这一反直觉的现象，我们发现现有多模态安全基准中存在视觉安全信息泄露（VSIL）问题，即图像中的潜在风险和敏感内容已经在文本查询中揭示。因此，MLLMs可以根据文本查询轻松拒绝这些敏感的文本-图像查询。然而，在现实世界场景中，没有VSIL的图像-文本对是常见的，并且被现有的多模态安全基准所忽视。为此，我们构建了一个多模态视觉无泄漏安全基准（VLSBench），防止图像到文本查询的视觉安全泄露，其中包含2400个图像-文本对。实验结果表明，VLSBench对开源和闭源MLLMs都构成了显著挑战，包括LLaVA、Qwen2-VL、Llama3.2-Vision和GPT-4o。本研究表明，在存在VSIL的情况下，文本对齐足以应对多模态安全场景，而在不存在VSIL的情况下，多模态对齐是更有效的解决方案。请参阅我们的代码和数据：http://hxhcreate.github.io/VLSBench|
|**2024-11-29**|**On Domain-Specific Post-Training for Multimodal Large Language Models**|Daixuan Cheng et.al.|[2411.19930](http://arxiv.org/abs/2411.19930)|null|近年来，通用多模态大型语言模型（MLLMs）得到了快速发展。然而，将这些通用模型适应到特定领域，如科学领域和工业应用，仍然研究较少。本文系统地探讨了通过后训练进行领域适应的MLLMs方法，重点关注数据合成、训练流程和任务评估。（1）数据合成：我们使用开源模型开发了一种视觉指令合成器，能够有效地从领域特定的图像-标题对生成多样化的视觉指令任务。我们合成的任务在提升MLLMs领域特定性能方面优于手动规则、GPT-4和GPT-4V生成的任务。（2）训练流程：虽然两阶段训练——最初基于图像-标题对，然后是视觉指令任务——被广泛用于开发通用MLLMs，但我们采用单阶段训练流程以增强领域特定后训练中的任务多样性。（3）任务评估：我们在两个领域——生物医学和食品领域——进行了实验，通过对不同来源和规模（例如，Qwen2-VL-2B，LLaVA-v1.6-8B，Llama-3.2-11B）的MLLMs进行后训练，并随后评估其在各种领域特定任务上的表现。为了支持MLLMs领域适应的进一步研究，我们将开放源代码我们的实现。|
|**2024-11-29**|**SIMS: Simulating Human-Scene Interactions with Real World Script Planning**|Wenjia Wang et.al.|[2411.19921](http://arxiv.org/abs/2411.19921)|null|模拟长期的人景交互是一个具有挑战性但又引人入胜的任务。先前的工作未能有效生成具有详细叙述的长期人景交互物理动画。本文介绍了一种新颖的框架，用于规划和控制长时间内物理上合理的长期人景交互。一方面，互联网上有大量风格化的角色动作或场景交互的电影和节目，为剧本规划提供了丰富的数据来源。另一方面，大规模语言模型（LLM）能够理解和生成逻辑连贯的故事线。这促使我们将两者结合起来，通过基于LLM的管道从视频中提取剧本，然后使用LLM来模仿和创作新的剧本，捕捉复杂的时序人类行为和环境交互。通过利用这一点，我们采用了一种双重视角策略，该策略既能理解语言又能理解场景，以在上下文和空间约束下引导角色动作。为了促进训练和评估，我们贡献了一个综合规划数据集，其中包含了从真实世界视频中提取的各种运动序列，并通过大规模语言模型进行了扩展。我们还收集并重新注释了现有动力学数据集中的运动片段，以使我们的策略能够学习各种技能。广泛的实验表明，我们的框架在多种任务执行中表现出色，并且具有良好的泛化能力，能够在各种场景中表现优异，与现有方法相比，性能显著提升。我们的代码和数据即将公开发布。|
|**2024-11-29**|**PDDLFuse: A Tool for Generating Diverse Planning Domains**|Vedant Khandelwal et.al.|[2411.19886](http://arxiv.org/abs/2411.19886)|null|各种现实世界的挑战需要能够适应广泛领域的规划算法。传统上，规划域的创建严重依赖于人工实现，这限制了可用域的数量和多样性。尽管最近的努力利用了生成式人工智能技术，如大型语言模型（LLM）来创建域，但这些努力主要集中在将现有域从自然语言描述转换过来，而不是生成新的域。相比之下，域随机化在强化学习中的成功应用通过在多样化的随机新域上进行训练来提高性能和泛化能力。受此成功的启发，我们的工具PDDLFuse旨在弥合Planning Domain Definition Language (PDDL)中的这一差距。PDDLFuse设计用于生成新的、多样的规划域，可用于验证新的规划器或测试基础规划模型。我们开发了方法来调整域生成器的参数，以调节所生成域的难度。这种适应性至关重要，因为现有的领域独立规划器通常难以处理更复杂的问题。初步测试表明，PDDLFuse能够高效地创建复杂且多样的域，这代表了传统域生成方法的重要进步，并为规划研究做出了贡献。|
|**2024-12-02**|**LUMIA: Linear probing for Unimodal and MultiModal Membership Inference Attacks leveraging internal LLM states**|Luis Ibanez-Lissen et.al.|[2411.19876](http://arxiv.org/abs/2411.19876)|null|大型语言模型（LLMs）在各种应用中越来越普遍，但与此同时也出现了对成员推理攻击（MIA）的担忧。先前的努力主要集中在黑盒到灰盒模型上，因此忽略了从内部LLM信息中可能获得的好处。为了解决这个问题，我们提出使用线性探针（LPs）作为检测成员推理攻击的方法，通过检查LLMs的内部激活来实现。我们的方法被称为LUMIA，它逐层应用LPs以获取关于模型内部工作的细粒度数据。我们在多种模型架构、大小和数据集上测试了这种方法，包括单模态和多模态任务。在单模态MIA中，LUMIA在曲线下面积（AUC）上比以前的技术平均提高了15.71%。值得注意的是，LUMIA在65.33%的情况下达到了AUC>60%，这比现有技术提高了46.80%。此外，我们的方法揭示了一些关键见解，例如哪些模型层最容易检测出MIA。在多模态模型中，LPs表明视觉输入可以显著有助于检测MIA——在85.90%的实验中达到了AUC>60%。|
|**2024-11-29**|**AIDetx: a compression-based method for identification of machine-learning generated text**|Leonardo Almeida et.al.|[2411.19869](http://arxiv.org/abs/2411.19869)|**[link](https://github.com/aidetx/aidetx)**|**本文介绍了一种名为AIDetx的新方法，用于使用数据压缩技术检测机器生成的文本。传统的方法，如深度学习分类器，通常存在高计算成本和有限的可解释性等问题。为了解决这些局限性，我们提出了一种基于压缩的分类框架，该框架利用了有限上下文模型（FCMs）。AIDetx为人类撰写和AI生成的文本分别构建了不同的压缩模型，并根据哪个模型能够实现更高的压缩比来对新输入进行分类。我们在两个基准数据集上评估了AIDetx，其F1得分分别超过了97%和99%，突显了其高精度。与当前的方法，如大型语言模型（LLMs）相比，AIDetx提供了一个更具有可解释性和计算效率更高的解决方案，显著减少了训练时间和硬件要求（例如，不需要GPU）。完整实现可在https://github.com/AIDetx/AIDetx 获取。**|
|**2024-11-29**|**Reverse Thinking Makes LLMs Stronger Reasoners**|Justin Chih-Yao Chen et.al.|[2411.19865](http://arxiv.org/abs/2411.19865)|null|逆向思维在人类推理中起着至关重要的作用。人类不仅能从问题推导出解决方案，还能反向思考，即从解决方案出发，反向推理回问题。这种方法通常能增强整体的推理表现，因为它能够使正向和逆向思维之间的一致性检查更为有效。为了使大型语言模型（LLMs）具备逆向思维能力，我们引入了逆向增强思维（RevThink），这是一个由数据增强和学习目标组成的框架。在RevThink中，我们通过收集教师模型的结构化正向和逆向推理来增强数据集，这些数据包括：（1）原始问题，（2）正向推理，（3）逆向问题，以及（4）逆向推理。然后，我们采用三个目标以多任务学习的方式训练一个较小的学生模型：（a）从问题生成正向推理，（b）从问题生成逆向问题，以及（c）从逆向问题生成逆向推理。实验涵盖了12个涉及常识、数学和逻辑推理的数据集，结果显示学生模型的零样本性能平均提高了13.53%，并且比最强的知识蒸馏基线方法高出6.84%。此外，我们的方法展示了样本效率——仅使用训练数据中10%的正确正向推理，其表现就超过了标准微调方法在10倍更多正向推理上训练的结果。RevThink还表现出对分布外保留数据集的强大泛化能力。|
|**2024-11-29**|**Cross-Domain Recommendation Meets Large Language Models**|Ajay Krishna Vajjala et.al.|[2411.19862](http://arxiv.org/abs/2411.19862)|**[link](https://github.com/ajaykv1/CDR_Meets_LLMs)**|**跨域推荐（CDR）已成为解决单域推荐系统面临的冷启动问题的一种有前景的解决方案。然而，现有的CDR模型依赖于复杂的神经架构、大型数据集和大量的计算资源，这使得它们在数据稀缺场景或需要简单性时效果不佳。在这项工作中，我们利用大规模语言模型（LLMs）的推理能力，并探索其在多个领域对组合中的CDR领域的表现。我们提出了两种专为CDR设计的新颖提示方法，并证明当有效提示时，LLMs在评分预测和排名任务中，在各种指标和领域组合上优于最先进的CDR基线。这项工作在LLMs和推荐系统之间架起了一座桥梁，展示了它们作为有效的跨域推荐系统的潜力。**|
|**2024-11-27**|**Cross-modal Information Flow in Multimodal Large Language Models**|Zhi Zhang et.al.|[2411.18620](http://arxiv.org/abs/2411.18620)|**[link](https://github.com/FightingFighting/cross-modal-information-flow-in-MLLM)**|近期在自回归多模态大型语言模型（MLLMs）方面的进展在视觉-语言任务上展示了有希望的进步。尽管已有许多研究探讨了大型语言模型内处理语言信息的方式，但对于MLLMs内部工作机制以及语言和视觉信息如何交互的了解仍然有限。在这项研究中，我们旨在填补这一空白，通过检查MLLMs中不同模态——语言和视觉——之间信息流，重点关注视觉问答任务。具体来说，给定一个图像-问题对作为输入，我们研究在模型中的哪些部分以及如何结合视觉和语言信息以生成最终预测。通过对一系列来自LLaVA系列的模型进行实验，我们发现在这个两种模态的信息整合过程中存在两个不同的阶段。在较低层，模型首先将整个图像的更一般的视觉特征转换为问题标记的表示。在中间层，它再次将与问题相关的特定对象的视觉信息转移到问题标记的位置。最后，在高层，生成的多模态表示被传播到输入序列的最后一位置以进行最终预测。总体而言，我们的研究结果为MLLMs中图像和语言处理的空间和功能方面提供了新的全面视角，从而促进未来关于多模态信息定位和编辑的研究。|
|**2024-11-27**|**Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation**|Nurshat Fateh Ali et.al.|[2411.18583](http://arxiv.org/abs/2411.18583)|null|本研究提出了多种方法来利用几种自然语言处理（NLP）技术和检索增强生成（RAG）与大型语言模型（LLM）自动生成文献综述。随着研究文章数量的不断增加，手动编写文献综述面临着巨大的挑战，这导致了对自动化需求的增加。本研究的主要目标是开发一个能够仅从PDF文件作为输入自动生成文献综述的系统。为了实现这一目标，评估了几种自然语言处理（NLP）策略的有效性，包括基于频率的方法（spaCy）、变压器模型（Simple T5）以及使用大型语言模型（GPT-3.5-turbo）的检索增强生成（RAG）。本研究选择了SciTLDR数据集，并使用三种不同的技术来实现三个不同系统的文献综述自动生成。所有三个系统均通过ROUGE评分进行评估。根据评估结果，大型语言模型GPT-3.5-turbo获得了最高的ROUGE-1分数，为0.364。变压器模型排名第二，spaCy位列最后。最终，基于大型语言模型的最佳系统创建了一个图形用户界面。|
|**2024-11-27**|**Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning**|Omkar Khade et.al.|[2411.18571](http://arxiv.org/abs/2411.18571)|null|大型语言模型（LLMs）在多语言能力方面已经表现出显著的性能，但在适应低资源语言时仍面临挑战。本研究调查了低秩适应（LoRA）参数高效微调（PEFT）对多语言Gemma模型在马拉地语（一种资源有限的语言）上的影响。我们使用了一个包含52,000个指令-响应对的翻译Alpaca数据集进行实验。研究结果表明，虽然评估指标通常显示微调后性能下降，但手动评估经常表明微调后的模型优于原始模型。观察结果显示，目标语言生成能力有所提高，但在语言适应后推理能力有所下降。这些结果强调了改进评估方法和创建高质量本地数据集以准确评估低资源设置下特定语言模型性能的必要性。|
|**2024-11-27**|**A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models**|Rong Wang et.al.|[2411.18564](http://arxiv.org/abs/2411.18564)|null|大型语言模型（LLMs）在各种任务中展示了令人印象深刻的性能。然而，LLMs经常在空间推理方面遇到困难，这是推理和推断的一个重要部分，需要理解空间中物体之间的复杂关系。本文提出了一种新颖的神经符号框架，以增强LLMs的空间推理能力。我们在两个基准数据集：StepGame和SparQA上评估了我们的方法，实施了三种不同的策略：(1)基于答案集编程（ASP）的符号推理，(2)使用DSPy的LLM+ASP流水线，以及(3)事实加逻辑规则。实验结果表明，与基线提示方法相比，我们的方法显著提高了准确性，在StepGame数据集上的准确率提高了40-50%，在更复杂的SparQA数据集上的准确率提高了3-13%。"LLM+ASP"流水线在寻找关系（FR）和寻找块（FB）问题的任务中取得了特别好的结果，但不同问题类型下的表现有所不同。这些令人印象深刻的结果表明，虽然神经符号方法为增强LLMs的空间推理提供了有希望的方向，但其效果很大程度上取决于具体任务特性和实现策略。我们提出了一种集成的、简单而有效的策略集，利用神经符号流水线来提升LLMs的空间推理能力。这种流水线及其策略显示出了在其他推理领域如时间推理、演绎推理等更强更广泛的应用潜力。|
|**2024-11-27**|**DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation**|Zhixuan Liang et.al.|[2411.18562](http://arxiv.org/abs/2411.18562)|null|灵巧操作在机器人领域至关重要，尤其是在接触丰富的交互场景下。虽然最近基于扩散的规划方法在较简单的操作任务中显示出潜力，但它们往往会产生不切实际的“幽灵状态”（例如，物体在没有手部接触的情况下自动移动），或者在处理复杂的顺序交互时缺乏适应性。在这项工作中，我们介绍了一种名为DexDiffuser的交互感知扩散规划框架，用于自适应灵巧操作。DexDiffuser通过双阶段扩散过程建模联合状态-动作动力学，该过程包括预接触对齐和接触后的目标导向控制，从而实现目标适应性的通用灵巧操作。此外，我们结合了基于动力学模型的双重引导，并利用大型语言模型自动生成引导函数，增强了物理交互的通用性，并通过语言提示促进了多样化的目标适应。在开门、笔和块的重新定向以及锤击钉子等物理交互任务中的实验表明，DexDiffuser在超出训练分布的目标上表现出色，平均成功率（59.2%）是现有方法（29.5%）的两倍以上。我们的框架在30度开门任务中达到了70.0%的成功率，在笔半侧重新定向任务中达到40.0%，在块半侧重新定向任务中达到36.7%，在锤击钉子任务中达到46.7%，展示了其在接触丰富的操作中的鲁棒性和灵活性。|
|**2024-11-27**|**Retrofitting (Large) Language Models with Dynamic Tokenization**|Darius Feher et.al.|[2411.18553](http://arxiv.org/abs/2411.18553)|null|当前的语言模型（LMs）使用固定的静态子词分词器。这种选择通常导致对英语以外的语言效率降低和功能受限，并且使得难以将LMs应用于新领域或语言。为了解决这些问题，我们提出对LMs进行动态分词改造：一种基于输入文本动态决定分词边界的策略。对于编码器风格的模型，我们引入了一种子词合并算法，灵感来源于字节对编码（BPE），但在批量级别上进行。我们在批量中合并频繁出现的子词序列，然后应用预训练的嵌入预测超网络来计算令牌嵌入。当与词级边界结合使用时，这在XNLI数据集上平均将14种语言的令牌序列长度减少了超过20%，同时仅使XLM-R的任务性能下降不到2%。对于解码器风格的模型，我们通过两种方式应用动态分词：1）在预填充阶段保持Mistral-7B的性能几乎完全不变，相对于词级而言序列长度最多减少40%；2）通过近似最近邻索引，实现了快速生成，拥有百万级别的词汇量，展示了对更大、动态词汇表的可扩展性。总体而言，我们的研究结果表明，动态分词显著提高了推理速度，并促进了跨语言的公平性，朝着克服静态分词的限制迈出了重要一步，使语言模型更加公平和适应性强。|
|**2024-11-27**|**Emergence of Self-Identity in AI: A Mathematical Framework and Empirical Study with Generative Large Language Models**|Minhyeok Lee et.al.|[2411.18530](http://arxiv.org/abs/2411.18530)|**[link](https://github.com/BrainJellyPie/self)**|**本文介绍了一种用于定义和量化人工智能（AI）系统自我意识的数学框架，填补了人工意识理论基础的关键空白。现有的人工自我意识方法通常依赖于启发式实现或哲学抽象，而我们提出了一种基于度量空间理论、测度论和泛函分析的正式框架。我们的框架认为，自我意识从两个可以用数学量化的基本条件产生：在度量空间 $(\mathcal{M}, d_{\mathcal{M}})$中存在一个连通的记忆连续统$C \subseteq \mathcal{M}$，以及一个保持这种连续统上一致自我认知的连续映射$I: \mathcal{M} \to \mathcal{S}$，其中$(\mathcal{S}, d_{\mathcal{S}})$ 表示可能的自我身份的度量空间。为了验证这一理论框架，我们使用Llama 3.2 1B模型进行了实验，并采用低秩自适应（LoRA）进行高效的微调。该模型在一个包含时间结构化记忆的人工合成数据集上进行训练，旨在捕捉连贯自我身份形成过程中的复杂性。我们的评估指标包括自我意识的定量测量、响应一致性和语言精度。实验结果显示，在可衡量的自我意识指标方面有显著改进，主要的自我意识得分从0.276提高到0.801。这使得构建具有验证过的自我身份特征的AI系统成为可能。本研究的结论对人形机器人和自主系统的领域具有直接的相关性。**|
|**2024-11-27**|**LLM-ABBA: Understand time series via symbolic approximation**|Erin Carson et.al.|[2411.18506](http://arxiv.org/abs/2411.18506)|null|大型语言模型（LLMs）在时间序列任务中的成功已在先前的研究中得到证明。通过使用符号化的时间序列表示，可以有效地弥合LLMs与时间序列之间的差距。然而，仍然存在的挑战是如何利用时间序列中隐藏的语义信息，并使用符号或现有的LLM令牌来调整LLMs的嵌入空间，以反映时间序列的隐含信息。一种名为自适应布朗桥符号聚合（ABBA）的符号时间序列近似（STSA）方法，在保留显著时间序列特征方面表现出色，通过建模时间序列模式的幅度和周期，同时使用现有令牌的LLMs。本文介绍了一种名为LLM-ABBA的方法，该方法将ABBA集成到大型语言模型中，用于各种下游时间序列任务。通过符号化时间序列，LLM-ABBA在UCR和三个医学时间序列分类任务中优于最近的最先进技术（SOTA）。同时，在ABBA中引入了固定折线技巧，以显著减轻符号到数值转换过程中因误用符号而导致的累积误差的影响，从而避免明显的漂移。在时间序列回归任务中，LLM-ABBA在时间序列外在回归（TSER）基准上达到了新的SOTA。LLM-ABBA在时间序列预测任务中的预测能力也具有竞争力，接近近期的SOTA结果。我们相信，这一框架也可以无缝扩展到其他时间序列任务。|
|**2024-11-27**|**GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**|Pengfei Zhou et.al.|[2411.18499](http://arxiv.org/abs/2411.18499)|null|多模态大型语言模型（MLLMs）在视觉理解和生成任务方面取得了显著进展。然而，生成交错的图像和文本内容仍然是一个挑战，这需要整合的多模态理解和生成能力。尽管统一模型的进步提供了新的解决方案，但现有的基准测试由于数据规模和多样性限制，不足以评估这些方法。为了填补这一空白，我们引入了GATE OpenING（简称OpenING），这是一个包含56个真实世界任务的5400个人工标注实例的综合基准。OpenING涵盖了多样化的日常场景，如旅行指南、设计和头脑风暴，提供了一个强大的平台来挑战交错生成方法。此外，我们提出了IntJudge，一种用于评估开放式多模态生成方法的评估模型。通过新颖的数据管道训练，我们的IntJudge与人类判断的一致率达到82.42%，比基于GPT的评估器高出11.34%。对OpenING的广泛实验表明，当前的交错生成方法仍有很大的改进空间。进一步提出了关于交错图像-文本生成的关键发现，以指导下一代模型的发展。OpenING已开源，网址为https://opening.github.io。|
|**2024-11-27**|**Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS**|Jinyang Wu et.al.|[2411.18478](http://arxiv.org/abs/2411.18478)|null|在上下文学习（ICL）的传统范式中，大型语言模型（LLMs）通过复杂的提示和高质量的示例来处理下游任务。然而，这种传统方法在面对复杂的数学推理任务时显示出局限性，主要是因为它对示例质量的高度依赖以及在具有挑战性的场景中需要人工干预。为了解决这些局限性，本文提出了一种名为HiAR-ICL的高层次自动化推理范式，该范式从具体的示例转向抽象的思维模式，扩展了ICL中传统的上下文概念。HiAR-ICL引入了五种原子推理动作作为构建链式结构模式的基本组件。通过使用蒙特卡洛树搜索，我们探索推理路径并构建思维卡片以指导后续推理。然后，我们开发了一个认知复杂性框架，该框架能够动态地将问题与适当的思维卡片匹配。实验结果表明，HiAR-ICL的有效性，在MATH基准测试中达到了79.6%的准确率（使用Qwen2.5-7B-Instruct），超过了GPT-4o（76.6%）和Claude 3.5（71.1%）。|
|**2024-11-26**|**Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats**|Jiaxin Wen et.al.|[2411.17693](http://arxiv.org/abs/2411.17693)|null|随着大型语言模型（LLMs）的能力不断增强，有必要评估即使这些模型有意尝试规避安全措施时，现有的安全措施是否仍然有效。先前的研究引入了控制评估，这是一种对抗性框架，用于测试不受信任模型的部署策略（即，那些可能试图规避安全措施的模型）。虽然以前的工作认为单一失败是不可接受的，但我们在此“分布式威胁环境”中进行控制评估——在这种环境中，没有单个行动是灾难性的，也没有单个行动提供压倒性的证据表明模型存在不一致对齐。我们采用了一个两级部署框架，使用自适应宏观协议在不同微观协议之间进行选择。微观协议针对单个任务操作，利用一个能力较低但经过广泛测试（可信）的模型来管理和监控不受信任的模型。同时，宏观协议根据其过去的行为动态调整对不受信任模型的对齐程度的信心，并据此选择更安全或风险更高的微观协议。我们在代码生成测试平台中评估了我们的方法，在该平台上红队试图通过LLM生成微妙的后门代码，而蓝队则负责保护该LLM的部署。我们绘制了安全性和实用性之间的帕累托前沿图，安全性的衡量标准是未被污染的解决方案数量，实用性的衡量标准是正确解决方案的数量。在给定的实用性水平下，我们的自适应部署策略相比非自适应基线减少了80%的后门。|
|**2024-11-26**|**Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens**|Xu Ouyang et.al.|[2411.17691](http://arxiv.org/abs/2411.17691)|null|我们发现低比特量化有利于未充分训练的大语言模型（LLMs），通过观察到较大的模型或训练样本较少的模型在应用低比特量化时经历的量化引起的退化（QiD）较小，而较小的模型在大量训练样本下则遭受显著的QiD。为了深入了解这一趋势，我们在受控环境中研究了超过1500个不同大小和不同训练水平（未充分训练或充分训练）的量化LLM检查点，推导出用于理解QiD与训练样本数量、模型大小和比特宽度之间关系的缩放定律。利用这些缩放定律，我们提出了一种新的观点，即可以使用QiD来衡量LLM的训练水平，并确定各种大小的LLM完全训练所需的训练样本数量。此外，我们使用这些缩放定律预测了用100万亿个训练样本训练的不同大小的LLM的量化性能。我们的预测显示，预计未来将用超过100万亿个训练样本训练的模型的低比特量化性能可能并不理想。这为低比特量化在未来提出了一个潜在挑战，并强调了在评估低比特量化研究时需要意识到模型的训练水平。为了促进对该问题的未来研究，我们将本工作中使用的1500多个量化检查点发布在https://huggingface.co/Xu-Ouyang。|
|**2024-11-26**|**Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration**|Yuhang Han et.al.|[2411.17686](http://arxiv.org/abs/2411.17686)|null|为了加速重型多模态大型语言模型（MLLMs）的推理，本研究重新审视了当前无训练令牌缩减研究的现状。遗憾的是，现有方法的关键组件紧密交织在一起，其相互联系和效果尚不清楚，难以进行比较、迁移和扩展。因此，我们提出了一种统一的“过滤-关联-压缩”范式，将令牌缩减分解成管道中的三个不同阶段，同时保持一致的设计目标和元素，允许独特的实现方式。此外，我们还揭示了流行的工作，并将其纳入我们的范式中，以展示其普适性。最后，我们提供了一系列基于该范式的推理各阶段速度与准确率之间平衡的方法。在10个基准测试中的实验结果表明，我们的方法可以在对性能影响极小的情况下实现高达82.4%的FLOPs减少，同时超越最先进的无训练方法。我们的项目页面位于https://ficoco-accelerate.github.io/。|
|**2024-11-26**|**Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning**|Zhu Xu et.al.|[2411.17679](http://arxiv.org/abs/2411.17679)|**[link](https://github.com/FloatFrank/TIPA)**|分词技术如字节对编码（BPE）和字节级BPE（BBPE）通过将文本分割成token显著提高了大型语言模型（LLMs）的计算效率和词汇表示稳定性。然而，这种分割往往掩盖了token内部的字符结构和序列，导致模型在训练过程中无法充分学习这些细节。因此，LLMs难以理解token内部的字符组成和位置关系，尤其是在使用有限数据微调下游任务时。本文介绍了一种名为Token Internal Position Awareness（TIPA）的新方法，该方法通过使用tokenizer自身的词汇表进行逆向字符预测任务来增强LLMs对token内部结构的理解。这种方法使模型能够有效地学习和泛化字符位置和内部结构。实验结果表明，使用TIPA训练的LLMs在预测token级别的字符位置方面优于基线模型。此外，当应用于下游任务中文拼写纠错（CSC）时，TIPA不仅加速了模型收敛，还显著提升了任务性能。|
|**2024-11-26**|**Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting**|Liyun Zhang et.al.|[2411.17674](http://arxiv.org/abs/2411.17674)|null|理解对话中的情绪通常需要外部知识以准确理解内容。随着大型语言模型（LLMs）变得越来越强大，我们不满足于预训练语言模型的有限能力。然而，LLMs要么只能处理文本模态，要么处理多媒体信息的成本过高。我们的目标是利用LLMs的能力和多媒体模态的补充特性。在本文中，我们提出了一种名为Lantern的框架，该框架通过使用感受野感知注意力加权来提示大型语言模型，从而提高特定普通模型的性能。该框架训练了一个多任务普通模型，以生成情绪类别的概率和维度分数。这些预测作为参考输入到LLMs中，以调整每个情绪类别的预测概率，并结合其外部知识和上下文理解。我们将对话切分为不同的感受野，每个样本恰好包含在t个感受野中。最后，LLMs的预测通过一个感受野感知注意力驱动加权模块进行合并。在实验中，普通模型CORECT和SDT部署在Lantern中，并使用GPT-4或Llama-3.1-405B。在IEMOCAP数据集上的4路和6路设置实验表明，Lantern可以显著提高当前普通模型的性能，最高可提升1.23%和1.80%。|
|**2024-11-26**|**SketchAgent: Language-Driven Sequential Sketch Generation**|Yael Vinker et.al.|[2411.17673](http://arxiv.org/abs/2411.17673)|null|素描作为一种多功能的工具，用于外部化想法，支持快速探索和视觉交流，涵盖了各种学科。尽管人工智能系统在内容创作和人机交互方面取得了显著进展，但捕捉人类素描的动态和抽象性质仍然具有挑战性。在这项工作中，我们介绍了SketchAgent，这是一种语言驱动的连续素描生成方法，使用户能够通过动态、对话式的互动来创建、修改和精炼素描。我们的方法不需要训练或微调。相反，我们利用了现成的多模态大型语言模型（LLMs）的顺序性和丰富的先验知识。我们提出了一种直观的素描语言，通过上下文示例引入模型，使其能够“绘制”基于字符串的动作。这些动作被处理成矢量图形，然后渲染到像素画布上，可以再次访问以进行进一步的任务。通过一笔一划地绘制，我们的代理捕获了素描固有的演变和动态特性。我们证明SketchAgent可以从不同的提示生成素描，进行对话式绘图，并与人类用户进行有意义的合作。|
|**2024-11-26**|**Synthetic Data Generation with LLM for Improved Depression Prediction**|Andrea Kang et.al.|[2411.17672](http://arxiv.org/abs/2411.17672)|null|自动检测抑郁症是一个快速发展的研究领域，涉及心理学和机器学习的交叉。然而，随着这一领域的兴趣呈指数增长，数据隐私和稀缺性问题也日益凸显，因为这类主题非常敏感。在本文中，我们提出了一种利用大型语言模型（LLMs）生成合成数据以提高抑郁症预测模型性能的流程。该流程从临床访谈记录的非结构化自然文本数据开始，通过链式思维提示利用开源LLM生成合成数据。该流程包括两个关键步骤：第一步是基于原始记录和抑郁评分生成概要和情感分析，第二步是基于第一步生成的概要和新的抑郁评分生成合成概要/情感分析。合成数据不仅在保真度和保护隐私方面令人满意，还平衡了训练数据集中严重程度的分布，从而显著提升了模型预测患者抑郁程度的能力。通过利用LLMs生成可以补充有限且不平衡的真实世界数据集的合成数据，我们展示了一种新颖的方法来解决自动抑郁症检测中常见的数据稀缺性和隐私问题，同时保持原始数据集的统计完整性。这种方法为未来的心理健康研究和应用提供了稳健的框架。|
|**2024-11-26**|**Toward High-Performance LLM Serving: A Simulation-Based Approach for Identifying Optimal Parallelism**|Yi-Chien Lin et.al.|[2411.17651](http://arxiv.org/abs/2411.17651)|null|高效地服务大型语言模型（LLMs）变得至关重要。LLMs通常使用多种设备，并采用数据并行、管道并行和张量并行等技术。每种并行技术都会在计算、内存和通信开销之间产生权衡，这使得确定最优的并行执行计划变得具有挑战性。此外，输入工作负载也会影响并行策略的选择。例如，具有长提示的任务如文章摘要往往是计算密集型的，而具有长生成长度的任务如代码生成则往往是内存密集型的；这些不同的特性导致了不同的最优执行计划。由于通过实际部署来寻找最优计划的成本过高，我们提出了一种名为APEX的LLM服务系统模拟器，它可以高效地识别出最优的并行执行计划。APEX捕捉了迭代级批处理的复杂特性，这是一种广泛应用于最先进的LLM服务系统的策略。APEX利用了LLMs的重复结构来减少设计空间，在扩展到万亿规模的模型时仍能保持类似的模拟开销。APEX支持广泛的LLMs、设备集群等，并且可以通过其高级模板轻松扩展。我们在CPU上运行APEX模拟，并使用8个H100 GPU对识别出的最优计划进行了评估，涵盖了广泛的LLMs和输入工作负载。我们表明，APEX可以找到最优执行计划，其端到端的服务延迟比启发式计划快达4.42倍。APEX还报告了LLM服务系统中使用的各种指标，如每输出令牌的时间和首次输出令牌的时间。此外，APEX可以在15分钟内使用CPU识别出最优的并行执行计划，这比在GPU集群上使用云服务的实际部署快71倍，成本效益高1234倍。APEX将在接受后开源。|
|**2024-11-26**|**On Limitations of LLM as Annotator for Low Resource Languages**|Suramya Jadhav et.al.|[2411.17637](http://arxiv.org/abs/2411.17637)|null|低资源语言由于缺乏足够的语言数据、资源和工具，在诸如有监督学习、注释和分类等任务中面临重大挑战。这种短缺阻碍了准确模型和数据集的发展，使得执行关键的自然语言处理（NLP）任务如情感分析或仇恨言论检测变得困难。为了弥补这一差距，大型语言模型（LLMs）为潜在注释者提供了机会，能够为这些代表性不足的语言生成数据集和资源。在本文中，我们专注于马拉地语，一种低资源语言，并评估封闭源和开源LLMs作为注释者的性能。我们评估了GPT-4o和Gemini 1.0 Pro、Gemma 2（2B和9B），以及Llama 3.1（8B）等模型在情感分析、新闻分类和仇恨言论检测等分类任务中的表现。我们的研究结果表明，虽然LLMs在英语等高资源语言的注释任务中表现出色，但在应用于马拉地语时仍显不足。即使是像Gemini和GPT这样的先进封闭模型，在与基于BERT的基线相比时也表现不佳，这突显了LLMs作为低资源语言注释者的局限性。|
|**2024-11-26**|**MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation**|Harsh Singh et.al.|[2411.17636](http://arxiv.org/abs/2411.17636)|null|大型语言模型（LLMs）在各个领域，包括机器人操作和导航，已经展示了卓越的规划能力。尽管最近在机器人领域的努力利用了LLMs进行高层次和低层次的规划，但这些方法经常面临显著的挑战，如长时序任务中的幻觉问题以及由于一次性生成计划而缺乏实时反馈导致的适应性限制。为了解决这些局限性，我们提出了一种新颖的多智能体LLM框架，称为多智能体大型语言模型用于操作（MALMM），该框架将高层次规划和低层次控制代码生成分配给专门化的LLM智能体，并由一个额外的智能体动态管理过渡。通过在每一步后纳入环境观察，我们的框架能够有效处理中间失败并实现自适应重规划。与现有方法不同，我们的方法不依赖于预训练的技能策略或上下文学习示例，并且能够泛化到各种新任务。我们在九个RLBench任务上评估了我们的方法，包括长时序任务，并证明了它能够在零样本设置下解决机器人操作问题，从而克服了现有基于LLM的操作方法的关键局限性。|
|**2024-11-25**|**Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?**|Sohee Yang et.al.|[2411.16679](http://arxiv.org/abs/2411.16679)|null|我们评估大型语言模型（LLMs）在回答多跳查询时潜在地回忆和组合事实的能力，例如“斯嘉丽·约翰逊出生那年，夏季奥运会是在哪个国家举办的”。一个主要的挑战是，这些模型可能已经通过遇到头部实体“斯嘉丽·约翰逊”和答案实体“美国”在同一训练序列中的方式形成了捷径，或者只是基于频率优先猜测答案。为了防止捷径形成，我们在预训练语料库中排除了头部实体和答案实体共同出现的测试查询。通过精心选择关系和事实，并系统地排除模型可能猜测答案或利用部分匹配的情况，我们构建了一个名为SOCRATES（无捷径潜伏推理）的评估数据集。观察表明，在不利用捷径的情况下，LLMs展示了在某些类型的查询上令人鼓舞的潜在多跳推理能力，但对于需要潜在回忆国家作为中间答案的查询，最佳模型达到了80%的潜在组合能力，而对于需要回忆年份的查询，这一比例仅降至5%。与链式思维组合能力的比较突显了模型在潜在推理与显式推理之间的显著差距。分析显示，在潜在组合能力较高的查询中，中间答案的潜在表示形式更常被构造，并揭示了在预训练期间潜在多跳推理的出现。|
|**2024-11-25**|**DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation**|Zun Wang et.al.|[2411.16657](http://arxiv.org/abs/2411.16657)|null|故事生成视频（SVG）最近成为了一项任务，旨在创建长、多动作、多场景的视频，这些视频能够一致地表现输入文本剧本中的故事。SVG在媒体和娱乐领域的多样化内容创作方面具有巨大潜力；然而，它也带来了显著的挑战：（1）对象必须表现出一系列精细复杂的运动，（2）多个对象需要在各个场景中保持一致性，以及（3）在单个场景内，主体可能需要进行多种动作，并且这些动作之间需要平滑过渡。为了解决这些挑战，我们提出了DreamRunner，一种新颖的故事到视频生成方法：首先，我们使用大型语言模型（LLM）对输入脚本进行结构化处理，以促进粗粒度场景规划以及细粒度对象级布局和运动规划。接下来，DreamRunner引入了检索增强的测试时适应技术，用于捕捉每个场景中对象的目标运动先验，支持基于检索视频的多样化运动定制，从而促进新视频中复杂、脚本化动作的生成。最后，我们提出了一种新颖的空间-时间区域基础的三维注意力和先验注入模块SR3AI，用于细粒度的对象-运动绑定和逐帧语义控制。我们将DreamRunner与各种SVG基线进行了比较，展示了其在角色一致性、文本对齐和平滑过渡方面的最先进性能。此外，DreamRunner在组合式文本到视频生成中的细粒度条件跟随能力上表现出色，显著优于基准测试集T2V-ComBench上的基线。最后，我们通过定性示例验证了DreamRunner在生成多对象交互方面的稳健能力。|
|**2024-11-25**|**Self-Generated Critiques Boost Reward Modeling for Language Models**|Yue Yu et.al.|[2411.16646](http://arxiv.org/abs/2411.16646)|null|奖励建模对于对齐大型语言模型（LLMs）与人类偏好至关重要，尤其是在基于人类反馈的强化学习（RLHF）中。然而，当前的奖励模型主要生成标量分数，并且难以结合自然语言格式的批评。我们假设预测批评和标量奖励可以提高奖励建模的能力。受此启发，我们提出了Critic-RM框架，该框架通过自生成批评来改进奖励模型，而无需额外的监督。Critic-RM采用两阶段过程：生成和过滤高质量批评，然后在奖励预测和批评生成上进行联合微调。实验结果表明，在基准测试中，Critic-RM相比标准奖励模型和LLM法官，提高了3.7%-7.3%的奖励建模准确性，展示了强大的性能和数据效率。进一步的研究还验证了生成批评在纠正错误推理步骤方面的有效性，使推理准确性提高了2.5%-3.2%。|
|**2024-11-25**|**Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective**|Jean Marie Tshimula et.al.|[2411.16642](http://arxiv.org/abs/2411.16642)|null|越狱提示在人工智能和网络安全领域构成重大威胁，因为它们被设计用来绕过大型语言模型中的伦理限制，可能使网络犯罪分子有机可乘。本文从网络安全的角度分析了越狱提示，探讨了诸如提示注入和上下文操纵等技术，这些技术允许生成有害内容、逃避内容过滤以及提取敏感信息。我们评估了成功越狱的影响，包括错误信息传播、自动化社会工程、有害内容生成（如生物武器和爆炸物）等方面。为了应对这些威胁，我们提出了包括高级提示分析、动态安全协议和持续的模型微调在内的策略，以增强人工智能的韧性。此外，我们强调了人工智能研究人员、网络安全专家和政策制定者之间合作的必要性，以制定保护人工智能系统的标准。通过案例研究，我们展示了这些网络安全方法，推动负责任的人工智能实践，以维护系统完整性和公众信任。**警告：本文包含读者可能感到反感的内容。**|
|**2024-11-25**|**Chat2SVG: Vector Graphics Generation with Large Language Models and Image Diffusion Models**|Ronghuan Wu et.al.|[2411.16602](http://arxiv.org/abs/2411.16602)|null|可缩放矢量图形（SVG）已成为数字设计中矢量图形的行业标准，提供了分辨率独立性和对单个元素的精确控制。尽管有这些优势，创建高质量的SVG内容仍然具有挑战性，因为它需要专业编辑软件的技术专业知识，并且需要大量时间来制作复杂的形状。最近的文本到SVG生成方法旨在使矢量图形创作更加易于访问，但它们在形状规则性、泛化能力和表现力方面仍面临限制。为了解决这些问题，我们引入了Chat2SVG，这是一种结合大型语言模型（LLM）和图像扩散模型优势的混合框架，用于文本到SVG生成。我们的方法首先使用LLM从基本几何原语生成语义上有意义的SVG模板。在图像扩散模型的引导下，双阶段优化流程在潜在空间中细化路径并调整点坐标以增强几何复杂性。广泛的实验表明，Chat2SVG在视觉保真度、路径规则性和语义对齐方面优于现有方法。此外，我们的系统通过自然语言指令实现直观编辑，使专业矢量图形创作对所有用户都易于访问。|
|**2024-11-25**|**From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge**|Dawei Li et.al.|[2411.16594](http://arxiv.org/abs/2411.16594)|**[link](https://github.com/llm-as-a-judge/awesome-llm-as-a-judge)**|**评估和评测一直是人工智能（AI）和自然语言处理（NLP）中的关键挑战。然而，传统的基于匹配或嵌入的方法往往难以判断微妙的属性并提供令人满意的结果。最近大型语言模型（LLMs）的发展启发了“LLM作为裁判”的范式，即利用LLMs在各种任务和应用中进行评分、排名或选择。本文对基于LLM的判断和评估进行了全面调查，提供了深入概述以推动这一新兴领域的发展。我们从输入和输出的角度给出了详细的定义。然后，我们引入了一个综合分类法，从三个维度探索LLM作为裁判的方式：评判什么、如何评判以及在哪里评判。最后，我们编制了基准来评估LLM作为裁判的表现，并强调了关键挑战和有前景的方向，旨在为该领域的未来研究提供有价值的见解和灵感。关于LLM作为裁判的论文列表和其他资源可以在\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge}和\url{https://llm-as-a-judge.github.io}找到。**|
|**2024-11-25**|**Large Language Model-based Decision-making for COLREGs and the Control of Autonomous Surface Vehicles**|Klinsmann Agyei et.al.|[2411.16587](http://arxiv.org/abs/2411.16587)|**[link](https://github.com/Psarhadi/Autonomous_ship_planning_collision_avoidance_control)**|在自主水面船舶（ASVs）领域，为解决主要针对人类操作员制定的海上碰撞规则（COLREGs）下的决策和避障问题一直是一个紧迫的挑战。最近，在可解释人工智能（AI）和机器学习方面的进展显示了实现类似人类决策的可能性。特别是大型语言模型（LLMs）在复杂系统（如自动驾驶汽车）决策中的应用取得了显著发展。COLREGs从算法角度来看具有一定的文本性和模糊性，这正好符合LLMs的能力，表明LLMs可能很快就会成为这一应用领域的合适选择。本文介绍了并展示了基于LLM的决策和控制在ASVs上的首次应用。所提出的方法建立了一个高级决策者，它使用在线碰撞风险指标和关键测量数据来做出安全操作的决策。为了支持在一个现实的ASV模型上进行训练和实时行动生成，开发了一种定制的设计和运行结构。局部规划和控制算法被集成以执行航点跟随和碰撞避免的命令。据作者所知，这项研究代表了首次尝试利用可解释的人工智能解决承认COLREGs规则的海上系统的动态控制问题，为这一具有挑战性的领域开辟了新的研究途径。在多个测试场景中获得的结果证明了该系统保持在线COLREGs合规性、准确的航点跟踪和可行控制的能力，同时为每个决策提供了人类可理解的推理。|
|**2024-11-25**|**MarketGPT: Developing a Pre-trained transformer (GPT) for Modeling Financial Time Series**|Aaron Wheeler et.al.|[2411.16585](http://arxiv.org/abs/2411.16585)|**[link](https://github.com/aaron-wheeler/marketgpt)**|本文介绍了一种用于建模金融时间序列的生成预训练变压器（GPT）。该GPT作为离散事件模拟器中的订单生成引擎，能够真实地复制限价订单簿的动态。我们的模型利用了大型语言模型的最新进展，以流式方式生成长序列的订单消息。结果表明，即使初始订单流提示不再存在于模型的上下文窗口中，该模型也能成功再现订单流数据的关键特征。此外，评估显示该模型捕捉了几种金融市场的统计特性，或“风格化事实”，以及更广泛的宏观尺度数据分布的特性。总体而言，这项工作标志着创建高保真、交互式市场模拟的重要一步。|
|**2024-11-25**|**Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision**|Zhiheng Xi et.al.|[2411.16579](http://arxiv.org/abs/2411.16579)|null|训练大型语言模型（LLMs）以在回应前花费更多时间进行思考和反思对于有效解决科学、编程和数学等领域的复杂推理任务至关重要。然而，自我反思和自我校正机制的有效性取决于模型准确评估自身表现的能力，这一能力可能受到初始准确性、问题难度以及缺乏外部反馈等因素的限制。本文探讨了一种将推理和评价角色分开的双玩家范式，其中评价模型在测试时和训练时都提供逐级反馈来监督推理（演员）模型。我们首先提出了AutoMathCritique，这是一种自动化且可扩展的框架，用于收集评价数据，从而生成包含76,321个配对响应和逐级反馈的数据集。通过使用该数据集微调语言模型，使其能够生成数学推理的自然语言反馈。我们证明，评价模型在测试时能持续提高演员模型在困难查询上的表现，尤其是在增加推理时间计算的情况下。受这些发现的启发，我们将基于评价的监督引入演员模型的自训练过程中，并提出了一种基于评价的自我改进方法。实验表明，该方法提高了演员模型的探索效率和解决方案多样性，特别是在具有挑战性的查询上，从而产生更强大的推理模型。最后，我们初步探讨了通过评价监督训练自我对话推理模型，并展示了其潜力。我们的代码和数据集位于\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}。|
|**2024-11-25**|**Predictive Power of LLMs in Financial Markets**|Jerick Shi et.al.|[2411.16569](http://arxiv.org/abs/2411.16569)|null|预测股票市场和其他资产的走势在过去几十年中一直具有重要价值。了解某一特定行业市场的未来走势可以为投资者提供大量信息，他们利用这些信息来制定策略以最大化利润或最小化风险。然而，市场数据非常嘈杂，选择合适的数据或合适的模型来进行此类预测颇具挑战性。随着大型语言模型的兴起，现在有更有效的方法来分析某些数据。我们的目标是确定GPT模型是否比其他传统的Transformer模型（如BERT模型）提供了更有用的信息。我们将使用来自联邦储备局“褐皮书”的数据，该报告提供了美国不同地区经济状况的总结。使用这些数据，我们随后利用LLM进行预测相关性的分析。通过这些相关性，我们然后将结果与众所周知的策略进行比较，以确定了解经济状况是否能改善投资决策。我们得出结论，褐皮书确实包含了不同资产之间的相关性信息，但GPT模型存在过多的前瞻偏差，传统模型仍然占据优势。|
|**2024-11-22**|**Measuring Bullshit in the Language Games played by ChatGPT**|Alessandro Trevisan et.al.|[2411.15129](http://arxiv.org/abs/2411.15129)|null|生成式大型语言模型（LLMs），它们创建的文本没有直接对应于真值的关系，被广泛认为与法兰克福在其著名的小册子《关于胡说八道》中描述的语言使用方式相似。在本文中，我们对这一主题进行严谨的研究，探讨这一现象是如何出现的以及如何进行分析。我们详细阐述这一论点，提出基于LLM的聊天机器人玩的是“胡说八道的语言游戏”。我们使用统计文本分析来探讨这种维特根斯坦式的语言游戏的特点，基于一个对比了1000篇科学出版物和典型伪科学文本的数据集，这些文本由ChatGPT生成。然后，我们探索在同一语言特征是否可以在两个著名的社会功能失调背景下被检测到：乔治·奥威尔对政治和语言的批判，以及大卫·格雷伯对胡言乱语工作的描述。通过简单的假设检验方法，我们证明了一个胡言乱语语言的统计模型可以可靠地将法兰克福提出的ChatGPT中的人工胡言乱语与自然人类语言中观察到的政治和工作场所功能上的胡言乱语联系起来。|
|**2024-11-22**|**AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution**|Fengyuan Liu et.al.|[2411.15102](http://arxiv.org/abs/2411.15102)|**[link](https://github.com/r-three/AttriBoT)**|**大语言模型（LLMs）的行为受到上下文输入的影响，这促使了上下文归属方法的发展，旨在量化每个上下文片段对LLMs生成结果的影响。留一法（LOO）误差通过测量移除给定上下文片段后LLM响应的可能性变化来提供一种计算上下文归属的方法，但这种方法对于大型模型来说可能过于昂贵。在这项工作中，我们引入AttriBoT，这是一种高效计算LOO误差近似值的新技术系列，用于上下文归属。具体而言，AttriBoT使用缓存激活以避免重复操作，进行分层归属以减少计算量，并模拟大型目标模型的行为以使用较小的代理模型。综合来看，AttriBoT可以提供超过300倍的速度提升，同时相比先前的上下文归属方法更忠实地反映目标模型的LOO误差。这种显著的性能提升使得计算给定响应的上下文归属比生成响应本身快30倍，从而支持需要大规模计算归属的实际应用。我们发布了用户友好且高效的AttriBoT实现，以促进LLM的可解释性，并鼓励未来开发高效的上下文归属方法。**|
|**2024-11-22**|**XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**|Yixin Dong et.al.|[2411.15100](http://arxiv.org/abs/2411.15100)|null|LLM代理的应用变得越来越复杂和多样化，导致对可以解析为代码、结构化函数调用和具身代理命令的结构化输出的需求日益增加。这些发展带来了在LLM推理中进行结构化生成的重大需求。上下文无关语法是一种通过受限解码实现结构化生成的灵活方法。然而，执行上下文无关语法需要在运行时遍历词汇表中的所有标记状态，这给结构化生成带来了不可忽视的开销。在本文中，我们提出XGrammar，这是一种针对大型语言模型的灵活且高效的结构生成引擎。XGrammar通过将词汇表划分为可以在预检查阶段处理的上下文无关标记和在运行时需要解释的上下文相关标记，从而加速上下文无关语法的执行。我们进一步构建转换以扩展语法上下文并减少上下文无关标记的数量。此外，我们建立了一个高效的持久堆栈以加速上下文相关标记的检查。最后，我们将语法引擎与LLM推理引擎协同设计，以重叠语法计算与GPU执行。评估结果显示，XGrammar相比现有解决方案可达到高达100倍的速度提升。结合LLM推理引擎，它可以在端到端的低LLM服务中实现接近零开销的结构化生成。|
|**2024-11-22**|**mR $^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA**|Tao Zhang et.al.|[2411.15041](http://arxiv.org/abs/2411.15041)|null|先进的多模态大型语言模型（MLLMs）在最近的知识型视觉问答任务，如INFOSEEK和Encyclopedic-VQA中，由于其知识范围有限且固定，往往导致回答模糊和不准确。因此，多模态检索增强生成（mRAG）自然被引入以提供全面和最新的知识，从而有效扩展知识范围。然而，当前的mRAG方法存在一些固有的缺点，包括：1）即使不需要外部知识时也执行检索；2）缺乏识别支持查询的证据的能力；3）由于额外的信息过滤模块或规则增加了模型复杂性。为了解决这些不足，我们提出了一种新的通用框架，称为多模态检索-反思-增强生成（mR$^2$AG），该框架通过两个易于实施的反思操作实现自适应检索和有用信息定位，从而避免高模型复杂性。在mR$^2$AG中，检索反思旨在区分不同的用户查询并避免冗余的检索调用，相关性反思则引导MLLM定位所检索内容中的有益证据，并相应地生成答案。此外，mR$^2$AG可以集成到任何训练良好的MLLM中，并通过在提出的mR$^2$AG指令调优数据集（mR$^2$AG-IT）上进行高效的微调来实现。mR$^2$ AG在INFOSEEK和Encyclopedic-VQA上显著优于最先进的MLLMs（例如，GPT-4v/o）和基于RAG的MLLMs，同时保持了基础MLLM在各种视觉依赖任务中的卓越能力。|
|**2024-11-22**|**One to rule them all: natural language to bind communication, perception and action**|Simone Colombani et.al.|[2411.15033](http://arxiv.org/abs/2411.15033)|null|近年来，人机交互领域的研究集中在开发能够理解复杂人类指令并在动态和多样化环境中执行任务的机器人系统。这些系统在从个人助理到工业机器人等多个应用领域中都具有重要意义，强调了机器人与人类灵活、自然和安全互动的重要性。本文提出了一种先进的机器人动作规划架构，该架构集成了通信、感知和规划，并结合了大型语言模型（LLM）。我们的系统旨在将用自然语言表达的命令转化为可执行的机器人动作，同时整合环境信息，并根据实时反馈动态更新计划。规划模块是系统的核心部分，在这里使用嵌入修改后的ReAct框架的LLM来解释和执行用户命令。通过利用其广泛的预训练知识，LLM能够有效地处理用户请求，而无需引入关于变化环境的新知识。修改后的ReAct框架进一步扩展了执行空间，提供了实时环境感知和物理行动的结果。通过结合强大的动态语义地图表示作为图形与控制组件和故障解释相结合，该架构增强了机器人在共享和动态环境中的适应性、任务执行和无缝协作能力。通过集成持续的环境反馈回路，系统可以动态调整计划以适应意外变化，优化机器人完成任务的能力。利用以往经验的数据集可以提供有关失败的详细反馈，更新LLM的下一次迭代上下文并提出克服问题的建议。|
|**2024-11-22**|**Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot**|Simone Colombani et.al.|[2411.15027](http://arxiv.org/abs/2411.15027)|null|机器人越来越多地被应用于动态环境，如工作场所、医院和家庭。因此，与机器人的互动必须简单直观，并且机器人的感知需要高效适应人类引起的变化。本文提出了一种机器人控制架构，旨在解决人机交互中的关键挑战，特别关注机器人状态表示的动态创建和持续更新。该架构使用大型语言模型（LLM）整合多种信息源，包括自然语言命令、机器人技能表示以及对感知场景的实时动态语义映射。这使得机器人能够在复杂、动态的环境中表现出灵活和适应性。传统的机器人系统通常依赖静态、预先编程的指令和设置，限制了其在动态环境和实时协作中的适应性。相比之下，本架构利用LLM解释复杂的高层次指令并生成可执行计划，从而增强人机协作。该系统的感知模块利用RGB-D传感器数据生成并持续更新语义场景图，提供详细且结构化的环境表示。粒子滤波器用于确保在动态现实世界环境中准确的对象定位。规划模块利用最新的语义地图将高层次任务分解为子任务，并将其与导航、物体操作（例如拾取和放置）和移动（例如前往某处）等机器人技能联系起来。通过结合实时感知、状态跟踪以及由LLM驱动的通信和任务规划，该架构增强了在动态环境下的适应性、任务效率和人机协作。|
|**2024-11-22**|**DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models**|Keda Tao et.al.|[2411.15024](http://arxiv.org/abs/2411.15024)|**[link](https://github.com/kd-tao/dycoke)**|视频大型语言模型（VLLMs）在处理复杂的视频内容方面取得了显著进步，但其推理效率仍然受限于从视频输入生成的数千个视觉标记所带来的高计算成本。我们观察到，与单图像输入不同，VLLMs通常在不同的解码迭代中从不同的帧中关注视觉标记，这使得一次性修剪策略容易错误地移除重要标记。基于此观察，我们提出了DyCoke，这是一种无需训练的标记压缩方法，旨在优化标记表示并加速VLLMs。DyCoke引入了一个即插即用的时间压缩模块，通过跨帧合并冗余标记来最小化时间冗余，并应用动态键值缓存减少来选择性地修剪空间冗余标记。它通过在每个解码步骤中动态保留关键标记来确保高质量的推理。广泛的实验结果表明，DyCoke能够超越先前的最先进方法，在不降低性能的情况下实现1.5倍的推理速度提升和1.4倍的内存减少，同时仍能提高性能，且无需进行额外训练。|
|**2024-11-22**|**FTA generation using GenAI with an Autonomy sensor Usecase**|Sneha Sudhir Shetiya et.al.|[2411.15007](http://arxiv.org/abs/2411.15007)|null|功能安全在系统设计中占有重要地位。其在汽车行业的重视程度多年来显著提升。迄今为止，已经开发出多种方法以适应各种与自动驾驶相关的场景和功能的故障树分析（FTA）。本文尝试探索使用生成式人工智能（GenAI）进行故障树分析（FTA）的可能性，并重点关注激光雷达传感器的故障情况。我们探讨了各种可用的开源大型语言模型（LLM），并深入研究其中一个模型以研究其响应并提供我们的分析。本文成功展示了通过提示工程训练现有大型语言模型以进行任何自动驾驶用例的故障树分析的可能性，并借助PlantUML工具辅助实现。|
|**2024-11-22**|**ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**|Junhong Shen et.al.|[2411.15004](http://arxiv.org/abs/2411.15004)|**[link](https://github.com/colonylabs/ScribeAgent)**|**大型语言模型（LLM）代理正在迅速进步，以处理日益复杂的网络任务。大多数这些代理依赖于通用的专有模型，如GPT-4，并专注于设计更好的提示来提升其规划能力。然而，通用的LLM并未专门训练以理解专业的网页上下文，例如HTML，并且它们通常在长期规划方面存在困难。我们探索了一种替代方法，即使用从超过250个领域收集的生产规模工作流数据（对应60亿个令牌）对开源LLM进行微调。这一简单而有效的方法在现有的基准测试上显著优于基于提示的代理——ScribeAgent在Mind2Web上的直接生成性能达到最先进水平，并在WebArena上将前最佳纯文本网络代理的任务成功率提高了14.1%。我们进一步对各种微调设计选择进行了详细的消融研究，并提供了关于LLM选择、训练配方、上下文窗口优化以及数据集大小影响的见解。**|
|**2024-11-22**|**Generative AI may backfire for counterspeech**|Dominik Bär et.al.|[2411.14986](http://arxiv.org/abs/2411.14986)|null|在线仇恨言论对个人福祉和社会凝聚力构成了严重威胁。一个有前景的解决方案是使用反击言论来遏制在线仇恨言论。反击言论旨在通过直接回复促使用户重新考虑其仇恨性帖子。然而，当前的方法由于需要人工干预而缺乏可扩展性，或者无法适应帖子的具体上下文。一种可能的补救措施是使用生成式人工智能，特别是大型语言模型（LLMs），来撰写定制的反击言论。在本文中，我们分析了由最先进的LLMs生成的语境化反击言论是否能够有效遏制在线仇恨言论。为此，我们在社交媒体平台Twitter/X上进行了一项大规模、预先注册的现场实验（N=2,664）。我们的实验采用了2x2被试间设计，并且还有一个没有反击言论的对照条件。一方面，发布仇恨内容的用户被随机分配接收（a）语境化的反击言论或（b）非语境化的反击言论。前者通过LLMs生成，而后者依赖于预定义的通用消息。另一方面，我们测试了两种反击策略：（a）促进同理心和（b）警告关于网络不当行为的后果。然后，我们测量了用户是否删除了最初的仇恨帖子，以及他们的行为在反击言论干预后是否发生了变化（例如，用户是否采用了更少的有毒语言）。研究发现，采用警告后果策略的非语境化反击言论显著减少了在线仇恨言论。然而，由LLMs生成的语境化反击言论证明是无效的，甚至可能会适得其反。|
|**2024-11-21**|**Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models**|Yuhao Dong et.al.|[2411.14432](http://arxiv.org/abs/2411.14432)|**[link](https://github.com/dongyh20/insight-v)**|**大型语言模型（LLMs）通过更多的推理展示出增强的能力和可靠性，从链式思维提示发展到像OpenAI o1这样的产品级解决方案。尽管有各种努力来改进LLM的推理能力，但在视觉-语言任务中，高质量的长链推理数据和优化的训练管道仍然探索不足。在本文中，我们提出了Insight-V，这是一个早期的努力，旨在1）规模化生成复杂多模态任务所需的长且稳健的推理数据，以及2）一个有效的训练管道以增强多模态大语言模型（MLLMs）的推理能力。具体来说，为了在没有人工劳动的情况下生成足够长且多样的推理路径，并确保数据质量，我们设计了一个具有渐进策略的两步管道和一种多粒度评估方法。我们观察到，直接用这种长且复杂的推理数据监督MLLMs并不能获得理想的推理能力。为了解决这个问题，我们设计了一个多智能体系统，该系统包括一个专注于执行长链推理的推理智能体和一个经过训练以判断并总结推理结果的摘要智能体。我们进一步结合了迭代DPO算法以增强推理智能体的生成稳定性和质量。基于流行的LLaVA-NeXT模型和我们更强大的基础MLLM，我们在需要视觉推理的挑战性多模态基准测试中展示了显著的性能提升。受益于我们的多智能体系统，Insight-V也能轻松保持或提高对感知导向的多模态任务的性能。**|
|**2024-11-21**|**Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding**|Yiming Zhang et.al.|[2411.14401](http://arxiv.org/abs/2411.14401)|null|近年来，多模态大型语言模型（MLLMs）的发展为视频理解开辟了新的途径。然而，在零样本视频任务中实现高保真度仍然具有挑战性。传统的视频处理方法严重依赖于微调以捕捉细微的空间-时间细节，这需要大量的数据和计算成本。相比之下，虽然无训练的方法效率较高，但通常缺乏在复杂视频内容中保持丰富上下文特征的鲁棒性。为此，我们提出了一种名为DYTO的新型动态令牌合并框架，用于零样本视频理解，该框架能够自适应地优化令牌效率，同时保留关键场景细节。DYTO结合了分层帧选择和二部图令牌合并策略，动态聚类关键帧并有选择地压缩令牌序列，从而在计算效率和语义丰富性之间取得平衡。广泛的实验结果表明，DYTO在多个基准测试中的表现优于微调和无训练方法，并为零样本视频理解设定了新的最先进水平。|
|**2024-11-21**|**Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings**|Aaron Zheng et.al.|[2411.14398](http://arxiv.org/abs/2411.14398)|null|随着大型语言模型（LLMs）的普及，企业能够快速开发概念验证和原型。因此，对实施稳健的护栏以监控、量化和控制LLM的行为的需求日益增长，确保使用过程可靠、安全、准确，并符合用户期望。以前的方法，如通过微调现有LLM来过滤不适当的用户提示或系统输出，例如LlamaGuard和OpenAI的MOD API，已经取得了显著的成功。然而，使用微调后的LLM作为护栏会引入更高的延迟和更高的维护成本，这可能对于成本效益部署来说并不实用或不可扩展。我们采取不同的方法，专注于微调一个轻量级架构：Sentence-BERT。这种方法将模型大小从LlamaGuard的70亿参数减少到大约6700万参数，同时在AEGIS安全基准上保持相当的性能。|
|**2024-11-21**|**UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages**|Bethel Melesse Tessema et.al.|[2411.14343](http://arxiv.org/abs/2411.14343)|**[link](https://github.com/bethelmelesse/unifiedcrawl)**|**大型语言模型（LLMs）在低资源语言上表现不佳，原因是训练数据有限。我们提出了一种方法，可以从整个Common Crawl语料库中高效收集文本数据。我们的方法，UnifiedCrawl，使用最少的计算资源对Common Crawl进行过滤和提取，生成的单语数据集比之前可用的源要大得多。我们证明，利用这些数据通过高效的适配器方法（QLoRA）微调多语言LLMs可以显著提升低资源语言的表现，同时最小化VRAM的使用。我们的实验显示，在语言建模困惑度和少量样本提示得分方面有大幅提高。我们的工作和发布的源代码提供了一种使用消费者硬件改善低资源语言LLMs的经济实惠的方法。我们的源代码可以在https://github.com/bethelmelesse/unifiedcrawl获取。**|
|**2024-11-21**|**Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training**|Zheheng Luo et.al.|[2411.14318](http://arxiv.org/abs/2411.14318)|null|众所周知，多样化的语料库对于训练大型语言模型至关重要，这些模型通常由多种不同领域的数据混合而成。一般来说，以往的努力主要集中在从不同领域以静态比例采样训练数据，以及在训练过程中调整数据比例。然而，很少有方法解决领域自适应连续预训练的复杂性。为填补这一空白，我们提出了Velocitune，这是一种新颖的框架，能够动态评估学习速度并相应地调整数据比例，从而更倾向于较慢学习的领域而避免较快学习的领域。该框架通过缩放律指导，以较低的成本指示每个领域所需的理想学习目标。为了评估Velocitune的有效性，我们在一个以推理为重点的数据集上使用CodeLlama进行了实验，并在专门用于系统命令生成的语料库上使用了Llama3和Mistral进行了实验。Velocitune在这两个数学和代码推理任务以及命令行生成基准测试中均取得了性能提升。进一步分析表明，推动Velocitune有效性的关键因素包括目标损失预测和数据排序。|
|**2024-11-21**|**Automated Generation of Code Debugging Exercises**|Victor-Alexandru Pădurean et.al.|[2411.14303](http://arxiv.org/abs/2411.14303)|null|调试是学习编程时的一项重要技能，但在入门课程中的教学和强调程度却各不相同。在代码生成大型语言模型（LLM）的时代，学生能够推理代码并识别错误的能力变得越来越重要。然而，学生们经常采用试错的方法来解决错误，而没有充分理解潜在的问题。培养识别和假设错误原因的能力至关重要，但通过传统方法有效教授这一能力可能会耗费大量时间。本文介绍了一款名为BugSpotter的创新工具，该工具利用LLM根据问题描述生成有错误的代码，并通过测试套件验证合成的错误。学生通过设计失败的测试用例与BugSpotter互动，在这些测试用例中，有错误代码的输出与问题说明中定义的预期结果不同。这不仅为学生提供了提高调试技能的机会，还让他们有机会练习阅读和理解问题说明。我们在一个大型课堂环境中部署了BugSpotter，并将其生成的调试练习与同一问题的手动创建练习进行了比较。我们发现，由BugSpotter生成的LLM练习在难度上有所变化，并且很好地符合问题说明。重要的是，LLM生成的练习在学生表现方面与教师手动创建的练习相当，这表明BugSpotter可能是一种有效且高效的辅助学习调试的工具。|
|**2024-11-21**|**Auto-SPICE: Leveraging LLMs for Dataset Creation via Automated SPICE Netlist Extraction from Analog Circuit Diagrams**|Jitendra Bhandari et.al.|[2411.14299](http://arxiv.org/abs/2411.14299)|**[link](https://github.com/jitendra-bhandari/auto-spice)**|Auto-SPICE是首个完全自动化的框架，利用大型语言模型（LLMs）生成侧重于集成电路仿真的模拟程序（SPICE）网表。它解决了在电路设计自动化中长期存在的挑战，即自动生成模拟电路的网表。自动化这一流程可以加速针对模拟电路设计和验证的定制化LLM的创建。我们确定了这一自动化中的关键挑战，并评估了最先进的多模态LLM，特别是GPT-4，以解决这些问题。我们提出了一种三步工作流程来克服当前的限制：标注模拟电路、提示调优和网表验证。这种方法旨在从电路原理图图像创建端到端的SPICE网表生成器，从而解决准确生成网表这一长期存在的难题。我们的框架在大约2,100个不同复杂度的原理图上进行了测试，展示了显著的性能提升。我们将此解决方案开源，以促进社区驱动的发展。|
|**2024-11-21**|**Efficient Aspect-Based Summarization of Climate Change Reports with Small Language Models**|Iacopo Ghinassi et.al.|[2411.14272](http://arxiv.org/abs/2411.14272)|**[link](https://github.com/ighina/llmclimate2024)**|**自然语言处理（NLP）在帮助决策者应对气候变化行动方面最近被强调为与更广泛的NLP技术用于社会公益的驱动方向一致。在此背景下，基于方面摘要（ABS）系统尤其有用，因为它们为利益相关者提供了一种方便的方式来查找专家编写的报告中的相关信息。在这项工作中，我们发布了一个新的气候改变报告的ABS数据集，并使用不同的大型语言模型（LLMs）和所谓的小型语言模型（SLMs）以无监督的方式解决这个问题。考虑到所面临的问题，我们还展示了小型语言模型在这个问题上并不明显逊色，同时还能减少碳足迹；我们通过首次应用一个现有的框架来评估零样本生成模型的能效和任务性能来进行这一展示。总体而言，我们的结果表明现代语言模型，无论是大是小，都能有效地解决气候改变报告的ABS问题，但在我们将问题构架为检索增强生成（RAG）问题时还需要更多的研究。我们的工作和数据集将有助于促进这一方向的努力。**|
|**2024-11-21**|**Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective**|Ernests Lavrinovics et.al.|[2411.14258](http://arxiv.org/abs/2411.14258)|null|大型语言模型（LLMs）在自然语言处理（NLP）应用领域，如自动文本生成、问答和聊天机器人等方面取得了革命性的进展。然而，它们面临一个重要的挑战：幻觉现象，即模型生成听起来合理但事实上不正确的响应。这会损害信任并限制LLMs在不同领域的适用性。另一方面，知识图谱（KGs）提供了结构化的事实集合，以实体（节点）及其关系（边）的形式表示。最近的研究表明，通过提供可以填补LLM对某些主题理解空白的上下文，KGs能够成为一个有前景的方法来减轻LLMs中的幻觉现象，从而提高其可靠性和准确性，同时利用其广泛的应用能力。尽管如此，这仍然是一个非常活跃的研究领域，存在许多未解决的开放问题。在本文中，我们将讨论这些开放挑战，并涵盖最新的数据集和基准测试方法，以及知识集成和评估幻觉现象的方法。在我们的讨论中，我们考虑了当前LLM系统中KG的使用情况，并确定了每个挑战中的未来方向。|
|**2024-11-21**|**Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models**|Javier Ferrando et.al.|[2411.14257](http://arxiv.org/abs/2411.14257)|null|大语言模型中的幻觉是一个普遍存在的问题，然而导致模型产生幻觉的机制尚不明确，这限制了我们解决这一问题的能力。通过使用稀疏自动编码器作为可解释性工具，我们发现这些机制的一个关键部分是实体识别，即模型检测它是否能回忆起关于某个实体的事实。稀疏自动编码器揭示了表示空间中有意义的方向，这些方向可以检测模型是否认识一个实体，例如检测它是否不了解某个运动员或电影。这表明模型可以具有自我知识：关于其自身能力的内部表征。这些方向具有因果关系：能够引导模型拒绝回答关于已知实体的问题，或者在本应拒绝回答时对未知实体进行属性幻觉。我们证明，尽管稀疏自动编码器是在基础模型上训练的，但这些方向对聊天模型的拒绝行为有因果影响，这表明聊天微调重新利用了这种现有的机制。此外，我们初步探索了这些方向在模型中的机制作用，发现它们会干扰下游头通常将实体属性传递到最终标记的注意力。|
|**2024-11-20**|**SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs**|Shirley Kokane et.al.|[2411.13547](http://arxiv.org/abs/2411.13547)|null|评估大型语言模型（LLMs）的输出是构建高性能复合AI系统的关键方面。由于LLM的输出会传播到下游步骤，识别LLM错误对于系统性能至关重要。在AI系统中，LLMs常见的任务之一是工具使用。虽然有许多基准环境用于评估LLMs在这项任务上的表现，但它们通常只提供成功率而没有解释失败案例的原因。为了解决这个问题，我们引入了SpecTool，这是一个新的基准，旨在识别LLM输出在工具使用任务中的错误模式。我们的数据集包括来自不同环境的查询，可用于测试七种新识别的错误模式的存在。通过使用SpecTool，我们展示了即使是当前最突出的LLMs也会在其输出中表现出这些错误模式。研究人员可以利用SpecTool的分析和见解来指导他们的错误缓解策略。|
|**2024-11-20**|**BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games**|Davide Paglieri et.al.|[2411.13543](http://arxiv.org/abs/2411.13543)|null|大型语言模型（LLMs）和视觉语言模型（VLMs）拥有广泛的知识并展现出有前景的推理能力，但在复杂动态环境中的表现仍不尽如人意。现实世界任务需要处理复杂的交互、高级空间推理、长期规划以及持续探索新策略——这些领域中我们缺乏有效的评估方法来全面衡量这些能力。为解决这一差距，我们引入了BALROG，这是一个旨在通过一系列具有挑战性的游戏评估LLMs和VLMs主动能力的新基准。我们的基准包括了一系列具有不同难度级别的现有强化学习环境，从非专家人类能在几秒钟内解决的任务到可能需要数年才能掌握的极其困难的任务（例如，NetHack学习环境）。我们设计了细粒度的指标来衡量性能，并对几种流行的开源和闭源LLMs和VLMs进行了广泛的评估。我们的研究结果表明，虽然当前的模型在较简单的游戏中部分成功，但它们在更困难的任务上遇到了显著的挑战。值得注意的是，我们观察到基于视觉决策方面存在严重的不足，因为当提供环境的视觉表示时，模型的表现变得更差。我们将BALROG作为开放且用户友好的基准发布，以促进未来在主动社区中的研究和发展。|
|**2024-11-20**|**Metacognition for Unknown Situations and Environments (MUSE)**|Rodolfo Valiente et.al.|[2411.13537](http://arxiv.org/abs/2411.13537)|null|元认知——即对自身认知过程的意识和调控——对于人类在未知情况下的适应性至关重要。相比之下，当前的自主代理在新环境中常常面临困难，因为它们的适应能力有限。我们假设，元认知是适应性自主系统中一个关键的缺失要素，它赋予系统应对未知挑战所需的认知灵活性。鉴于元认知能力的广泛性，我们重点关注两个关键方面：能力意识和针对新任务的策略选择。为此，我们提出了“未知情况与环境的元认知框架”（MUSE），该框架将元认知过程——特别是自我意识和自我调节——整合到自主代理中。我们介绍了MUSE的两种初始实现方式：一种基于世界建模，另一种利用大型语言模型（LLM），这两种方法都实现了元认知循环。我们的系统持续学习评估其在特定任务中的能力，并利用这种自我意识来指导策略选择的迭代循环。与基于Dreamer-v3的强化学习方法和纯粹基于提示的LLM代理方法相比，MUSE代理在自我意识和自我调节方面表现出显著改进，使它们能够更有效地解决新颖、分布外的任务。这项工作强调了受认知和神经系统的启发的方法在使自主系统适应新环境方面的潜力，克服了当前依赖大量训练数据的方法的局限性。|
|**2024-11-20**|**Advancing Complex Medical Communication in Arabic with Sporo AraSum: Surpassing Existing Large Language Models**|Chanseo Lee et.al.|[2411.13518](http://arxiv.org/abs/2411.13518)|null|不断增加的多语言需求在医疗保健领域凸显了对擅长处理多种语言的AI模型的需求，特别是在临床文档和决策方面。阿拉伯语因其复杂的形态学、句法和双语现象，在医学背景下给自然语言处理（NLP）带来了独特的挑战。本案例研究评估了Sporo AraSum，一种专为阿拉伯语临床文档设计的语言模型，与JAIS，目前领先的阿拉伯语NLP模型的表现。研究使用了合成数据集，并修改了PDQI-9指标以适应评估不同语言的模型性能。研究重点评估了模型在总结患者与医生互动方面的表现，包括准确性、全面性、临床实用性和语言文化能力。结果显示，Sporo AraSum在AI中心化的定量指标以及我们修改版PDQI-9所测量的所有定性属性上显著优于JAIS。AraSum的架构实现了精确且具有文化敏感性的文档记录，解决了阿拉伯语的语言细微差别，同时减少了AI幻觉的风险。这些发现表明，Sporo AraSum更适合满足阿拉伯语医疗环境的需求，为多语言临床工作流程提供了一种变革性解决方案。未来的研究应纳入真实世界的数据，以进一步验证这些发现并探索更广泛地整合到医疗系统中的可能性。|
|**2024-11-20**|**Disentangling Memory and Reasoning Ability in Large Language Models**|Mingyu Jin et.al.|[2411.13504](http://arxiv.org/abs/2411.13504)|**[link](https://github.com/mingyuj666/disentangling-memory-and-reasoning)**|**大型语言模型（LLMs）在处理需要广泛知识和推理能力的复杂任务时表现出色。然而，现有的LLM推理管道作为一个不透明的过程运行，没有明确区分知识检索和推理步骤，这使得模型的决策过程模糊且杂乱无章。这种模糊性可能导致诸如幻觉和知识遗忘等问题，这些问题显著影响了LLM在高风险领域的可靠性。在这篇论文中，我们提出了一种新的推理范式，将复杂的推理过程分解为两个明确且清晰的动作：(1)记忆回溯：检索相关知识；(2)推理：基于检索到的知识进行逻辑步骤。为了促进这种分解，我们引入了两个特殊标记——memory和reason，引导模型区分需要知识检索的步骤和涉及推理的步骤。实验结果表明，这种分解不仅提高了模型性能，还增强了推理过程的可解释性，使用户能够识别错误来源并有效改进模型响应。代码可在https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning获取。**|
|**2024-11-20**|**Utilizing Large Language Models to Synthesize Product Desirability Datasets**|John D. Hastings et.al.|[2411.13485](http://arxiv.org/abs/2411.13485)|null|本研究探讨了大型语言模型（LLMs）在生成合成数据集以用于产品可取性工具包（PDT）测试中的应用，这是评估用户情绪和产品体验的关键组成部分。使用gpt-4o-mini，这是一种成本效益高的大型商用LLM替代方案，分别采用了Word+Review、Review+Word和Supply-Word三种方法各生成了1000篇产品评论。生成的数据集从情感一致性、文本多样性以及数据生成成本方面进行了评估。结果显示，所有方法在情感一致性方面表现良好，皮尔逊相关系数在0.93到0.97之间。Supply-Word在多样性及PDT术语覆盖率方面表现最佳，但生成成本也较高。尽管存在轻微的积极情感偏向，在测试数据有限的情况下，LLM生成的合成数据具有显著优势，包括可扩展性、成本节约以及数据集生产的灵活性。|
|**2024-11-20**|**PatentEdits: Framing Patent Novelty as Textual Entailment**|Ryan Lee et.al.|[2411.13477](http://arxiv.org/abs/2411.13477)|null|专利必须被认为具有新颖性和非显而易见性才能被美国专利商标局（USPTO）授予。如果不符合这些条件，美国专利审查员会引用先前的技术或现有技术来质疑其新颖性，并发出非最终驳回通知。预测在现有技术下需要修改的发明权利要求是确保发明权利的重要且关键步骤，但此前尚未将其作为可学习任务进行研究。在这项工作中，我们介绍了PatentEdits数据集，其中包含105,000个成功的修订示例，这些修订克服了对新颖性的异议。我们设计了算法逐句标注这些编辑，然后探讨了大型语言模型（LLMs）在预测这些编辑方面的表现。我们证明，评估引用参考文献和草稿句子之间的文本蕴含特别有效，可以预测哪些发明权利要求保持不变或与现有技术相比仍具有新颖性。|
|**2024-11-20**|**When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training**|Haonan Wang et.al.|[2411.13476](http://arxiv.org/abs/2411.13476)|**[link](https://github.com/haonan3/anchorcontext)**|**扩展上下文窗口大小使得大型语言模型（LLMs）能够处理更长的序列并应对更复杂的任务。旋转位置嵌入（RoPE）因其相对位置编码特性已成为长上下文训练的标准方法。然而，我们观察到使用RoPE与BFloat16格式时会出现数值问题，导致其偏离了预期的相对位置编码，尤其是在长上下文场景中。这一问题源于BFloat16的有限精度，并且随着上下文长度增加而累积，其中第一个token对这个问题有显著影响。为了解决这个问题，我们开发了AnchorAttention，这是一种即插即用的注意力机制，可以缓解由BFloat16引起的数值问题，提高长上下文处理能力，并加快训练速度。AnchorAttention通过将第一个token视为共享锚点并赋予其一致的位置ID，使其对所有文档可见，从而减少不必要的注意力计算，保持语义连贯性，并提高计算效率。在三种不同类型的LLMs上进行的实验表明，AnchorAttention显著提高了长上下文性能，并将训练时间减少了50%以上，同时保留了原始LLM在一般任务上的能力。我们的代码可在https://github.com/haonan3/AnchorContext获取。**|
|**2024-11-20**|**SoK: A Systems Perspective on Compound AI Threats and Countermeasures**|Sarbartha Banerjee et.al.|[2411.13459](http://arxiv.org/abs/2411.13459)|null|大型语言模型（LLMs）在企业中广泛应用，这些模型通常使用专有模型，并处理敏感输入和数据。先前的研究已经确定了针对训练和推理过程中使用的各种软件和硬件组件的广泛攻击向量，这使得执行保密性和完整性策略变得极其困难。随着我们逐步构建集成多个大型语言模型（LLMs）的复合AI推理管道，攻击面显著扩大。攻击者现在不仅关注AI算法，还关注与这些系统相关的软件和硬件组件。尽管当前研究通常单独考察这些元素，但我们发现结合跨层攻击观察可以实现端到端攻击，且对威胁模型的假设最小化。鉴于每一层都存在大量现有攻击，我们需要对不同层次的攻击向量进行全面而系统化的理解。  这篇论文综述了适用于复合AI系统的不同软件和硬件攻击，并展示了如何通过结合多种攻击机制来减少孤立攻击所需的威胁模型假设。接下来，我们根据Mitre Att&ck框架系统化地整理ML攻击，以便更好地根据威胁模型定位每个攻击。最后，我们概述了软件和硬件层现有的对策，并讨论了制定全面防御策略的必要性，以确保复合AI系统的安全和高性能部署。|
|**2024-11-20**|**AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations**|Gaurav Verma et.al.|[2411.13451](http://arxiv.org/abs/2411.13451)|null|当前构建网页代理的策略依赖于底层多模态大语言模型（MLLMs）的泛化能力和通过提示进行引导的能力，以及在与网页相关的任务上的大规模微调。然而，网页代理仍然难以自动化处理未见过的网站和领域，这限制了它们在企业特定和专有平台上的适用性。除了从大规模预训练和微调中获得的泛化能力外，我们提出了一种利用人类演示来实现少量样本适应性的方法。我们引入了AdaptAgent框架，该框架使专有和开放权重的多模态网页代理能够在使用少量人类演示（最多2个）的情况下适应新的网站和领域。我们在两个流行的基准数据集——Mind2Web和VisualWebArena上进行了实验，结果显示，使用上下文演示（对于专有模型）或元适应演示（对于经过元学习的开放权重模型）可以将任务成功率提高3.36%到7.21%，相对提高了21.03%到65.75%。此外，我们的额外分析显示：(a) 多模态演示比纯文本演示更有效；(b) 揭示了元学习过程中不同数据选择策略对代理泛化能力的影响；(c) 展示了少量样本演示数量对网页代理成功率的影响。总体而言，我们的研究结果解锁了一个补充维度，用于开发广泛适用的多模态网页代理，超越了大规模预训练和微调，强调了少量样本适应性的重要性。|
|**2024-11-19**|**ACING: Actor-Critic for Instruction Learning in Black-Box Large Language Models**|Salma Kharrat et.al.|[2411.12736](http://arxiv.org/abs/2411.12736)|**[link](https://github.com/salmakh1/ACING)**|**大型语言模型（LLMs）在解决任务时的有效性很大程度上取决于指令的质量，这通常需要通过大量的人工努力进行微调。这突显了自动化指令优化的需求；然而，在处理黑盒LLMs时，这种优化尤其具有挑战性，因为模型参数和梯度无法访问。我们提出了ACING，这是一种针对特定任务的提示优化方法，将其构架为一个无状态的连续动作强化学习（RL）问题，即连续乐队设置。ACING利用基于演员-评论家的方法来优化提示，从不可微的奖励信号中学习。我们通过在30个基于指令的任务上优化ChatGPT的提示来验证ACING的有效性。结果显示，ACING始终优于基线方法，实现了中位数10个百分点的提升。此外，ACING不仅恢复了而且超越了人工编写的专家指令，相对于人类基准，最高可提高39个百分点。**|
|**2024-11-19**|**Information Theory of Meaningful Communication**|Doron Sivan et.al.|[2411.12728](http://arxiv.org/abs/2411.12728)|**[link](https://github.com/DoronSivan/info_narrative_data)**|在香农的经典论文中，印刷英语的熵被估计为大约每个字符1比特。然而，作为交流手段的语言与它的印刷形式有很大的不同：（i）信息的基本单位不是字符或单词，而是句子，即最短的有意义的语义部分；（ii）传达的主要内容是所说或所写内容的意义，而用来传达意义的确切措辞通常被忽略。在这项研究中，我们展示了可以利用最近开发的大语言模型来量化有意义叙述中传达的信息，以每句意义的比特数来衡量。|
|**2024-11-19**|**Strengthening Fake News Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques. Defying BERT?**|Ahmed Akib Jawad Karim et.al.|[2411.12703](http://arxiv.org/abs/2411.12703)|null|快速传播的虚假信息，尤其是在在线平台上的传播，凸显了可靠检测系统的重要性和紧迫性。本研究探讨了使用机器学习和自然语言处理技术，特别是支持向量机（SVM）和BERT，来检测假新闻的方法。我们为SVM采用了三种不同的文本向量化方法：词频逆文档频率（TF-IDF）、Word2Vec和词袋模型（BoW），以评估它们在区分真实和虚假新闻方面的有效性。此外，我们将这些方法与大型语言模型BERT进行了比较。我们的综合方法包括详细的预处理步骤、严格的模型实现和全面的评估，以确定最有效的方法。结果显示，尽管BERT取得了卓越的准确率，达到99.98%，F1分数为0.9998，但使用线性核函数和支持向量机（SVM）的词袋模型（BoW）同样表现优异，准确率达到99.81%，F1分数为0.9980。这些结果表明，尽管BERT性能更优，但使用词袋模型（BoW）和TF-IDF向量化方法的支持向量机（SVM）模型接近其性能，并且具有较低的计算需求优势。|
|**2024-11-19**|**When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations**|Huaizhi Ge et.al.|[2411.12701](http://arxiv.org/abs/2411.12701)|null|大型语言模型（LLMs）容易受到后门攻击，其中隐藏的触发器可以恶意操纵模型行为。尽管已经提出了几种后门攻击方法，但LLMs中后门功能的工作机制仍需进一步探索。在本文中，我们超越了对LLMs的攻击，通过自然语言解释的新颖视角来研究后门功能。具体而言，我们利用LLMs的生成能力为其决策生成人类可理解的解释，从而能够比较清洁样本和中毒样本的解释。我们探讨了各种后门攻击，并将后门嵌入到多个任务中的LLaMA模型。我们的实验表明，中毒后的模型在生成清洁数据的解释时质量更高，而在生成中毒数据的解释时则显著更加一致。我们进一步分析了解释生成过程，揭示在令牌级别上，中毒样本的解释令牌仅出现在LLM的最后一两个变换层中。在句子级别上，注意力动态表明，在生成解释时，中毒输入会将注意力从输入上下文中转移开。这些发现加深了我们对LLMs中后门攻击机制的理解，并提供了一个通过可解释性技术检测此类漏洞的框架，有助于开发更安全的LLMs。|
|**2024-11-19**|**SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference**|Jiho Shin et.al.|[2411.12692](http://arxiv.org/abs/2411.12692)|null|利用稀疏性对于优化大型语言模型的推理至关重要。然而，采用SiLU作为激活函数的现代LLM表现出最小的激活稀疏性。最近的研究提出用ReLU代替SiLU以诱导显著的激活稀疏性，并通过微调显示下游任务准确性没有下降。然而，要充分利用这一点，需要训练一个预测器来估计这种稀疏性。在本文中，我们引入了SparseInfer，这是一种简单、轻量且无需训练的预测器，用于预测ReLU领域LLM的激活稀疏性，在该预测器中，激活稀疏性是通过比较输入和权重的符号位来预测的。为了补偿可能的预测不准确，启用了自适应调整预测器的保守程度，这也可以作为一个控制旋钮来优化LLM的推理。所提出的方法比最先进的方法快大约1倍，且精度损失在1%以内。|
|**2024-11-19**|**Neurosymbolic Graph Enrichment for Grounded World Models**|Stefano De Giorgis et.al.|[2411.12671](http://arxiv.org/abs/2411.12671)|null|复杂现实世界场景的理解和推理是人工智能系统开发中的一个重大挑战。在这项工作中，我们提出了一种新颖的方法来增强和利用大型语言模型（LLM）的反应能力，以解决复杂问题并深入理解情境化的现实世界含义。我们介绍了一种方法和工具，用于创建一种多模态、知识增强的形式化意义表示，该表示结合了大型语言模型的优势与结构化语义表示的优势。我们的方法始于图像输入，利用最先进的大型语言模型生成自然语言描述。然后，这个描述被转化为抽象意义表示（AMR）图，该图被形式化并丰富了逻辑设计模式以及来自语言和事实知识库的分层语义。随后，生成的图形被反馈到LLM中，通过复杂的启发式学习扩展，包括语义隐含、道德价值、具身认知和隐喻表示。通过弥合非结构化语言模型与正式语义结构之间的鸿沟，我们的方法为解决自然语言理解和推理中的复杂问题开辟了新的途径。|
|**2024-11-19**|**DLBacktrace: A Model Agnostic Explainability for any Deep Learning Models**|Vinay Kumar Sankarapu et.al.|[2411.12643](http://arxiv.org/abs/2411.12643)|**[link](https://github.com/aryaxai/dlbacktrace)**|**人工智能的快速发展催生了越来越复杂的深度学习模型，这些模型经常作为不透明的“黑箱”运作，其决策过程缺乏透明度。这种缺乏可解释性带来了相当大的挑战，尤其是在高风险应用中，理解模型输出背后的理由与输出本身同样重要。本研究针对人工智能系统中的紧迫需求，强调了可解释性在建立信任、确保问责以及在关键任务领域负责任部署方面的作用。为了解决深度学习中的可解释性问题，我们介绍了DLBacktrace技术，这是一种由AryaXAI团队开发的创新技术，旨在阐明各种领域的模型决策，包括简单的多层感知器（MLP）、卷积神经网络（CNN）、大型语言模型（LLM）、计算机视觉模型等。我们提供了DLBacktrace算法的全面概述，并通过多样化的基于任务的指标，将其性能与已建立的可解释性方法（如SHAP、LIME、GradCAM、Integrated Gradients、SmoothGrad和Attention Rollout）进行基准测试比较。提出的DLBacktrace技术兼容多种在PyTorch和TensorFlow中构建的模型架构，支持像Llama 3.2这样的自然语言处理架构，以及其他诸如BERT和LSTM的模型，计算机视觉模型如ResNet和U-Net，以及用于表格数据的自定义深度神经网络（DNN）。这种灵活性突显了DLBacktrace在增强模型透明度方面的适应性和有效性。该库是开源的，并可在https://github.com/AryaXAI/DLBacktrace获取。**|
|**2024-11-19**|**Improving Controllability and Editability for Pretrained Text-to-Music Generation Models**|Yixiao Zhang et.al.|[2411.12641](http://arxiv.org/abs/2411.12641)|null|在AI辅助音乐创作领域已经取得了显著进展，但现有的系统通常难以满足迭代和精细音乐制作的需求。这些挑战包括对生成内容的控制不足以及缺乏灵活、精确的编辑功能。本论文通过一系列逐步推进的改进来解决这些问题，从而增强文本到音乐生成模型的可控性和可编辑性。  首先，我们介绍了Loop Copilot系统，旨在解决音乐创作中迭代优化的需求。Loop Copilot利用大型语言模型（LLM）协调多个专业化的AI模型，使用户能够通过对话界面交互式地生成和优化音乐。该系统的核心是全局属性表，它在整个迭代过程中记录并保持关键的音乐属性，确保在任何阶段的修改都能保持音乐的整体连贯性。虽然Loop Copilot在协调音乐创作过程方面表现出色，但它并不能直接解决对生成内容进行详细编辑的需求。  为了解决这一局限性，我们提出了MusicMagus作为进一步的解决方案，用于编辑AI生成的音乐。MusicMagus引入了一种零样本文本到音乐编辑方法，允许用户修改特定的音乐属性，如流派、情绪和乐器编配，而无需重新训练模型。通过操作预训练扩散模型中的潜在空间，MusicMagus确保这些编辑在风格上连贯，并且非目标属性保持不变。此系统特别有效于在编辑过程中保持音乐的结构完整性，但在更复杂和现实世界的音频场景中遇到挑战。|
|**2024-11-19**|**AdaCM $^2$: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction**|Yuanbin Man et.al.|[2411.12593](http://arxiv.org/abs/2411.12593)|null|大型语言模型（LLMs）的进步推动了视频理解任务的发展，通过将视觉模型与LLMs结合，但大多数现有的基于LLM的模型（例如VideoLLaMA、VideoChat）仅限于处理短时长视频。最近尝试通过提取并压缩视觉特征到固定内存大小来理解长视频。然而，这些方法仅利用视觉模态来合并视频令牌，并忽略了视觉和文本查询之间的相关性，导致在处理复杂的问答任务时面临困难。为了解决长视频和复杂提示的问题，我们提出了AdaCM$^2$，这是首次引入了一种自适应跨模态内存缩减方法，在视频流上以自回归方式实现视频-文本对齐。我们在各种视频理解任务上的广泛实验，如视频描述生成、视频问答和视频分类，表明AdaCM$^2$ 在多个数据集上实现了最先进的性能，同时显著减少了内存使用。特别地，在LVU数据集上，它在多个任务中取得了4.5%的提升，并且GPU内存消耗减少了高达65%。|
|**2024-11-19**|**Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models**|Laura Ruis et.al.|[2411.12580](http://arxiv.org/abs/2411.12580)|**[link](https://github.com/pomonam/kronfluence)**|**大型语言模型（LLMs）的能力和局限性在近年来得到了详细描述，这提供了一个引人入胜但又相互矛盾的图景。一方面，LLMs展示了解决问题的一般能力；另一方面，它们在与人类相比时显示出令人惊讶的推理差距，这质疑了它们泛化策略的稳健性。由于设计LLMs所使用的数据量巨大，我们无法使用传统方法来衡量泛化能力：即训练-测试集分离。为了克服这一点，我们研究了当执行推理任务时，LLMs采用何种泛化策略，并通过调查它们依赖的预训练数据来对此进行探讨。对于两种不同规模的模型（7B和35B参数量）以及其中25亿个预训练令牌，我们识别了影响模型输出的文档，并将其与回答事实性问题的数据进行对比。我们发现，虽然模型对每个事实性问题依赖的数据集大多是不同的，但在同一任务的不同推理问题中，一个文档往往具有相似的影响，这表明存在程序性知识。进一步发现，事实性问题的答案通常会出现在最具影响力的数据中。然而，对于推理问题，答案通常不会显示出高度影响力，中间推理步骤的答案也是如此。当我们定性地分析推理问题的排名靠前的文档时，我们确认这些有影响力的文档通常包含了程序性知识，例如通过公式或代码演示如何获得解决方案。我们的研究结果表明，模型采用的推理方法不同于检索，而更像是一种能够从执行类似推理形式的文档中综合程序性知识的通用策略。**|
|**2024-11-18**|**Bi-Mamba: Towards Accurate 1-Bit State Space Models**|Shengkun Tang et.al.|[2411.11843](http://arxiv.org/abs/2411.11843)|null|典型的Mamba选择性状态空间模型（SSM）解决了Transformer的一些限制，如序列长度导致的二次计算复杂度和由于键值缓存导致的显著推理时间内存需求。然而，Mamba模型规模的不断扩大仍然带来了训练和部署挑战，并引发了能源消耗方面的环境问题。在这项工作中，我们引入了Bi-Mamba，这是一种可扩展且强大的1比特Mamba架构，旨在实现更高效的大型语言模型，涵盖780M、1.3B和2.7B等多种规模。Bi-Mamba模型从与常规大语言模型相同的大量数据中从头开始训练，并使用自回归蒸馏损失进行训练。在语言建模的广泛实验结果表明，Bi-Mamba在性能上与全精度模型（例如FP16或BF16）相当，并且比后训练二值化（PTB）Mamba基线模型具有更好的准确性，同时相比原始Mamba模型显著减少了内存占用和能源消耗。我们的研究开创了一种低比特表示下的线性计算复杂度的大语言模型框架，并促进了针对高效1比特Mamba大语言模型的专用硬件设计。|
|**2024-11-18**|**Tackling prediction tasks in relational databases with LLMs**|Marek Wydmuch et.al.|[2411.11829](http://arxiv.org/abs/2411.11829)|null|尽管大型语言模型（LLMs）在许多问题上展示了卓越的性能，但它们在关系数据库预测任务中的应用尚未得到充分探索。在这项工作中，我们探讨了LLMs无法在关系数据库上获得满意结果的观点，因为这些数据库包含相互连接的表、复杂的关联以及异构的数据类型。通过最近引入的RelBench基准测试，我们证明了即使是对LLMs进行简单的应用也能在这些任务上取得具有竞争力的表现。这些发现确立了LLMs作为关系数据库机器学习的新基准，并鼓励进一步的研究。|
|**2024-11-18**|**Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods**|Egor Kovalev et.al.|[2411.11795](http://arxiv.org/abs/2411.11795)|null|对抗性鲁棒性是神经网络研究中的一个重要领域，涵盖了计算机视觉模型、大型语言模型（LLMs）等的研究。随着JPEG AI——首个端到端神经图像压缩（NIC）方法的标准的发布，其鲁棒性问题变得尤为关键。JPEG AI是首批国际上实际应用的基于神经网络的模型之一，并被嵌入到消费设备中。然而，NIC鲁棒性的研究仅限于开源编解码器和有限范围的攻击。本文提出了一种新的方法来衡量NIC对对抗性攻击的鲁棒性。我们进行了首次大规模评估，以比较JPEG AI与其他NIC模型的鲁棒性。我们的评估结果和代码将在网上公开提供（链接在盲审中被隐藏）。|
|**2024-11-18**|**LLM-IE: A Python Package for Generative Information Extraction with Large Language Models**|Enshuo Hsu et.al.|[2411.11779](http://arxiv.org/abs/2411.11779)|null|尽管最近采用了大型语言模型（LLMs）进行生物医学信息提取，但在提示工程和算法方面仍然存在挑战，并且没有专门的软件可用。为了解决这一问题，我们开发了LLM-IE：一个用于构建完整信息提取管道的Python包。我们的主要创新是一个交互式的LLM代理，用以支持模式定义和提示设计。材料与方法：LLM-IE支持命名实体识别、实体属性提取和关系提取任务。我们在i2b2数据集上进行了基准测试并进行了系统评估。结果：基于句子的提示算法表现最佳，但需要更长的推理时间。系统评估提供了直观的可视化效果。讨论：LLM-IE的设计基于在医疗保健领域的实际NLP经验，并已在内部项目中采用。它应该对生物医学NLP社区具有巨大价值。结论：我们开发了一个Python包LLM-IE，提供构建稳健的信息提取管道的构建模块。|
|**2024-11-18**|**sMoRe: Enhancing Object Manipulation and Organization in Mixed Reality Spaces with LLMs and Generative AI**|Yunhao Xing et.al.|[2411.11752](http://arxiv.org/abs/2411.11752)|null|在混合现实（MR）环境中，理解和创建虚拟对象对于提供直观且丰富的用户体验至关重要。本文介绍了一种名为sMoRe（Spatial Mapping and Object Rendering Environment，空间映射与对象渲染环境）的MR应用程序，该应用结合了生成式人工智能（GenAI）和大型语言模型（LLM），以帮助用户在物理空间中创建、放置和管理虚拟对象。sMoRe允许用户使用语音或文本命令来利用生成式AI创建和放置虚拟对象，并指定空间约束。该系统利用LLM解释用户的命令，分析当前场景，并识别最佳位置。此外，sMoRe集成了文本到3D生成式AI，可以根据用户的描述动态创建3D对象。我们的用户研究证明了sMoRe在增强用户对MR环境的理解、交互和组织方面的有效性。|
|**2024-11-18**|**BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration**|Yuzong Chen et.al.|[2411.11745](http://arxiv.org/abs/2411.11745)|**[link](https://github.com/yc2367/bitmod-hpca-25)**|大型语言模型（LLMs）在各种机器学习任务中表现出色。然而，这些模型的庞大内存占用显著阻碍了它们的部署。本文提出了一种名为BitMoD的算法与硬件协同设计解决方案，旨在以低权重精度实现高效加速。在算法层面，BitMoD引入了细粒度的数据类型自适应技术，使用不同的数值数据类型来量化一组（例如，128个）权重。通过精心设计这些新的数据类型，BitMoD能够在保持高准确率的同时将LLM权重量化到非常低的精度（例如，4位和3位）。在硬件层面，BitMoD采用位串行处理单元，轻松支持多种数值精度和数据类型；其硬件设计包括两项关键创新：首先，它采用统一表示法处理不同权重数据类型，从而降低了硬件成本。其次，它采用位串行去量化单元，以最小的硬件开销重新缩放每组部分和。我们在六个代表性LLM上的评估表明，BitMoD显著优于最先进的LLM量化和加速方法。对于判别任务，BitMoD可以将LLM权重量化到4位，平均准确率损失小于0.5%。对于生成任务，BitMoD能够将LLM权重量化到3位，同时获得比先前LLM量化方案更好的困惑度。结合优秀的模型性能与高效的加速器设计，BitMoD相比先前的LLM加速器ANT和OliVe分别实现了平均1.69倍和1.48倍的速度提升。|
|**2024-11-18**|**Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment**|Allison Huang et.al.|[2411.11731](http://arxiv.org/abs/2411.11731)|**[link](https://github.com/acyhuang/moral-persuasion)**|我们探讨了大型语言模型（LLMs）如何通过提示来改变其最初的决策，并使其与既定的伦理框架保持一致。我们的研究基于两个实验，旨在评估LLMs对道德说服的敏感性。在第一个实验中，我们通过评估一个基础代理LLM在道德模糊场景中的决策，并观察一个说服者代理如何试图改变基础代理的初始决策，来检验其对道德模糊性的敏感性。第二个实验则评估了LLMs对预定义伦理框架的敏感性，通过提示它们采纳根植于已建立的哲学理论的具体价值对齐。结果表明，LLMs确实可以在具有道德争议的情境中被说服，说服的成功与否取决于诸如所使用的模型、场景的复杂度和对话长度等因素。值得注意的是，来自同一家公司的不同规模的LLMs产生了明显不同的结果，这突显了它们在伦理说服方面的敏感性差异。|
|**2024-11-18**|**Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via Skill Library and Tactile Representation**|Mingchao Qi et.al.|[2411.11714](http://arxiv.org/abs/2411.11714)|**[link](https://github.com/mingchaoqi/skill_transfer)**|**在开放世界环境中部署机器人涉及复杂的任务，这些任务具有长序列和丰富的交互性，需要在各种复杂场景中高效地转移机器人的技能。为了解决这一挑战，我们提出了一种基于知识图谱的技能库框架，该框架使机器人具备高级技能意识和空间语义理解能力。该框架通过构建“任务图”和“场景图”来分层组织操作知识，分别表示任务和场景语义信息。我们引入了“状态图”，以促进高层次任务规划与低层次场景信息之间的交互。此外，我们提出了一个用于操作技能的分层迁移框架。在任务层面，该框架在一个四阶段提示范式中集成了上下文学习和连锁思维提示，利用大型语言模型（LLM）的推理和泛化能力，实现任务层面子任务序列的迁移。在运动层面，我们开发了一种使用A*算法和技能库的自适应轨迹迁移方法，实现了运动层面的自适应轨迹迁移。在物理层面，我们引入了一种基于触觉感知的自适应轮廓提取和姿势感知方法，这种方法可以从视觉-触觉纹理数据中动态获取高精度的轮廓和姿势信息，并调整迁移的技能，如接触位置和姿势，以确保在新环境中的有效性。实验结果验证了所提出方法的有效性。项目网址：https://github.com/MingchaoQi/skill_transfer**|
|**2024-11-18**|**FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large and Small Language Models**|Tao Fan et.al.|[2411.11707](http://arxiv.org/abs/2411.11707)|null|通过将大型语言模型（LLMs）适应于特定领域的任务或为其注入特定领域的知识，我们可以充分利用LLMs的能力。然而，目前仍存在一个差距，即无法同时实现服务器端的LLMs和下游客户端的小型语言模型（SLMs）之间的相互增强。为了解决这一问题，我们提出了FedCoLLM，这是一种新颖且参数高效的联邦框架，旨在协同调优LLMs和SLMs。该方法旨在自适应地将服务器端LLMs的知识转移到客户端的SLMs，同时利用客户端的数据丰富LLMs的领域知识。为了实现这一点，FedCoLLM使用轻量级适配器与SLMs结合，促进了在尊重数据隐私的同时，减少计算和通信开销的知识交换。我们的评估显示，在各种公开的LLMs和SLMs以及一系列自然语言处理文本生成任务中，借助LLMs的帮助，客户端的SLMs性能得到了显著提升。与此同时，通过FedCoLLM增强的LLMs的表现与直接在客户端数据上微调的效果相当。|
|**2024-11-18**|**Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search**|Jinhao Jiang et.al.|[2411.11694](http://arxiv.org/abs/2411.11694)|null|最近，测试时扩展在研究社区引起了广泛关注，这主要归功于OpenAI发布的o1模型所带来的显著进步。通过在推理阶段分配更多的计算资源，大型语言模型（LLMs）可以更广泛地探索解空间，通过生成更多的思考令牌或多样化解决方案，从而产生更准确的响应。然而，开发类似o1的推理方法具有挑战性，研究人员一直在尝试推进这一开放的研究领域。本文提出了一种初步探索，通过奖励引导的树搜索算法来增强LLMs的推理能力。该框架通过整合策略模型、奖励模型和搜索算法来实现。它主要围绕一个树搜索算法构建，其中策略模型在由专门训练的奖励模型引导的动态扩展树中进行导航。我们详细探讨了实现此框架所需的各种设计考虑，并提供了技术方面的详细报告。为了评估我们的方法的有效性，我们专注于数学推理任务，并在四个具有挑战性的数据集上进行了广泛的评估，显著提升了LLMs的推理能力。|
|**2024-11-15**|**Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization**|Weiyun Wang et.al.|[2411.10442](http://arxiv.org/abs/2411.10442)|null|现有的开源多模态大型语言模型（MLLMs）通常采用预训练和有监督微调的训练过程。然而，这些模型受到分布变化的影响，这限制了它们的多模态推理能力，特别是在链式思维（CoT）表现方面。为了解决这个问题，我们引入了一种偏好优化（PO）过程以增强MLLMs的多模态推理能力。具体来说，（1）在数据方面，我们设计了一个自动化的偏好数据构建管道，创建了一个高质量的大规模多模态推理偏好数据集MMPR。并且（2）在模型方面，我们探索将PO与MLLMs集成，开发了一种简单而有效的方法，称为混合偏好优化（MPO），这种方法提高了多模态CoT的表现。我们的方法在多个基准测试中展示了改进的性能，尤其是在多模态推理任务中。特别地，我们的模型InternVL2-8B-MPO在MathVista上的准确率达到了67.0，比InternVL2-8B高出8.7个百分点，并且达到了与10倍大的InternVL2-76B相当的性能。我们希望这项研究能激发MLLMs领域的进一步发展。代码、数据和模型将会公开发布。|
|**2024-11-15**|**LLaVA-o1: Let Vision Language Models Reason Step-by-Step**|Guowei Xu et.al.|[2411.10440](http://arxiv.org/abs/2411.10440)|**[link](https://github.com/PKU-YuanGroup/LLaVA-CoT)**|大型语言模型在推理能力方面已经取得了显著的进步，特别是在通过推理时间扩展方面，例如OpenAI的o1模型。然而，当前的视觉-语言模型（VLM）在执行系统性和结构化推理时往往表现不佳，尤其是在处理复杂的视觉问答任务时。在这项工作中，我们介绍了LLaVA-o1，这是一种新型的VLM，旨在进行自主的多阶段推理。与思维链提示不同，LLaVA-o1独立地进行总结、视觉解释、逻辑推理和结论生成的顺序阶段。这种结构化的方法使LLaVA-o1在推理密集型任务中的精度有了显著提升。为此，我们编制了LLaVA-o1-100k数据集，整合了来自各种视觉问答来源的样本，并提供了结构化的推理注释。此外，我们提出了一种推理时间阶段级束搜索方法，这使得推理时间扩展成为可能。值得注意的是，仅使用100k训练样本和一种简单但有效的推理时间扩展方法，LLaVA-o1不仅在其广泛使用的多模态推理基准测试中比其基础模型提高了8.9%，而且超过了更大甚至闭源模型的表现，如Gemini-1.5-pro、GPT-4o-mini和Llama-3.2-90B-Vision-Instruct。|
|**2024-11-15**|**MARS: Unleashing the Power of Variance Reduction for Training Large Models**|Huizhuo Yuan et.al.|[2411.10438](http://arxiv.org/abs/2411.10438)|**[link](https://github.com/AGI-Arena/MARS)**|训练深度神经网络——以及最近的大模型——需要高效且可扩展的优化器。自适应梯度算法如Adam、AdamW及其变体在这一任务中起到了核心作用。尽管在过去十年中开发了许多旨在加速凸和非凸设置下随机优化的方差缩减算法，但方差缩减并未在训练深度神经网络或大型语言模型中获得广泛成功。因此，在现代人工智能中，这种方法较少被采用。在这篇论文中，为了释放方差缩减在高效训练大型模型中的潜力，我们提出了一种统一的优化框架MARS（Make vAriance Reduction Shine），该框架通过一种缩放随机递归动量技术将预条件梯度方法与方差缩减相结合。在我们的框架内，我们分别引入了三种基于AdamW、Lion和Shampoo的MARS实例。我们还探讨了我们的算法与现有优化器之间的联系。实验结果表明，在训练GPT-2模型时，MARS的表现始终大幅优于AdamW。|
|**2024-11-15**|**Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization**|Yuhan Fu et.al.|[2411.10436](http://arxiv.org/abs/2411.10436)|null|多模态大型语言模型（MLLMs）因其幻觉现象而受到限制，这限制了它们的实际应用。近期的研究尝试通过直接偏好优化（DPO）来提升MLLMs的性能，但这些方法在减少幻觉方面显示出不一致的效果。为了更有效地解决这个问题，我们引入了一种针对幻觉的直接偏好优化方法（HDPO），以减少MLLMs中的幻觉。与先前的方法不同，我们的方法针对幻觉的不同形式和原因进行处理。具体来说，我们开发了三种类型的偏好对数据，分别针对以下导致MLLMs幻觉的原因：（1）视觉能力不足，（2）长上下文生成，以及（3）多模态冲突。实验结果表明，我们的方法在多个幻觉评估数据集上取得了优越的性能，超过了大多数最先进的方法，并突显了我们方法的潜力。消融研究和深入分析进一步证实了我们方法的有效性，并提出了通过扩大规模进一步改进的可能性。|
|**2024-11-15**|**Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash**|Parsa Hejabi et.al.|[2411.10422](http://arxiv.org/abs/2411.10422)|**[link](https://github.com/parsahejabi/simulation-framework-for-multi-agent-balderdash)**|**大型语言模型（LLMs）在复杂任务和交互式环境中展示了令人印象深刻的技能，但它们的创造力仍需深入探索。本文介绍了一个利用游戏“Balderdash”来评估LLMs创造力和逻辑推理能力的仿真框架。在“Balderdash”游戏中，玩家为生僻词汇编造虚构定义以欺骗他人，同时识别正确的定义。我们的框架使多个LLM代理能够参与此游戏，评估它们生成可信定义的能力以及基于游戏规则和历史进行策略规划的能力。我们实现了一个集中式游戏引擎，其中包含各种LLM作为参与者，并且有一个判断LLM来评估语义等效性。通过一系列实验，我们分析了不同LLM的表现，考察了诸如真实定义比率、欺骗比率和正确猜测比率等指标。结果提供了对LLMs创造性和欺骗能力的见解，突出了它们的优势和改进空间。研究还表明，LLMs输入中的生僻词汇较少会导致其对游戏规则和历史背景的推理能力较差。具体实现可参见(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash)。**|
|**2024-11-15**|**Interactive Cycle Model -- The Linkage Combination among Automatic Speech Recognition, Large Language Models and Smart Glasses**|Libo Wang et.al.|[2411.10362](http://arxiv.org/abs/2411.10362)|**[link](https://github.com/brucewang123456789/GeniusTrail)**|本研究提出了一个名为“ASR-LLM-智能眼镜”的交互循环模型，该模型结合了自动语音识别、大规模语言模型和智能眼镜，以促进无缝的人机交互。研究方法涉及将交互过程分解为不同的阶段和元素。语音通过自动语音识别系统捕捉并处理，然后由大规模语言模型进行分析和解释。结果随后传输到智能眼镜上显示。当用户与显示的数据进行交互时，反馈循环即完成。为了量化模型的性能，使用了数学公式，这些公式围绕核心评估点：在自动语音识别的语音转文本转换中的准确性、连贯性和延迟。研究结果通过理论测试和评估来验证模型的可行性和性能。尽管这种人机交互产品尚未在行业中出现，但该模型在提高依赖于人机交互领域的用户体验方面的表现也验证了其作为促进人机交互的技术的实用性。此外，本研究开创性地将诸如生成预训练Transformer模型等前沿技术整合到独特的交互模型中，大规模语言模型通过强大的评估技术和创新应用提供了原始价值，为评估和增强人机交互提供了新的视角。关键词：自动语音识别，大规模语言模型，智能眼镜，交互机制|
|**2024-11-15**|**Bias Unveiled: Investigating Social Bias in LLM-Generated Code**|Lin Ling et.al.|[2411.10351](http://arxiv.org/abs/2411.10351)|null|大型语言模型（LLMs）在自动化代码生成领域取得了显著进展。然而，研究中存在一个值得注意的空白，即评估LLMs生成代码中的社会偏见问题。为了解决这一问题，我们提出了一种新颖的公平性框架，即Solar，用于评估和减轻LLM生成代码中的社会偏见。具体而言，Solar能够自动生成测试用例，以定量地揭示LLM生成代码中的社会偏见。为了量化生成代码中的社会偏见严重程度，我们开发了一个涵盖各种社会问题的数据集。我们将Solar和该数据集应用于四种最先进的LLM代码生成模型进行评估。我们的评估结果显示，所有被测LLM生成的代码中都存在严重的偏见。此外，我们探索了几种偏见缓解策略，包括思维链（CoT）提示、结合积极角色扮演与CoT提示以及迭代提示。实验表明，迭代提示可以有效减少LLM生成代码中的社会偏见，最多可降低90%。Solar具有高度的扩展性，可用于评估新的社会问题。|
|**2024-11-15**|**Number it: Temporal Grounding Videos like Flipping Manga**|Yongliang Wu et.al.|[2411.10332](http://arxiv.org/abs/2411.10332)|**[link](https://github.com/yongliang-wu/numpro)**|**视频大型语言模型（Vid-LLMs）在视频内容理解方面取得了显著进展，但在需要精确时间定位的任务上，即视频时间定位（VTG），表现不佳。为了解决这一差距，我们引入了一种名为Number-Prompt（NumPro）的新方法，通过为每个视频帧添加唯一的数字标识符，使Vid-LLMs能够将视觉理解与时间定位相结合。将视频视为一系列编号的帧图像，NumPro将VTG转换成一个直观的过程：如同翻阅漫画面板一样按顺序查看。这使得Vid-LLMs能够“阅读”事件的时间线，准确地将视觉内容与相应的时间信息关联起来。我们的实验表明，NumPro显著提升了顶级Vid-LLMs的时间定位性能，且不会增加额外的计算成本。此外，在NumPro增强的数据集上进行微调定义了新的VTG领域最佳水平，相比之前的顶级方法，在时刻检索的mIoU上提高了最多6.9%，在亮点检测的mAP上提高了最多8.5%。代码将在https://github.com/yongliang-wu/NumPro 获取。**|
|**2024-11-15**|**Modification Takes Courage: Seamless Image Stitching via Reference-Driven Inpainting**|Ziqi Xie et.al.|[2411.10309](http://arxiv.org/abs/2411.10309)|**[link](https://github.com/yayoyo66/rdistitcher)**|**当前的图像拼接方法在处理具有挑战性的场景（如不均匀色调和大视差）时经常会产生明显的接缝。为了解决这个问题，我们提出了参考驱动的图像修补拼接器（RDIStitcher），该方法将图像融合和矩形化重新表述为基于参考的图像修补模型，相比以前的方法，它采用了更大的修改融合区域和更强的修改强度。此外，我们引入了一种自监督的模型训练方法，使得在无需标记数据的情况下通过微调文本到图像（T2I）扩散模型来实现RDIStitcher成为可能。鉴于评估拼接图像质量的难度，我们提出了基于多模态大语言模型（MLLMs）的指标，为评估拼接图像质量提供了新的视角。与最先进的方法相比，广泛的实验表明，我们的方法显著提高了拼接图像的内容连贯性和无缝过渡。特别是在零样本实验中，我们的方法展示了强大的泛化能力。代码：https://github.com/yayoyo66/RDIStitcher**|
|**2024-11-15**|**Static network structure cannot stabilize cooperation among Large Language Model agents**|Jin Han et.al.|[2411.10294](http://arxiv.org/abs/2411.10294)|null|大型语言模型（LLMs）越来越多地被用于模拟人类社交行为，最近的研究探索了它们在模拟社交动态方面的能力。在此，我们测试LLMs是否能在社会困境中反映人类行为，在这些困境中个人利益和集体利益相冲突。人类在实验室环境中通常比预期更加合作，在混合人群中合作较少但在固定网络中更多。相反，LLMs在混合环境中表现出更多的合作。这引发了一个关键问题：LLMs是否即将在固定网络中的合作困境中模仿人类行为？在这项研究中，我们在混合和结构化网络配置中检查了代理反复参与囚徒困境的网络交互，旨在识别LLMs与人类之间的合作行为的相似之处。我们的研究结果表明存在关键区别：虽然人类在结构化网络中更倾向于合作，但LLMs主要在混合环境中表现出更高的合作度，并且对网络环境的适应有限。值得注意的是，LLMs的合作程度也因模型类型而异，这说明了在人工代理中复制人类社交适应性的复杂性。这些结果突显了一个重要的差距：LLMs难以在固定网络中模仿人类部署的细微、适应性的社交策略。与人类参与者不同，LLMs不会根据网络结构或不断变化的社会环境调整其合作行为，未能采用人类适应性使用的互惠规范。这一局限性指出了未来LLM设计中的一个基本需求——整合对社交规范的深入理解，从而实现更真实地模拟人类合作和适应能力在网络环境中的表现。|
|**2024-11-14**|**MagicQuill: An Intelligent Interactive Image Editing System**|Zichen Liu et.al.|[2411.09703](http://arxiv.org/abs/2411.09703)|**[link](https://github.com/ant-research/MagicQuill)**|图像编辑涉及多种复杂的任务，并且需要高效而精确的操作技术。在本文中，我们介绍了MagicQuill，一个集成的图像编辑系统，能够快速实现创意想法。我们的系统具有简洁但功能强大的界面，允许通过最少的输入来表达编辑操作（例如插入元素、擦除对象、改变颜色）。这些交互由一个多模态大型语言模型（MLLM）实时监控，以预判编辑意图，从而无需显式地输入提示。最后，我们应用了一个强大的扩散先验，并通过一个精心学习的双分支插件模块进行增强，以实现对编辑请求的精确控制。实验结果证明了MagicQuill在实现高质量图像编辑方面的有效性。请访问https://magic-quill.github.io尝试我们的系统。|
|**2024-11-14**|**Advancing Fine-Grained Visual Understanding with Multi-Scale Alignment in Multi-Modal Models**|Wei Wang et.al.|[2411.09691](http://arxiv.org/abs/2411.09691)|null|多模态大型语言模型（MLLMs）在细粒度视觉理解方面取得了显著的成功，但在细粒度知识对齐方面仍面临重大挑战。这些挑战限制了它们准确捕捉局部细节和实现全面全局感知的能力。尽管最近的研究重点在于将对象表达与定位信息对齐，但通常缺乏显式整合对象图像，而对象图像包含了超越单纯文本或坐标的丰富信息。为了弥合这一差距，我们提出了一种新颖的细粒度视觉知识对齐方法，该方法能够有效地对齐并整合对象的多尺度知识，包括文本、坐标和图像。这种方法基于我们开发的多尺度细粒度增强数据合成管道，提供了超过30万条关键训练数据，以增强对齐效果并提升整体性能。此外，我们还推出了TinyGroundingGPT，这是一系列优化用于高级别对齐的小型模型。这些模型参数量约为30亿，不仅在定位任务中表现出色，在复杂视觉场景中的表现也与更大的MLLMs相当。|
|**2024-11-14**|**Squeezed Attention: Accelerating Long Context Length LLM Inference**|Coleman Hooper et.al.|[2411.09688](http://arxiv.org/abs/2411.09688)|**[link](https://github.com/SqueezeAILab/SqueezedAttention)**|新兴的大规模语言模型（LLM）应用需要较长的输入提示来执行复杂的下游任务，如文档分析和代码生成。对于这些长上下文长度的应用，输入提示的长度对推理效率提出了显著挑战，因为推理成本随序列长度线性增加。然而，对于许多这些应用，提示中的大量上下文在不同的用户输入之间是固定的，因此提供了机会在离线优化处理用户输入的速度。在这项工作中，我们提出了一种称为压缩注意力的机制，以加速那些固定上下文占很大比例的LLM应用。首先，我们在离线阶段利用K均值聚类根据语义相似性将固定上下文的键进行分组，并用单个质心值表示每个簇。在推理过程中，我们将用户输入的查询令牌与质心进行比较，预测哪些来自固定上下文的键在语义上相关并需要在推理过程中加载。然后，我们仅使用这些重要的键从固定上下文中计算精确注意力，从而减少带宽和计算成本。我们还扩展了该方法，使用层次质心查找来识别重要的键，这可以将注意力复杂度从线性降低到对上下文长度的对数级。我们实现了优化的Triton内核来进行质心比较和使用重要键的稀疏FlashAttention，在长上下文推理的预填充和生成阶段实现了超过4倍的速度提升。此外，我们已在各种长上下文基准测试中广泛评估了该方法，包括LongBench，在没有精度损失的情况下实现了KV缓存预算3倍的减少，并且对于各种模型，精度差距小于0.5点时可实现高达8倍的减少。|
|**2024-11-14**|**Local deployment of large-scale music AI models on commodity hardware**|Xun Zhou et.al.|[2411.09625](http://arxiv.org/abs/2411.09625)|null|我们介绍了MIDInfinite，一个能够在普通硬件上本地生成符号音乐的网络应用程序。创建此演示涉及将Anticipatory Music Transformer（一种在Lakh MIDI数据集上预训练的大规模语言模型）移植到机器学习编译（MLC）框架中。一旦模型被移植，MLC可以在多种运行时环境中促进推理，包括C++、移动设备和浏览器。我们设想MLC有望弥合越来越强大的音乐AI模型与更熟悉的音乐软件开发技术之间的差距。作为概念验证，我们构建了一个网络应用程序，允许用户在浏览器中生成连续的多乐器MIDI流，这些流可以从头开始生成或根据提示进行生成。在普通硬件（如M3 MacBook Pro）上，我们的演示可以每秒生成51个音符，在72.9%的情况下生成速度超过实时播放，并且在提前缓冲2秒后，这一比例增加到86.3%。|
|**2024-11-14**|**PTR: Precision-Driven Tool Recommendation for Large Language Models**|Hang Gao et.al.|[2411.09613](http://arxiv.org/abs/2411.09613)|null|通过为大型语言模型（LLMs）配备外部工具，它们解决复杂问题的能力得到了显著提升。然而，尽管LLMs的解析能力在不断进步，但在提示中同时引入所有可用工具仍然是不切实际的，因为外部工具有很多。因此，为特定任务提供一套精确的工具（包括数量和质量方面）变得至关重要。当前的工具检索方法主要集中在优化工具排名列表，并直接打包固定数量的顶级工具作为工具集。然而，这些方法往往无法在执行前为LLMs配备最佳工具集，因为不同任务所需的最优工具数量可能不同，导致诸如冗余或不合适的工具等问题，从而阻碍了即时访问最相关的工具。本文解决了为LLMs推荐精确工具集的挑战。我们介绍了工具推荐的问题，定义了其范围，并提出了一种新颖的精度驱动工具推荐（PTR）方法。PTR通过利用历史工具包使用情况捕获一个初始且简洁的工具集，并通过执行工具匹配动态调整工具集，最终形成多视图基础上的工具添加。此外，我们还提出了一个新的数据集RecTools和一个名为TRACC的新指标，旨在评估LLMs的工具推荐效果。我们进一步通过全面的实验验证了我们的设计选择，在两个开放基准和我们的RecTools数据集上展示了有前景的准确性。|
|**2024-11-14**|**The Moral Foundations Weibo Corpus**|Renjie Cao et.al.|[2411.09612](http://arxiv.org/abs/2411.09612)|null|表达在自然语言中的道德情感对线上线下环境都有显著影响，塑造了行为风格和互动模式，包括社交媒体自我展示、网络欺凌、遵守社会规范以及伦理决策。为了有效测量自然语言处理文本中的道德情感，利用大型标注数据集至关重要，这些数据集提供了细致的理解以实现准确的分析和模型训练。然而，现有的语料库虽然有价值，但常常面临语言局限性。为了解决中文领域的这一差距，我们介绍了道德基础微博语料库。该语料库包含25,671条关于微博的评论，涵盖了六个不同的主题领域。每条评论至少由三位系统培训的注释员根据从道德理论中衍生出的十个道德类别进行人工标注。为了评估注释员的一致性，我们展示了kappa检验结果，这是衡量一致性的黄金标准。此外，我们应用了最新的大规模语言模型来补充人工注释，进行了分析实验以比较它们的表现，并报告了道德情感分类的基线结果。|
|**2024-11-14**|**Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the AutoNuggetizer Framework**|Ronak Pradeep et.al.|[2411.09607](http://arxiv.org/abs/2411.09607)|null|本报告提供了TREC 2024检索增强生成（RAG）轨道部分结果的初步分析。我们已经确定了RAG评估是继续在信息获取（更广泛地说，在自然语言处理和人工智能领域）取得进展的一个障碍，希望我们能够为解决这一领域的许多挑战做出贡献。本研究探讨的核心假设是，最初为2003年TREC问答赛道开发的金块（nugget）评估方法为评估RAG系统提供了一个坚实的基础。因此，我们的工作重点在于“重构”该方法，具体来说是应用大型语言模型来自动创建金块并自动将金块分配给系统答案。我们称这种方法为AutoNuggetizer框架。在TREC设置下，我们能够将完全自动化的流程与半手动由人工评估者创建金块并手动分配到系统答案的过程进行校准。基于来自45个运行中的21个主题的初步结果，我们观察到全自动金块评估所得到的分数与人类评估者半手动评估的分数之间存在强烈的关联性。这表明，我们的全自动评估过程可以用于指导未来的RAG系统迭代。|
|**2024-11-14**|**Accelerating Knowledge Graph and Ontology Engineering with Large Language Models**|Cogan Shimizu et.al.|[2411.09601](http://arxiv.org/abs/2411.09601)|null|大型语言模型有望显著加速包括本体建模、扩展、修改、填充、对齐以及实体消歧在内的关键知识图谱和本体工程任务。我们概述了基于大型语言模型的知识图谱和本体工程作为一个新的研究领域，并认为模块化方法在本体工程中将至关重要。|
|**2024-11-14**|**LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models**|Zhengyi Wang et.al.|[2411.09595](http://arxiv.org/abs/2411.09595)|null|本文探讨了扩展大型语言模型（LLMs）在文本预训练基础上生成3D网格的能力，并将其整合到统一的模型中。这种方法具有两大优势：（1）利用LLMs中已嵌入的空间知识，这些知识来源于3D教程等文本来源；（2）实现基于对话的3D生成和网格理解。主要挑战在于有效地将3D网格数据分词成离散的令牌，以便LLMs能够无缝处理。为此，我们引入了一种名为LLaMA-Mesh的新方法，该方法将3D网格的顶点坐标和面定义表示为纯文本，从而允许直接与LLMs集成而不必扩展词汇表。我们构建了一个有监督微调（SFT）数据集，使预训练的LLMs能够（1）根据文本提示生成3D网格，（2）生成所需的交错文本和3D网格输出，以及（3）理解和解释3D网格。我们的工作首次证明了LLMs可以通过文本格式微调以获得用于3D网格生成的复杂空间知识，从而有效统一3D和文本模态。LLaMA-Mesh在网格生成质量方面达到了与其他从头开始训练的模型相当的水平，同时保持了强大的文本生成性能。|
|**2024-11-14**|**Adopting RAG for LLM-Aided Future Vehicle Design**|Vahid Zolfaghari et.al.|[2411.09590](http://arxiv.org/abs/2411.09590)|null|在本文中，我们探讨了大型语言模型（LLMs）与检索增强生成（RAG）的集成，以增强汽车行业的自动化设计和软件开发。我们提出了两个案例研究：一个标准化合规聊天机器人和一个设计副驾驶，两者都利用RAG提供准确、上下文感知的回复。我们评估了四种LLM模型——GPT-4o、LLAMA3、Mistral和Mixtral，比较它们的回答准确性和执行时间。我们的结果显示，虽然GPT-4表现出色，但LLAMA3和Mistral也展示了在本地部署中的潜力，解决了汽车行业中的数据隐私问题。本研究突显了RAG增强的LLM在改善汽车工程中的设计流程和合规性的潜力。|
|**2024-11-13**|**The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models**|Daniel P. Jeong et.al.|[2411.08870](http://arxiv.org/abs/2411.08870)|**[link](https://github.com/taekb/eval-medical-dapt)**|近年来，一些研究试图开发专门用于医疗应用的基础模型，通过在公开的生物医学语料库上进行持续预训练来适应通用的大规模语言模型（LLMs）和视觉-语言模型（VLMs）。这些研究通常声称这种领域适应性预训练（DAPT）可以提高下游医疗任务的表现，例如回答医学执照考试问题。在本文中，我们对比了十个公共“医疗”LLM和两个VLM与其对应的基线模型，得出了不同的结论：所有医疗VLM和几乎所有医疗LLM在零样本/少样本提示和监督微调模式下进行医学问答任务时，并未始终优于其基线模型。例如，在所有任务和模型对的3次提示设置中，医疗LLM仅在22.7%的情况下优于其基线模型，在36.8%的情况下与基线模型持平，而在其余40.5%的情况下则显著劣于基线模型。我们的结论基于以下几点：(i) 直接对比每个医疗模型与其对应的基线模型；(ii) 分别优化每个模型的提示以进行零样本/少样本提示；(iii) 考虑比较中的统计不确定性。虽然这些基本做法在文献中并未被一致采用，但我们的消融实验表明，它们对结论有重大影响。同时，我们发现经过特定QA任务微调后，医疗LLM可以表现出性能提升，但这种优势并未转移到基于临床记录的任务上。我们的研究结果表明，最先进的通用领域模型可能已经具备强大的医学知识和推理能力，并提供了加强未来研究结论的建议。|
|**2024-11-13**|**LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs**|Piyush Jha et.al.|[2411.08862](http://arxiv.org/abs/2411.08862)|null|我们介绍了LLMStinger，这是一种新颖的方法，利用大型语言模型（LLMs）自动生成对抗后缀以进行越狱攻击。与传统方法不同，这些方法需要复杂的提示工程或白盒访问，LLMStinger使用强化学习（RL）循环来微调攻击者LLM，基于现有的攻击生成有害问题的新的后缀。我们的方法显著优于现有的红队技术（我们对比了最新的15种方法），在LLaMA2-7B-chat上的攻击成功率（ASR）提高了57.2%，在Claude 2上的ASR提高了50.3%，这两种模型都以其广泛的安全措施而闻名。此外，我们在GPT-3.5上的ASR达到了94.97%，在Gemma-2B-it上的ASR达到了99.4%，这证明了LLMStinger在开放和封闭源模型中的鲁棒性和适应性。|
|**2024-11-13**|**Multimodal Instruction Tuning with Hybrid State Space Models**|Jianing Zhou et.al.|[2411.08840](http://arxiv.org/abs/2411.08840)|null|处理长上下文对于增强多模态大语言模型（MLLMs）在处理高分辨率图像或高帧率视频等应用中的识别和理解能力至关重要。随着图像分辨率和帧率的提高，计算需求显著增加，因为输入标记的数量增多。这一挑战因自注意力机制相对于序列长度的二次复杂性而进一步加剧。大多数先前的工作要么通过预训练模型来应对长上下文问题，但忽略了效率问题，要么试图通过降采样（例如，识别关键图像块或帧）来减少上下文长度，但这可能导致信息丢失。为了在保持MLLMs卓越效果的同时解决这个问题，我们提出了一种使用混合Transformer-MAMBA模型的新方法，以高效处理多模态应用中的长上下文。我们的多模态模型能够有效处理超过10万令牌的长上下文输入，在各种基准测试中优于现有模型。值得注意的是，与当前模型相比，我们的模型在处理高分辨率图像和高帧率视频时的推理效率提高了约4倍，且随着图像分辨率或视频帧数的增加，效率提升更加明显。此外，我们的模型是首个能够在低分辨率图像或低帧率视频上进行训练，同时能够对高分辨率图像和高帧率视频进行推理的模型，从而在多种场景下提供灵活性。|
|**2024-11-13**|**FinRobot: AI Agent for Equity Research and Valuation with Large Language Models**|Tianyu Zhou et.al.|[2411.08804](http://arxiv.org/abs/2411.08804)|**[link](https://github.com/ai4finance-foundation/finrobot)**|**随着金融市场日益复杂化，对自动化工具的需求也在不断增加，这些工具能够有效协助人类分析师进行股票研究，特别是在卖方研究领域。虽然生成式人工智能（GenAI）在这一领域吸引了大量关注，但现有的AI解决方案往往因为其技术因素的狭隘关注以及有限的自由裁量判断能力而表现不佳。这些限制阻碍了它们实时适应新数据的能力，并准确评估风险，从而降低了它们对投资者的实际价值。  本文介绍了FinRobot，这是首个专门设计用于股票研究的AI代理框架。FinRobot采用多代理链式思考（CoT）系统，整合定量和定性分析，以模拟人类分析师的全面推理过程。该系统围绕三个专业代理构建：数据CoT代理，它整合多种数据源以实现稳健的财务整合；概念CoT代理，它模仿分析师的推理以生成可操作的见解；以及主题CoT代理，它将这些见解综合成一个连贯的投资主题和报告。FinRobot提供基于精确数字数据、行业适当的估值指标以及现实风险评估的公司分析。其动态更新的数据管道确保研究保持及时和相关性，灵活适应新的财务信息。与现有的自动化研究工具（如CapitalCube和Wright Reports）不同，FinRobot提供的见解可以媲美大型经纪公司和基础研究供应商所生产的见解。我们已将FinRobot开源，网址为<https://github.com/AI4Finance-Foundation/FinRobot>。**|
|**2024-11-13**|**Evaluating World Models with LLM for Decision Making**|Chang Yang et.al.|[2411.08794](http://arxiv.org/abs/2411.08794)|null|世界模型在决策制定中扮演着关键角色，其中MuZero和Dreamer在复杂任务中取得了显著的成功。最近的研究利用大规模语言模型（LLMs）作为通用的世界模拟器来模拟世界的动态，因为它们具有泛化能力。这些大规模语言模型也作为世界模型用于推理规划（Reasoning via Planning, RAP）和思维树（Tree of Thought, ToT）中的深思熟虑的推理。然而，世界模型要么被评估为通用的世界模拟器，要么作为辅助规划预测的代理功能模块。在这项工作中，我们从决策制定的角度提出对使用大规模语言模型作为世界模型进行综合评估。具体来说，我们利用了来自Wang等人（2023；2024）的31个不同的环境，并为每个环境制定了基于规则的策略以实现多样化评估。然后，我们设计了三个主要任务：策略验证、动作建议和策略规划，在这些任务中世界模型可以单独用于决策制定。最后，我们在各种设置下对先进的大规模语言模型（即GPT-4o和GPT-4o-mini）在这些环境中进行了这三个主要任务的综合评估。关键观察结果包括：i) GPT-4o在三个主要任务上显著优于GPT-4o-mini，尤其是在需要领域知识的任务上；ii) 使用大规模语言模型作为世界模型进行长期决策任务时性能会下降；iii) 结合世界模型的不同功能可能会导致性能的额外不稳定。|
|**2024-11-13**|**Can sparse autoencoders be used to decompose and interpret steering vectors?**|Harry Mayne et.al.|[2411.08790](http://arxiv.org/abs/2411.08790)|**[link](https://github.com/harrymayne/sv_interpretability)**|**转向向量是控制大型语言模型行为的一种有前景的方法。然而，其底层机制仍然不甚明了。虽然稀疏自编码器（SAE）可能提供了一种解释转向向量的潜在方法，但最近的研究发现，由SAE重构的向量往往缺乏原始向量的转向特性。本文研究了为何直接将SAE应用于转向向量会产生误导性的分解，并确定了两个原因：（1）转向向量落在SAE设计的输入分布之外；（2）转向向量在特征方向上可以具有有意义的负投影，而这不是SAE所设计来处理的。这些限制阻碍了直接使用SAE来解释转向向量。**|
|**2024-11-13**|**Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers**|Clément Dumas et.al.|[2411.08745](http://arxiv.org/abs/2411.08745)|**[link](https://github.com/butanium/llm-lang-agnostic)**|**在多语言语言建模中的一个核心问题是大型语言模型（LLMs）是否发展出了与特定语言分离的通用概念表示。本文通过分析基于变换器的LLMs在词翻译任务中的潜在表示（潜伏变量），探讨了这一问题。我们通过从源翻译提示中提取潜伏变量，并将其插入到目标翻译提示的前向传递中，战略性地提取这些潜伏变量。通过这样做，我们发现输出语言在较早的层就被编码到潜伏变量中，而要翻译的概念则被编码在较晚的层。基于这一见解，我们进行了两个关键实验。首先，我们证明仅通过激活修补就可以改变概念而不改变语言，反之亦然。其次，我们表明使用不同语言潜伏变量的平均值进行修补不会损害模型的性能，反而提高了模型翻译概念的能力。我们的结果为所研究模型内部存在语言无关的概念表示提供了证据。**|
|**2024-11-13**|**A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models**|Dingdong Wang et.al.|[2411.08742](http://arxiv.org/abs/2411.08742)|null|随着语音大语言模型（Speech LLMs）的兴起，人们对离散语音标记的兴趣日益增加，因为它们能够与基于文本的标记无缝集成。尽管大多数研究集中在连续语音特征上，但基于离散标记的LLMs在某些任务上已经显示出有希望的结果，然而这两种范式之间的性能差距很少被探讨。在这篇论文中，我们通过使用一个轻量级的LLM（Qwen1.5-0.5B）对多种语义相关的任务进行了离散和连续特征的公平而全面的比较。我们的发现表明，连续特征通常比离散标记表现更好，特别是在需要细粒度语义理解的任务中。此外，本研究不仅停留在表面比较，还通过识别导致离散标记表现不佳的关键因素（如有限的标记粒度和低效的信息保留）来深入分析。为了提高离散标记的性能，我们基于分析探索了潜在的改进方面。我们希望我们的结果能为推进Speech LLMs中的离散语音标记提供新的见解。|
|**2024-11-13**|**Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models**|Somanshu Singla et.al.|[2411.08733](http://arxiv.org/abs/2411.08733)|**[link](https://github.com/Singla17/DRPO)**|对齐大型语言模型（LLMs）传统上依赖于昂贵的训练和人类偏好注释。自我对齐旨在通过使模型能够自行对齐来降低成本。为了进一步降低成本，并在没有任何昂贵调优或注释的情况下实现对齐，我们引入了一种新的无需调优的自我对齐方法，即动态奖励与提示优化（\ours）。我们的方法利用了一种基于搜索的优化框架，使LLMs能够在没有额外训练或人工干预的情况下迭代地自我改进并制定最优的对齐指令。核心的\ours是一种动态奖励机制，它识别并纠正特定于模型的对齐弱点，使LLMs能够高效适应多样的对齐挑战。在八种最近的LLMs（包括开源和闭源模型）上的实证评估表明，\ours显著提升了对齐性能，基础模型的表现超过了经过SFT/RLHF调优的模型。此外，由\ours自动优化的提示超越了由人类专家精心设计的提示，进一步验证了我们方法的有效性。我们的研究结果强调了当前LLMs通过推理时间优化实现自适应自我对齐的巨大潜力，补充了基于调优的对齐方法。|
|**2024-11-13**|**Polymetis:Large Language Modeling for Multiple Material Domains**|Chao Huang et.al.|[2411.08728](http://arxiv.org/abs/2411.08728)|null|随着大型语言模型在各个领域的应用不断扩大，材料科学也迎来了人工智能驱动创新的机会。传统上依赖手动搜索材料科学相关的信息现在正在利用人工智能技术作为辅助工具来提高材料科学研究的效率。为了加速研究人员在材料科学研究中的知识获取和智能化决策支持，本文提出了一种名为Polymetis的大型语言模型，旨在为材料领域提供高度专业的知识解答，涵盖能源材料、功能材料、合金材料、物理化学、生物等材料方向。该模型使用了大约200万条材料知识指令的数据集，在构建数据集的过程中，我们开发了智能提取大型模型（IELM），专门用于从科学文本中提取并形成结构化的知识，避免了大量的手动标注成本，提高了效率。我们将这些数据注入到GLM4-9B模型中进行学习，以增强其在多种材料领域的推理能力。此外，我们引入了增强提示策略，确保模型的回答更加有组织和全面，为材料科学探索的多样化需求提供高效和全面的智能化支持，推动材料科学的发展。|
|**2024-11-12**|**Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data**|Juanhui Li et.al.|[2411.08028](http://arxiv.org/abs/2411.08028)|null|在实际的自然语言处理应用中，大型语言模型（LLMs）由于其在大规模数据集上的广泛训练而提供了有前景的解决方案。然而，这些模型的大尺寸和高计算需求限制了它们在许多应用中的实用性，特别是当需要进一步微调时。为了解决这些限制，通常会选择较小的模型进行部署。然而，较小的模型训练受到标注数据稀缺的阻碍。相反，未标注的数据通常是容易获得的，并且可以通过使用LLMs生成伪标签来训练较小的模型。这使得较小的模型（学生）可以从LLMs（教师）那里获取知识，同时减少了计算成本。这个过程引入了一些挑战，比如潜在的噪声伪标签。因此，选择高质量和信息丰富的数据对于提高模型性能和改善数据利用效率至关重要。为了解决这个问题，我们提出了LLKD，这是一种能够在较少计算资源和较少数据的情况下从LLMs进行知识蒸馏的学习方法。LLKD是一种自适应样本选择方法，结合了来自教师和学生的信号。具体来说，它优先选择那些教师对其标签具有高置信度的样本，这表明标签是可靠的，并且那些学生表现出高度信息需求的样本，这识别出了需要进一步学习的具有挑战性的样本。我们的全面实验表明，LLKD在各种数据集上实现了卓越的性能，并提高了数据效率。|
|**2024-11-12**|**LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models**|Anoop Cherian et.al.|[2411.08027](http://arxiv.org/abs/2411.08027)|null|物理推理是机器人在现实世界中操作的重要技能。然而，解决这类推理问题通常涉及假设和反思复杂的多体相互作用，这些相互作用受到多种物理力的影响，因此学习所有这些相互作用对包括大型语言模型（LLMs）在内的最先进的机器学习框架来说是一个巨大的挑战。为研究这个问题，我们提出了一项新的物理推理任务和一个数据集，名为TraySim。我们的任务涉及预测托盘上几个物体在外力冲击下的动态——由此引发的物体相互作用及其动态提供了一个具有挑战性但又受控的设置，推理的目标是推断冲击后物体的稳定性。为了解决这个复杂的物理推理任务，我们提出了LLMPhy，这是一种零样本黑盒优化框架，它利用了LLMs的物理知识和程序合成能力，并将这些能力与现代物理引擎内置的世界模型相结合。具体而言，LLMPhy使用LLM生成代码，通过隐式的分析-综合方法迭代估计系统的物理超参数（摩擦、阻尼、布局等），并通过循环中的非可微模拟器来实现这一目标，并使用推断出的参数来想象场景的动态以解决推理任务。为了展示LLMPhy的有效性，我们在我们的TraySim数据集上进行了实验，以预测物体的稳态姿态。我们的结果显示，LLM和物理引擎的结合导致了最先进的零样本物理推理性能，同时展示了优于标准黑盒优化方法的收敛性，并且对物理参数有更好的估计。|
|**2024-11-12**|**Language Models as Causal Effect Generators**|Lucius E. J. Bynum et.al.|[2411.08019](http://arxiv.org/abs/2411.08019)|**[link](https://github.com/lbynum/sequence-driven-scms)**|**我们提出了一种基于大型语言模型（LLM）的具有可控因果结构的数据生成框架。具体来说，我们定义了一个过程，可以将任何语言模型和任何有向无环图（DAG）转化为序列驱动的结构因果模型（SD-SCM）。广义上讲，SD-SCM是一种具有用户定义结构和LLM定义结构方程的因果模型。我们描述了如何根据所需的因果结构从观察、干预和反事实分布中进行采样。然后，我们利用这一过程提出了一种新的因果推断方法基准测试，生成个体层面的反事实数据而无需手动指定变量之间的函数关系。我们创建了一个包含数千个数据集的例子基准，并在这些数据集上测试了一系列流行的估计方法，用于平均、条件平均和个体治疗效果估计，同时考虑隐藏混杂因素的影响。除了生成数据外，同一过程还允许我们检测可能编码在LLM中的因果效应的存在。此过程可以作为审计LLM以检测错误信息、歧视或其他不希望的行为的工具。我们认为SD-SCM可以成为任何需要顺序数据且具有可控因果结构的应用中的有用工具。**|
|**2024-11-12**|**ExpressivityArena: Can LLMs Express Information Implicitly?**|Joshua Tint et.al.|[2411.08010](http://arxiv.org/abs/2411.08010)|null|尽管大型语言模型（LLMs）在某些方面展示了卓越的性能，但它们在表达人类用于有效沟通的隐含语言线索方面的表现仍不清楚。本文介绍了ExpressivityArena，一个用于衡量LLMs隐含沟通能力的Python库。我们提供了一个全面的框架来评估任意LLMs的表达性，并探讨其实际应用。为此，我们精炼了“表达性”的定义和测量方法，并使用该框架进行了一系列小型实验。这些实验测试了LLMs在诗歌创作、编程和情感反应等创造性和逻辑任务中的表现。然后通过ExpressivityArena由自动化评分器对它们进行评估，我们验证这是测试表达性的最实用方法。在此基础上，我们通过评估LLMs在对话中保持表达性能力来深化对LLMs表达性的理解。我们的研究结果表明，LLMs能够生成和理解具有表达力的内容，但也存在一些局限性。这些见解将指导未来LLMs的开发和部署。我们将在论文中提供ExpressivityArena的代码。|
|**2024-11-12**|**Can adversarial attacks by large language models be attributed?**|Manuel Cebrian et.al.|[2411.08003](http://arxiv.org/abs/2411.08003)|null|在对抗性设置（如网络攻击和虚假信息）中，对大型语言模型（LLMs）的输出进行归因提出了显著的挑战，这些挑战可能会变得越来越重要。我们使用形式语言理论来研究这一归因问题，特别是Gold引入并在Angluin的工作中进一步扩展的语言识别极限。通过将LLM的输出建模为形式语言，我们分析了有限文本样本是否可以唯一地确定原始模型。我们的研究表明，由于某些语言类别的不可识别性，在一些关于微调模型重叠输出的温和假设下，理论上不可能以确定性的方式将输出归因于特定的LLM。即使考虑到Transformer架构的表达能力限制，这一结论仍然成立。即使有直接访问模型或全面监控，显著的计算障碍也会阻碍归因工作。这些发现强调了需要采取主动措施来减轻由对抗性LLM使用带来的风险，因为它们的影响继续扩大。|
|**2024-11-12**|**Derivational Morphology Reveals Analogical Generalization in Large Language Models**|Valentin Hofmann et.al.|[2411.07990](http://arxiv.org/abs/2411.07990)|null|在大型语言模型（LLMs）中的语言泛化机制是什么？这个问题引起了相当大的关注，大多数研究分析了LLMs的语言技能与规则的相似程度。目前尚不清楚LLMs中的语言泛化是否同样可以被解释为存储实例的相似性操作的结果。先前研究的一个关键不足在于其关注高度规律性的语言现象，对于这些现象，基于规则和基于类比的方法做出了相同的预测。我们在此考察派生词法，特别是英语形容词名词化，它显示了显著的变化性。我们引入了一种新的方法来研究LLMs中的语言泛化：专注于GPT-J，我们将基于规则和基于类比学习的认知模型拟合到LLM的训练数据，并比较它们对一组非正式形容词的预测与LLM的预测，从而能够直接得出有关潜在机制的结论。正如预期的那样，对于具有规则名词化模式的形容词，基于规则和基于类比的模型对GPT-J的预测解释得同样好。然而，对于具有变化名词化模式的形容词，类比模型提供了更好的匹配度。此外，GPT-J的行为对个体词频敏感，即使对于规则形式也是如此，这种行为与规则形式的类比解释一致，但不与基于规则的解释一致。这些发现反驳了GPT-J在形容词名词化方面的语言泛化涉及规则的假设，表明存储实例的相似性操作是潜在机制。总体而言，我们的研究表明，类比过程在LLMs的语言泛化中起着比以前认为更大的作用。|
|**2024-11-12**|**JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation**|Yiyang Ma et.al.|[2411.07975](http://arxiv.org/abs/2411.07975)|**[link](https://github.com/deepseek-ai/janus)**|我们介绍了JanusFlow，这是一个强大的框架，能够在单一模型中统一图像理解和生成。JanusFlow引入了一种极简架构，将自回归语言模型与在生成建模中处于领先地位的校正流（rectified flow）相结合。我们的主要发现表明，校正流可以简单地在大型语言模型框架内进行训练，无需复杂的架构修改。为了进一步提高我们统一模型的性能，我们采用了两种关键策略：(i) 解耦理解编码器和生成编码器，(ii) 在统一训练过程中对它们的表示进行对齐。广泛的实验表明，JanusFlow在各自领域中的表现可与专门模型相媲美或更优，同时在标准基准上显著优于现有的统一方法。这项工作朝着更高效和多功能的视觉-语言模型迈出了重要一步。|
|**2024-11-12**|**From General to Specific: Utilizing General Hallucation to Automatically Measure the Role Relationship Fidelity for Specific Role-Play Agents**|Chuyi Kong et.al.|[2411.07965](http://arxiv.org/abs/2411.07965)|null|先进的角色扮演能力使得开发角色扮演游戏代理（RPAs）成为可能。然而，现有的基准测试如HPD（通过将人工评分的角色关系纳入上下文以供大型语言模型（LLMs）进行连贯性排序）和社会基准测试（SocialBench，它使用由LLMs生成的特定角色配置文件，在多选任务的背景下评估角色偏好），面临着诸如泛化能力差、判断隐晦且不准确以及上下文长度过长等问题。为了解决这些问题，我们提出了一种自动、可扩展且具有广泛适用性的范式。具体而言，我们通过从通用知识图谱中提取关系来构建基准，并利用RPA固有的幻觉特性来促使它在不同角色之间进行互动，使用ChatGPT进行立场检测，并定义了关系幻觉及其三个相关指标。广泛的实验验证了这些指标的有效性和稳定性。我们的研究进一步探讨了影响这些指标的因素，并讨论了关系幻觉与事实性之间的权衡。|
|**2024-11-12**|**Towards Low-bit Communication for Tensor Parallel LLM Inference**|Harry Dong et.al.|[2411.07942](http://arxiv.org/abs/2411.07942)|null|张量并行性提供了一种有效的方法来提高服务器上大型语言模型（LLM）的推理效率，尽管这会增加额外的通信成本。然而，随着服务器LLM的规模继续扩大，它们需要分布在更多的设备上，这会放大通信成本。一种解决这个问题的方法是使用量化，但目前针对LLM的方法往往避免量化张量并行性需要通信的特征。利用通信特征中一致的异常值，我们介绍了一种量化方法，该方法将平均通信值从16位减少到4.2位，同时几乎保留了原始性能。例如，我们的方法保持了Gemma 2 27B和Llama 2 13B在所有评估任务上的原始性能分别约为98.0%和99.5%。|
|**2024-11-12**|**Leveraging Multimodal Models for Enhanced Neuroimaging Diagnostics in Alzheimer's Disease**|Francesco Chiumento et.al.|[2411.07871](http://arxiv.org/abs/2411.07871)|null|快速发展的大规模语言模型（LLMs）和视觉-语言模型（VLMs）在医学诊断领域，尤其是在放射学方面，展现出了巨大潜力。这些领域的数据集如X射线通常与人类生成的诊断报告配对使用。然而，在神经影像学领域，特别是在阿尔茨海默病等病症的研究中，由于缺乏可用于模型微调的综合诊断报告，存在显著的研究空白。本文通过在OASIS-4数据集上使用GPT-4o-mini生成合成诊断报告来解决这一问题。OASIS-4数据集包含了663名患者的数据。利用这些合成报告作为训练和验证的真实数据，我们进一步利用预训练的BiomedCLIP和T5模型直接从数据集中的图像生成神经学报告。我们提出的方法获得了BLEU-4评分为0.1827，ROUGE-L评分为0.3719，METEOR评分为0.4163，这表明该方法在生成临床相关且准确的诊断报告方面具有潜力。|
|**2024-11-11**|**UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts**|Bo Yang et.al.|[2411.07240](http://arxiv.org/abs/2411.07240)|**[link](https://github.com/utmathgroup/utmath)**|数学推理能力的评估对于推进通用人工智能（AGI）至关重要。虽然大型语言模型（LLMs）在解决数学问题方面表现出了令人印象深刻的能力，但现有的基准测试如GSM8K和MATH存在局限性，包括问题定义狭窄且特定数字以及依赖于预设规则，这阻碍了对推理和适应性的准确评估。本文介绍了一个名为UTMath基准的新测试方法，该方法通过广泛的单元测试来全面评估模型。它涵盖了9个数学领域的1,053个问题，每个问题有超过68个测试用例。我们提出了一种创新的评估框架，灵感来源于软件开发中的单元测试，重点关注结果的准确性和可靠性。此外，我们引入了“思考推理到编码”（Reasoning-to-Coding of Thoughts，RCoT）方法，鼓励LLMs在生成代码之前进行明确的推理，从而生成更高级的解决方案并提高性能。此外，我们不仅会发布UTMath基准，还会发布UTMath-Train训练数据集（超过70,000个样本），以支持社区进一步探索数学推理。|
|**2024-11-11**|**OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model**|Sumeth Yuenyong et.al.|[2411.07238](http://arxiv.org/abs/2411.07238)|null|OpenThaiGPT 1.5是一款基于Qwen v2.5的先进泰语聊天模型，经过超过200万条泰语指令对的微调。本报告从工程角度介绍了该模型的开发、功能和性能。我们讨论了模型架构、训练过程以及多项关键特性，包括多轮对话支持、检索增强生成（RAG）兼容性和工具调用功能。基准测试结果表明，OpenThaiGPT 1.5在各种泰语任务上表现出最先进的性能，超越了其他开源泰语模型。此外，我们还探讨了实际应用中的考虑因素，如GPU内存需求和部署策略。|
|**2024-11-11**|**Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving**|Botao Yu et.al.|[2411.07228](http://arxiv.org/abs/2411.07228)|null|为了增强大型语言模型（LLMs）在化学问题解决中的能力，已经提出了几种配备了工具的LLM基代理，如ChemCrow和Coscientist。然而，它们的评估范围狭窄，对于理解工具在各种化学任务中的益处存在很大差距。为了弥补这一差距，我们开发了ChemAgent，这是一种基于ChemCrow的增强型化学代理，并对其在专门化学任务和普通化学问题上的表现进行了全面评估。令人惊讶的是，ChemAgent并不总是比没有工具的基础LLM表现出色。通过与化学专家进行错误分析，我们发现：对于专门的化学任务，例如合成预测，我们应该为代理配备专门的工具；然而，对于像考试中的普通化学问题，代理正确运用化学知识进行推理的能力更为重要，工具的配备并不总能提供帮助。|
|**2024-11-11**|**Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks**|Madeline Brumley et.al.|[2411.07213](http://arxiv.org/abs/2411.07213)|null|一个大型语言模型（LLMs）解释性研究的关键目标是开发方法以稳健地引导模型实现所需的行为。为此，提出了两种不同的解释方法——“自下而上”和“自上而下”。然而，这两种方法之间的定量比较很少。我们通过案例研究来比较两种代表性向量引导方法的效果：功能向量（FV；arXiv:2310.15213），作为自下而上的方法，以及情境向量（ICV；arXiv:2311.06668），作为自上而下的方法。尽管两者都旨在捕捉广泛情境学习任务的紧凑表示，但我们发现它们在特定类型的任务上效果不同：ICV在行为转变任务中优于FV，而FV在需要更高精度的任务中表现出色。我们讨论了这些发现对未来引导方法评估以及进一步研究自上而下和自下而上引导方法的影响。|
|**2024-11-11**|**DLCR: A Generative Data Expansion Framework via Diffusion for Clothes-Changing Person Re-ID**|Nyle Siddiqui et.al.|[2411.07205](http://arxiv.org/abs/2411.07205)|**[link](https://github.com/croitorualin/dlcr)**|**随着生成扩散模型的近期展示出的强大能力，一个开放的研究问题是“由这些模型生成的图像是否可以用于学习更好的视觉表征”。虽然这种生成数据扩展可能足以应对较简单的视觉任务，但我们探索了其在更困难的判别任务中的有效性：衣服更换人员重识别（CC-ReID）。CC-ReID的目标是在非重叠摄像头中匹配出现的人，即使他们在不同摄像头中改变了衣服。当前的CC-ReID模型不仅受到现有CC-ReID数据集中服装多样性有限的限制，而且生成保留重要个人特征以实现准确识别的额外数据也是一个当前挑战。为了解决这一问题，我们提出了一种名为DLCR的新数据扩展框架，该框架利用预训练的扩散和大型语言模型（LLMs）来准确生成具有不同服装的个体的多样化图像。我们为五个基准CC-ReID数据集（PRCC、CCVID、LaST、VC-Clothes和LTCC）生成了额外的数据，并将其服装多样性增加了10倍，总计生成了超过210万张图像。DLCR采用基于扩散的文本引导修复技术，条件是使用LLMs构建的服装提示，以生成仅修改主体服装同时保留其个人可识别特征的合成数据。通过这种大规模的数据增加，我们引入了两种新策略——渐进学习和测试时预测优化——分别减少训练时间和进一步提升CC-ReID性能。在PRCC数据集上，通过使用DLCR生成的数据训练CAL（一种以前的最先进方法），我们获得了显著的前1名准确率提升11.3%。我们将代码和每个数据集生成的数据公开发布在这里：https://github.com/CroitoruAlin/dlcr。**|
|**2024-11-11**|**The Super Weight in Large Language Models**|Mengxia Yu et.al.|[2411.07191](http://arxiv.org/abs/2411.07191)|**[link](https://github.com/mengxiayu/llmsuperweight)**|**近期的研究表明，大型语言模型（LLM）中的少量参数异常值对模型的质量至关重要。尽管LLM包含数十亿个参数，但这些小比例的异常值，如0.01%，相当于数万个参数。在这项工作中，我们提出了一个更为惊人的发现：仅仅剪枝一个参数就能破坏LLM生成文本的能力——使困惑度增加三个数量级，并将零样本准确率降低到随机猜测水平。我们提出了一种无需数据的方法，通过单次前向传递模型来识别这些参数，称为超级权重。我们进一步发现，这些超级权重会引起相应罕见且大的激活异常值，称为超级激活。当超级激活被高精度保留时，简单的四舍五入量化可以变得与最先进的方法竞争。对于权重量化，我们同样发现，通过保留超级权重并裁剪其他权重异常值，四舍五入量化可以扩展到比以前考虑的更大的块大小。为了促进对超级权重的进一步研究，我们提供了常见且公开可用的LLM的超级权重坐标索引。**|
|**2024-11-11**|**NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics**|David Robinson et.al.|[2411.07186](http://arxiv.org/abs/2411.07186)|null|大型语言模型（LLMs）在文本和音频提示下的表现代表了各种听觉任务的最先进水平，包括语音、音乐和一般音频，并展示了在未见过的任务中的新兴能力。然而，这些能力尚未在生物声学任务中得到充分展示，例如在大量记录中检测动物发声、分类稀有和濒危物种以及标注情境和行为——这些任务对于保护、生物多样性监测和动物行为研究至关重要。在这项工作中，我们介绍了NatureLM-音频，这是第一个专门设计用于生物声学的音频-语言基础模型。我们的精心策划的训练数据集包含了涵盖生物声学、语音和音乐数据多样范围的文本-音频对，旨在解决该领域有限注释数据集带来的挑战。我们证明了从音乐和语音中学到的表示可以成功转移到生物声学，并且我们的模型在未见过的分类群和任务上显示出有希望的泛化能力。重要的是，我们在一个新的基准（BEANS-Zero）上测试NatureLM-音频，并在几个生物声学任务上，包括未见物种的零样本分类，设定了新的最先进技术（SotA）。为了推进生物声学研究，我们还开源了生成训练和基准数据的代码，以及训练模型的代码。|
|**2024-11-11**|**Continual Memorization of Factoids in Large Language Models**|Howard Chen et.al.|[2411.07175](http://arxiv.org/abs/2411.07175)|**[link](https://github.com/princeton-nlp/continual-factoid-memorization)**|**大型语言模型可以通过预训练吸收大量知识，但预训练对于获取长尾或专门事实效率低下。因此，对反映世界变化的新知识进行微调变得流行起来，尽管这可能会破坏模型的原有能力。我们研究了这种脆弱性在持续记忆背景下的表现，即模型在一个小的长尾事实集上进行训练，并且在多次后续训练其他数据集后仍需保留这些事实。通过广泛的实验，我们发现LLM在多个后续任务中存在遗忘问题，简单的重放技术并不能完全防止遗忘，特别是在事实集在后期阶段进行训练时。我们认为有两条途径可以减轻遗忘：1）保护模型学习事实的过程；2）减少后期训练中的干扰。基于这一见解，我们开发了一种有效的缓解策略：REMIX（随机和通用数据混合）。REMIX通过在每个阶段混合来自预训练语料库或甚至随机生成的单词序列的通用数据来防止遗忘，尽管这些数据与第一阶段记忆的事实无关。REMIX能够在严重遗忘的情况下恢复性能，通常优于那些在第一阶段可以访问事实的基于重放的方法。然后我们分析了REMIX如何改变学习过程，并发现成功防止遗忘与一个模式相关：模型比平常更早地存储事实，并且多样化存储这些事实的层。REMIX的有效性为进一步研究记忆和遗忘的潜在动态开启了令人兴奋的研究可能性。**|
|**2024-11-11**|**A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis: Evaluating Mental Health Sentiment on Social Media during COVID-19**|Vedant Khandelwal et.al.|[2411.07163](http://arxiv.org/abs/2411.07163)|null|在健康危机（如COVID-19大流行）期间，通过社交媒体监控公众情绪可能是有帮助的。然而，传统的基于频率的数据驱动神经网络方法可能会错过新出现的相关内容，因为语言在动态变化的环境中不断演变。人为编写的符号知识源，例如标准语言和俚语词汇表，可能有助于提升动态语言中的社交媒体信号。我们介绍了一种神经符号方法，该方法结合了神经网络与符号知识源，增强了对与COVID-19相关的心理健康相关推文的检测和解释能力。我们的方法使用了大型数据集（约120亿条推文、250万条子版块数据和70万篇新闻文章）以及多个知识图谱进行了评估。这种方法能够动态适应不断变化的语言，在F1评分上超过了纯数据驱动模型，得分超过92%。此外，这种方法在适应新数据方面也更快，并且比微调预训练的大规模语言模型（LLM）具有更低的计算需求。本研究展示了神经符号方法在动态环境中进行文本解释（如健康监测）任务中的优势。|
|**2024-11-11**|**Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models**|Yancheng He et.al.|[2411.07140](http://arxiv.org/abs/2411.07140)|null|新提出的大型语言模型（LLMs）评估基准对于跟上LLMs的快速发展至关重要。在这项工作中，我们提出了Chinese SimpleQA，这是第一个全面的中文基准，用于评估语言模型回答简短问题的事实准确性能力。Chinese SimpleQA 主要有五个特性：中文、多样性、高质量、静态和易于评估。具体来说，首先，我们专注于涵盖6个主要话题和99个多样化子话题的中文语言。其次，我们进行了全面的质量控制过程，以确保问题和答案的高质量，其中参考答案是静态的，不会随时间改变。第三，遵循SimpleQA的原则，问题和答案都非常简短，并且根据OpenAI API进行易于评估的评分过程。基于Chinese SimpleQA，我们对现有LLMs的事实准确性能力进行了全面评估。最后，我们希望Chinese SimpleQA 能够帮助开发者更好地了解其模型的中文事实准确性能力，并促进基础模型的发展。|
|**2024-11-08**|**Recycled Attention: Efficient inference for long-context language models**|Fangyuan Xu et.al.|[2411.05787](http://arxiv.org/abs/2411.05787)|null|在给定长上下文输入的情况下生成长序列的标记对大型语言模型（LLMs）施加了沉重的计算负担。其中一个计算瓶颈来自于在每个生成步骤中对长序列输入计算注意力。在本文中，我们提出了一种名为“循环注意力”的推理时间方法，该方法在全上下文注意力和仅针对输入标记子集的注意力之间交替进行。在执行部分注意力时，我们循环利用之前执行全注意力的标记的注意力模式，并仅关注最受关注的前K个标记，从而减少了数据移动和注意力计算的成本。与之前提出的仅关注局部上下文或具有高累积注意力分数的标记的推理时间加速方法相比，我们的方法灵活地选择了与当前解码步骤相关的标记。我们在RULER上评估了我们的方法，RULER是一组任务，旨在全面评估长上下文能力以及长上下文语言建模任务。将我们的方法应用于现成的LLMs可以实现与仅考虑局部上下文的基线相当的速度提升，同时性能提升了2倍。我们进一步探索了两种改进性能-效率权衡的方法：（1）根据查询相似性动态决定何时执行循环注意力或全注意力步骤；（2）继续使用循环注意力对模型进行预训练。|
|**2024-11-08**|**Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?**|Veronica Chatrath et.al.|[2411.05775](http://arxiv.org/abs/2411.05775)|null|政治性错误信息对民主进程构成了重大挑战，影响了公众舆论和对媒体的信任。传统的手动事实核查方法存在可扩展性和注释者偏见的问题，而机器学习模型则需要大规模且成本高昂的标注数据集。本研究探讨了使用最先进的大型语言模型（LLMs）作为可靠的注释器来检测新闻文章中的政治真实性。我们利用开源的LLMs创建了一个具有政治多样性的数据集，并通过LLM生成的注释进行标注。这些注释由人类专家验证，并进一步通过基于LLM的判断者进行评估，以检验注释的准确性和可靠性。我们的方法提供了一种可扩展且稳健的替代传统事实核查的方案，增强了媒体透明度和公众信任。|
|**2024-11-08**|**Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024**|Christopher Malon et.al.|[2411.05762](http://arxiv.org/abs/2411.05762)|null|分离网络上的虚假信息和事实长期以来一直考验着人类的搜索和推理能力。我们展示了大型语言模型（LLMs）的推理能力和现代搜索引擎的检索能力可以结合起来，以自动化这一过程并可解释地验证主张。我们将LLMs和搜索结合在一个多跳证据追索策略下。该策略使用一个序列到序列模型根据输入的主张生成初始问题，搜索并回答该问题，并使用LLM迭代生成后续问题以追求缺失的证据。我们在FEVER 2024（AVeriTeC）共享任务上展示了我们的系统。与一次性生成所有问题的策略相比，我们的方法获得了0.045更高的标签准确率和0.155更高的AVeriTeC评分（评估证据的充分性）。通过消融研究，我们展示了各种设计选择的重要性，如问题生成方法、中等大小的上下文、一次对一份文档进行推理、添加元数据、改写、将问题简化为两类以及重新考虑最终判决。我们的提交系统在开发集上达到了0.510的AVeriTeC评分，在测试集上达到了0.477的AVeriTeC评分。|
|**2024-11-08**|**Unmasking the Limits of Large Language Models: A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal**|Fuka Matsuzaki et.al.|[2411.05665](http://arxiv.org/abs/2411.05665)|**[link](https://github.com/isfhub/maskcode)**|**本文通过严格评估大型语言模型（LLMs）处理被遮蔽文本的能力，揭示了其存在的局限性。我们引入了两个新任务：MskQA，用于衡量在像RealtimeQA这样的被遮蔽问答数据集上的推理能力；MskCal，用于评估在被遮蔽算术问题上的数值推理能力。通过对GPT-4o和4o-mini的测试发现，虽然这些大语言模型在处理被遮蔽文本时表现出一定的韧性，但它们的表现高度依赖于遮蔽率和语义线索。特别是当“完全遮蔽”时，即没有语义线索可用的情况下，性能显著下降，这表明大语言模型依赖于表面模式。有趣的是，GPT-4o在MskCal中的表现始终优于4o-mini，显示出更强的处理被遮蔽文本中的数值推理能力。这强调了语义线索在大语言模型推理过程中的关键作用。我们的研究揭示了背景知识与推理能力在处理被遮蔽文本时的相互作用，为进一步理解大语言模型的能力和局限性铺平了道路，并突显了需要更稳健的评估方法来准确评估它们的真实理解能力。**|
|**2024-11-08**|**The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent**|Leon O. H. Kroczek et.al.|[2411.05653](http://arxiv.org/abs/2411.05653)|null|大型语言模型（LLMs）在对话任务中展示了卓越的能力。将LLM具象化为虚拟人类允许用户在虚拟现实中进行面对面的社交互动。然而，在虚拟现实中的LLM控制代理与人之间的社交互动中，人格和任务相关因素的影响尚不清楚。在这项研究中，46名参与者与一个虚拟代理进行了互动，该代理的人格被操纵为外向或内向，并在三种不同的对话任务（闲聊、知识测试、说服）中进行了测试。通过评分评估了社会评价、情感体验和真实感。互动参与度通过量化参与者的词汇量和对话轮次来测量。最后，我们测量了参与者在知识测试期间寻求代理帮助的意愿。我们的研究结果表明，外向型代理得到了更积极的评价，引发了更愉快的体验和更高的参与度，并被认为比内向型代理更具真实性。然而，人格并没有影响求助倾向，但当参与者得到LLM的帮助时，他们通常对自己获得的答案更有信心。因此，LLM控制的具象化虚拟代理的人格特征变化会影响虚拟互动中的社会情感处理和行为。具象化的虚拟代理允许在虚拟环境中展示自然的社会互动。|
|**2024-11-08**|**LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution**|Yuheng Zhao et.al.|[2411.05651](http://arxiv.org/abs/2411.05651)|null|可视化分析（VA）要求分析师根据观察结果迭代地提出分析任务，并通过创建可视化和交互式探索来执行这些任务以获得洞察。这一过程需要编程、数据处理和可视化工具方面的技能，突显了对更智能、更精简的VA方法的需求。最近开发的大语言模型（LLM）作为代理，具备动态规划和使用工具的能力，为增强VA的效率和多功能性提供了潜力。我们提出了LightVA，这是一种轻量级的VA框架，通过人机协作支持任务分解、数据分析和交互式探索。我们的方法旨在帮助用户逐步将高层次的分析目标转化为低层次的任务，生成可视化并得出洞见。具体而言，我们引入了一种基于LLM代理的任务规划和执行策略，采用涉及规划者、执行者和控制器的递归过程。规划者负责推荐和分解任务，执行者处理任务执行，包括数据分析、可视化生成和多视图组合，而控制器则协调规划者和执行者之间的互动。在此框架基础上，我们开发了一个具有混合用户界面的系统，包括用于监控和管理任务规划过程的任务流程图、用于交互式数据探索的可视化面板以及用于通过自然语言指令引导模型的聊天视图。我们通过使用场景和专家研究检验了该方法的有效性。|
|**2024-11-08**|**Evaluating Large Language Model Capability in Vietnamese Fact-Checking Data Generation**|Long Truong To et.al.|[2411.05641](http://arxiv.org/abs/2411.05641)|null|大型语言模型（LLMs）随着阅读理解和推理能力的逐步提高，被应用于各种复杂的语言任务，包括为各种目的自动生成语言数据。然而，针对低资源语言如越南语的自动数据生成研究仍不充分，缺乏全面的评估。在本文中，我们探索了使用LLMs进行越南语事实核查任务的自动数据生成，该任务面临显著的数据限制。具体而言，我们关注从多条证据句子中合成声明的事实核查数据，以评估LLMs的信息合成能力。我们开发了一种使用简单提示技术的自动数据构建过程，并探索了几种提高生成数据质量的方法。为了评估LLMs生成数据的质量，我们进行了手动质量评估和使用语言模型的性能评估。实验结果和手动评估表明，尽管通过微调技术显著提高了生成数据的质量，但LLMs仍然无法与人类产生的数据质量相匹配。|
|**2024-11-08**|**Assessing Open-Source Large Language Models on Argumentation Mining Subtasks**|Mohammad Yeghaneh Abkenar et.al.|[2411.05639](http://arxiv.org/abs/2411.05639)|null|我们探讨了四种开源大型语言模型（LLMs）在论辩挖掘（AM）中的能力。我们在三个不同的语料库上进行实验：说服性文章（PE）、论辩微文本（AMT）第一部分和第二部分，基于两个论辩挖掘子任务：（i）论辩话语单元分类（ADUC）和（ii）论辩关系分类（ARC）。本研究旨在评估包括Mistral 7B、Mixtral8x7B、LlamA2 7B和LlamA3 8B在内的开源LLMs在零样本和少量样本场景下的论辩能力。我们的分析有助于未来的研究进一步评估使用开源LLMs进行计算论辩。|
|**2024-11-08**|**A Two-Step Concept-Based Approach for Enhanced Interpretability and Trust in Skin Lesion Diagnosis**|Cristiano Patrício et.al.|[2411.05609](http://arxiv.org/abs/2411.05609)|**[link](https://github.com/cristianopatricio/2-step-concept-based-skin-diagnosis)**|临床环境中深度学习系统应用的主要障碍是标注数据的稀缺以及对这些系统的可解释性和信任度不足。概念瓶颈模型（CBMs）通过将最终疾病预测限制在一组人类可理解的概念上来提供内在的可解释性。然而，这种内在的可解释性以更大的注释负担为代价，此外，添加新概念需要重新训练整个系统。在这项工作中，我们介绍了一种新颖的两步方法来解决这两个挑战。通过模拟CBM的两个阶段，我们利用预训练的视觉语言模型（VLM）自动预测临床概念，然后利用大型语言模型（LLM）基于预测的概念生成疾病诊断。我们在三个皮肤病变数据集上验证了我们的方法，结果表明该方法优于传统的CBMs和最先进的可解释方法，且无需任何训练，并仅使用少量标注示例。代码可在https://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis获取。|
|**2024-11-08**|**Evaluating and Adapting Large Language Models to Represent Folktales in Low-Resource Languages**|JA Meaney et.al.|[2411.05593](http://arxiv.org/abs/2411.05593)|null|民间故事是了解一个文明的社会和文化的宝贵资源。数字民俗研究旨在利用自动化技术更好地理解这些民间故事，并依赖于文本数据的抽象表示。尽管许多大型语言模型（LLMs）声称能够表示像爱尔兰语和盖尔语这样低资源的语言，我们提出了两个分类任务来探索这些表示的有用性，并进行了三种适应性改进以提高这些模型的性能。我们发现，调整模型以处理更长的序列，并继续在民间故事领域进行预训练可以提高分类性能，但这些发现受到基线支持向量机（SVM）使用非上下文特征时表现优异的影响。|
|**2024-11-07**|**SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models**|Muyang Li et.al.|[2411.05007](http://arxiv.org/abs/2411.05007)|**[link](https://github.com/mit-han-lab/deepcompressor)**|**扩散模型在生成高质量图像方面已被证明非常有效。然而，随着这些模型变得更大，它们需要显著更多的内存，并且延迟更高，这给部署带来了重大挑战。在这项工作中，我们旨在通过将权重和激活量化到4位来加速扩散模型。在如此激进的量化级别上，权重和激活都非常敏感，传统的用于大型语言模型的后训练量化方法如平滑处理变得不够充分。为了解决这一限制，我们提出了一种新的4位量化范式SVDQuant。与平滑处理不同，后者在权重和激活之间重新分配异常值，我们的方法使用低秩分支吸收这些异常值。我们首先通过将异常值从激活转移到权重来集中这些异常值，然后利用高精度低秩分支并通过奇异值分解（SVD）处理权重异常值。这一过程减轻了两侧的量化难度。然而，简单地独立运行低秩分支会由于激活的额外数据移动导致显著的开销，从而抵消了量化带来的速度提升。为了解决这个问题，我们设计了一个名为Nunchaku的推理引擎，将低秩分支的内核融合到低比特分支中，以减少冗余的内存访问。它还可以无缝支持现成的低秩适配器（LoRAs），而无需重新量化。我们在SDXL、PixArt- $\Sigma$ 和FLUX.1上的广泛实验验证了SVDQuant在保持图像质量方面的有效性。我们将12B FLUX.1模型的内存使用减少了3.5倍，在16GB笔记本电脑4090 GPU上比4位仅权重量化基线快3.0倍，为PC上的更多交互式应用铺平了道路。我们的量化库和推理引擎已开源。**|
|**2024-11-07**|**Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?**|Jonathan Roberts et.al.|[2411.05000](http://arxiv.org/abs/2411.05000)|null|随着大型语言模型（LLMs）的上下文限制增加，可能的应用和下游功能范围也在扩大。在许多实际任务中，决策依赖于分散在多个通常不相关的文档中的细节，这些文档大多包含无关信息。长上下文LLMs似乎非常适合这种复杂的信息检索和推理，这在传统上证明是昂贵且耗时的。然而，尽管近年来较长上下文模型的发展取得了快速进展，但我们对LLMs如何有效使用其上下文的理解并未跟上步伐。为了解决这个问题，我们进行了一系列检索实验，旨在评估17个领先LLMs的能力，例如它们通过上下文窗口跟踪信息线索的能力。令人惊讶的是，我们发现许多模型具有非常强的“线程安全”能力：能够在不显著降低性能的情况下同时跟踪多个线索。然而，对于许多模型，我们发现有效的上下文限制明显短于支持的上下文长度，在上下文窗口增大时准确度会下降。我们的研究还强调了一个重要观点，即不同分词器的token计数不应直接比较——它们通常对应着显著不同的书面字符数量。我们将代码和长上下文实验数据公开。|
|**2024-11-07**|**LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation**|Weiquan Huang et.al.|[2411.04997](http://arxiv.org/abs/2411.04997)|**[link](https://github.com/microsoft/LLM2CLIP)**|**CLIP是当今最重要的多模态基础模型之一。其强大的能力源自自然语言提供的丰富监督信号，自然语言作为人类知识的载体，塑造了一个强大的跨模态表示空间。然而，随着大型语言模型（LLM）如GPT-4和LLaMA的快速发展，语言理解和生成的边界不断被拓展。这引发了一个有趣的问题：能否利用LLM的能力进一步提升多模态表示学习？将LLM整合到CLIP中的潜在好处显而易见。LLM在文本理解方面具有很强的能力，可以从根本上提高CLIP处理图像描述的能力，显著增强其处理长且复杂文本的能力，这是原始CLIP的一个已知限制。此外，LLM在庞大的文本语料库上进行训练，拥有开放世界的知识。这使其在训练过程中能够扩展描述信息，从而提高学习过程的效率。在本文中，我们提出了一种名为LLM2CLIP的新方法，该方法利用LLM的力量来释放CLIP的潜力。通过在描述空间中使用对比学习微调LLM，我们将文本能力提取到输出嵌入中，显著提高了输出层的文本辨别力。然后，我们设计了一种高效的训练过程，在此过程中，微调后的LLM作为CLIP视觉编码器的强大教师。由于LLM的存在，我们现在可以使用更长、更复杂的描述，而不受原始CLIP文本编码器上下文窗口和能力限制的影响。我们的实验表明，这种方法在跨模态任务中带来了显著的改进。**|
|**2024-11-07**|**Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models**|Weixin Liang et.al.|[2411.04996](http://arxiv.org/abs/2411.04996)|null|大型语言模型（LLMs）的发展已经扩展到多模态系统，这些系统能够在统一框架内处理文本、图像和语音。然而，训练这些模型需要比仅处理文本的LLMs显著更大的数据集和计算资源。为了解决这一扩展挑战，我们引入了Mixture-of-Transformers（MoT），这是一种稀疏的多模态变压器架构，能够显著减少预训练所需的计算成本。MoT通过模态解耦模型的非嵌入参数——包括前馈网络、注意力矩阵和层归一化——从而实现特定于模态的处理，并对整个输入序列进行全局自注意力处理。我们在多个设置和模型规模下评估了MoT的表现。在Chameleon 7B设置（用于文本和图像生成的自回归任务）中，MoT使用仅为密集基线模型55.8%的浮点运算（FLOPs）就达到了与之相当的性能。当扩展到包括语音时，MoT在语音性能上达到与密集基线模型相当的水平，但只使用了37.2%的FLOPs。在Transfusion设置中，其中文本和图像以不同的目标进行训练，一个7B的MoT模型在图像模态上的表现与密集基线模型相当，但使用的FLOPs仅为后者的一半；而一个760M的MoT模型在关键的图像生成指标上优于一个1.4B的密集基线模型。系统分析进一步展示了MoT的实际优势，在AWS p4de.24xlarge实例（配备NVIDIA A100 GPU）上，MoT实现了与密集基线模型相当的图像质量，所需时间仅为后者的47.2%，而在文本质量方面则只需75.6%的时间。|
|**2024-11-07**|**Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives**|Hao Sun et.al.|[2411.04991](http://arxiv.org/abs/2411.04991)|**[link](https://github.com/holarissun/rewardmodelingbeyondbradleyterry)**|**Bradley-Terry（BT）模型在大型语言模型（LLM）对齐的奖励建模中是一种常见且成功的方法。然而，目前尚不清楚为什么最初为多玩家随机博弈匹配而开发的这一模型可以被用于将成对响应比较转换为奖励值并进行预测，尤其是在只有有限数量的提示-响应对稀疏地与其他对进行比较的情况下。在本文中，我们首先重新审视了使用BT模型进行奖励建模的基础，并基于使用嵌入的深度神经网络建立了BT奖励模型的收敛率，为其使用提供了理论基础。尽管从理论上讲是合理的，但我们认为从下游优化的角度来看，BT模型并不是必要的选择。这是因为奖励模型只需要通过真奖励的单调变换来保持正确的排名预测。我们强调了奖励建模中的关键概念——顺序一致性，并证明BT模型具备这种特性。因此，我们提出了一种简单直接的上限算法，与现成的二元分类器兼容，作为顺序一致的奖励建模目标。为了提供实用的见解，我们在超过12,000个实验设置中对这些不同的奖励建模方法进行了实证评估，使用了6种基础LLM、2个数据集以及多样化的注释设计，这些设计在数量、质量和偏好注释的配对选择上有所不同。**|
|**2024-11-07**|**Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries**|Dylan Manuel et.al.|[2411.04981](http://arxiv.org/abs/2411.04981)|null|安全专家通过反汇编（反编译）二进制代码来识别关键的安全漏洞。在关键基础设施（CI）中使用的固件、驱动程序和专有软件等重要系统中，源代码的访问受限，这使得在二进制级别上进行这种分析变得尤为重要。即使有可用的源代码，编译后源代码与处理器执行的二进制代码之间也存在语义差距，这可能阻碍对源代码中漏洞的检测。目前，关于大型语言模型（LLMs）的研究忽略了在这一领域分析反编译二进制代码的重要性，而仅关注源代码。在这项工作中，我们首次通过实证研究揭示了最先进的LLMs在分析反编译二进制代码中的漏洞时存在的重大语义限制，主要是由于缺乏相关数据集。为了弥补这一差距，我们引入了DeBinVul，这是一个新的反编译二进制代码漏洞数据集。我们的数据集是多架构和多优化的，专注于C/C++，因为它们在CI中的广泛应用以及与众多漏洞的关联。具体来说，我们为任务（i）识别；（ii）分类；（iii）描述漏洞；以及（iv）恢复反编译二进制代码中的函数名称，整理了150,872个漏洞和非漏洞样本。随后，我们使用DeBinVul微调了最先进的LLMs，并报告了CodeLlama、Llama3和CodeGen2在检测二进制代码漏洞方面的性能分别提高了19%、24%和21%。此外，使用DeBinVul，我们在漏洞分类任务中报告了80-90%的高性能。另外，我们还报告了在函数名称恢复和漏洞描述任务中的性能提升。|
|**2024-11-07**|**SuffixDecoding: A Model-Free Approach to Speeding Up Large Language Model Inference**|Gabriele Oliaro et.al.|[2411.04975](http://arxiv.org/abs/2411.04975)|null|我们提出了SuffixDecoding，这是一种新颖的无模型方法，通过推测性解码来加速大型语言模型（LLM）的推理过程。与依赖于草稿模型或专门解码头的现有方法不同，SuffixDecoding利用从先前生成的输出构建的后缀树来高效预测候选令牌序列。我们的方法能够在不增加维护和协调额外模型开销的情况下实现灵活的树结构推测。SuffixDecoding构建并动态更新后缀树以捕捉生成文本中的模式，并使用基于经验令牌频率的有原则的评分机制来构建推测树。SuffixDecoding仅需要CPU内存，而典型的LLM服务节点上这种内存非常充裕且未充分利用。我们证明了SuffixDecoding在各种工作负载下，包括开放式聊天、代码生成和文本到SQL任务，实现了与基于模型的方法竞争的速度提升。对于开放式聊天和代码生成任务，SuffixDecoding实现了高达1.4倍的输出吞吐量提升，以及比SpecInfer低1.1倍的时间每令牌（TPOT）延迟。对于一个专有的多LLM文本到SQL应用，SuffixDecoding实现了高达2.9倍的输出吞吐量提升和3倍的延迟降低。我们的评估表明，即使参考语料库很小，只有256个示例，SuffixDecoding也能保持高接受率，同时随着更多历史输出的纳入，性能继续提升。|
|**2024-11-07**|**BitNet a4.8: 4-bit Activations for 1-bit LLMs**|Hongyu Wang et.al.|[2411.04965](http://arxiv.org/abs/2411.04965)|null|最近的研究表明，如BitNet b1.58这样的1比特大语言模型（LLMs）在减少推理成本的同时保持了模型性能，这是一个很有前景的方向。在这项工作中，我们介绍了BitNet a4.8，它通过启用4比特激活来实现1比特LLMs的优化。BitNet a4.8采用了一种混合量化和稀疏化策略，以减轻由异常通道引入的量化误差。具体来说，我们在注意力和前馈网络层的输入中使用4比特激活，同时对中间状态进行稀疏化处理，并随后进行8比特量化。广泛的实验表明，BitNet a4.8在等效的训练成本下实现了与BitNet b1.58相当的性能，同时由于启用了4比特（INT4/FP4）内核而加快了推理速度。此外，BitNet a4.8仅激活55%的参数，并支持3比特KV缓存，进一步提高了大规模LLM部署和推理的效率。|
|**2024-11-07**|**Position Paper On Diagnostic Uncertainty Estimation from Large Language Models: Next-Word Probability Is Not Pre-test Probability**|Yanjun Gao et.al.|[2411.04962](http://arxiv.org/abs/2411.04962)|null|大型语言模型（LLMs）正在被探索用于诊断决策支持，但它们估计先验概率的能力，这对于临床决策至关重要，仍然有限。本研究评估了两种大型语言模型，即Mistral-7B和Llama3-70B，使用结构化的电子健康记录数据在三个诊断任务上进行测试。我们检查了从LLM概率估计中提取的三种当前方法，并揭示了它们的局限性。我们的目的是强调改进LLM置信度估计技术的需求。|
|**2024-11-07**|**CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM**|Jingwei Xu et.al.|[2411.04954](http://arxiv.org/abs/2411.04954)|null|本文旨在设计一个统一的计算机辅助设计（CAD）生成系统，该系统能够根据用户的输入（如文本描述、图像、点云或它们的组合）轻松生成CAD模型。为此，我们引入了CAD-MLLM，这是第一个能够在多模态输入条件下生成参数化CAD模型的系统。具体来说，在CAD-MLLM框架内，我们利用CAD模型的命令序列，并采用先进的大型语言模型（LLM）来对齐这些不同多模态数据和CAD模型向量表示之间的特征空间。为了促进模型训练，我们设计了一个全面的数据构建和注释管道，使每个CAD模型都配备相应的多模态数据。由此产生的数据集名为Omni-CAD，是首个包含每个CAD模型的文字描述、多视角图像、点云以及命令序列的多模态CAD数据集。该数据集包含大约450,000个实例及其CAD构造序列。为了全面评估生成的CAD模型的质量，我们超越了当前侧重于重建质量的评估指标，引入了额外的指标来评估拓扑质量和表面封闭程度。广泛的实验结果表明，CAD-MLLM显著优于现有的条件生成方法，并且在面对噪声和缺失点时仍然保持高度稳健。项目页面和更多可视化内容可以在https://cad-mllm.github.io/找到。|
|**2024-11-06**|**Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?**|Daniel P. Jeong et.al.|[2411.04118](http://arxiv.org/abs/2411.04118)|**[link](https://github.com/taekb/eval-medical-dapt)**|近年来，有几项研究致力于开发专门用于医学应用的基础模型，通过在公开的生物医学语料库上进行持续预训练来改进通用大型语言模型（LLMs）和视觉-语言模型（VLMs）。这些研究通常声称这种领域适应性预训练（DAPT）能提高下游医学任务的表现，例如回答医学执照考试问题。在本文中，我们比较了七个公共“医学”LLMs和两个VLMs与其对应的基线模型，并得出了不同的结论：所有医学VLMs和几乎所有医学LLMs在零样本/少样本提示下进行医学问答（QA）任务时，并没有始终如一地优于其基线模型。例如，在我们考虑的3次提示设置下的任务和模型对中，医学LLMs仅在其基线模型表现更差的情况下占12.1%，与基线模型持平的情况占49.8%，而在其余38.2%的情况下则显著逊色于基线模型。我们的结论基于以下几点：(i) 将每个医学模型直接与对应的基线模型进行头对头比较；(ii) 为每个模型单独优化提示；(iii) 考虑到比较中的统计不确定性。虽然这些基本做法在文献中并不总是被采用，但我们的消融实验表明，它们对结论有重大影响。我们的发现表明，最先进的通用领域模型可能已经具备较强的医学知识和推理能力，并提出了加强未来研究结论的建议。|
|**2024-11-06**|**How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis**|Guan Zhe Hong et.al.|[2411.04105](http://arxiv.org/abs/2411.04105)|null|大型语言模型（LLMs）在需要规划和推理的任务上表现出色。受此启发，我们研究了网络执行复杂逻辑推理的内部机制。我们首先构建了一个命题逻辑问题作为网络训练和评估的具体测试平台。关键在于，这个问题需要非平凡的规划才能解决，但我们能够训练一个小的变压器以实现完美的准确性。在此设置的基础上，我们进一步研究了一个三层变压器，从零开始训练它如何解决这个问题。我们能够识别出某些“规划”和“推理”电路，这些电路需要注意力块之间的合作来实现所需的逻辑。为了扩展我们的发现，我们还研究了更大的模型Mistral 7B。通过激活修补技术，我们描述了内部组件中对解决我们的逻辑问题至关重要的部分。总体而言，我们的工作系统地揭示了小型和大型变压器的新颖方面，并继续研究它们如何进行规划和推理。|
|**2024-11-06**|**Textual Decomposition Then Sub-motion-space Scattering for Open-Vocabulary Motion Generation**|Ke Fan et.al.|[2411.04079](http://arxiv.org/abs/2411.04079)|null|文本到动作生成是计算机视觉中的一个重要任务，通过给定的文本生成目标三维动作。现有的标注数据集规模有限，导致大多数现有方法对小数据集过拟合，无法推广到开放域的动作。一些方法试图通过与CLIP空间对齐或使用预训练然后微调的方法来解决开放词汇动作生成问题。然而，当前标注数据集的有限规模使得它们只能实现从子文本空间到子动作空间的映射，而不是实现全文本空间和全动作空间之间的映射（完整映射），这是实现开放词汇动作生成的关键。为此，本文提出利用原子动作（短时间内的简单身体部分动作）作为中间表示，并采用两个有序耦合步骤，即文本分解和子动作空间散射，以解决完整映射问题。对于文本分解，我们设计了一种细粒度描述转换算法，并结合大型语言模型的泛化能力，将任何给定的动作文本转换为原子文本。子动作空间散射学习从原子动作到目标动作的合成过程，使学习到的子动作空间散射形成完整的动作空间。对于给定的开放域动作，它将外推转换为插值，从而显著提高泛化能力。我们的网络 $DSO$-Net结合了文本分解和子动作空间散射来解决开放词汇动作生成问题。广泛的实验表明，我们的$DSO$ -Net在开放词汇动作生成方面显著优于最先进的方法。代码可在<https://vankouf.github.io/DSONet/>获取。|
|**2024-11-06**|**Beemo: Benchmark of Expert-edited Machine-generated Outputs**|Ekaterina Artemova et.al.|[2411.04032](http://arxiv.org/abs/2411.04032)|**[link](https://github.com/Toloka/beemo)**|大规模语言模型（LLMs）的迅速普及增加了机器生成文本（MGTs）的数量，并在各种领域模糊了文本作者身份。然而，大多数现有的MGT基准测试包括单一作者的文本（人类撰写和机器生成）。这种传统的设计未能捕捉到更实际的多作者场景，在这些场景中，用户会根据自然流畅性、连贯性和事实准确性对LLM的响应进行润色。我们的论文介绍了Beemo基准，这是一个包含6500篇由人类撰写、十个经过指令微调的LLM生成并由专家编辑的文本的数据集，涵盖了从创意写作到总结的各种应用场景。此外，Beemo还包括13100篇机器生成并经过LLM编辑的文本，从而允许对各种MGT检测进行多样化评估。我们记录了Beemo的创建协议，并展示了在不同实验设置下基准测试33种MGT检测配置的结果。我们发现，基于专家的编辑能够逃避MGT检测，而经过LLM编辑的文本不太可能被识别为人类撰写。Beemo及其所有材料均公开可用。|
|**2024-11-06**|**Prompt Engineering Using GPT for Word-Level Code-Mixed Language Identification in Low-Resource Dravidian Languages**|Aniket Deroy et.al.|[2411.04025](http://arxiv.org/abs/2411.04025)|null|语言识别（LI）对于各种自然语言处理任务至关重要，是情感分析、机器翻译和信息检索等应用的基础步骤。在印度这样的多语言社会中，特别是在年轻人在社交媒体上交流时，文本常常表现出代码混合现象，即在不同语言层次上将本地语言与英语混合。这种现象给LI系统带来了巨大挑战，尤其是在单个单词内语言交织的情况下。德拉威语系语言在印度南部广泛使用，具有丰富的形态结构，但由于数字平台上的代表性不足，导致采用罗马字母或混合脚本进行交流。本文介绍了一种基于提示的方法，旨在解决德拉威语系语言在单词级语言识别中的挑战。在这项工作中，我们利用GPT-3.5 Turbo来了解大型语言模型是否能够正确地对单词进行分类。我们的研究结果表明，卡纳达语模型在大多数指标上均优于泰米尔语模型，显示出更高的准确性和可靠性，能够在识别和分类卡纳达语实例方面表现更好。相比之下，泰米尔语模型表现一般，特别是在精确度和召回率方面需要改进。|
|**2024-11-06**|**Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning**|Jiawei Yao et.al.|[2411.03978](http://arxiv.org/abs/2411.03978)|**[link](https://github.com/alexander-yao/multi-sub)**|多聚类旨在从不同方面发现数据中的各种潜在结构。深度多聚类方法通过利用数据中的复杂模式和关系取得了显著的性能。然而，现有的工作在灵活适应用户特定需求的数据分组方面存在困难，这可能需要手动理解每种聚类。为了解决这些限制，我们在本文中引入了Multi-Sub，这是一种新颖的端到端多聚类方法，采用了多模态子空间代理学习框架。通过利用CLIP和GPT-4的协同能力，Multi-Sub将表达用户偏好的文本提示与其对应的视觉表示对齐。这是通过自动从大规模语言模型生成代理词来实现的，这些代理词充当子空间基底，从而允许根据用户的兴趣定制数据表示。我们的方法在广泛的视觉多聚类任务数据集上始终优于现有基线。我们的代码可在https://github.com/Alexander-Yao/Multi-Sub获取。|
|**2024-11-06**|**What Really is Commonsense Knowledge?**|Quyet V. Do et.al.|[2411.03964](http://arxiv.org/abs/2411.03964)|null|常识数据集在自然语言处理领域得到了很好的发展，主要通过众包方式进行人工标注。然而，对于常识推理基准存在一些争议。具体来说，某些常识基准中的很大一部分实例并不涉及常识知识。这一问题会削弱对评估模型真正常识推理能力的测量。此外，这个问题源于常识知识概念模糊，与其它类型的知识区分不明显。为了澄清所有上述主张，本研究调查了现有常识知识的定义，基于三个框架来定义概念，并将它们整合成一个多框架统一的常识知识定义（即综合定义）。然后，我们使用综合定义对CommonsenseQA和CommonsenseQA 2.0数据集进行标注和实验，以检验上述主张。我们的研究表明，在这两个数据集中存在大量非常识知识实例，而且在这两个子集上大型语言模型（LLMs）的表现较差，特别是在常识知识实例上。|
|**2024-11-06**|**How Does A Text Preprocessing Pipeline Affect Ontology Syntactic Matching?**|Zhangcheng Qiang et.al.|[2411.03962](http://arxiv.org/abs/2411.03962)|**[link](https://github.com/qzc438/ontology-llm)**|在许多本体匹配（OM）系统中，已经实现了包含分词、标准化、停用词去除和词干提取/词形还原的通用文本预处理管道。然而，文本预处理标准的缺乏导致了映射结果的多样性。本文研究了文本预处理管道在句法层面的OM任务中的影响。通过对8个本体对齐评估倡议（OAEI）轨道存储库中的49个不同对齐实验表明：（1）分词和标准化目前比停用词去除和词干提取/词形还原更有效；（2）词形还原和词干提取的选择是任务特定的。我们建议使用独立的词形还原或词干提取，并辅以后续校正。我们发现（3）Porter词干提取器和Snowball词干提取器的表现优于Lancaster词干提取器；（4）词性标注对词形还原没有帮助。为了修复OM任务中效果不佳的停用词去除和词干提取/词形还原，我们提出了一种新的基于上下文的管道修复方法，该方法显著提高了匹配正确性和整体匹配性能。我们还讨论了大型语言模型（LLMs）时代中文本预处理管道的使用。|
|**2024-11-06**|**Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in Retrieval-Augmented Generation**|Yuhang Liu et.al.|[2411.03957](http://arxiv.org/abs/2411.03957)|null|检索增强生成（RAG）已被证明是解决大型语言模型（LLMs）固有幻觉问题的有效方法。先前的方法通常基于语义相似性训练检索器，缺乏对RAG的优化。更近的研究提出了让检索器与LLMs的偏好信号对齐的方法。然而，这些偏好信号对于密集检索器（通常具有较弱的语言能力）来说往往难以理解和有效学习。受到指导发现学习等教学理论的启发，我们提出了一种新的框架FiGRet（细粒度引导检索器），该框架利用LLMs的语言能力从更精细、信息为中心的角度构建示例，以引导检索器的学习。具体而言，我们的方法利用LLMs从检索器表现不佳的样本中构建易于理解的示例，重点关注与RAG场景高度相关的三个学习目标：相关性、全面性和纯净性。这些示例作为脚手架，最终使检索器与LLMs的偏好对齐。此外，我们采用双重课程学习策略，并利用LLM和检索器之间的互反馈进一步提升RAG系统的性能。一系列实验表明，我们提出的框架提升了配备不同检索器的RAG系统的性能，并适用于各种LLMs。|
|**2024-11-06**|**Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of Study in Tabletop Role-Playing Games Soundtracks**|Felipe Marra et.al.|[2411.03948](http://arxiv.org/abs/2411.03948)|**[link](https://github.com/felipemarra/babel-bardo)**|本文研究了文本到音频音乐生成模型在生成长篇音乐方面的表现，特别是在提示随时间变化的情况下，重点是为桌面角色扮演游戏（TRPG）创作配乐。我们介绍了一种名为Babel Bardo的系统，该系统利用大型语言模型（LLM）将语音转录转换为音乐描述，以控制文本到音乐的生成模型。在两个TRPG活动中比较了Babel Bardo的四个版本：一个基线版本使用直接的语音转录，以及三个基于LLM的版本，它们采用了不同的音乐描述生成方法。评估考虑了音频质量、故事情节的一致性和过渡的流畅性。结果表明，详细的音乐描述可以提高音频质量，而连续描述之间的一致性能增强故事情节的一致性和过渡的流畅性。|
|**2024-11-05**|**LLMs for Domain Generation Algorithm Detection**|Reynier Leyva La O et.al.|[2411.03307](http://arxiv.org/abs/2411.03307)|null|本文分析了使用大型语言模型（LLMs）来检测域名生成算法（DGAs）的应用。我们详细评估了两种重要技术：情境学习（ICL）和监督微调（SFT），展示了它们如何提高检测效果。SFT通过使用特定领域的数据提高了性能，而ICL则帮助检测模型快速适应新威胁，而无需大量的再训练。我们使用Meta的Llama3 8B模型，在一个自定义数据集上进行实验，该数据集包含了68个恶意软件家族和正常域名，涵盖了多个难以检测的方案，包括最近的基于词汇的DGAs。结果证明，基于LLM的方法在DGA检测方面可以达到具有竞争力的结果。特别是基于SFT的LLM DGA检测器在使用注意力层的最先进模型基础上实现了超越，达到了94%的准确率和4%的误报率，并且在检测基于词汇的DGA域名方面表现出色。|
|**2024-11-05**|**VERITAS: A Unified Approach to Reliability Evaluation**|Rajkumar Ramamurthy et.al.|[2411.03300](http://arxiv.org/abs/2411.03300)|null|大型语言模型（LLMs）通常无法从上下文中综合信息以生成准确的响应。这使得它们在知识密集型场景中变得不可靠，在这些场景中，输出的可靠性至关重要。对于可靠的LLM来说，集成一个强大的事实核查系统以检测各种格式中的幻觉是一个关键组成部分。虽然有一些开放访问的事实核查模型可用，但它们的功能往往局限于特定任务，如基于事实的问题回答或蕴涵验证，并且在对话设置中的表现较差。另一方面，封闭访问的模型如GPT-4和Claude提供了更大的灵活性，适用于不同的上下文，包括基于事实的对话验证，但受到高成本和延迟的限制。在这项工作中，我们介绍了VERITAS，这是一个幻觉检测模型家族，旨在灵活地跨多种上下文运行，同时最小化延迟和成本。VERITAS在所有主要幻觉检测基准上的平均性能达到了最先进的水平，与类似大小的模型相比，其平均性能提高了10%，并且接近GPT4涡轮在大模型作为裁判设置下的性能。|
|**2024-11-05**|**Examining Human-AI Collaboration for Co-Writing Constructive Comments Online**|Farhana Shahid et.al.|[2411.03295](http://arxiv.org/abs/2411.03295)|null|本文研究了大型语言模型（LLMs）如何帮助人们在涉及分裂性社会问题的在线辩论中撰写建设性的评论，并探讨了不同文化背景下对建设性的理解是否存在差异。通过针对来自印度和美国的600名参与者进行的控制实验，这些参与者审查并撰写了关于伊斯兰恐惧症和同性恋恐惧症在线帖子的建设性评论，我们发现LLMs与人类对在线评论的建设性感知存在潜在不一致。尽管LLM更倾向于认为辩证性评论更具建设性，但参与者更重视逻辑性和事实性。尽管存在这些差异，参与者仍认为LLM生成的和人机协作撰写的评论比独立由人类撰写的评论更具建设性。我们的分析还显示，LLM生成的和人机协作撰写的评论表现出更多与建设性相关的语言特征，相比人类撰写的关于分裂性话题的评论而言。当参与者使用LLMs来改进他们的评论时，最终的评论更长、更有礼貌、更积极、毒性更低且更易读，增加了论证特征，保留了原始意图，但偶尔会失去一些细微之处。基于这些发现，我们讨论了在利用LLMs促进在线建设性讨论时的伦理和设计考虑。|
|**2024-11-05**|**Interaction2Code: How Far Are We From Automatic Interactive Webpage Generation?**|Jingyu Xiao et.al.|[2411.03292](http://arxiv.org/abs/2411.03292)|**[link](https://github.com/webpai/interaction2code)**|将网页设计转换为功能性的用户界面代码是构建网站的关键步骤，但这一过程可能非常繁琐且耗时。为了自动化这一设计到代码的转换过程，已经提出了各种基于学习网络和多模态大语言模型（MLLMs）的方法。然而，这些研究仅在少量静态网页上进行评估，并忽略了动态交互元素，这使得它们在实际网站部署中的应用价值有限。为此，我们首次系统地研究了MLLMs在生成交互式网页方面的表现。具体来说，我们首先定义了交互到代码的任务，并构建了Interaction2Code基准数据集，该数据集包含97个独特的网页和213种不同的交互，涵盖15种网页类型和30种交互类别。然后，我们使用三种最先进的MLLMs进行了全面实验，结合自动度量指标和人工评估，总结出六个发现。实验结果突显了MLLMs在生成细粒度交互特征以及处理复杂转换和细微视觉修改的交互方面存在的局限性。我们进一步分析了失败案例及其根本原因，识别出10种常见的失败类型并评估了其严重程度。此外，我们的发现揭示了三个关键影响因素，即提示、视觉显著性和文本描述，这些因素可以提升MLLMs在交互生成方面的性能。基于这些发现，我们为研究人员和开发者提供了启示，为该领域的未来进展奠定了基础。数据集和源代码可在https://github.com/WebPAI/Interaction2Code获取。|
|**2024-11-05**|**The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare**|Souren Pashangpour et.al.|[2411.03287](http://arxiv.org/abs/2411.03287)|null|大型语言模型（LLMs）在医疗机器人中的潜在应用可以帮助应对全球医疗系统面临的巨大需求，特别是在人口老龄化和医疗专业人员短缺的情况下。尽管LLMs已经被整合到医学领域以协助医生和患者，但在临床环境中将LLMs集成到医疗机器人中尚未得到探索。在这篇视角论文中，我们研究了机器人技术和LLMs的突破性发展，以独特地确定设计面向健康领域的基于LLM的机器人的所需系统要求，包括通过人机交互（HRIs）进行多模态通信、语义推理和任务规划。此外，我们还讨论了这一新兴创新领域的伦理问题、开放挑战以及潜在的未来研究方向。|
|**2024-11-05**|**SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents**|Dawei Li et.al.|[2411.03284](http://arxiv.org/abs/2411.03284)|**[link](https://github.com/david-li0406/smoa)**|**尽管多智能体系统在各种任务和应用中显著提升了大型语言模型（LLMs）的性能，但智能体之间的密集交互可能会妨碍其效率和多样性。为了解决这些挑战，我们从稀疏混合智能体（SMoE）框架中汲取灵感，并提出了一种稀疏混合智能体（SMoA）框架，以提升多智能体LLMs的效率和多样性。与完全连接的结构不同，SMoA引入了响应选择和提前停止机制来稀疏化个体LLM智能体之间的信息流，从而在性能和效率之间取得平衡。此外，受SMoE框架中专家多样性原则的启发，我们为每个LLM智能体分配了不同的角色描述，促进了多样性和发散性思维。广泛的实验表明，在推理、对齐和公平性基准测试中，SMoA的表现与传统的混合智能体方法相当，但计算成本显著降低。进一步分析表明，SMoA更加稳定，具有更大的扩展能力，并且通过超参数优化提供了相当大的潜力。代码和数据将在：https://github.com/David-Li0406/SMoA 获取。**|
|**2024-11-05**|**Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities**|Ryosuke Takata et.al.|[2411.03252](http://arxiv.org/abs/2411.03252)|null|我们从零开始研究代理的出现，通过使用基于大型语言模型（LLM）的代理。在以往对基于LLM的代理的研究中，每个代理的性格特征，包括个性和记忆，通常是预先定义好的。我们关注的是如何从一个未分化的状态中分化出个体性，如行为、个性和记忆。当前的LLM代理在一个群体模拟中进行合作交流，以自然语言交换基于上下文的消息。通过分析这一多代理模拟，我们报告了关于社会规范、合作和个人特质如何自发产生的有价值的新见解。本文展示了自主交互的LLM驱动代理会生成幻觉和话题标签来维持交流，这反过来增加了他们互动中的词汇多样性。随着交流的进行，每个代理的情绪会发生变化，当它们形成社区时，代理的个性随之出现并发展。这种计算建模方法及其发现将为分析集体人工智能提供一种新的方法。|
|**2024-11-05**|**DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models**|Ying Zhou et.al.|[2411.03250](http://arxiv.org/abs/2411.03250)|null|近期大型语言模型（LLM）的进步显著增强了其知识和生成能力，引发了利用LLM进行高质量数据合成的浓厚兴趣。然而，通过提示LLM进行合成数据生成仍然面临挑战，因为LLM对目标数据分布的理解有限，并且提示工程复杂，尤其是对于结构化格式的数据。为了解决这些问题，我们引入了DiffLM，这是一种基于变分自编码器（VAE）的可控数据合成框架，进一步利用扩散模型保留原始分布和格式结构中的更多信息，并通过即插即用的潜在特征注入模块将目标分布知识的学习与LLM的生成目标解耦。由于观察到VAE的潜在表示与真实数据分布之间存在显著差异，我们在框架中引入了潜在扩散模块，以学习一个完全表达的潜在分布。在七个具有结构化格式数据（即表格、代码和工具数据）的真实世界数据集上的评估表明，DiffLM生成了高质量的数据，在某些情况下，下游任务的表现超过了真实数据2-7个百分点。数据和代码将在内部审查完成后公开。|
|**2024-11-05**|**From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice**|Alicia Guo et.al.|[2411.03137](http://arxiv.org/abs/2411.03137)|null|创意作家们热爱他们的创作工艺，然而使用大型语言模型（LLMs）的AI系统可以自动化写作过程中的许多部分。那么，为什么一些创意作家选择将AI整合到他们的工作流程中呢？为了探讨这个问题，我们采访并观察了18位已经定期在写作实践中使用AI的创意作家的写作会话。我们的研究发现，创意作家在整合AI时是有意为之的，他们根据对写作的核心价值观，如真实性和工艺，以及与AI的关系和使用方式，做出许多有意识的决定，以确定他们希望在哪些方面保持控制权。通过分析，我们提出了一个作家价值观、作家与AI的关系以及整合策略的分类，并讨论了这三个要素之间的相互关系。|
|**2024-11-05**|**"Create a Fear of Missing Out" -- ChatGPT Implements Unsolicited Deceptive Designs in Generated Websites Without Warning**|Veronika Krauß et.al.|[2411.03108](http://arxiv.org/abs/2411.03108)|null|随着大型语言模型（LLMs）的最新进展，网络开发者越来越多地利用它们的代码生成能力进行网站设计。然而，由于这些模型是基于现有的设计师知识进行训练的，它们可能会无意中复制不良甚至非法的做法，特别是欺骗性设计（DD）。本文研究了用户是否可能在为一个虚构的网上商店创建功能时意外地生成欺骗性设计模式。我们招募了20名参与者，让他们使用ChatGPT生成产品概览或结账功能，然后使用中立提示对其进行修改以实现商业目标（例如，“提高我们销售产品的可能性”）。我们发现，所有20个生成的网站都至少包含一种欺骗性设计模式（平均值：5，最大值：9），且GPT-4没有发出任何警告。当参与者反思这些设计时，只有4名参与者表达了担忧，而大多数认为结果令人满意，并不认为这在道德上存在问题，尽管这对终端用户和采纳ChatGPT建议的人来说存在潜在的伦理和法律问题。|
|**2024-11-04**|**Training-free Regional Prompting for Diffusion Transformers**|Anthony Chen et.al.|[2411.02395](http://arxiv.org/abs/2411.02395)|**[link](https://github.com/antonioo-c/regional-prompting-flux)**|**扩散模型在文本到图像生成方面展示了出色的能力。随着大型语言模型（如T5、Llama）的应用，它们对语义的理解能力，即遵循提示的能力也得到了极大的提升。然而，现有的模型无法完美处理长且复杂的文本提示，尤其是当这些文本提示包含具有众多属性和相互关联的空间关系的多个对象时。尽管已经提出了许多基于UNet的模型（如SD1.5、SDXL）的区域提示方法，但基于最近的扩散变换器（DiT）架构（如SD3和FLUX）的方法尚未实现。在这份报告中，我们提出并实现了基于注意力操作的FLUX.1的区域提示方法，这使得DiT能够在无需训练的情况下具备细粒度的组合式文本到图像生成能力。代码可在<https://github.com/antonioo-c/Regional-Prompting-FLUX>获取。**|
|**2024-11-04**|**Adaptive Length Image Tokenization via Recurrent Allocation**|Shivam Duggal et.al.|[2411.02393](http://arxiv.org/abs/2411.02393)|**[link](https://github.com/shivamduggal4/adaptive-length-tokenizer)**|**当前的视觉系统通常为图像分配固定长度的表示，而不考虑信息内容。这与人类智能以及大型语言模型不同，后者根据熵、上下文和熟悉度分配不同的表征容量。受此启发，我们提出了一种方法来学习二维图像的变长令牌表示。我们的编解码器架构递归地处理二维图像令牌，在多次迭代的循环展开过程中将其提炼为一维潜在令牌。每次迭代都会细化二维令牌，更新现有的一维潜在令牌，并通过添加新令牌自适应地增加表征容量。这使得图像可以压缩成一个可变数量的令牌，范围从32到256。我们使用重建损失和FID指标验证了我们的标记化方法，结果表明令牌数量与图像熵、熟悉度和下游任务要求相匹配。在每次迭代中随着表征容量的增加进行循环令牌处理显示出令牌专业化的迹象，揭示了对象/部分发现的潜力。**|
|**2024-11-04**|**Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models**|Guangzhi Xiong et.al.|[2411.02382](http://arxiv.org/abs/2411.02382)|null|大型语言模型（LLMs）在各个科学领域展示了非凡的能力，从自然语言处理到复杂的问题解决任务。它们理解并生成类似人类文本的能力开启了推进科学研究的新可能性，使数据处理、文献综述甚至实验设计等任务成为可能。在这种背景下，LLMs最有前景的应用之一是假设生成，通过分析现有知识，它们可以识别新的研究方向。然而，尽管有这些潜力，LLMs容易生成“幻觉”，即听起来合理但实际上不正确的输出。这一问题在需要严格准确性和可验证性的科学领域提出了重大挑战，可能导致错误或误导性的结论。为了克服这些挑战，我们提出了一种名为KG-CoI（基于知识图谱的思路链）的新系统，该系统通过整合来自知识图谱（KGs）的外部结构化知识来增强LLM假设生成。KG-CoI引导LLMs经历一个结构化的推理过程，并将其输出组织成一个思路链（CoI），还包括一个基于知识图谱支持的模块来检测幻觉。通过在我们新构建的假设生成数据集上进行的实验，我们证明了KG-CoI不仅提高了LLM生成假设的准确性，还减少了其推理链中的幻觉，突显了它在推进现实世界科学研究方面的有效性。|
|**2024-11-04**|**Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI**|Ramneet Kaur et.al.|[2411.02381](http://arxiv.org/abs/2411.02381)|null|在本文中，我们提出了一种动态语义聚类方法，该方法受到中国餐馆过程的启发，旨在解决大型语言模型（LLMs）推理中的不确定性问题。我们通过计算生成的语义聚类的熵来量化LLM对给定查询的不确定性。此外，我们建议利用这些聚类的（负）似然性作为（非）一致性得分，在符合性预测框架内使用，使模型能够预测一组响应而不是单一输出，从而考虑其预测中的不确定性。我们通过两个著名的问答基准测试COQA和TriviaQA验证了我们不确定性量化（UQ）技术的有效性，使用的两种LLMs分别为Llama2和Mistral。我们的方法在AUROC、AUARC和AURAC等指标下实现了最先进的UQ性能。所提出的符合性预测器也被证明能够在保持相同概率保证包含正确答案的同时，产生更小的预测集，与现有的最先进符合性预测基线相比。|
|**2024-11-04**|**DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution**|Yang Yue et.al.|[2411.02359](http://arxiv.org/abs/2411.02359)|**[link](https://github.com/yueyang130/deer-vla)**|**多模态大语言模型（MLLMs）在处理复杂的语言和视觉数据方面展现了卓越的理解和推理能力。这些进展激发了建立一种通用机器人多模态大语言模型的愿景，这种模型能够理解复杂的人类指令并完成各种具身任务。然而，由于机器人平台通常具有有限的计算和内存容量，将MLLMs应用于现实世界的机器人面临挑战。相比之下，MLLMs的推理过程需要存储数十亿参数并执行大量计算，对硬件提出了很高的要求。在本文中，我们提出了一种针对机器人视觉-语言-动作模型的动态早退框架（DeeR-VLA，或简称DeeR），该框架能够根据具体情况自动调整激活的MLLM的大小。这种方法利用了MLLMs中的多出口架构，使得模型可以在特定情况下激活适当大小的模型后终止处理，从而避免进一步的冗余计算。此外，我们开发了新的算法，基于预定义的需求（如平均计算成本即功耗、峰值计算消耗即延迟以及GPU内存使用量）来建立DeeR的早期终止标准。这些改进确保了DeeR能够在不同的资源约束下高效运行，同时保持竞争力的表现。在CALVIN机器人操作基准测试中，DeeR展示了LLM的计算成本降低了5.2到6.5倍，LLM的GPU内存减少了2到6倍，且未影响性能。代码和检查点可在<https://github.com/yueyang130/DeeR-VLA>获取。**|
|**2024-11-04**|**"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization**|Eldar Kurtic et.al.|[2411.02355](http://arxiv.org/abs/2411.02355)|null|尽管大规模语言模型（LLM）的量化技术在推理加速方面取得了显著进展，但对于各种量化格式所带来的准确性和性能之间的权衡仍存在很大的不确定性。我们对量化精度进行了全面的经验研究，评估了流行的量化格式（FP8、INT8、INT4）在学术基准和现实任务中的表现，涵盖了整个Llama-3.1模型系列。此外，我们的研究还探讨了量化模型与未压缩模型生成文本之间的差异。除了基准测试外，我们还提出了一些量化改进措施，使我们能够获得最先进的准确度恢复结果。我们的调查涵盖了超过50万次独立评估，得出了几个关键发现：（1）FP8权重和激活量化（W8A8-FP）在所有模型规模下都是无损的；（2）当适当调优时，INT8权重和激活量化（W8A8-INT）仅导致1-3%的准确度下降，令人惊讶的是，其性能仍然良好；（3）INT4权重只量化（W4A16-INT）在性能上与8位整数权重和激活量化相当。为了确定给定部署环境下的“最佳”格式，我们使用流行的开源vLLM框架在不同的GPU架构上进行了推理性能分析。我们发现，W4A16在同步部署中提供了最佳的成本效益，并且在中端GPU上的异步部署也表现出色。同时，W8A8格式在高端GPU上进行中型和大型模型的异步“连续批处理”部署中表现出色。我们的研究结果为在不同规模和性能需求下部署量化LLM提供了一套实用指南。|
|**2024-11-04**|**Social-RAG: Retrieving from Group Interactions to Socially Ground Proactive AI Generation to Group Preferences**|Ruotong Wang et.al.|[2411.02353](http://arxiv.org/abs/2411.02353)|null|人工智能代理越来越多地被赋予在协作的在线空间中提出主动建议的任务，但有时会因为不符合团队的偏好或以不适当的社会方式行事而显得无益甚至令人厌烦。幸运的是，团队空间拥有丰富的先前社会互动历史和社交反馈机制，可以支持创建符合团队兴趣和规范的代理。我们提出了Social-RAG工作流程，该流程将代理与关于团队的社会信息联系起来，从先前的团队互动中检索信息，选择相关社交信号，然后将上下文输入大型语言模型以生成对团队的消息。我们将这一流程实施到PaperPing系统中，该系统在团队聊天中发布学术论文推荐，利用了通过对39名研究人员进行形成性研究确定的社交信号。在为期三个月的部署中，PaperPing在不干扰现有社交实践的情况下，在18个频道发布了相关消息，促进了团队的共同理解。|
|**2024-11-04**|**Can Large Language Models generalize analogy solving like people can?**|Claire E. Stevenson et.al.|[2411.02348](http://arxiv.org/abs/2411.02348)|null|当解决类比问题时，我们将已知情境中的信息转移到新情境中，通过抽象规则和关系相似性来实现。在人类中，解决类比问题的能力（例如，“身体：脚 :: 桌子：？”）在儿童时期出现，并且似乎可以轻松转移到其他领域，如视觉领域“（：）:: <：？”。最近的研究表明，大型语言模型（LLMs）能够解决各种形式的类比问题。然而，LLMs能否像人类一样将类比解决能力泛化到新的领域？为了研究这个问题，我们让儿童、成人和LLMs解决一系列字母串类比问题（例如，a b : a c :: j k : ？）在拉丁字母中，在近迁移领域（希腊字母），以及远迁移领域（符号列表）。正如预期的那样，儿童和成人都能轻松地将其知识泛化到不熟悉的领域，而LLMs则没有做到这一点。这种人类与AI表现的关键差异是证据，表明这些LLMs仍然难以实现稳健的人类类比迁移。|
|**2024-11-04**|**WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning**|Zehan Qi et.al.|[2411.02337](http://arxiv.org/abs/2411.02337)|**[link](https://github.com/THUDM/WebRL)**|大型语言模型（LLMs）在作为自主代理方面展现出了显著的潜力，特别是在网络任务方面。然而，现有的基于LLM的网络代理严重依赖昂贵的专有LLM API，而开源的LLM缺乏必要的决策能力。本文介绍了一种名为WebRL的自我进化在线课程强化学习框架，旨在使用开源LLM训练高性能网络代理。WebRL解决了构建LLM网络代理的三个关键挑战，包括训练任务的稀缺、稀疏反馈信号以及在线学习中的策略分布漂移。具体而言，WebRL包含了1）一个自我进化的课程，该课程从失败尝试中生成新任务；2）一个稳健的结果监督奖励模型（ORM）；3）自适应强化学习策略以确保持续改进。我们将WebRL应用于将开源Llama-3.1和GLM-4模型转化为高效的网络代理。在WebArena-Lite上，WebRL将Llama-3.1-8B的成功率从4.8%提高到42.4%，并将GLM-4-9B的成功率从6.1%提高到43%。这些开源模型显著超过了GPT-4-Turbo（17.6%）和GPT-4o（13.9%）的表现，并且优于之前基于开源LLM训练的最佳网络代理（AutoWebGLM，18.2%）。我们的研究结果表明，WebRL在缩小开源和专有LLM网络代理之间的差距方面是有效的，为更易访问和强大的自主网络交互系统铺平了道路。|
|**2024-11-04**|**Sparsing Law: Towards Large Language Models with Greater Activation Sparsity**|Yuqi Luo et.al.|[2411.02335](http://arxiv.org/abs/2411.02335)|**[link](https://github.com/thunlp/SparsingLaw)**|激活稀疏性指的是在激活输出中存在的大量贡献较弱的元素，这些元素可以被消除，从而有利于与大规模语言模型（LLMs）相关的许多重要应用。尽管促进LLMs中的更大激活稀疏性值得深入研究，但现有工作缺乏对激活稀疏性和潜在影响因素之间相关性的全面和定量研究。在本文中，我们对解码器-only的Transformer基础LLMs中的激活稀疏性的量化缩放特性和影响因素进行了全面研究。具体而言，我们提出了PPL- $p\%$稀疏性，这是一种精确且性能感知的激活稀疏性度量，适用于任何激活函数。通过广泛的实验，我们发现了几个重要的现象。首先，不同的激活函数表现出相似的性能，但在训练时间稀疏性趋势上相反。激活比率（即$1-\mathrm{稀疏率}$ ）随着训练数据量的变化，在SiLU激活和ReLU激活的LLMs中分别遵循收敛的幂律增加和对数空间幂律减少。这表明ReLU作为激活函数比SiLU更高效，并且能够利用更多的训练数据来提高激活稀疏性。其次，激活比率在某个瓶颈点以下线性增加与宽度深度比的关系，表明固定参数规模下更深架构的潜在优势。最后，在类似的宽度深度比下，我们惊讶地发现激活稀疏性的极限值随参数规模变化较弱，即LLMs内的激活模式对参数规模不敏感。这些针对具有更大激活稀疏性的LLMs的经验法则对于使LLMs更加高效和可解释具有重要意义。|
|**2024-11-01**|**SelfCodeAlign: Self-Alignment for Code Generation**|Yuxiang Wei et.al.|[2410.24198](http://arxiv.org/abs/2410.24198)|**[link](https://github.com/bigcode-project/selfcodealign)**|**指令微调是一种监督微调方法，显著提高了大型语言模型（LLMs）遵循人类指令的能力。我们提出了SelfCodeAlign，这是首个完全透明且许可宽松的管道，用于自我对齐代码LLMs，而无需大量的手动标注或蒸馏。SelfCodeAlign在整个数据生成过程中使用相同的基模型进行推理。它首先从高质量的种子代码片段中提取多样化的编码概念以生成新任务。然后，它为每个任务采样多个响应，并将其与测试用例配对，在沙盒环境中进行验证。最后，通过选择通过测试的示例进行指令微调。在我们的主要实验中，我们使用SelfCodeAlign与CodeQwen1.5-7B一起生成了一个包含74k个指令-响应对的数据集。在此数据集上进行微调后，该模型在HumanEval+上的pass@1达到了67.1%，超过了CodeLlama-70B-Instruct，尽管其规模小了十倍。在所有基准测试中，这个经过微调的模型始终优于之前最先进的无需人工标注或蒸馏的指令微调方法OctoPack。此外，我们展示了SelfCodeAlign在各种规模的LLMs（从3B到33B）上都是有效的，并且基模型可以从与自身数据分布的对齐中受益更多。我们还验证了管道中每个组件的有效性，显示SelfCodeAlign在直接从GPT-4o蒸馏和领先的基于GPT-3.5的蒸馏方法（如OSS-Instruct和Evol-Instruct）方面均表现出色。SelfCodeAlign还促成了StarCoder2-Instruct的创建，这是首个完全透明、许可宽松且自我对齐的代码LLM，实现了最先进的编码性能。**|
|**2024-10-31**|**Constraint Back-translation Improves Complex Instruction Following of Large Language Models**|Yunjia Qi et.al.|[2410.24175](http://arxiv.org/abs/2410.24175)|null|大型语言模型（LLMs）在遵循具有复杂格式、长度等约束的指令时存在困难。传统上，先前的工作通过向先进的LLMs提供复杂的指令-响应对来进行后训练，以处理这些复杂指令。然而，即使是先进的LLMs也难以很好地遵循复杂的指令，从而限制了生成数据的质量。在这项工作中，我们发现现有的数据集内在地包含了隐含的复杂约束，并提出了一种新颖的数据生成技术——约束回译。具体来说，我们采用现有数据集中高质量的指令-响应对，并仅使用先进的LLMs将响应已满足的复杂约束添加到指令中，这自然降低了成本和数据噪声。在实验中，我们使用Llama3-70B-Instruct进行约束回译，创建了一个高质量的复杂指令-响应数据集，命名为CRAB。我们展示了在CRAB上进行后训练可以提高多种基础LLMs的复杂指令遵循能力，在广泛的指令遵循基准上进行了评估。我们进一步发现，约束回译也可以作为后训练中的有用辅助训练目标。我们的代码、数据和模型将被发布，以促进未来的研究。|
|**2024-10-31**|**Thought Space Explorer: Navigating and Expanding Thought Space for Large Language Model Reasoning**|Jinghan Zhang et.al.|[2410.24155](http://arxiv.org/abs/2410.24155)|null|近年来，大型语言模型（LLMs）在处理复杂推理任务方面展现出了巨大的潜力，通常通过构建思维链来指导模型进行多步推理。然而，现有的方法往往局限于先前探索过的解决方案空间，从而忽略了LLMs认知范围内的关键盲点。为了解决这些问题，我们设计了Thought Space Explorer (TSE)，这是一种新颖的框架，旨在扩展和优化思维结构，以引导LLMs探索其思维盲点。通过基于原始思维结构生成新的推理步骤和分支，并采用各种设计策略，TSE扩展了思维空间并减轻了盲点对LLM推理的影响。在多个级别的推理任务上的实验结果证明了TSE的有效性。我们还进行了广泛的分析，以理解结构化和扩展化的思维如何有助于释放LLM推理能力的潜力。|
|**2024-10-31**|**Language-Driven Policy Distillation for Cooperative Driving in Multi-Agent Reinforcement Learning**|Jiaqi Liu et.al.|[2410.24152](http://arxiv.org/abs/2410.24152)|null|合作驾驶技术对于提升交通系统的效率和安全性至关重要。基于学习的方法，如多智能体强化学习（MARL），在合作决策任务中展示了强大的能力。然而，现有的MARL方法仍然面临学习效率和性能方面的挑战。近年来，大规模语言模型（LLM）迅速发展，并在各种顺序决策任务中表现出色。为了增强合作代理的学习能力，同时确保决策效率和成本效益，我们提出了一种名为LDPD的语言驱动策略蒸馏方法来引导MARL探索。在这个框架中，基于LLM的教师代理训练较小的学生代理通过其自身的决策演示实现合作决策。教师代理增强了自动驾驶车辆的观察信息，并利用LLM进行复杂的合作决策推理，同时也利用精心设计的决策工具实现专家级决策，提供高质量的教学经验。学生代理通过梯度策略更新将教师的先验知识提炼到自己的模型中。实验表明，学生可以在最少的教师指导下快速提高其能力，并最终超越教师的表现。广泛的实验表明，我们的方法在性能和学习效率方面优于基线方法。|
|**2024-10-31**|**Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing**|Akash Dhruv et.al.|[2410.24119](http://arxiv.org/abs/2410.24119)|**[link](https://github.com/neucol/llm-conversion-performance)**|**基础模型和生成式人工智能（GenAI）的出现有望改变科学计算中的生产力，特别是在代码开发、重构以及从一种编程语言转换到另一种编程语言方面。然而，由于GenAI的输出不能保证正确性，因此仍然需要人工干预。部分这种干预可以通过任务特定工具以及用于正确性验证和有效提示开发的附加方法来自动化。我们研究了GenAI在辅助代码转换、语言互操作性和在用于模拟大型强子对撞机（LHC）粒子相互作用的遗留Fortran代码库中进行代码库检查方面的应用。在此过程中，我们开发了一款名为CodeScribe的工具，结合提示工程与用户监督，建立了一个高效的代码转换流程。在本文中，我们展示了CodeScribe如何帮助将Fortran代码转换为C++，生成Fortran-C API以集成遗留系统与现代C++库，并提供开发者支持以实现代码组织和算法实施。我们还讨论了AI驱动的代码转换面临的挑战，并强调其在提高科学计算工作流程生产力方面的优势。**|
|**2024-10-31**|**Repository-Level Compositional Code Translation and Validation**|Ali Reza Ibrahimzada et.al.|[2410.24117](http://arxiv.org/abs/2410.24117)|**[link](https://github.com/Intelligent-CAT-Lab/AlphaTrans)**|代码翻译是将程序从一种编程语言转换为另一种编程语言的过程。一些基于规则的转译器已经被设计出来，以实现不同编程语言对之间的自动化代码翻译。然而，这些规则可能会因编程语言的发展而变得过时，并且无法推广到其他编程语言。近期的研究探索了使用大型语言模型（LLMs）来自动化代码翻译。一个关键观察是，这样的技术可能在精心设计的基准测试中表现良好，但在真实世界的项目中，由于依赖关系、自定义类型、特定于编程语言的功能等因素的存在，它们可能难以泛化。  我们提出了AlphaTrans，这是一种神经符号方法，用于自动化整个代码仓库级别的代码翻译。AlphaTrans不仅翻译源代码，还翻译测试代码，并采用多级验证确保翻译后的代码保留了源程序的功能。为了分解问题以便让LLMs处理，AlphaTrans利用程序分析将程序分解成片段，并按逆调用顺序进行翻译。我们使用AlphaTrans翻译了十个现实世界中的开源项目，这些项目包含的类、方法和测试分别有<836, 8575, 2719>个。AlphaTrans成功翻译了这些项目的所有代码库，共包括6899个代码片段。99.1%的翻译代码片段在语法上是正确的，AlphaTrans验证了其中25.8%的运行时行为和功能正确性。平均而言，集成翻译和验证过程需要36小时来翻译一个项目，显示出其在实际应用中的可扩展性。对于那些在语法或语义上不正确的翻译，AlphaTrans生成一份报告，其中包括现有的翻译、堆栈跟踪、测试错误或断言失败。我们向两位开发者提供了这些辅助材料，帮助他们在四个项目中修复翻译错误。他们平均花费20.1小时解决了这些问题，并使所有测试通过。|
|**2024-10-31**|**Matchmaker: Self-Improving Large Language Model Programs for Schema Matching**|Nabeel Seedat et.al.|[2410.24105](http://arxiv.org/abs/2410.24105)|null|实体匹配——即在具有不同表和层次结构的异构数据源之间找到属性之间的匹配——对于创建可用于机器学习（ML）的数据至关重要。这一基础性的数据问题在医疗、金融和电子商务等领域尤为重要，同时也能够更广泛地通过增加用于训练ML模型的数据量来使ML模型受益。然而，由于不同模式之间的结构/层次和语义异质性，实体匹配是一个具有挑战性的ML任务。先前的自动化实体匹配的ML方法要么需要大量的标注数据进行模型训练，这通常是不现实的，要么零样本性能较差。为此，我们提出了Matchmaker——一种用于实体匹配的组合式语言模型程序，该程序由候选生成、优化和置信度评分组成。Matchmaker还通过一种新颖的优化方法实现在零样本情况下自我改进，该方法构建合成上下文演示以引导语言模型的推理过程。实证研究表明，在真实世界的医学实体匹配基准上，Matchmaker优于之前的基于ML的方法，突显了其加速数据集成和ML就绪数据互操作性的潜力。|
|**2024-10-31**|**Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs**|Muhammed Saeed et.al.|[2410.24049](http://arxiv.org/abs/2410.24049)|null|大型语言模型（LLMs）在广泛应用的同时引发了伦理问题，因为它们内置了社会偏见。本研究在包括女性权利、恐怖主义和反犹太主义在内的八个领域中考察了LLMs对阿拉伯人与西方人的偏见，并评估了这些模型抵抗延续这些偏见的能力。为此，我们创建了两个数据集：一个用于评估LLM对阿拉伯人与西方人的偏见，另一个用于测试模型对放大负面特征的提示的安全性（“越狱”）。我们评估了六种LLM——GPT-4、GPT-4o、LlaMA 3.1（8B & 405B）、Mistral 7B和Claude 3.5 Sonnet。我们发现79%的案例显示出对阿拉伯人的负面偏见，其中LlaMA 3.1-405B是最具偏见的模型。我们的“越狱”测试显示，尽管GPT-4o是经过优化的版本，但它却是最易受攻击的，其次是LlaMA 3.1-8B和Mistral 7B。除了Claude外，所有LLM在三个类别中的攻击成功率均超过87%。我们还发现Claude 3.5 Sonnet的安全性最高，但仍然在八个类别中的七个显示出偏见。尽管GPT-4o是GPT-4的一个优化版本，但我们发现它更容易受到偏见和“越狱”的影响，这表明优化存在缺陷。我们的研究结果强调了需要更强大的偏见缓解策略和强化安全措施的紧迫性。|
|**2024-10-31**|**Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks**|Yingzhe Peng et.al.|[2410.24032](http://arxiv.org/abs/2410.24032)|null|大规模语言模型（LLM）的兴起已经彻底改变了用户与知识系统之间的交互方式，使得聊天机器人能够整合大量的信息并协助处理复杂的探索性任务。然而，基于LLM的聊天机器人往往难以提供个性化支持，尤其是在用户以模糊查询开始或缺乏足够的上下文信息时。本文介绍了一种名为“个性化探索协作助理”（CARE）的系统，该系统通过结合多代理LLM框架和结构化的用户界面来增强个性化在探索性任务中的应用。CARE的界面包括聊天面板、解决方案面板和需求面板，使迭代式查询细化和动态解决方案生成成为可能。多代理框架协同工作，以识别显性和隐性用户需求，从而提供定制化的、可操作的解决方案。在一项涉及22名参与者的被试内用户研究中，CARE相对于基线LLM聊天机器人一直受到欢迎，用户称赞其能够减轻认知负担、激发创造力，并提供更加个性化的解决方案。我们的研究结果表明，CARE有可能将基于LLM的系统从被动的信息检索者转变为个性化问题解决和探索中的积极合作伙伴。|
|**2024-10-31**|**AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents**|Yifan Xu et.al.|[2410.24024](http://arxiv.org/abs/2410.24024)|**[link](https://github.com/THUDM/Android-Lab)**|自主代理在与现实世界互动中的重要性日益增加。特别是，安卓代理作为一种交互方法被频繁提及。然而，现有的用于训练和评估安卓代理的研究缺乏对开源和闭源模型系统的系统性研究。在这项工作中，我们提出了AndroidLab作为系统化的安卓代理框架。它包括一个具有不同模态的操作环境、动作空间以及可重复使用的基准测试。它支持在同一动作空间下的大型语言模型（LLMs）和多模态模型（LMMs）。AndroidLab基准测试包括预定义的安卓虚拟设备和九个应用上的138个任务。通过使用AndroidLab环境，我们开发了一个安卓指令数据集，并训练了六个开源的LLMs和LMMs，将LLMs的成功率从4.59%提升到21.50%，LMMs的成功率从1.93%提升到13.28%。AndroidLab已开源并公开提供，网址为<https://github.com/THUDM/Android-Lab>。|
|**2024-10-30**|**EMMA: End-to-End Multimodal Model for Autonomous Driving**|Jyh-Jing Hwang et.al.|[2410.23262](http://arxiv.org/abs/2410.23262)|null|我们介绍了EMMA，这是一种用于自动驾驶的端到端多模态模型。该模型基于多模态大型语言模型基础，直接将原始相机传感器数据映射到各种与驾驶相关的输出，包括规划轨迹、感知对象和道路图元素。EMMA通过将所有非传感器输入（例如导航指令和自车状态）和输出（例如轨迹和三维位置）表示为自然语言文本，最大限度地利用了预训练大型语言模型中的世界知识。这种方法使EMMA能够在统一的语言空间中联合处理各种驾驶任务，并使用特定任务提示生成每个任务的输出。实证研究表明，EMMA在nuScenes上的运动规划方面达到了最先进的性能，并在Waymo开放运动数据集（WOMD）上取得了具有竞争力的结果。此外，EMMA在Waymo开放数据集（WOD）上作为主要摄像头的三维目标检测也取得了具有竞争力的结果。我们展示了通过同时训练EMMA进行规划轨迹、目标检测和道路图任务可以在这三个领域都取得改进，突显了EMMA作为自动驾驶应用中的通用模型的潜力。然而，EMMA也表现出一些局限性：它只能处理少量图像帧，不包含准确的三维传感模态如激光雷达或雷达，并且计算成本较高。我们希望我们的结果能够激发进一步的研究，以解决这些问题并进一步发展自动驾驶模型架构。|
|**2024-10-30**|**Evaluating Cultural and Social Awareness of LLM Web Agents**|Haoyi Qiu et.al.|[2410.23252](http://arxiv.org/abs/2410.23252)|null|随着大型语言模型（LLMs）扩展到执行现实世界应用中的代理任务，超越传统的自然语言处理任务，评估其鲁棒性变得越来越重要。然而，现有的基准测试往往忽视了文化和社会意识等关键维度。为了解决这些问题，我们引入了CASA，这是一个旨在评估LLM代理在两个基于网络的任务（在线购物和社交讨论论坛）中对文化和社会规范的敏感性的基准。我们的方法评估了LLM代理检测并适当回应违反规范的用户查询和观察的能力。此外，我们提出了一种全面的评估框架，该框架测量代理对文化和社会规范的意识覆盖率、在管理用户查询时的实用性以及面对误导性网络内容时的违规率。实验表明，当前的LLM在非代理环境中的表现显著优于在网络代理环境中，代理的意识覆盖率不到10%，违规率超过40%。为了提高性能，我们探索了两种方法：提示和微调，并发现这两种方法可以互补——针对特定文化的数据集进行微调可以显著增强代理在不同地区的泛化能力，而提示则能提升代理处理复杂任务的能力。这些发现突显了在开发周期中不断基准测试LLM代理的文化和社会意识的重要性。|
|**2024-10-30**|**Carrot and Stick: Eliciting Comparison Data and Beyond**|Yiling Chen et.al.|[2410.23243](http://arxiv.org/abs/2410.23243)|null|比较数据通常来自于人们的主观判断，并且难以直接验证。这些数据对于许多机器学习任务至关重要，包括基于人类反馈的强化学习和排名模型估计。如何诚实地从理性个体那里获取这样的比较数据？我们设计了同伴预测机制来利用奖金-惩罚支付方式来获取比较数据。我们的设计依赖于比较数据的强随机传递性，从而创建对称的严格真实机制，使得说实话不仅形成严格的贝叶斯纳什均衡，而且在所有对称均衡中获得最高报酬。在我们的机制下，每个个体只需要评估一对项目并报告她的比较结果。  我们进一步将奖金-惩罚支付的概念扩展到网络化数据的获取上，设计了一种当代理人的私人信号根据Ising模型采样时，对称地严格真实的机制。我们提供了奖金-惩罚支付成为严格贝叶斯纳什均衡的必要和充分条件。在两个现实世界的数据集上的实验进一步支持了我们的理论发现。|
|**2024-10-30**|**A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment**|Matteo G. Mecattaf et.al.|[2410.23242](http://arxiv.org/abs/2410.23242)|**[link](https://github.com/kinds-of-intelligence-cfi/llm-aai)**|作为通用工具，大型语言模型（LLMs）必须经常推理日常物理环境。在问答场景中，理解物理对象的相互作用可能是给出适当回答的必要条件。此外，LLMs越来越多地被用作自主系统中的推理引擎，设计和控制它们的动作序列。大多数研究通过静态基准来解决这个问题，这些基准由关于物理世界的文本或图像问题组成。然而，这些基准无法捕捉现实生活中的物理过程的复杂性和细微差别。在这里，我们提倡第二种相对未被充分探索的方法：通过在一个3D环境中赋予LLMs对代理的控制权来“具身化”它们。我们提出了第一个具身且认知上有意义的LLM物理常识推理评估框架。我们的框架允许直接比较LLMs与其他具身代理，如基于深度强化学习的代理，以及人类和非人类动物。我们使用Animal-AI（AAI）环境，一个模拟的3D虚拟实验室，来研究LLMs的物理常识推理能力。为此，我们使用AAI测试平台，该平台是一系列实验，复制了非人类动物的实验室研究，以研究物理推理能力，包括距离估计、跟踪看不见的物体和工具使用。我们证明，没有微调的状态-of-the-art多模态模型能够完成这种任务，使得与2019年Animal-AI奥运会参赛者和人类儿童进行有意义的比较成为可能。我们的结果显示，LLMs目前在这类任务上的表现不如人类儿童。我们认为这种方法允许使用直接从认知科学中提取的生态有效的实验来研究物理推理，从而提高LLMs的预测性和可靠性。|
|**2024-10-30**|**EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning**|Peide Huang et.al.|[2410.23234](http://arxiv.org/abs/2410.23234)|null|本文介绍了一个名为EMOTION的框架，用于在人形机器人中生成富有表现力的动作序列，从而增强其进行类人非语言交流的能力。非语言线索如面部表情、手势和身体动作在有效的人际互动中起着至关重要的作用。尽管在机器人的行为方面已经取得了进展，但现有的方法往往难以模仿人类非语言交流的多样性和细微差别。为了解决这一差距，我们的方法利用大型语言模型（LLM）的上下文学习能力，动态生成适合社会交往的手势动作序列，以促进人机交互。我们使用该框架生成了10种不同的表情手势，并进行了在线用户研究，比较由EMOTION和其加入人类反馈版本EMOTION++生成的动作与人类操作员生成的动作之间的自然度和可理解性。结果显示，在某些情况下，我们的方法在生成可理解且自然的机器人动作方面要么与人类表现相当，要么超越人类。我们还提供了未来研究的设计启示，考虑在生成富有表现力的机器人手势时需要考虑的一系列变量。|
|**2024-10-31**|**Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval**|Sheryl Hsu et.al.|[2410.23214](http://arxiv.org/abs/2410.23214)|null|大型语言模型（LLMs）的幻觉现象通过允许模型搜索信息并将答案与真实来源挂钩，得到了一定程度的缓解。然而，LLMs在处理复杂或间接主题时，往往难以提出正确的搜索查询。我们观察到，通过让LLMs尝试不同的查询并学习对那些成功产生相关结果的查询赋予更高的权重，LLMs可以学会检索相关的事实。为此，我们引入了LeReT（Learning to Retrieve by Trying），这是一种强化学习框架，通过探索搜索查询并使用基于偏好的优化来提高查询质量。LeReT可以将绝对检索准确性提高多达29%，并将下游生成器评估提高17%。LeReT的简单性和灵活性使其能够应用于任意现成的检索器，并成为改进通用LLM管道的一种有前途的技术。项目网站：http://sherylhsu.com/LeReT/。|
|**2024-10-30**|**ProTransformer: Robustify Transformers via Plug-and-Play Paradigm**|Zhichao Hou et.al.|[2410.23182](http://arxiv.org/abs/2410.23182)|**[link](https://github.com/chris-hzc/ProTransformer)**|近年来，基于Transformer的架构在机器学习的各个领域占据主导地位。在这篇论文中，我们介绍了一种新颖的鲁棒注意力机制，旨在增强基于Transformer的架构的韧性。这项技术可以作为插件层集成到现有的Transformer模型中，从而提高其鲁棒性，而无需额外的训练或微调。通过全面的实验和消融研究，我们证明了ProTransformer显著提升了各种预测任务、攻击机制、骨干架构和数据域中的Transformer模型的鲁棒性。值得注意的是，在经典的TextFooler攻击下，无需进一步微调，ProTransformer分别将BERT、ALBERT、DistilBERT和RoBERTA这四种模型的性能提高了19.5%、28.3%、16.1%和11.4%。此外，ProTransformer在大型语言模型（LLMs）面对基于提示的攻击时表现出良好的韧性，分别将T5和LLaMA的性能提高了24.8%和17.8%，并且平均将Vicuna在Jailbreaking攻击下的性能提高了10.4%。除了语言领域外，ProTransformer还在视觉和图领域展示了出色的鲁棒性。|
|**2024-10-30**|**ReasoningRec: Bridging Personalized Recommendations and Human-Interpretable Explanations through LLM Reasoning**|Millennium Bismay et.al.|[2410.23180](http://arxiv.org/abs/2410.23180)|**[link](https://github.com/millenniumbismay/reasoningrec)**|**本文介绍了一种名为ReasoningRec的推理推荐框架，该框架利用大语言模型（LLMs）来弥合推荐与人类可解释性解释之间的差距。与依赖于隐式用户-项目交互的传统推荐系统不同，ReasoningRec使用LLMs来建模用户和项目，重点在于用户的偏好、厌恶和解释性推理。该框架利用一个较大的LLM生成用户偏好的合成解释，随后用于微调较小的LLM以提高推荐准确性及提供人类可理解的解释。我们的实验研究调查了推理和上下文信息对个性化推荐的影响，结果显示上下文和个人化数据的质量显著影响LLM生成合理解释的能力。实证评估表明，ReasoningRec在推荐预测方面比最先进的方法高出12.5%，同时提供了易于理解的解释。代码可在以下链接获取：https://github.com/millenniumbismay/reasoningrec。**|
|**2024-10-30**|**SciPIP: An LLM-based Scientific Paper Idea Proposer**|Wenxiao Wang et.al.|[2410.23166](http://arxiv.org/abs/2410.23166)|**[link](https://github.com/cheerss/scipip)**|知识的指数增长和跨学科研究的复杂性给研究人员带来了显著挑战，包括信息过载和探索新想法的困难。大型语言模型（LLMs）如GPT-4在增强想法提案方面显示出巨大潜力，但如何有效利用大模型进行合理的想法提案尚未得到充分探讨。本文提出了一种科学论文想法提案器（SciPIP）。基于用户提供的研究背景，SciPIP从文献数据库中检索有用论文，同时利用LLMs的能力生成更多新颖且可行的想法。为此，我们构建了一个文献检索数据库，提取大量论文的多维度信息以便快速访问。然后，提出了一种基于语义、实体和引用共现的文献检索方法，从多个方面根据用户提供的背景搜索相关文献。在文献检索之后，我们引入了双路径想法提案策略，其中一条路径从检索到的文献中推断解决方案，另一条路径通过模型头脑风暴生成原创想法。然后我们将两者结合起来以实现可行性与原创性的良好平衡。通过在自然语言处理（NLP）领域的广泛实验，我们证明SciPIP可以检索与现有顶级会议论文类似的引文，并生成许多与其一致的想法。此外，我们使用大型语言模型评估了SciPIP生成的其他想法的原创性，进一步验证了我们提出方法的有效性。代码和数据库已发布在https://github.com/cheerss/SciPIP。|
|**2024-10-30**|**Real-Time Personalization for LLM-based Recommendation with Customized In-Context Learning**|Keqin Bao et.al.|[2410.23136](http://arxiv.org/abs/2410.23136)|**[link](https://github.com/ym689/rec_icl)**|**频繁更新基于大型语言模型（LLM）的推荐系统以适应新的用户兴趣，就像传统推荐系统所做的那样，由于高昂的训练成本，即使有加速方法也是不切实际的。本文探讨了在不进行任何模型更新的情况下，通过利用情境学习（ICL）来适应动态用户兴趣的方法，这种方法使LLM能够从输入中的少量示例中学习新任务。使用新的兴趣示例作为ICL的少量示例，LLM可以实时学习兴趣，从而避免了模型更新的需求。然而，现有的基于LLM的推荐器在推荐调优过程中经常失去在情境学习的能力，而原始LLM的情境学习缺乏针对推荐任务的关注。为了解决这个问题，我们提出了RecICL，它定制了针对推荐任务的情境学习，用于实时推荐。RecICL以情境学习格式组织训练示例，确保在调优过程中保留情境学习能力并与其推荐任务对齐。广泛的实验表明，RecICL在无需模型更新的情况下实现了实时推荐的有效性。我们的代码可在https://github.com/ym689/rec_icl获取。**|
|**2024-10-29**|**Enhancing Code Annotation Reliability: Generative AI's Role in Comment Quality Assessment Models**|Seetharam Killivalavan et.al.|[2410.22323](http://arxiv.org/abs/2410.22323)|null|本文探索了一种新颖的方法，通过利用生成式人工智能技术来提升二元分类模型在评估代码注释质量方面的性能。我们通过将来自多个GitHub仓库的1,437个新生成的代码-注释对（标记为“有用”或“无用”）整合到一个现有的C语言数据集中（该数据集包含9,048对），展示了模型性能的显著提升。采用先进的大语言模型后，我们的方法使得支持向量机（SVM）模型的精确率提高了5.78%，从0.79提升至0.8478，同时人工神经网络（ANN）模型的召回率提高了2.17%，从0.731提升至0.7527。这些结果突显了生成式人工智能在改进代码注释分类模型中的价值，为软件开发和质量控制中的模型准确性提升提供了重要的潜力。本研究为在实际软件工程环境中整合生成技术以优化机器学习模型提供了乐观的前景。|
|**2024-10-29**|**Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing by Betting**|Can Chen et.al.|[2410.22318](http://arxiv.org/abs/2410.22318)|**[link](https://github.com/canchen-cc/online-llm-detection)**|**近年来，区分机器生成文本和人类撰写文本的算法研究引起了广泛关注。现有方法通常是在离线设置下进行，即在给定的数据集中包含真实文本和机器生成文本的混合样本，任务是确定数据集中的每个样本是由大型语言模型（LLM）还是由人类生成的。然而，在许多实际场景中，如新闻网站、社交媒体账户或其他论坛发布的文章是以流式方式发布的。因此，在这种在线场景中，如何快速且准确地确定这些来源是否为LLM，并具有强大的统计保证，对于这些媒体或平台有效地运作并防止错误信息和其他潜在的LLM误用至关重要。为了解决在线检测的问题，我们开发了一种基于顺序假设检验的算法，该算法不仅建立并补充了现有的离线检测技术，而且还具备统计保证，包括控制错误发现率和正确识别来源为LLM的预期时间。实验结果证明了我们方法的有效性。**|
|**2024-10-29**|**Natural Language Inference Improves Compositionality in Vision-Language Models**|Paola Cascante-Bonilla et.al.|[2410.22315](http://arxiv.org/abs/2410.22315)|null|在视觉-语言模型（VLMs）中，组合推理仍然是一个挑战，因为这些模型通常难以关联对象、属性和空间关系。最近的方法试图通过依赖文本描述的语义，利用大规模语言模型（LLMs）将问题和答案分解成子集来解决这些问题。然而，这些方法主要在表面层次上操作，未能引入更深的词汇理解，同时还会引入由LLM生成的错误假设。针对这些问题，我们提出了Caption Expansion with Contradictions and Entailments (CECE)，这是一种基于原理的方法，利用自然语言推理（NLI）从给定的前提生成蕴涵和矛盾。CECE生成词汇上多样的句子，同时保持其核心意义。通过广泛的实验，我们展示了CECE增强了可解释性，并减少了对有偏见或表面特征的过度依赖。通过平衡原始前提与CECE，我们在无需额外微调的情况下显著优于先前的方法，在衡量图像-文本对齐的人类判断得分的基准测试中取得了最先进的结果，并在Winoground上实现了+19.2%（组分数）和在EqBen上实现+12.9%（组分数）的性能提升，超过了最佳现有工作（使用针对性数据微调）。|
|**2024-10-29**|**GPT-4o reads the mind in the eyes**|James W. A. Strachan et.al.|[2410.22309](http://arxiv.org/abs/2410.22309)|null|大型语言模型（LLMs）能够从文本中重现人类类似推理的能力，包括关于情绪和心理状态的推理。然而，这种能力是否扩展到其他模态尚不清楚。人类具有通过他人的眼睛读心的复杂能力。在此研究中，我们测试了这一能力是否也存在于GPT-4o这一多模态LLM中。我们使用了两种广泛使用的心理理论测试版本，即“眼睛中的心智阅读测试”和“多元种族眼睛中的心智阅读测试”。结果发现，GPT-4o在解释来自直立面部的心理状态方面优于人类，但在面部倒置时表现较差。尽管我们样本中的人类在白人和非白人面孔之间没有表现出差异，但GPT-4o对白人面孔的准确度高于非白人面孔。GPT-4o的错误并非随机出现，而是揭示了一种高度一致但错误的处理心理状态信息的方式，在不同试验中呈现出方向依赖的错误结构，这种结构在面对倒置面孔时与人类存在定性差异，而在面对直立面孔时则无明显区别。这些发现强调了先进的心理状态推理能力和人类类似的面部处理特征，如反转效应，在GPT-4o中共存，同时其信息处理方式与人类存在显著差异。|
|**2024-10-29**|**SVIP: Towards Verifiable Inference of Open-source Large Language Models**|Yifan Sun et.al.|[2410.22307](http://arxiv.org/abs/2410.22307)|null|开源的大语言模型（LLMs）在自然语言理解和生成方面展示了显著的能力，并在各个领域得到了广泛应用。然而，随着模型规模的增大，本地部署变得不切实际，许多用户不得不依赖计算服务提供商通过黑盒API进行推理。这种依赖引入了一种新的风险：计算服务提供商可能在未经用户同意的情况下，用较小且能力较弱的模型替代用户请求的LLM，从而提供质量较差的结果，同时节省成本。在这篇论文中，我们形式化了LLM可验证推理的问题。现有的基于密码学或博弈论技术的可验证计算解决方案要么在计算上不经济，要么基于较强的假设。我们引入了SVIP，这是一种基于秘密的可验证LLM推理协议，它利用LLM的中间输出作为唯一的模型标识符。通过在这些输出上训练代理任务，并要求计算服务提供商返回生成的文本和处理过的中间输出，用户可以可靠地验证计算服务提供商是否诚实行事。此外，结合秘密机制进一步增强了我们的协议的安全性。我们在多种强适应性对抗场景下全面分析了我们的协议。广泛的实验表明，SVIP是准确的、可泛化的、计算高效的，并且对各种攻击具有抵抗力。值得注意的是，SVIP的假阴性率低于5%，假阳性率低于3%，并且每次查询的验证时间少于0.01秒。|
|**2024-10-29**|**Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning**|Yihe Deng et.al.|[2410.22304](http://arxiv.org/abs/2410.22304)|null|数学推理是大型语言模型（LLMs）的关键能力，但生成详细且准确的推理轨迹仍然是一个重大挑战。本文介绍了一种利用在线学习流的新方法，以产生高质量的推理轨迹用于LLM微调。我们的方法采用增量输出生产流，其中组件LLM通过迭代通信协作构建解决方案。我们使用带有滚动的在线直接偏好优化（DPO）学习来训练该流，为每个训练样本生成DPO对，并实时更新模型。我们直接比较了通过我们方法与直接模型推理生成的推理轨迹的质量，证明了我们方法在提高LLM在数学推理任务中的性能方面的有效性。|
|**2024-10-29**|**LLMs are Highly-Constrained Biophysical Sequence Optimizers**|Angelica Chen et.al.|[2410.22296](http://arxiv.org/abs/2410.22296)|null|大型语言模型（LLMs）在各种生物任务中，如蛋白质工程和分子设计方面，最近展示了显著的潜力。这些任务通常涉及黑盒离散序列优化，挑战在于生成不仅在生物学上可行而且严格符合细粒度约束的序列。然而，LLMs往往难以应对这些约束，特别是在生物学背景下，验证候选解决方案既昂贵又耗时。在这项研究中，我们探索了将LLMs作为高度约束的双层优化器的可能性，通过一种我们称之为语言模型优化边缘期望（LLOME）的方法。该方法结合了离线和在线优化，利用有限的oracle评估迭代地增强由LLM生成的序列。此外，我们提出了一种新的训练目标——边缘对齐期望（MargE），该目标训练LLM平滑地在奖励分布和参考分布之间插值。最后，我们引入了一个合成测试套件，该套件与实际生物物理问题具有强烈的几何相似性，并且能够在不进行耗时的实验室验证的情况下快速评估LLM优化器。我们的发现表明，与遗传算法基线相比，LLMs在要求较少测试函数评估的情况下实现了显著更低的遗憾解。然而，我们也观察到LLMs表现出适度的校准偏差，容易发生生成器崩溃，并且在没有明确的地面真值奖励可用时难以找到最优解。|
|**2024-10-29**|**Fine-Tuning LLMs for Code Mutation: A New Era of Cyber Threats**|Mohammad Setak et.al.|[2410.22293](http://arxiv.org/abs/2410.22293)|null|近年来，大型语言模型（LLMs）在自然语言处理和代码合成方面取得了显著进展，使其能够应用于不同领域更复杂的任务。本文探讨了LLMs在代码变异中的应用，这是一个在不改变程序代码功能的前提下改变其结构的过程。传统上，代码变异被用于提高关键任务应用程序的软件健壮性。此外，变异引擎也被恶意软件开发者用来逃避基于特征码的检测方法。现有的恶意软件使用的变异引擎通常只产生有限的代码变化，这些变化仍然可以通过静态代码分析被识别。然而，预训练的LLM所展示的灵活性可能显著改变这种威胁态势，通过允许进行更复杂的代码变异，这些变异不容易通过静态分析检测到。我们可以通过微调和再训练增加由预训练LLM生成的代码的变化。我们称之为代码变异训练。在本文中，我们为基于预训练LLM的代码合成器提出了一个新的代码变异训练定义，并在一个轻量级的预训练模型上展示了这种方法。我们的方法涉及在子例程级别重组（即变异）代码，这使得变异更加可控同时保持语义完整性，并通过单元测试验证。实验结果表明，我们的方法有效地提高了基于LLM的程序合成器在生成多样化且功能正确的代码解决方案方面的变异能力，展示了它们在改变代码变异格局以及与之相关的威胁方面的潜力。|
|**2024-10-29**|**Embedding-based classifiers can detect prompt injection attacks**|Md. Ahsan Ayub et.al.|[2410.22284](http://arxiv.org/abs/2410.22284)|**[link](https://github.com/AhsanAyub/malicious-prompt-detection)**|**大型语言模型（LLMs）因其卓越的生成能力而在各类组织中得到广泛应用。然而，LLMs容易受到各种对抗性攻击，特别是提示注入攻击，这种攻击通过精心设计的恶意提示欺骗LLMs，使其生成有害或不适当的内容。在这篇论文中，我们提出了一种基于嵌入式机器学习（ML）分类器的新方法，以保护基于LLM的应用程序免受这种严重威胁。我们利用三种常用的嵌入模型来生成恶意和良性提示的嵌入，并使用ML分类器预测输入提示是否为恶意。在几种传统的ML方法中，我们使用随机森林和XGBoost构建的分类器表现最佳。我们的分类器在性能上优于开源实现中的最先进的提示注入分类器，后者使用的是仅编码器的神经网络。**|
|**2024-10-29**|**Whose ChatGPT? Unveiling Real-World Educational Inequalities Introduced by Large Language Models**|Renzhe Yu et.al.|[2410.22282](http://arxiv.org/abs/2410.22282)|null|自2022年底以来，ChatGPT等类似工具的广泛可用性引发了公众对大型语言模型（LLMs）在提高学习体验和成果方面的潜力的巨大兴趣和实验努力，特别是对于来自弱势背景的学习者。然而，很少有研究系统地考察了LLMs的实际可用性对教育公平性的现实影响，除了理论预测和创新LLM应用的控制研究之外。为了描绘LLMs不平等趋势，我们分析了一所美国公立少数族裔服务院校2021年至2024年间2391门课程中16791名大学生提交的1140328篇学术写作作业。研究发现，在LLMs可用之后，学生的整体写作质量逐渐提高，并且语言优势和劣势学生之间的写作质量差距逐渐缩小。然而，这种平等化效应更多集中在较高社会经济地位的学生身上。这些发现揭示了LLMs时代的数字鸿沟，并提出了关于LLMs在早期阶段的公平效益的问题，强调了研究人员和从业者需要制定负责任的做法以通过LLMs改善教育公平性。|
|**2024-10-28**|**Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics**|Yaniv Nikankin et.al.|[2410.21272](http://arxiv.org/abs/2410.21272)|**[link](https://github.com/technion-cs-nlp/llm-arithmetic-heuristics)**|为了探讨大型语言模型（LLMs）在解决推理任务时是通过学习稳健的可泛化算法，还是通过记忆训练数据，我们选择了算术推理作为代表性任务进行研究。通过因果分析，我们识别出模型的一个子部分（一个电路），该部分解释了基本算术逻辑中模型大部分的行为，并检查了其功能。通过关注单个电路神经元的层面，我们发现了一组重要的稀疏神经元，它们实现了简单的启发式方法。每个启发式方法识别数值输入模式并输出相应的答案。我们假设，这些启发式神经元的组合是生成正确算术答案的机制。为了验证这一点，我们将每个神经元分类为几种启发式类型——例如，当操作数落在某个范围内时激活的神经元——并发现这些启发式类型的无序组合是解释模型在算术提示上准确性的主要机制。最后，我们证明这种机制在训练早期就是算术准确性的重要来源。总的来说，我们在多个LLM上进行的实验结果表明，LLMs执行算术运算既不是依靠稳健的算法，也不是依靠记忆；相反，它们依赖于“一组启发式方法”。|
|**2024-10-28**|**LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior**|Hanyu Wang et.al.|[2410.21264](http://arxiv.org/abs/2410.21264)|null|我们介绍了LARP，这是一种新颖的视频标记器，旨在克服当前用于自回归（AR）生成模型的视频标记方法的局限性。与传统的基于补丁的标记器直接将局部视觉补丁编码为离散标记不同，LARP引入了一种整体标记方案，通过一组学习到的整体查询来收集视觉内容的信息。这种设计使LARP能够捕捉更全局和语义化的表示，而不仅仅是局限于局部补丁级别的信息。此外，它还提供了灵活性，支持任意数量的离散标记，从而根据任务的具体需求实现自适应和高效的标记。为了使离散标记空间与下游AR生成任务对齐，LARP集成了一个轻量级的AR变换器作为训练时的先验模型，该模型在离散潜在空间上预测下一个标记。通过在训练过程中结合先验模型，LARP学习了一个不仅优化了视频重建的潜在空间，而且结构上更适合自回归生成的潜在空间。此外，这一过程定义了离散标记的顺序，在训练过程中逐步将其推向最优配置，确保推理时更平滑和准确的AR生成。全面的实验表明，LARP表现强劲，在UCF101分类条件下的视频生成基准上达到了最先进的FVD分数。LARP增强了AR模型与视频的兼容性，并开启了构建统一的高保真多模态大型语言模型（MLLMs）的可能性。|
|**2024-10-28**|**LongReward: Improving Long-context Large Language Models with AI Feedback**|Jiajie Zhang et.al.|[2410.21252](http://arxiv.org/abs/2410.21252)|**[link](https://github.com/THUDM/LongReward)**|尽管在开发长上下文大型语言模型（LLMs）方面取得了显著进展，但这些模型合成的数据质量往往影响了有监督微调（SFT）模型的长上下文性能，并导致固有的局限性。原则上，适当的奖励信号可以利用强化学习（RL）进一步提升模型的能力。然而，在长上下文场景中如何获得可靠的奖励仍然是一个未探索的问题。为此，我们提出了LongReward，这是一种新颖的方法，它利用现成的LLM从四个维度（即：有用性、逻辑性、准确性和完整性）提供长上下文模型响应的奖励，并为每个维度设计了详细的评估流程。通过结合LongReward和离线RL算法DPO，我们能够有效地改进长上下文SFT模型。我们的实验表明，LongReward不仅显著提升了模型的长上下文性能，还增强了它们遵循短指令的能力。我们还发现，带有LongReward的长上下文DPO和传统的短上下文DPO可以一起使用而不损害任何一方的性能。|
|**2024-10-28**|**Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback**|Nour Jedidi et.al.|[2410.21242](http://arxiv.org/abs/2410.21242)|null|构建有效的密集检索系统在缺乏相关性监督时仍然具有挑战性。近期的研究试图通过使用大型语言模型（LLM）来生成假设文档，从而找到最接近的真实文档来解决这一问题。然而，这种方法完全依赖于LLM具备与查询相关的领域特定知识，这在实践中可能不可行。此外，生成假设文档的方法效率低下，因为对于每个查询，LLM需要生成大量的标记。为了解决这些挑战，我们引入了基于相关反馈的真实文档嵌入（ReDE-RF）。受相关反馈的启发，ReDE-RF提出将假设文档生成重新定义为相关性估计任务，利用LLM选择哪些文档应被用于最近邻搜索。通过这种重新定义，LLM不再需要领域特定的知识，而只需要判断什么是相关的。此外，相关性估计仅要求LLM输出一个标记，从而提高每次查询的搜索延迟。我们的实验表明，ReDE-RF在广泛的低资源检索数据集上始终超越最先进的零样本密集检索方法，并且在每次查询的延迟方面也取得了显著改进。|
|**2024-10-28**|**Hierarchical Knowledge Graph Construction from Images for Scalable E-Commerce**|Zhantao Yang et.al.|[2410.21237](http://arxiv.org/abs/2410.21237)|null|知识图谱（KG）在各种AI系统中扮演着越来越重要的角色。对于电子商务而言，构建高效且低成本的自动化知识图谱是实现众多成功下游应用的基础。本文提出了一种新颖的方法，可以从原始产品图像中构建结构化的产品知识图谱。该方法充分利用了视觉语言模型（VLM）和大型语言模型（LLM）的最新进展，实现了整个过程的完全自动化，并允许及时更新图谱。我们还提供了一个经过人工标注的电子商务产品数据集，用于评估知识图谱构建中的产品属性提取。我们的方法在所有指标和评估属性上都优于基线方法，展示了其有效性和广阔的应用潜力。|
|**2024-10-28**|**Flaming-hot Initiation with Regular Execution Sampling for Large Language Models**|Weizhe Chen et.al.|[2410.21236](http://arxiv.org/abs/2410.21236)|null|自ChatGPT发布以来，大型语言模型（LLMs）在各个领域展示了显著的能力。在开发这些通用能力时，一个关键的挑战是高效地获取多样化且高质量的数据。这在与沙盒检查器相关的推理任务中尤为重要，例如数学或代码问题，目标是提高生成正确解决方案的概率。在这项工作中，我们介绍了Flaming-hot Initiation with Regular Execution（FIRE）采样方法，这是一种简单但非常有效的方法，可以高效地找到好的响应。我们的实证结果表明，FIRE采样提高了推理时间生成的质量，并且在对齐阶段的训练中也受益。此外，我们探讨了FIRE采样如何通过促进多样性来提升性能，并分析了在响应的不同位置使用FIRE的影响。|
|**2024-10-28**|**LoRA vs Full Fine-tuning: An Illusion of Equivalence**|Reece Shuttleworth et.al.|[2410.21228](http://arxiv.org/abs/2410.21228)|null|微调是将预训练的大规模语言模型适应到下游任务中的关键范式。最近的研究表明，低秩自适应（LoRA）方法在各种任务上能够以极小的可训练参数量达到与完全微调模型相当的性能。即使两种方法学习到的模型准确性相似，它们的学习解决方案真的等价吗？我们通过分析模型权重矩阵的谱属性来研究不同的微调方法如何改变预训练模型。我们发现，全微调和LoRA生成的权重矩阵在奇异值分解结构上表现出很大的不同；此外，当在超出适应任务分布的情况下测试时，经过微调的模型显示出不同的泛化行为。更具体地说，我们首先展示了使用LoRA训练的权重矩阵具有新的高排名奇异向量，我们称之为“入侵维度”。这些入侵维度在全微调过程中不会出现。其次，我们展示了尽管具有入侵维度的LoRA模型在目标任务上的表现与全微调模型相当，但它们对预训练分布的建模效果较差，并且在顺序适应多个任务时的鲁棒性较低。高秩、秩稳定的LoRA模型甚至在与低秩LoRA模型执行相同任务时，也与全微调模型非常接近。这些结果表明，即使在相同的微调分布上表现相同，LoRA更新的模型和全微调模型访问了参数空间的不同部分。我们最后探讨了为什么入侵维度会在LoRA微调模型中出现，为什么它们是不理想的，以及如何最小化其影响。|
|**2024-10-28**|**Lifting the Veil on the Large Language Model Supply Chain: Composition, Risks, and Mitigations**|Kaifeng Huang et.al.|[2410.21218](http://arxiv.org/abs/2410.21218)|null|大规模语言模型（LLM）在智力和生产力方面引发了显著的影响。近年来，商业和开源LLM的引入呈现出巨大的增长趋势。许多企业已将LLM集成到其应用中以解决特定领域的任务。然而，将LLM整合到具体业务场景中不仅仅需要使用这些模型本身，而是一个系统的过程，涉及大量的组成部分，这些组成部分统称为LLM供应链。LLM供应链内在地承载着风险。因此，理解可能引入供应链的组件类型以及相关的风险至关重要，这有助于不同的利益相关者实施有效的缓解措施。虽然一些文献涉及与LLM供应链相关的风险，但目前还没有论文明确界定其范围、识别固有风险并探讨潜在的缓解策略。鉴于LLMs已成为新时代的重要基础设施，我们认为对LLM供应链及其固有风险和缓解策略进行彻底审查对于行业从业者避免潜在损失具有重要价值，并且对于学术研究人员重新思考现有方法和探索新的研究途径也具有启发意义。我们的论文提供了LLM供应链的全面概述，详细介绍了利益相关者、组成元素以及供应类型。我们开发了与各种供应链利益相关者和组件相关的风险类型、风险行为和缓解措施的分类法。总而言之，我们的工作探讨了LLM供应链的技术和操作方面，为研究和工程人员在不断发展的LLM领域提供有价值的见解。|
|**2024-10-28**|**BongLLaMA: LLaMA for Bangla Language**|Abdullah Khan Zehady et.al.|[2410.21200](http://arxiv.org/abs/2410.21200)|null|孟加拉语（或“ Bengali”）是一种使用约2.4亿母语者和大约3亿人使用的语言。尽管它是世界上第五大使用语言，孟加拉语仍被视为一种“低资源”语言，现有的预训练语言模型在孟加拉语处理（BLP）任务上往往表现不佳。本研究通过引入BongLLaMA（即孟加拉语-LLaMA），解决了这一问题，这是一种专门针对大型孟加拉语语料库和指令调优数据集进行微调的开源大型语言模型。我们介绍了我们的方法论、数据增强技术、微调细节以及全面的基准测试结果，展示了BongLLaMA在孟加拉语处理任务中的效用。我们相信BongLLaMA将成为孟加拉语模型的新标准基线，从而促进未来专注于这种广泛使用但“低资源”的语言的基准研究。所有BongLLaMA模型均可供公众使用，网址为https://huggingface.co/BanglaLLM。|
|**2024-10-29**|**Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction**|Qintong Zhang et.al.|[2410.21169](http://arxiv.org/abs/2410.21169)|null|文档解析对于将非结构化和半结构化文档（如合同、学术论文和发票）转换为结构化的、机器可读的数据至关重要。文档解析从非结构化输入中提取可靠且结构化的数据，为众多应用提供了极大的便利。特别是随着大型语言模型的最新进展，文档解析在知识库构建和训练数据生成方面发挥着不可或缺的作用。本文综述了当前文档解析的状态，涵盖了关键的方法论，从模块化流水线系统到由大型视觉-语言模型驱动的端到端模型。详细探讨了核心组件，包括布局检测、内容提取（包括文本、表格和数学表达式）以及多模态数据集成。此外，本文还讨论了模块化文档解析系统和视觉-语言模型在处理复杂布局、整合多个模块和识别高密度文本时所面临的挑战。文章强调了开发更大和更多样化数据集的重要性，并概述了未来的研究方向。|
|**2024-10-25**|**The Potential and Value of AI Chatbot in Personalized Cognitive Training**|Zilong Wang et.al.|[2410.19733](http://arxiv.org/abs/2410.19733)|null|近年来，全球人口老龄化加速导致认知障碍，如阿尔茨海默病的发病率增加，给公共卫生带来了巨大挑战。尽管目前尚无有效治疗方法可以逆转阿尔茨海默病，但预防和早期干预，包括认知训练，至关重要。本报告探讨了AI聊天机器人在增强个性化认知训练方面的潜力。我们介绍了ReMe，这是一个基于网络的框架，旨在创建AI聊天机器人以促进认知训练研究，特别是针对从个人生活日志中提取的情节记忆任务。通过利用大型语言模型，ReMe提供了更友好、互动和个性化的培训体验。案例研究表明，ReMe通过生活回忆和开放式语言谜题有效地吸引了用户，突显了其在改善认知训练设计方面的潜力。尽管取得了令人鼓舞的结果，但仍需要进一步研究，通过包括认知能力评估在内的大规模研究来验证培训的有效性。总体而言，ReMe为个性化认知训练提供了一种有前景的方法，利用AI技术满足日益增长的认知健康非药物干预需求，未来的研究旨在扩展其应用范围和有效性。|
|**2024-10-25**|**Counting Ability of Large Language Models and Impact of Tokenization**|Xiang Zhang et.al.|[2410.19730](http://arxiv.org/abs/2410.19730)|**[link](https://github.com/juntaic7/impact-of-tokenization-in-the-counting-ability-of-language-models)**|Transformers作为现代大型语言模型（LLMs）的基石，面临着固有的架构限制，这限制了它们的推理能力。与循环网络不同，Transformers缺乏循环连接，使其只能进行恒定深度的计算。这种限制使它们在TC $^0$ 复杂性类中，从理论上讲，无法解决那些需要输入长度增加时推理深度也相应增加的任务。计数作为许多推理任务的基本组成部分，也需要推理深度随着任务复杂度线性增长才能进行归纳。尽管先前的研究已经确定了基于Transformer的专家模型在计数任务中的能力上限，但这些发现并不能直接应用于通用LLM，因为它们的推理机制存在差异。最近的研究指出，链式思考（CoT）推理可以在一定程度上缓解Transformer在计数任务中的架构限制。然而，关于分词在这些模型中的作用却很少受到关注。不同于通常使用字符级分词的专家模型，LLM通常依赖于字节级（BPE）分词器，这从根本上改变了推理处理的方式。我们的研究探讨了分词对LLM计数能力的影响，揭示了基于分词方式的不同导致显著的性能变化。我们提供了理论和实验分析，为如何通过选择合适的分词方法来增强模型的理论可计算性提供了见解，从而启发设计新的分词方法以提高LLM的推理能力。|
|**2024-10-25**|**FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning**|Nicole Cho et.al.|[2410.19727](http://arxiv.org/abs/2410.19727)|null|从大量数据源生成金融智能通常依赖于传统的方法，如知识图谱构建或数据库工程。近年来，针对金融领域的特定大型语言模型（LLM）已经出现。尽管这些进展令人鼓舞，但仍存在一些限制，例如高推理成本、幻觉以及同时分析高维金融数据的复杂性。这促使我们发明了FISHNET（金融智能从子查询、协调、神经条件、专家集群和任务规划），这是一种代理架构，能够完成超过98,000份监管文件的极其复杂的分析任务，这些文件在语义、数据层次或格式上差异巨大。FISHNET在金融洞察生成方面表现出色（成功率为61.8%，路由为5.0%，RAG R-精确度为45.6%）。我们进行了严格的消融实验，以实证证明FISHNET的成功、每个代理的重要性以及所有代理组装优化性能。我们的模块化架构可以应用于各种用例，提供可扩展性、灵活性和对金融任务至关重要的数据完整性。|
|**2024-10-25**|**2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision**|Shilong Li et.al.|[2410.19720](http://arxiv.org/abs/2410.19720)|null|近年来，直接偏好优化（DPO）在使大型语言模型（LLMs）与人类偏好对齐方面取得了显著进展，这得益于其简单性和有效性。然而，现有的方法通常优化一个标量分数或排名奖励，从而忽略了人类偏好的多维性质。在这项工作中，我们提出将DPO的偏好扩展到两个维度：片段和方面。我们首先引入了一个名为HelpSteer-2D的二维监督数据集。对于片段维度，我们将响应分成句子并为每个片段分配分数。对于方面维度，我们精心设计了几项标准以涵盖响应质量评估标准。利用二维信号作为反馈，我们开发了一个2D-DPO框架，将总体目标分解为多片段和多方面的目标。在流行的基准测试中进行的广泛实验表明，2D-DPO的表现优于那些优化标量或一维偏好的方法。|
|**2024-10-25**|**TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning**|Xiangyu Zeng et.al.|[2410.19702](http://arxiv.org/abs/2410.19702)|null|多模态大型语言模型（MLLMs）在短视频理解方面已经展示了令人印象深刻的性能。然而，对于长视频的理解仍然具有挑战性。本文提出了一套新的设计来适应现有的短视频MLLM，以实现长视频理解，包括一个简单而高效的框架来处理长视频序列、一个高质量的视频数据集用于MLLM的接地调优，以及一个精心设计的指令调优任务，以显式地将接地监督纳入传统的问答格式。具体而言，基于VideoChat，我们提出了我们的长视频MLLM，称为VideoChat-T，通过实现令牌洗牌来压缩长视频令牌，并引入时间自适应位置编码（TAPE）来增强视觉表示的时间感知。同时，我们引入了TimePro，这是一个综合性的接地为中心的指令调优数据集，由9个任务和34.9万个高质量的接地标注组成。值得注意的是，我们设计了一种新的指令调优任务类型，称为时间接地字幕，用于执行详细视频描述与相应时间戳预测。这种明确的时间位置预测将指导MLLM在生成描述时正确关注视觉内容，从而减少因LLMs引起的幻觉风险。实验结果表明，我们的TimeSuite成功地提高了短视频MLLM在长视频理解方面的能力，在Egoschema和VideoMME基准测试上分别提高了5.6%和6.8%。此外，VideoChat-T在零样本时间接地能力方面表现出色，显著优于现有的最先进的MLLM。经过微调后，它的表现与传统的监督专家模型相当。|
|**2024-10-25**|**IPPON: Common Sense Guided Informative Path Planning for Object Goal Navigation**|Kaixian Qu et.al.|[2410.19697](http://arxiv.org/abs/2410.19697)|null|在未探索的环境中高效导航到目标物体是通用智能机器人的一项关键技术。最近的方法采用模块化策略，结合经典的探索算法（特别是前沿探索）与学习的语义映射/探索模块来解决这一物体导航问题。本文介绍了一种新颖的信息路径规划和三维物体概率映射方法。该映射模块通过语义分割和贝叶斯滤波计算感兴趣物体的概率。此外，它还存储常见物体的概率，这些概率基于大型语言模型中的常识先验，从而从语义上引导探索。当当前视角捕获了足够多且置信度高的物体为感兴趣物体的体素时，规划器终止。尽管我们的规划器采用了零样本方法，但在Habitat物体导航挑战2023中，它在成功加权路径长度（SPL）和软SPL指标上达到了最先进的性能，比其他工作高出20%以上。此外，我们在真实机器人上验证了其有效性。项目网页：https://ippon-paper.github.io/|
|**2024-10-25**|**Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient Finetuning of LLMs**|Yifei Zhang et.al.|[2410.19694](http://arxiv.org/abs/2410.19694)|null|微调大型语言模型（LLMs）已成为将预训练模型适应下游任务的重要技术。然而，LLMs的巨大规模带来了显著的计算复杂性和资源需求挑战。低秩适应（LoRA）作为一种有前景的解决方案应运而生。然而，实际应用中的低秩适应与理论最优之间存在差距。在这项工作中，我们提出了极端梯度提升LoRA（XGBLoRA），这是一种新的框架，通过利用集成学习的力量来弥合这一差距。受梯度提升启发，XGBLoRA迭代地学习并融合一系列LoRA适应以优化模型预测。它在性能上优于标准LoRA，同时保持了秩-1适应的计算效率。我们提供了理论分析以证明方法的收敛性和最优性，并在各种自然语言处理任务上进行了广泛的实验。结果表明，XGBLoRA始终优于标准LoRA，并且在显著减少可训练参数的情况下实现了与全微调相当的性能。这项工作推进了LLMs的参数高效微调技术，并为优化性能和效率的同时将LLMs适应到下游任务提供了有前景的解决方案。|
|**2024-10-25**|**APRICOT: Active Preference Learning and Constraint-Aware Task Planning with LLMs**|Huaxiaoyue Wang et.al.|[2410.19656](http://arxiv.org/abs/2410.19656)|null|家庭机器人在执行个性化任务时，必须巧妙地平衡用户偏好与环境限制。我们专注于在受限空间内进行组织任务，例如将物品放入冰箱，其中用户的放置偏好常常与物理限制相冲突。机器人必须根据少量演示来推断用户的偏好，这比详细定义所有需求更容易让用户操作。虽然最近的研究使用大型语言模型（LLMs）从用户演示中学习偏好，但它们面临两个基本挑战。首先，在解释用户行为时存在固有的模糊性，因为单一观察到的行为可能对应多种偏好。其次，并非所有用户偏好在环境中都是实际可行的，因为存在几何约束。为了解决这些挑战，我们引入了APRICOT，这是一种新颖的方法，结合了基于LLM的贝叶斯主动偏好学习和考虑环境约束的任务规划。APRICOT通过主动查询用户来优化生成的偏好，并动态调整其计划以尊重环境限制。我们在多样化的组织任务数据集上评估了APRICOT，并展示了其在现实场景中的有效性，证明了其在偏好满意度和计划可行性方面的显著提升。该项目网站位于https://portal-cornell.github.io/apricot/|
|**2024-10-25**|**Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina**|Yuan Gao et.al.|[2410.19599](http://arxiv.org/abs/2410.19599)|null|近期研究表明，大型语言模型（LLMs）可以展现出类似人类的推理能力，在经济实验、调查和政治讨论中与人类行为一致。这促使许多人提出可以将LLMs作为人类在社会科学中的替代品。然而，LLMs与人类之间存在根本性的差异，它们依赖于概率模式，缺乏塑造人类认知的具身经验或生存目标。我们通过11-20金钱请求游戏来评估LLMs的推理深度。几乎所有先进的方法都无法在许多模型中复制人类的行为分布，除非在使用大量人类行为数据进行微调的情况下。失败的原因多种多样，涉及输入语言、角色和保护措施等因素。这些结果提醒我们不要将LLMs用于研究人类行为或将其作为人类的替代品。|
|**2024-10-25**|**Diverse Sign Language Translation**|Xin Shen et.al.|[2410.19586](http://arxiv.org/abs/2410.19586)|**[link](https://github.com/XinS0909/Diverse_Sign_Language_Translation)**|类似于口语，一个手语表达可能对应多个有效的文本解释。因此，对手语翻译（SLT）模型进行单一的映射学习可能是不充分的，尤其是在数据有限的情况下。在这项工作中，我们引入了多样化的手语翻译（DivSLT）任务，旨在为手语视频生成多样且准确的翻译。首先，我们利用大型语言模型（LLM）为广泛使用的CSL-Daily和PHOENIX14T SLT数据集生成多个参考。这里，仅邀请母语人士来润色不准确的参考，从而显著提高了注释效率。其次，我们提供了一个基准模型以推动该任务的研究。具体来说，我们研究了多参考训练策略，以使我们的DivSLT模型能够实现多样化的翻译。然后，为了提高翻译准确性，我们采用了最大化翻译结果奖励的强化学习目标。此外，我们使用多种指标来评估DivSLT任务的准确性、多样性和语义精度。在丰富后的数据集上的实验结果表明，我们的DivSLT方法不仅实现了更好的翻译性能，还获得了多样化的翻译结果。|
|**2024-10-24**|**Unbounded: A Generative Infinite Game of Character Life Simulation**|Jialu Li et.al.|[2410.18975](http://arxiv.org/abs/2410.18975)|null|我们介绍了生成无限游戏的概念，这是一种视频游戏，它超越了传统固定、硬编码系统的边界，通过使用生成模型来实现。受James P. Carse关于有限游戏和无限游戏区别的启发，我们利用最近在生成式人工智能方面的进展来创建《无界》——一款完全封装在生成模型中的角色生活模拟游戏。具体来说，《无界》受到沙盒生活模拟游戏的启发，允许你通过喂养、玩耍和引导等方式与你在虚拟世界中的自主虚拟角色互动，其中一些机制是开放式的，并且可以是突发性的。为了开发《无界》，我们在语言模型和视觉生成领域提出了技术上的创新。具体而言，我们提出了：(1)一种专门设计的、经过蒸馏的大规模语言模型（LLM），该模型能够实时动态生成游戏机制、叙事和角色互动，(2)一种新的动态区域图像提示适配器（IP-Adapter），用于视觉模型，确保角色在多个环境中的视觉生成既一致又灵活。我们通过定性和定量分析对我们的系统进行了评估，结果显示，在角色生活模拟、用户指令遵循、叙事连贯性和视觉一致性方面，与传统相关方法相比有显著改进。|
|**2024-10-24**|**Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms**|Zhangheng Li et.al.|[2410.18967](http://arxiv.org/abs/2410.18967)|null|构建一个通用的用户界面（UI）理解模型面临着诸多挑战，包括平台多样性、分辨率变化和数据限制等问题。本文介绍了一种名为Ferret-UI 2的新模型，这是一种多模态大语言模型（MLLM），旨在实现跨多种平台的通用UI理解，包括iPhone、Android、iPad、网页和Apple TV等平台。Ferret-UI 2在原有Ferret-UI的基础上引入了三项关键创新：支持多种平台类型、通过自适应缩放实现高分辨率感知，以及利用GPT-4o结合集合标记视觉提示生成高级任务训练数据。这些改进使Ferret-UI 2能够执行复杂的、以用户为中心的交互，使其在不断扩展的平台生态系统中具有高度的通用性和适应性。广泛的实验证明，在指向、定位、以用户为中心的高级任务（包含9个子任务×5个平台）、GUIDE下一步预测数据集和GUI-World多平台基准测试中，Ferret-UI 2显著优于Ferret-UI，并且展示了强大的跨平台迁移能力。|
|**2024-10-24**|**Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions**|Yujuan Fu et.al.|[2410.18966](http://arxiv.org/abs/2410.18966)|null|大型语言模型（LLMs）在各种基准测试中表现出色，显示出作为通用任务解决者的潜力。然而，由于这些模型通常是在大量数据上进行训练的，因此对其评估的一个重要问题是数据污染问题，即训练数据和评估数据集之间的重叠会夸大性能评估。虽然已经开发了多种方法来识别数据污染，但这些方法依赖于特定的假设，而这些假设可能并不普遍适用于不同的设置。为了弥补这一差距，我们系统地回顾了47篇关于数据污染检测的论文，对其中的基础假设进行了分类，并评估了它们是否经过严格的验证。我们确定并分析了八类假设，并以三个假设作为案例研究。我们的分析表明，在对用于预训练LLMs的实例进行分类时，基于这三种假设的检测方法的表现接近于随机猜测，这表明当前的LLMs学习的是数据分布而不是记忆个别实例。总体而言，这项工作强调了方法明确陈述其基础假设并在各种场景下测试其有效性的重要性。|
|**2024-10-24**|**OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning**|Xiaoqiang Wang et.al.|[2410.18963](http://arxiv.org/abs/2410.18963)|null|大型语言模型（LLMs）和大型多模态模型（LMMs）在自动化复杂任务如网页浏览和游戏方面展现出了巨大的潜力。然而，它们在跨多样化应用中的泛化能力仍然有限，这限制了其更广泛的应用。为了解决这一挑战，我们提出了OSCAR：通过状态感知推理和重规划的操作系统控制。OSCAR是一种通用代理，旨在通过标准化的控制方式（如鼠标和键盘输入）自主导航和与各种桌面和移动应用程序进行交互，同时处理屏幕图像以完成用户命令。OSCAR将人类指令转换为可执行的Python代码，从而实现对图形用户界面（GUI）的精确控制。为了增强稳定性和适应性，OSCAR作为一个状态机运行，并配备了错误处理机制和动态任务重规划功能，使其能够高效地实时调整以应对反馈和异常情况。我们通过广泛的实验在多样化的基准测试上展示了OSCAR的有效性，在这些测试中，它将复杂的操作流程简化为简单的自然语言命令，显著提高了用户的生产力。我们的代码将在发表后开源。|
|**2024-10-24**|**Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in Low-Resource Code**|Jipeng Zhang et.al.|[2410.18957](http://arxiv.org/abs/2410.18957)|null|大型语言模型（LLMs）在生成高资源编程语言（HRPLs）如Python的代码方面表现出色，但在低资源编程语言（LRPLs）如Racket或D上的表现则显著逊色。这种性能差距加剧了数字鸿沟，阻碍了使用LRPLs的开发者从LLM的进步中受益，并在一定程度上强化了未充分代表的编程社区之间的创新差异。虽然为LRPLs生成额外训练数据是一个有前景的方法，但它面临着两个关键挑战：人工标注既费时又昂贵，而LLM生成的LRPL代码质量通常较差。这一问题的根本原因在于自然语言到编程语言的差距（NL-PL Gap），在LRPLs中尤其明显，因为对齐的数据有限。在这项工作中，我们介绍了一种名为Bridge-Coder的新方法，该方法利用LLMs的内在能力来增强其在LRPLs上的性能。我们的方法包括两个关键阶段。首先是桥接生成，通过利用LLMs对一般知识的理解、对HRPLs的熟练程度和上下文学习能力来创建高质量的数据集。然后是桥接对齐，逐步改善自然语言指令与LRPLs之间的对齐。实验结果在多种LRPLs中显示，Bridge-Coder显著提升了模型性能，证明了我们方法的有效性和泛化能力。此外，我们还详细分析了方法的关键组成部分，为未来解决与LRPLs相关挑战的研究提供了有价值的见解。|
|**2024-10-24**|**BioMistral-NLU: Towards More Generalizable Medical Language Understanding through Instruction Tuning**|Yujuan Velvin Fu et.al.|[2410.18955](http://arxiv.org/abs/2410.18955)|null|大型语言模型（LLMs）如ChatGPT通过在大规模和多样化的指令跟随语料库上进行微调，能够泛化到新的任务。然而，这些经过指令微调的LLMs在需要领域知识、细粒度文本理解和结构化数据提取的专业医学自然语言理解（NLU）任务中往往表现不佳。为了解决这一问题，我们：(1) 提出了一种统一的提示格式，适用于7个重要的NLU任务，通过跨度提取和多选题问答（QA）来实现；(2) 创建了一个指令微调数据集MNLU-Instruct，利用了多种现有的开源医学NLU语料库；(3) 通过在MNLU-Instruct上对BioMistral进行微调，开发了BioMistral-NLU，一个具有通用性的医学NLU模型。我们在零样本设置下评估了BioMistral-NLU，在两个广泛采用的医学NLU基准测试中，即生物医学语言理解评估（BLUE）和生物医学语言理解和推理基准（BLURB）中的6个重要NLU任务。实验结果表明，我们的BioMistral-NLU在性能上优于原始的BioMistral以及专有的LLMs——ChatGPT和GPT-4。我们与数据集无关的提示策略和在各种NLU任务上的指令微调步骤增强了LLMs在各种医学NLU任务中的泛化能力。消融实验显示，即使总的训练实例数量保持不变，指令微调的任务种类越广，下游零样本泛化能力也越强。|
|**2024-10-24**|**Dynamic Vocabulary Pruning in Early-Exit LLMs**|Jort Vincenti et.al.|[2410.18952](http://arxiv.org/abs/2410.18952)|**[link](https://github.com/matteonulli/vocabulary_pruning)**|**增加大型语言模型（LLMs）的规模已被证明可以提高其性能。然而，这也带来了推理速度变慢和成本增加的问题。早期退出是一种有前景的方法，通过在中间层进行预测来提高LLM推理的效率。然而，现代LLMs中的大词汇量使得所需的置信度估计在计算上非常昂贵，从而降低了效率提升的效果。为了解决这个问题，我们提出在测试时动态剪枝词汇表。具体来说，词汇表在最初的某一层被剪枝，并在整个前向传递过程中使用较小的词汇表。我们的实验表明，这种后处理动态词汇表剪枝方法提高了早期退出LLM中置信度估计的效率，同时保持了具有竞争力的性能。**|
|**2024-10-24**|**SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models**|Zonghao Ying et.al.|[2410.18927](http://arxiv.org/abs/2410.18927)|null|多模态大型语言模型（MLLMs）在用户生成有害输出方面表现出强烈的安全隐患，这促使了安全评估基准的发展。然而，我们观察到现有的MLLMs安全基准存在查询质量低和评估可靠性差的问题，这些问题限制了对MLLMs安全影响的检测，因为随着MLLMs的不断发展，这些基准已显得不足。在本文中，我们提出了一种名为\toolns的综合框架，用于对MLLMs进行安全评估。我们的框架包括一个全面的有害查询数据集和一种自动评估协议，分别旨在解决上述问题。我们首先设计了一个自动安全数据集生成管道，在这个管道中，我们使用一组LLM评判者来识别和分类对MLLMs最具危害性和多样性的风险场景；基于这种分类，我们进一步要求这些评判者相应地生成高质量的有害查询，从而产生了23种风险场景和2300个多模态有害查询对。在安全评估过程中，我们借鉴司法程序中的陪审团制度，开创了一种陪审团审议评估协议，该协议采用协作式LLM来评估目标模型是否表现出特定的有害行为，从而提供可靠且无偏见的内容安全风险评估。此外，我们的基准还可以扩展到音频模态，显示出高度的可扩展性和潜力。基于我们的框架，我们对15种广泛使用的开源MLLMs和6种商业MLLMs（如GPT-4o、Gemini）进行了大规模实验，揭示了现有MLLMs中存在的广泛安全问题，并实例化了关于MLLMs安全性能的一些见解，如图像质量和参数大小。|
|**2024-10-24**|**From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical Integrity on Faulty Mathematical Problems**|A M Muntasir Rahman et.al.|[2410.18921](http://arxiv.org/abs/2410.18921)|null|考虑一个数学问题：“莉莉昨天从她最好的朋友那里收到了3块饼干，并在早餐时吃了5块。今天，她的朋友又给了她3块饼干。现在莉莉有多少块饼干？”许多大型语言模型（LLMs）在先前的研究中通过计算“3-5+3”的等式来得出答案“1”。然而，从人类的角度来看，我们认识到这个问题的内在缺陷：如果莉莉最初只有3块饼干，她不可能在早餐时吃掉5块。这种差异引发了一个关键问题：当前的LLMs是仅仅作为盲目的解题者，机械地应用数学运算而不进行更深层次的推理，还是能够作为一个逻辑思考者，识别逻辑上的不一致？  为了探讨这个问题，我们提出了一套基准数据集FaultyMath，其中包括多样化的有缺陷的数学问题：i）涵盖多个数学类别，如代数、几何、数论等；ii）具有不同的难度级别；iii）不同类型的缺陷来源——包括常识违反、模糊陈述、数学矛盾等。我们使用FaultyMath对广泛的LLMs进行评估，包括开源、闭源和数学专业模型，从三个方面进行评估：(i) 在没有明确提示的情况下，这些模型能多准确地检测出有缺陷的数学问题？(ii) 当提供关于问题有效性的提示——无论是正确的还是误导性的——LLMs在多大程度上能够适应成为可靠的逻辑思考者？(iii) 当LLMs识别出一个数学问题是错误的时，它们生成的解释有多可靠？通过广泛的实验和详细的分析，我们的结果表明，现有的LLMs大多表现为盲目的解题者，未能具备成为逻辑思考者所需的推理能力。|
|**2024-10-25**|**A Survey on Speech Large Language Models**|Jing Peng et.al.|[2410.18908](http://arxiv.org/abs/2410.18908)|null|大型语言模型（LLMs）在上下文理解和多任务处理方面表现出色。因此，研究人员一直在寻求将LLMs集成到口语理解（SLU）领域的大框架中。不同于传统的通过自动语音识别（ASR）生成文本并依次处理的方法，新的研究集中在设计以音频特征提取为中心、结合多模态信息融合和LLM推理的架构——即所谓的语音LLMs。这种方法能够更丰富地提取音频特征，同时促进音频和文本模态的端到端融合，从而实现从音频数据中进行更深层次的理解和推理。本文阐明了语音LLMs的发展，提供了系统架构和训练策略的深入分析。通过广泛的研究和一系列针对性实验，本文评估了语音LLMs在丰富音频转写方面的进展及其在SLU领域跨任务整合的潜力。此外，本文还指出了通过实验发现的关键挑战，例如在某些条件下LLMs的惰性问题。文章进一步探讨了语音LLMs的训练策略，并基于这些发现提出了潜在解决方案，为该领域的未来研究以及LLMs在多模态环境中的应用提供了有价值的见解和参考。|
|**2024-10-23**|**TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts**|Yuxuan Xie et.al.|[2410.18071](http://arxiv.org/abs/2410.18071)|null|最近，多模态大型语言模型（MLLMs）因其令人印象深刻的性能而备受关注。对MLLMs的评估变得至关重要，因为这有助于分析这些模型的特性并提供有价值的见解。然而，当前的基准测试忽视了提示敏感性的问题——轻微的提示变化可能会导致显著的性能波动。因此，不适当的提示可能会掩盖模型的能力，从而低估模型的性能。此外，不同的模型对于不同提示有不同的偏好，因此使用相同的提示来评估所有模型会导致评估偏差。本文分析了现有基准测试中的这一缺陷，并进一步引入了一个新的评估框架TP-Eval。该框架通过引入提示定制方法来减少评估偏差并挖掘模型的潜力。TP-Eval将重写原始提示，为不同的模型生成不同的定制化提示。特别是，我们针对MLLM评估场景设计了一些模块来实现提示定制。广泛的实验表明，我们的方法能够有效揭示模型的潜力，TP-Eval有望为社区开发更全面和有说服力的MLLM评估基准做出贡献。|
|**2024-10-23**|**LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering**|Qingfei Zhao et.al.|[2410.18050](http://arxiv.org/abs/2410.18050)|**[link](https://github.com/qingfei1/longrag)**|**长上下文问答（LCQA）是一项具有挑战性的任务，旨在通过推理长篇文档来准确回答问题。现有的长上下文大语言模型（LLMs）在LCQA中常常面临“迷失中间”问题。检索增强生成（RAG）通过提供外部事实证据来缓解这一问题。然而，其分块策略破坏了全局长上下文信息，并且在长上下文中低质量的检索会阻碍大语言模型识别有效的事实细节，因为存在大量噪声。为此，我们提出了LongRAG，这是一种通用的、双重视角的、健壮的大语言模型为基础的RAG系统范式，用于增强RAG对复杂长上下文知识的理解（即全局信息和事实细节）。我们将LongRAG设计为一种即插即用的范式，便于适应各种领域和大语言模型。在三个多跳数据集上的广泛实验表明，LongRAG显著优于长上下文大语言模型（提升6.94%），先进的RAG（提升6.16%）和原始RAG（提升17.25%）。此外，我们进行了定量消融研究和多维度分析，强调了系统组件和微调策略的有效性。数据和代码可在https://github.com/QingFei1/LongRAG获取。**|
|**2024-10-23**|**Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for Russian Scientific Keyphrases**|Anna Glazkova et.al.|[2410.18040](http://arxiv.org/abs/2410.18040)|null|关键词选择是自然语言处理中的一个具有广泛应用的挑战性任务。由于俄语丰富的形态学特征以及有限的训练数据集，将现有的监督和非监督解决方案应用于俄语面临诸多限制。最近对英文文本的研究表明，大型语言模型（LLMs）成功地解决了生成关键词的任务。这些模型能够在不进行特定任务微调的情况下取得令人印象深刻的结果，使用文本提示即可。在这项工作中，我们评估了基于提示的方法在生成俄文科学摘要关键词方面的表现。首先，我们比较了零样本和少量样本提示方法、微调模型和非监督方法的性能。然后，我们评估了少量样本设置中关键词示例的选择策略。我们展示了人工评估生成的关键词的结果，并通过专家评估分析了模型的优势和劣势。我们的结果显示，即使使用简单的文本提示，基于提示的方法也可以超越常见的基线模型。|
|**2024-10-23**|**MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning**|Jingfan Zhang et.al.|[2410.18035](http://arxiv.org/abs/2410.18035)|null|低秩适应（LoRA）及其混合专家（MOE）变体是高度有效的参数高效微调（PEFT）方法。然而，由于在Transformer层的多个线性模块中添加了LoRA模块和MOE路由器，这些方法在多租户设置中引入了显著的延迟。为了解决这个问题，我们提出了Mixture of Low-Rank Adaptation (MiLoRA)，这是一种新颖且高效的LoRA变体。MiLoRA与之前的MOE风格LoRA方法不同，它将每个LoRA模块视为一个专家，并采用提示感知路由机制。这种机制在生成第一个新标记之前计算一次专家路由结果，并在后续标记中重用这些结果，从而减少延迟。广泛的实验和分析表明，在常识推理任务、数学推理任务以及广泛使用的LLM评估基准上，MiLoRA始终优于强大的PEFT基线，同时具有可比的可调参数预算。此外，与之前的基于LoRA的方法相比，MiLoRA在多租户设置中显著降低了延迟。|
|**2024-10-23**|**GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration**|Xin Li et.al.|[2410.18032](http://arxiv.org/abs/2410.18032)|**[link](https://github.com/bupt-gamma/graphteam)**|**图是现实世界场景中建模关系数据的常用工具，例如社交网络和城市计算。现有的基于大型语言模型（LLM）的图分析方法要么集成图神经网络（GNN）以用于特定的机器学习任务，从而限制了其可移植性；要么完全依赖于LLM自身的推理能力，导致性能不佳。为了解决这些局限性，我们利用了LLM基代理的最新进展，这些进展表明它们能够利用外部知识或工具解决问题。通过模拟人类的问题解决策略，如类比和协作，我们提出了一种基于LLM的多代理系统，名为GraphTeam，用于图分析。GraphTeam由三个模块中的五个LLM基代理组成，这些具有不同专长的代理可以相互协作以应对复杂问题。具体来说，（1）输入-输出规范化模块：问题代理从原始问题中提取并精炼四个关键参数，促进问题理解，而答案代理则组织结果以满足输出要求；（2）外部知识检索模块：我们首先构建了一个包含相关文档和经验信息的知识库，然后搜索代理针对每个问题检索最相关的条目。（3）问题解决模块：给定搜索代理检索到的信息，编码代理使用编程方式生成解决方案；如果编码代理不起作用，则推理代理将直接计算结果而不进行编程。在六个图分析基准上的广泛实验表明，GraphTeam达到了最先进的性能，在准确率方面平均比最佳基线高出25.85%。代码和数据可在<https://github.com/BUPT-GAMMA/GraphTeam>获取。**|
|**2024-10-23**|**MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC Meeting**|Sungil Seok et.al.|[2410.18012](http://arxiv.org/abs/2410.18012)|null|美国联邦基金利率在国内外金融市场中扮演着重要角色。然而，研究主要集中在该利率调整的影响上，而不是决策过程本身。最近大型语言模型（LLM）的进步提供了一种可能的方法来重构负责设定联邦基金利率的联邦公开市场委员会（FOMC）会议。在本文中，我们提出了一种五阶段的FOMC会议模拟框架MiniFed，该框架使用LLM代理来模拟现实世界中的FOMC会议成员，并优化FOMC结构。此框架有效地重新激活了FOMC会议过程，并有助于预测联邦基金利率。实验结果表明，我们提出的MiniFed框架在联邦基金利率预测方面具有高准确度，并且在代理行为上与现实世界的对应者保持一致。鉴于目前很少有研究利用LLM代理来模拟大规模的现实世界会议，我们的工作可以作为未来发展的基准。|
|**2024-10-23**|**ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference**|Xin He et.al.|[2410.17954](http://arxiv.org/abs/2410.17954)|null|稀疏混合专家（MoE）模型在性能上优于密集的大语言模型（LLMs），但在推理部署过程中面临显著的内存需求挑战。现有的卸载技术涉及在GPU和CPU之间交换激活和空闲的专家，但这些技术通常受到刚性专家缓存机制的限制。这些机制无法适应动态路由，导致缓存利用率低下，或在预测训练中产生高昂的成本。为了解决这些特定于推理的挑战，我们引入了ExpertFlow，这是一个专门设计的系统，旨在通过适应灵活路由并实现专家在CPU和GPU之间的高效调度来增强推理效率。这减少了开销并提升了系统性能。我们的方法核心是一个基于预测路由路径的卸载机制，利用轻量级预测器在计算开始前准确预测路由路径。这种主动策略允许实时纠正专家缓存中的错误，显著提高缓存命中率并减少专家传输的频率，从而最小化I/O开销。此外，我们还实施了一种动态令牌调度策略，通过在不同批次间重新排列输入令牌来优化MoE推理。这种方法不仅减少了每批次激活的专家数量，还提高了计算效率。我们的广泛实验表明，ExpertFlow实现了高达93.72%的GPU内存节省，并将推理速度提升至基线方法的2到10倍，突显了其有效性和作为资源受限推理场景下的稳健解决方案的重要性。|
|**2024-10-23**|**SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains**|Ran Xu et.al.|[2410.17952](http://arxiv.org/abs/2410.17952)|null|检索增强生成（RAG）通过整合外部知识增强了大型语言模型（LLMs）的问题回答（QA）能力。然而，将通用的RAG系统适应到科学和医学等专业领域时，由于分布差异和有限的领域特定数据访问，会面临独特的挑战。为了解决这一问题，我们提出了SimRAG，这是一种自训练方法，使LLM具备问题回答和问题生成的联合能力，以实现领域适应。我们的方法首先在指令跟随、问答和搜索相关数据上对LLM进行微调。然后，它提示相同的LLM从无标签语料库中生成多样化的领域相关问题，并采用额外的过滤策略来保留高质量的合成示例。通过利用这些合成示例，LLM可以在特定领域的RAG任务中提升性能。在跨越两个基础模型大小和三个领域的11个数据集上的实验表明，SimRAG比基线方法高出1.2%至8.6%。|
|**2024-10-23**|**Benchmarking Floworks against OpenAI & Anthropic: A Novel Framework for Enhanced LLM Function Calling**|Nirav Bhan et.al.|[2410.17950](http://arxiv.org/abs/2410.17950)|null|大型语言模型（LLMs）在各个领域展示了非凡的能力，但由于工具使用和功能调用方面的挑战，其经济影响受到了限制。本文介绍了一种名为ThorV2的新架构，该架构显著增强了LLMs的功能调用能力。我们开发了一个全面的基准测试，专注于HubSpot CRM操作，以评估ThorV2与OpenAI和Anthropic的领先模型。我们的结果显示，ThorV2在单个和多API调用任务的准确性、可靠性、延迟和成本效率方面均优于现有模型。我们还表明，ThorV2在多步骤任务中的可靠性更强，并且可扩展性更好，相比传统模型具有明显优势。我们的工作提供了令人兴奋的可能性，即使用显著更小的LLMs实现比当今最佳模型更准确的功能调用。这些进展对于开发更强大的AI助手以及LLMs在现实场景中的广泛应用具有重要意义。|
|**2024-10-23**|**Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced Defense in Large Language Models**|He Cao et.al.|[2410.17922](http://arxiv.org/abs/2410.17922)|**[link](https://github.com/idea-xl/g4d)**|随着大规模语言模型（LLMs）的广泛部署，确保其安全性变得越来越重要。然而，现有的防御方法往往存在两个关键问题：(i) 防御能力不足，尤其是在化学等特定领域场景下，缺乏专门知识可能导致对恶意查询生成有害响应。(ii) 过度防御，这会损害LLMs的一般实用性和响应性。为了解决这些问题，我们引入了一种基于多代理的防御框架，称为Guide for Defense (G4D)，该框架利用准确的外部信息提供用户意图的无偏总结以及分析性安全响应指导。广泛的实验表明，在流行的手册逃脱攻击和良性数据集上，我们的G4D可以在不损害模型一般功能的情况下增强LLM在通用和特定领域的鲁棒性。|
|**2024-10-22**|**Large Language Models Empowered Personalized Web Agents**|Hongru Cai et.al.|[2410.17236](http://arxiv.org/abs/2410.17236)|null|网络代理作为自动化基于用户指令的Web任务完成的一种有前景的方向，显著提升了用户体验。最近，网络代理从传统的代理发展到基于大语言模型（LLM）的网络代理。尽管取得了成功，现有的基于LLM的网络代理忽略了个性化数据（如用户资料和历史Web行为）在辅助理解用户的个性化指令和执行定制化操作方面的重要性。为克服这一局限，我们首先制定了一个基于LLM的个性化网络代理任务，该任务结合了个性化数据和用户指令来实现个性化的指令理解和操作执行。为了应对缺乏全面评估基准的问题，我们构建了一个个性化网络代理基准（PersonalWAB），该基准包含了用户指令、个性化用户数据、Web功能，并提供了三个个性化Web任务的两种评估范式。此外，我们提出了一种个性化用户记忆增强对齐（PUMA）框架，以适应个性化网络代理任务。PUMA利用具有特定任务检索策略的记忆库来筛选相关的历史Web行为。然后，根据这些行为，PUMA通过微调和直接偏好优化来调整LLM进行个性化的操作执行。广泛的实验验证了PUMA在PersonalWAB上优于现有网络代理的优越性。|
|**2024-10-22**|**Automated Spinal MRI Labelling from Reports Using a Large Language Model**|Robin Y. Park et.al.|[2410.17235](http://arxiv.org/abs/2410.17235)|**[link](https://github.com/robinyjpark/autolabelclassifier)**|**我们提出了一种通用的管道，用于使用大型语言模型自动化提取放射学报告中的标签，并在脊柱MRI报告上进行了验证。该标签提取方法的有效性在五种不同的情况中进行了评估：脊柱癌、狭窄、脊椎滑脱、马尾神经受压和疝气。使用开源模型，我们的方法在保留的一组报告上等于或超过了GPT-4的表现。此外，我们展示了所提取的标签可以用来训练影像模型以识别伴随的MRI扫描中的这些已识别的状况。所有使用自动标签训练的分类器表现与使用临床医生手动标注的扫描训练的模型相当。代码可以在<https://github.com/robinyjpark/AutoLabelClassifier>找到。**|
|**2024-10-22**|**Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy**|Benedict Aaron Tjandra et.al.|[2410.17234](http://arxiv.org/abs/2410.17234)|null|大型语言模型（LLMs）以其生成合理但不准确文本的能力而闻名，这种现象在医学或法律等关键应用中带来了显著的风险，因此需要采取稳健的幻觉缓解策略。尽管最近的研究提出了通过微调来教导模型避免回答超出其知识或能力范围的问题的方法，但这些方法依赖于外部的真实标签，或者仅限于短文本回应。为了解决这些限制，我们提出了一种利用语义熵进行微调的方法，这是一种从模型内部进行自我反思得出的不确定性度量，不需要外部标签。我们证明了我们的方法在使用先前研究进行微调的模型上达到了同等或更好的表现，并在多种数据集上实现了对短文本和长文本生成的强大性能。|
|**2024-10-22**|**Few-shot In-Context Preference Learning Using Large Language Models**|Chao Yu et.al.|[2410.17233](http://arxiv.org/abs/2410.17233)|null|设计奖励函数是强化学习中的核心组成部分，但对于非常复杂的行为来说可能具有挑战性。通过用从人类反馈中学习到的奖励函数替代手工编写的奖励函数，基于人类反馈的强化学习（RLHF）已经用于缓解这一挑战。然而，学习这些奖励函数通常效率低下，因为它们往往是从头开始学习的。我们研究了大型语言模型（LLM）是否可以通过将一系列人类偏好转换为表示奖励的代码来减少查询的低效性。我们提出了一种称为“上下文偏好学习”（ICPL）的方法，该方法利用LLM的背景知识来加速从偏好中学习奖励函数的过程。ICPL采用环境上下文和任务描述，合成一组奖励函数，然后反复使用人类对政策结果视频的排名来更新这些奖励函数。通过合成偏好，我们证明ICPL比RLHF高效几个数量级，并且甚至与使用真实奖励函数的方法相比也具有竞争力。最后，我们进行了一系列人类偏好学习试验，观察到ICPL不仅适用于合成设置，还可以在人类参与的循环中有效工作。更多相关信息和视频可以在https://sites.google.com/view/few-shot-icpl/home 获取。|
|**2024-10-22**|**Context-aware Prompt Tuning: Advancing In-Context Learning with Adversarial Methods**|Tsachi Blau et.al.|[2410.17222](http://arxiv.org/abs/2410.17222)|null|微调大型语言模型（LLMs）通常涉及更新数十亿个参数。一种更为参数高效的方法是提示调优（PT），它仅更新少数可学习的标记。另一种方法是情境学习（ICL），它通过在输入中包含示例来适应新任务，而无需进行训练。当应用基于优化的方法，如微调和PT进行少样本学习时，模型会特别适应少量的训练示例，而ICL则不改变模型本身。这种区别使得传统的学习方法更容易过拟合；相反，ICL对少量样本的情况不太敏感。虽然ICL不容易过拟合，但它并不能完全提取训练示例中存在的信息。本文介绍了一种名为情境感知提示调优（CPT）的方法，该方法受到ICL、PT和对抗攻击的启发。我们在ICL策略的基础上，将示例与输入串联起来，但通过PT式的优化，迭代地优化上下文嵌入，以从训练示例中提取更深层次的信息。我们仔细修改特定的上下文标记，考虑输入和输出格式的独特结构。受对抗攻击的启发，我们根据上下文中存在的标签调整输入，旨在最小化而不是最大化损失。此外，我们应用投影梯度下降算法，使标记嵌入保持在接近原始值的状态，假设用户提供的数据本质上是有价值的。我们的方法在多个分类任务中使用各种LLM模型，已显示出优越的准确性。|
|**2024-10-22**|**Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh through Large Language Modeling**|Azmine Toushik Wasi et.al.|[2410.17210](http://arxiv.org/abs/2410.17210)|**[link](https://github.com/ciol-researchlab/ukil)**|**目的：孟加拉国的法律系统面临着重大挑战，如案件积压、复杂性、高昂的成本以及数百万未决案件等问题，这些问题导致许多人因缺乏知识或经济限制而无法寻求法律救济。本研究旨在开发一个专门的大语言模型（LLM）以协助孟加拉国的法律系统。方法：我们通过收集和爬取各种法律法案的数据，创建了UKIL-DB-EN，即孟加拉国法律文件的英文语料库。然后在该数据集上对GPT-2模型进行了微调，开发了GPT2-UKIL-EN，这是一个专注于提供英语法律援助的LLM。结果：该模型通过包括专家意见支持的案例研究在内的语义评估进行了严格评估，结果显示模型具有潜在的法律事务辅助能力。结论：我们的工作代表了建立孟加拉国AI法律助手的第一个有组织的努力。尽管结果令人鼓舞，但仍需要进一步改进以提高模型的准确性、可信度和安全性。这是朝着创建能够满足1.8亿人口需求的法律AI的重要一步。**|
|**2024-10-22**|**VoiceBench: Benchmarking LLM-Based Voice Assistants**|Yiming Chen et.al.|[2410.17196](http://arxiv.org/abs/2410.17196)|**[link](https://github.com/matthewcym/voicebench)**|**基于大型语言模型（LLMs）的成功，近期的进展如GPT-4o使得通过基于LLM的语音助手实现实时语音交互成为可能，与传统的基于文本的交互相比，这大大提升了用户体验。然而，缺乏专门用于评估这些语音交互能力的基准测试，阻碍了基于LLM的语音助手的发展。当前的评估主要集中在自动语音识别（ASR）或使用清晰语音的一般知识评估上，忽视了更复杂的现实场景，这些场景涉及多样的说话者特征、环境和内容因素。为了解决这一问题，我们介绍了VoiceBench，这是首个旨在提供多方面评估的基于LLM的语音助手基准测试。VoiceBench还包括既包括真实的也包括合成的口语指令，这些指令融合了上述三个关键的现实世界变化因素。广泛的实验揭示了当前基于LLM的语音助手模型的局限性，并为该领域的未来研究和发展提供了宝贵的见解。**|
|**2024-10-23**|**Non-myopic Generation of Language Model for Reasoning and Planning**|Chang Ma et.al.|[2410.17195](http://arxiv.org/abs/2410.17195)|**[link](https://github.com/chang-github-00/llm-predictive-decoding)**|大型语言模型在推理和规划方面展示了惊人的能力，通过将复杂问题分解成一系列步骤来解决。尽管它们在数学问题求解和编码等各种领域取得了成功，但由于其固有的自回归解码方式，这些模型在确保可靠且最优的规划时仍面临挑战。本文从最优控制的角度重新审视了大型语言模型的推理方法，提出了一种新颖的方法——预测解码。该方法利用模型预测控制来增强规划准确性。通过根据前瞻轨迹重新加权语言模型的分布，预测解码旨在减轻早期错误并促进非短视规划。我们的实验表明，在数学、编码和智能体任务的广泛范围内，这种方法显著提高了性能。此外，预测解码还表现出计算效率，使用较少的计算资源就优于搜索基线。本研究为优化大型语言模型的规划能力提供了见解。|
|**2024-10-22**|**From Attention to Activation: Unravelling the Enigmas of Large Language Models**|Prannay Kaul et.al.|[2410.17174](http://arxiv.org/abs/2410.17174)|null|我们研究了自回归Transformer中的两种奇怪现象：（1）注意力头中第一个令牌的主导性；（2）隐藏状态中出现大的异常激活值。我们发现，流行的大语言模型（如Llama）在98%的注意力头中对第一个令牌的关注度最大，我们将这种行为归因于softmax函数。为了缓解这个问题，我们提出了一种softmax-1的重新公式化方法。此外，我们确定自适应优化器（例如Adam）是导致这些大异常激活值的主要原因，并引入OrthoAdam，一种新的优化器，它使用正交矩阵来转换梯度，以解决这一问题。最后，我们的方法不仅防止了这些现象的发生，而且还使Transformer能够在使用基本算法进行量化时保持其性能，这是标准方法无法做到的。总之，我们的方法将第一个令牌的注意力比例从65%降低到3.3%，隐藏状态中的激活峰度从1657降低到3.1，在4位权重量化下困惑度惩罚从3565降低到0.3。|
|**2024-10-22**|**Improving Pinterest Search Relevance Using Large Language Models**|Han Wang et.al.|[2410.17152](http://arxiv.org/abs/2410.17152)|null|为了提高Pinterest搜索的相关性评分，我们将大型语言模型（LLMs）集成到我们的搜索相关性模型中，利用精心设计的文本表示来有效地预测Pin的相关性。我们的方法使用搜索查询以及包含从生成式视觉语言模型中提取的字幕的内容表示。这些表示进一步通过链接文本数据、历史高质量交互查询、用户创建的板、Pin标题和Pin描述进行丰富，从而创建出强大的模型来预测搜索相关性。我们采用半监督学习方法以高效地扩展训练数据量，超越仅限于昂贵的人工标注数据。通过利用多语言LLMs，我们的系统将训练数据扩展到包括未见过的语言和领域，尽管初始数据和注释员的专业知识仅限于英语。此外，我们将基于LLM的模型提炼成实时可服务的模型架构和特征。我们提供了全面的离线实验验证我们提出的技术，并展示了在大规模部署系统中所取得的成果。|
|**2024-10-21**|**Reflection-Bench: probing AI intelligence with reflection**|Lingyu Li et.al.|[2410.16270](http://arxiv.org/abs/2410.16270)|**[link](https://github.com/yabyum/reflectionbench)**|**适应性地调整信念或行为以应对意外结果的反思能力，是智能系统与世界互动的核心原则。从认知科学的角度来看，这一原则适用于人类和人工智能系统。为了应对关于大型语言模型（LLMs）智能性的辩论，我们提出了Reflection-Bench，这是一个包含七个任务的综合基准，这些任务涵盖了反思所需的核心认知功能，包括感知、记忆、信念更新、决策、预测、反事实思维和元反思。我们评估了13个著名的LLMs，如OpenAI o1、GPT-4、Claude 3.5 Sonnet等的表现。结果显示，目前的LLMs在反思能力方面仍不令人满意。我们讨论了这些结果背后的原因，并提出了未来研究的潜在方向。总之，Reflection-Bench不仅提供了评估工具，也为开发能够可靠地与环境交互的AI提供了灵感。我们的数据和代码可在https://github.com/YabYum/ReflectionBench获得。**|
|**2024-10-21**|**Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance**|Zhangwei Gao et.al.|[2410.16261](http://arxiv.org/abs/2410.16261)|**[link](https://github.com/opengvlab/internvl)**|**多模态大型语言模型（MLLMs）在广泛的领域内展示了在视觉-语言任务中的出色性能。然而，由于模型规模庞大和相关的高计算成本，在消费者级GPU或边缘设备上训练和部署这些模型面临着巨大挑战，从而限制了它们的广泛应用。在这项工作中，我们引入了Mini-InternVL系列模型，其参数量从1B到4B不等，这些模型仅使用5%的参数就能达到90%的性能。这种显著的效率和效果提升使我们的模型更加易于访问和适用于各种实际场景。为了进一步促进这些模型的采用，我们开发了一个统一的适应框架，使得Mini-InternVL模型能够转移并在下游任务中超越专门模型，包括自动驾驶、医学影像和遥感等领域。我们相信，我们的研究可以为高效且有效的MLLMs的发展提供有价值的见解和资源。代码可在https://github.com/OpenGVLab/InternVL获取。**|
|**2024-10-21**|**Elucidating the design space of language models for image generation**|Xuantong Liu et.al.|[2410.16257](http://arxiv.org/abs/2410.16257)|**[link](https://github.com/Pepper-lll/LMforImageGeneration)**|自回归（AR）语言模型在文本生成中的成功激发了计算机视觉社区采用大规模语言模型（LLMs）进行图像生成。然而，考虑到文本和图像模态之间的基本差异，用于图像生成的语言模型的设计空间仍需深入探索。我们观察到图像标记表现出比文本标记更大的随机性，这在训练过程中带来了挑战。尽管如此，AR模型通过有效地学习即使是从看似次优的优化问题中提取的模式，展示了其潜力。我们的分析还表明，虽然所有模型都成功地理解了局部信息在图像生成中的重要性，但较小的模型难以捕捉全局上下文。相比之下，较大的模型在这方面表现出更好的能力，解释了当扩大模型规模时性能提升的原因。我们进一步通过广泛的对比实验阐明了用于视觉生成的语言模型的设计空间，包括标记器选择、模型选择、模型可扩展性、词汇设计和采样策略。我们的工作首次分析了语言模型在视觉生成中的优化行为，我们认为它能够启发更有效的设计，当将LMs应用于其他领域时。最后，我们阐明了一种用于图像生成的语言模型，称为ELM，在ImageNet 256*256基准测试中达到了最先进的性能。代码可在<https://github.com/Pepperlll/LMforImageGeneration.git>获取。|
|**2024-10-21**|**CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution**|Maosong Cao et.al.|[2410.16256](http://arxiv.org/abs/2410.16256)|**[link](https://github.com/open-compass/compassjudger)**|**高效且准确的评估对于大型语言模型（LLMs）的持续改进至关重要。在各种评估方法中，主观评估因其与现实世界使用场景和人类偏好的高度一致而备受关注。然而，基于人类的评估既昂贵又缺乏可重复性，因此精确的自动化评估者（评判者）在这个过程中变得尤为重要。在这份报告中，我们介绍了\textbf{CompassJudger-1}，这是第一个开源的\textbf{一体化}评判LLM。CompassJudger-1是一个通用的LLM，表现出显著的多功能性。它能够：1. 作为奖励模型进行单一评分和双模型比较；2. 根据指定格式进行评估；3. 生成批评；4. 执行各种任务，就像一个通用的LLM。为了在一个统一的设置下评估不同评判模型的能力，我们还建立了\textbf{JudgerBench}，这是一个新的基准测试，涵盖了各种主观评估任务，并涉及广泛的主题。CompassJudger-1提供了一个全面的解决方案来处理各种评估任务，同时保持适应多样化需求的灵活性。CompassJudger和JudgerBench均已发布并可供研究社区访问https://github.com/open-compass/CompassJudger。我们相信通过开源这些工具，我们可以促进合作并加速LLM评估方法的进步。**|
|**2024-10-21**|**Can Knowledge Editing Really Correct Hallucinations?**|Baixiang Huang et.al.|[2410.16251](http://arxiv.org/abs/2410.16251)|**[link](https://github.com/llm-editing/HalluEditBench)**|**大型语言模型（LLMs）在生成内容时常常会出现幻觉，即包含不真实的信息，尽管它们在各种任务上表现出色。同时，知识编辑作为一种新的流行范式，被用来纠正LLMs中错误的事实知识，其优势在于避免了从头开始重新训练的需要。然而，现有用于知识编辑的评估数据集的一个常见问题是，它们并不能确保LLMs在编辑前对评估问题生成幻觉性答案。当经过不同技术编辑后的LLMs在这类数据集上进行评估时，很难直接采用这些性能来评估不同知识编辑方法在纠正幻觉方面的有效性。因此，一个基本的问题仍未得到充分验证：知识编辑真的能纠正LLMs中的幻觉吗？我们提出了HalluEditBench，以全面基准测试知识编辑方法在纠正现实世界幻觉方面的能力。首先，我们严格构建了一个包含9个领域、26个主题和超过6000个幻觉的大规模幻觉数据集。然后，我们在五个维度——包括有效性、泛化能力、可移植性、局部性和鲁棒性——上全面评估了知识编辑方法的性能。通过HalluEditBench，我们提供了对不同知识编辑方法在纠正幻觉方面的潜力和局限性的新见解，这可以启发未来的改进并促进知识编辑领域的进展。**|
|**2024-10-21**|**Analyzing Context Contributions in LLM-based Machine Translation**|Emmanouil Zaranis et.al.|[2410.16246](http://arxiv.org/abs/2410.16246)|null|大型语言模型（LLMs）在机器翻译（MT）方面已经达到了最先进的性能，并且通过少量示例展示了利用上下文进行学习的能力。然而，关于LLMs如何使用输入的不同部分的机制仍然很大程度上未被探索。在这项工作中，我们对机器翻译中的上下文利用进行了全面分析，研究了当生成翻译时，LLMs如何使用各种上下文部分，如少量示例和源文本。我们强调了几个关键发现：（1）在不同翻译方向下，少量示例的源部分似乎比其对应的目标部分贡献更大；（2）用平行数据微调LLMs会改变不同上下文部分的贡献模式；（3）存在位置偏差，即更早的少量示例对翻译序列的贡献更高。最后，我们证明检查异常的上下文贡献可以潜在地揭示病态翻译，例如幻觉。我们的发现揭示了基于LLM的机器翻译的内部运作机制，这些机制超越了标准编码器-解码器机器翻译模型已知的知识。|
|**2024-10-21**|**IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in Communicative Multi-Agent Systems**|Yihuan Mao et.al.|[2410.16237](http://arxiv.org/abs/2410.16237)|null|随着大型语言模型（LLM）代理越来越多地集成到我们的基础设施中，它们的稳健协调和消息同步变得至关重要。拜占庭将军问题（BGP）是构建在对抗性攻击下具有弹性的多智能体系统（MAS）的关键模型。它描述了一种情景，在这种情景中系统内存在恶意代理，这种情况可能源于LLM代理的幻觉或外部攻击。在BGP中，整个系统的目的是就采取的行动达成共识。传统的BGP要求所有代理之间实现全局共识；然而，在实际场景中，全局共识并不总是必要，甚至可能是低效的。因此，迫切需要探索一种与MAS中观察到的局部协调模式相一致的改进版BGP。在我们的研究中，我们称这种改进版本为不完美BGP（IBGP），旨在解决这一差异。为了应对这一问题，我们提出了一种框架，该框架利用了通用MAS设置中的共识协议，提供了对通信攻击的可证明的弹性以及适应不断变化环境的能力，并通过实证结果进行了验证。此外，我们还通过一个传感器网络环境的案例研究来说明我们协议的实际应用。|
|**2024-10-21**|**LLaVA-KD: A Framework of Distilling Multimodal Large Language Models**|Yuxuan Cai et.al.|[2410.16236](http://arxiv.org/abs/2410.16236)|**[link](https://github.com/Fantasyele/LLaVA-KD)**|大型语言模型（LLM）的成功促使研究者探索多模态大型语言模型（MLLM），以实现视觉和语言的统一理解。然而，随着模型规模和计算复杂性的增加，MLLM在资源受限环境中的应用受到限制。小规模多模态大型语言模型（s-MLLM）旨在保留大规模模型（l-MLLM）的能力，同时减少计算需求，但会导致性能显著下降。为了解决这些问题，我们提出了一种名为LLaVA-KD的新框架，用于将知识从l-MLLM转移到s-MLLM。具体来说，我们引入了多模态蒸馏（MDist）来最小化l-MLLM和s-MLLM之间视觉-文本输出分布的差异，并引入关系蒸馏（RDist）来转移l-MLLM建模视觉特征之间相关性的能力。此外，我们提出了一个三阶段训练方案，以充分发挥s-MLLM的潜力：1）蒸馏预训练对齐视觉-文本表示；2）监督微调使模型具备多模态理解能力；3）蒸馏微调进一步转移l-MLLM的能力。我们的方法显著提高了性能，而无需改变小模型的架构。广泛的实验和消融研究验证了每个提出的组件的有效性。代码将在https://github.com/caiyuxuan1120/LLAva-KD获取。|
|**2024-10-21**|**ToW: Thoughts of Words Improve Reasoning in Large Language Models**|Zhikun Xu et.al.|[2410.16235](http://arxiv.org/abs/2410.16235)|**[link](https://github.com/ARC-ASU/fine-nwp)**|我们介绍了Thoughts of Words（ToW），这是一种新颖的训练时数据增强方法，用于下个词预测。ToW将下个词预测视为一个核心推理任务，并在预训练文本中注入精细的思考，解释下个词应该是什么以及它与前文上下文的关系。我们的方法解决了现有下个词预测学习方案的两个基本缺点：它们会引起事实性幻觉，并且对于模型来说难以有效学习原始文本中的隐含推理过程。虽然获取这些单词的思想有很多方法，但我们探索了通过蒸馏从更大模型中获取ToW注释的第一步。经过仅使用70K个ToW注释的持续预训练后，我们在平均情况下提高了模型推理性能7%到9%，并将模型幻觉减少了高达10%。同时，ToW完全独立于任务和应用，不会对标签或语义引入额外的偏见。|
|**2024-10-21**|**Building A Coding Assistant via the Retrieval-Augmented Language Model**|Xinze Li et.al.|[2410.16229](http://arxiv.org/abs/2410.16229)|**[link](https://github.com/NEUIR/CONAN)**|预训练语言模型在代码相关任务中表现出强大的有效性，如代码检索、代码生成、代码总结和代码补全等任务。本文提出了一种名为CONAN（通过检索增强语言模型实现的代码助手）的方法，旨在通过模仿人类在编程过程中寻求知识的行为来构建代码助手。具体来说，CONAN由一个代码结构感知检索器（CONAN-R）和一个基于双重视图代码表示的检索增强生成模型（CONAN-G）组成。CONAN-R通过使用Code-Documentation对齐和掩码实体预测任务来预训练CodeT5，从而使语言模型具备代码结构感知能力，并学习有效的代码片段和文档表示。然后，CONAN-G设计了一种双重视图代码表示机制来实现检索增强的代码生成模型。CONAN-G将代码文档描述视为提示，帮助语言模型更好地理解代码语义。我们的实验表明，CONAN在不同的代码生成任务上取得了令人信服的性能，并显著优于先前的检索增强代码生成模型。进一步分析显示，CONAN通过对代码文档数据对进行对齐以及通过掩码和预测代码中的实体来捕获结构语义，从而为代码片段和文档学习定制化表示。此外，检索到的代码片段和文档提供了来自程序语言和自然语言的必要信息，以协助代码生成过程。CONAN还可以作为大型语言模型（LLMs）的助手，在较短的代码文档长度下提供外部知识，以提高其在各种代码任务上的有效性。这显示了CONAN提取必要信息并帮助过滤检索到的代码文档中的噪声的能力。|
|**2024-10-18**|**Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts**|German Gritsai et.al.|[2410.14677](http://arxiv.org/abs/2410.14677)|**[link](https://github.com/Advacheck-OU/ai-dataset-analysing)**|快速发展的自回归大型语言模型（LLMs）显著提升了生成文本的质量，这促使了可靠机器生成文本检测器的出现。大量检测器和包含人工智能片段的数据集应运而生，一些检测方法在这些数据集中达到了高达99.9%的目标指标识别质量。然而，在实际应用中，这些检测器的质量往往会大幅下降，这引发了疑问：这些检测器是否真正具有高度的可靠性，还是其高基准分数仅仅是由于评估数据集质量较差所致？在这篇论文中，我们强调了需要建立稳健且高质量的方法来评估生成的数据，以防止未来模型中的偏差和低泛化能力。我们对专门用于AI生成内容检测的竞赛中的数据集进行了系统回顾，并提出了评估包含AI生成片段的数据集质量的方法。此外，我们还讨论了使用高质量生成数据以实现两个目标的可能性：提高检测模型的训练效果和改善训练数据集本身。我们的贡献旨在促进对人与机器文本之间动态关系的更好理解，从而最终支持在一个日益自动化的世界中信息的完整性。|
|**2024-10-18**|**SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment**|Qin Liu et.al.|[2410.14676](http://arxiv.org/abs/2410.14676)|null|现有的偏好对齐机制是一种一刀切的对齐方式，其中大型语言模型（LLM）参数化知识中的非偏好特征被统一屏蔽，适用于所有用户。然而，这部分知识对于那些具有专业知识并能够处理这些信息的高级用户来说可能是有用的。这种一刀切的对齐机制削弱了这些合格用户的LLM效用。为了解决这个问题，我们提出了SudoLM框架，该框架通过授权对齐让LLM学习针对不同用户凭证的具体参数化知识的访问控制。SudoLM允许授权用户通过分配的SUDO密钥解锁对所有参数化知识的访问，而非授权用户则无法访问这些知识。在两个应用场景的实验表明，SudoLM能够有效控制用户对参数化知识的访问，并保持其总体效用。|
|**2024-10-18**|**Enhancing Large Language Models' Situated Faithfulness to External Contexts**|Yukun Huang et.al.|[2410.14675](http://arxiv.org/abs/2410.14675)|**[link](https://github.com/kkkevinkkkkk/situated_faithfulness)**|**大型语言模型（LLMs）通常会使用外部信息作为上下文，但这些外部信息有时可能是不准确的，甚至可能是故意误导的。我们认为，稳健的LLMs应该展示出情境真实性，根据它们对内部知识和外部上下文的信心动态调整对外部信息的信任度。为了评估这种能力，我们对LLMs进行了多项QA数据集的测试，包括一个新创建的数据集RedditQA，该数据集包含了来自Reddit帖子中的实际错误上下文。结果显示，当提供正确和不正确的上下文时，无论是开源模型还是专有模型，都倾向于过度依赖外部信息，而不管其事实准确性如何。为了增强情境真实性，我们提出了两种方法：自引导置信度推理（SCR）和基于规则的置信度推理（RCR）。SCR使模型能够根据自身内部知识相对地评估外部信息的置信度，从而生成最准确的答案。相比之下，RCR从LLM中提取显式的置信度信号，并利用预定义的规则来确定最终答案。我们的结果表明，对于具有强大推理能力的模型，如GPT-4o和GPT-4o mini，SCR优于RCR，在直接输入增强基线上的提升幅度最高可达24.2%。相反，对于较小的模型，如Llama-3-8B，RCR则优于SCR。通过我们的置信度推理直接偏好优化（CR-DPO）方法对SCR进行微调，可以提高在已见和未见过的数据集上的性能，平均提升幅度为8.9%。除了定量结果外，我们还提供了关于SCR和RCR相对优势的见解。我们的研究结果强调了提高LLMs情境真实性的有前景途径。相关数据和代码已经发布。**|
|**2024-10-18**|**MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps**|Xiongtao Zhou et.al.|[2410.14668](http://arxiv.org/abs/2410.14668)|**[link](https://github.com/alenai97/miceval)**|**Multimodal Chain of Thought（MCoT）是一种流行的提示策略，用于提高多模态大型语言模型（MLLMs）在各种复杂推理任务中的性能。尽管这种方法很受欢迎，但在评估多模态链式思维推理步骤的质量方面仍缺乏自动化方法。为了解决这一问题，我们提出了Multimodal Chain-of-Thought Evaluation（MiCEval），这是一个框架，旨在通过评估描述和每个推理步骤的质量来评估推理链的正确性。描述部分的评估侧重于图像描述的准确性，而推理步骤则根据前续步骤条件生成时的质量进行评估。MiCEval基于一个细粒度的数据集，该数据集根据正确性、相关性和信息量对每个步骤进行标注。对四种最先进的MLLMs进行的广泛实验表明，使用MiCEval进行逐步评估与人类判断更加吻合，相比现有基于余弦相似度或微调的方法更为准确。MiCEval数据集和代码可以在https://github.com/alenai97/MiCEval找到。**|
|**2024-10-18**|**A Large Language Model-Driven Reward Design Framework via Dynamic Feedback for Reinforcement Learning**|Shengjie Sun et.al.|[2410.14660](http://arxiv.org/abs/2410.14660)|null|大型语言模型（LLMs）在设计强化学习（RL）任务的奖励函数方面显示出显著的潜力。然而，获取高质量的奖励代码通常需要人工干预、大量的LLM查询或重复的RL训练。为了解决这些问题，我们提出了CARD，这是一种由LLM驱动的奖励设计框架，它迭代地生成和改进奖励函数代码。具体来说，CARD包括一个编码器，用于生成和验证代码，同时还有一个评估器提供动态反馈来指导编码器改进代码，从而消除了对人工反馈的需求。除了过程反馈和轨迹反馈外，我们还引入了轨迹偏好评估（TPE），该评估基于轨迹偏好来评估当前的奖励函数。如果代码未能通过TPE，评估器将提供偏好反馈，避免了在每次迭代时进行RL训练，并使奖励函数更好地与任务目标对齐。在Meta-World和ManiSkill2上的实证结果表明，我们的方法在任务性能和令牌效率之间实现了有效的平衡，在所有任务上都优于或匹配基线。在12个任务中的10个任务上，CARD的表现优于或可与使用专家设计奖励训练的策略相媲美，甚至在3个任务上超越了最优解。|
|**2024-10-18**|**EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search**|Oliver Sieberling et.al.|[2410.14649](http://arxiv.org/abs/2410.14649)|**[link](https://github.com/ist-daslab/evopress)**|高计算成本是大型语言模型（LLMs）面临的一个主要问题，因此对模型压缩的研究层出不穷，这些方法包括量化、稀疏化或结构化剪枝等。一个新的研究前沿是由所谓的“动态、非均匀”压缩方法构成的，这些方法通过调整每块或甚至每层的压缩级别（例如稀疏性），以最小化精度损失，同时确保全局压缩阈值。然而，当前的方法依赖于启发式方法来识别给定层对误差的重要性，这基于诸如“误差单调性”的假设，即端到端模型压缩误差与各层误差之和成比例。在本文中，我们重新审视了这一领域，并提出了一种新的通用方法，该方法在给定输入范围内被证明是最佳的。我们的动机观察到，通常情况下，“误差单调性”对于LLMs并不成立：具有较低各层误差总和的压缩模型可能表现得比误差总和较高的模型更差。为了解决这个问题，我们提出了一种新的通用进化框架，称为EvoPress，它具有理论上的收敛性和低样本及评估复杂度。我们展示了这些理论保证导致了在动态压缩Llama、Mistral和Phi模型方面高度竞争的实际性能。通过EvoPress，我们在所有压缩方法上都达到了最新的成果：结构剪枝（块/层删除）、非结构化稀疏性以及具有动态位宽的量化。我们的代码可在https://github.com/IST-DASLab/EvoPress获取。|
|**2024-10-18**|**Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs**|Runchu Tian et.al.|[2410.14641](http://arxiv.org/abs/2410.14641)|**[link](https://github.com/Rachum-thu/LongPiBench)**|**位置偏差在大型语言模型（LLMs）中限制了它们处理长输入的能力。一个显著的例子是“迷失在中间”现象，即LLMs难以利用位于输入中间的相关信息。尽管先前的研究主要集中在单个相关信息上，但现实世界的应用通常涉及多个相关的信息片段。为了弥补这一差距，我们提出了LongPiBench，这是一个旨在评估涉及多个相关片段的位置偏差的基准测试。通过五种商业模型和六种开源模型进行的详细实验表明，虽然大多数当前模型对“迷失在中间”的问题具有鲁棒性，但存在与相关信息片段间距显著相关的偏差。这些发现强调了评估和减少位置偏差的重要性，以提升LLMs的能力。**|
|**2024-10-18**|**GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings**|Raghuveer Thirukovalluru et.al.|[2410.14635](http://arxiv.org/abs/2410.14635)|**[link](https://github.com/raghavlite/GenEOL)**|训练-free嵌入方法直接利用预训练的大规模语言模型（LLMs）来嵌入文本，避免了对比学习的昂贵和复杂的流程。先前的训练-free嵌入方法主要集中在优化嵌入提示上，而忽略了利用LLMs的生成能力的好处。我们提出了一种新颖的方法GenEOL，该方法使用LLMs生成保留句子含义的不同变换，并聚合这些变换的嵌入结果以增强整体句子嵌入。GenEOL在多个LLMs的句子语义文本相似性（STS）基准测试中平均比现有的训练-free嵌入方法高出2.85分。我们的分析表明，GenEOL在LLM层面上稳定了表征质量，并且对嵌入提示的扰动具有鲁棒性。GenEOL还在MTEB基准测试中的多个聚类、重排序和配对分类任务中取得了显著的提升。|
|**2024-10-18**|**DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual Distillation in Conversational Search**|Simon Lupart et.al.|[2410.14609](http://arxiv.org/abs/2410.14609)|null|会话搜索（CS）任务是在语境内从文档集中检索相关文档，结合检索与会话上下文建模。随着大规模语言模型（LLMs）的兴起，CS领域通过LLMs重写用户查询并考虑会话上下文得到了显著改进。然而，在推理时使用这些模型会影响效率。当前方法通过从人类重写的查询中蒸馏嵌入来学习上下文建模任务以解决此问题。然而，这些方法主要关注于上下文建模，并且仅在独立于蒸馏的损失项中处理检索任务中的对比部分。为了应对这些限制，我们提出了一种新的蒸馏方法，作为对先前目标的放松，统一检索和上下文建模。我们通过蒸馏对话和文档之间的相似性分数来放松现有的训练目标，而不是仅仅依赖表示学习。我们提出的蒸馏目标允许在表示空间中有更多的自由度，并利用文档相关性的对比性质。通过在5个CS数据集上的Learned Sparse Retrieval（LSR）实验，我们的方法在域内和域外检索性能方面均显示出显著改善，超越了最先进水平，在域外数据集上召回率提高了多达6个百分点。此外，通过放松目标，我们提出了多教师蒸馏，使用多个LLM作为教师，从而获得额外收益，并在域内实验中超越这些教师本身。最后，对模型稀疏性的分析表明，我们的蒸馏方法能够更好地控制训练模型的稀疏性。|
|**2024-10-18**|**Teaching Models to Balance Resisting and Accepting Persuasion**|Elias Stengel-Eskin et.al.|[2410.14596](http://arxiv.org/abs/2410.14596)|**[link](https://github.com/esteng/persuasion_balanced_training)**|**大型语言模型（LLMs）容易受到说服的影响，这在模型面对敌对对话者时可能带来风险。我们迈出了防御模型免受说服的第一步，同时认为防御负面说服只是问题的一半：模型还应该能够接受有益的说服以改进其答案。我们发现，仅优化模型一方面会导致在另一方面表现不佳。为了平衡正面和负面说服，我们引入了说服平衡训练（PBT），该方法利用多智能体递归对话树来生成数据，并通过偏好优化训练模型在适当时候接受说服。PBT一致提高了模型对抗错误信息的抵抗力和应对挑战的韧性，同时也使模型在包含正反两面说服的整体数据上表现最佳。至关重要的是，我们发现PBT模型在多智能体辩论中是更好的队友。我们发现，没有PBT的情况下，更强和较弱模型的组合表现出不稳定性能，模型回答的顺序决定了团队获得较强或较弱模型的表现。PBT带来了更好且更稳定的性能结果，并减少了顺序依赖性，较强模型能够持续提升较弱模型的表现。**|
|**2024-10-17**|**Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens**|Lijie Fan et.al.|[2410.13863](http://arxiv.org/abs/2410.13863)|null|在视觉领域，扩大自回归模型的效果并不像在大型语言模型中那样显著。在这项工作中，我们研究了文本到图像生成中的这一扩展问题，重点关注两个关键因素：模型是否使用离散或连续的标记，以及标记是否以随机或固定栅格顺序使用类似于BERT或GPT的变换器架构生成。我们的实证结果表明，虽然所有模型在验证损失方面都能有效扩展，但它们的评估性能——通过FID、GenEval分数和视觉质量来衡量——则呈现出不同的趋势。基于连续标记的模型在视觉质量上显著优于使用离散标记的模型。此外，生成顺序和注意力机制对GenEval分数有显著影响：随机顺序的模型在GenEval分数上明显优于栅格顺序的模型。受这些发现的启发，我们训练了一种名为Fluid的随机顺序自回归模型，该模型基于连续标记。Fluid 10.5B模型在MS-COCO 30K上的零样本FID达到了新的最先进水平，即6.16，并且在GenEval基准测试中的整体得分为0.69。我们希望我们的发现和结果能鼓励未来进一步缩小视觉和语言模型之间的扩展差距。|
|**2024-10-17**|**PUMA: Empowering Unified MLLM with Multi-granular Visual Generation**|Rongyao Fang et.al.|[2410.13861](http://arxiv.org/abs/2410.13861)|**[link](https://github.com/rongyaofang/puma)**|**近年来，多模态基础模型在视觉-语言理解方面取得了显著进展。初步尝试也探索了多模态大语言模型（MLLM）在视觉内容生成中的潜力。然而，现有工作未能充分解决统一MLLM范式下不同图像生成任务对不同粒度需求的问题——从文本到图像生成所需的多样性到图像操作所需的精确可控性。在这项工作中，我们提出了PUMA，即通过多粒度视觉生成赋予统一MLLM以力量。PUMA将多粒度视觉特征统一作为MLLM的输入和输出，优雅地解决了不同粒度要求的各种图像生成任务在统一MLLM框架下的问题。经过多模态预训练和任务特定指令微调后，PUMA在广泛的多模态任务中表现出色。这项工作标志着向真正统一的MLLM迈出了重要一步，这种MLLM能够适应各种视觉任务对粒度的需求。代码和模型将在https://github.com/rongyaofang/PUMA发布。**|
|**2024-10-17**|**$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models**|Yaxin Luo et.al.|[2410.13859](http://arxiv.org/abs/2410.13859)|null|尽管多模态大型语言模型（MLLMs）取得了显著进展，但其高昂的计算成本仍然是实际部署中的一个障碍。受自然语言处理中深度混合（MoD）的启发，我们从“激活标记”的角度来解决这一限制。我们的关键见解是，如果大多数标记对于层计算是冗余的，那么可以通过MoD层直接跳过它们。然而，直接将MLLMs的密集层转换为MoD层会导致显著的性能下降。为了解决这个问题，我们提出了一种创新的MoD适应策略，称为$\gamma$-MoD，用于现有的MLLMs。在$\gamma$-MoD中，我们提出了一个新的指标来指导MLLM中MoD的部署，即注意力图的秩（ARank）。通过ARank，我们可以有效地识别哪些层是冗余的，并应被替换为MoD层。基于ARank，我们进一步提出了两种新颖的设计，以最大限度地提高MLLM的计算稀疏性，同时保持其性能，即共享视觉-语言路由器和掩码路由学习。通过这些设计，MLLM的90%以上的密集层可以有效转换为MoD层。为了验证我们的方法，我们在三个流行的MLLM上进行了应用，并在9个基准数据集上进行了广泛的实验。实验结果不仅验证了$\gamma$-MoD对现有MLLMs的显著效率提升，还证实了其在各种MLLM上的泛化能力。例如，$\gamma$ -MoD仅导致轻微的性能下降，即-1.5%，但可以分别将LLaVA-HR的训练时间和推理时间减少31.0%和53.2%。|
|**2024-10-17**|**How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs**|Guhao Feng et.al.|[2410.13857](http://arxiv.org/abs/2410.13857)|null|尽管基于Transformer的大型语言模型（LLMs）在各个领域取得了显著的成功，但理解和提升它们的数学能力仍然是一个重要的挑战。在这篇论文中，我们对LLMs的数学能力进行了严格的理论分析，特别关注它们的算术表现。我们发现数值精度是影响其在数学任务中表现的关键因素。我们的研究结果表明，使用低数值精度的Transformer在处理算术任务（如迭代加法和整数乘法）时，除非模型大小相对于输入长度呈超多项式增长，否则无法有效解决这些问题。相比之下，使用标准数值精度的Transformer可以高效地处理这些任务，并且所需的模型尺寸要小得多。我们还通过实验进一步验证了这一理论发现，探索了不同数值精度对算术任务的影响，为提高LLMs的数学推理能力提供了宝贵的见解。|
|**2024-10-17**|**Can MLLMs Understand the Deep Implication Behind Chinese Images?**|Chenhao Zhang et.al.|[2410.13854](http://arxiv.org/abs/2410.13854)|**[link](https://github.com/MING-ZCH/CII-Bench)**|**随着多模态大型语言模型（MLLMs）的能力不断提升，对这些模型进行更高阶能力评估的需求也在增加。然而，目前缺乏针对MLLMs的高阶感知和理解中文视觉内容的评估工作。为了填补这一空白，我们介绍了中文图像隐含理解基准（CII-Bench），旨在评估MLLMs对中文图像的高阶感知和理解能力。与现有基准相比，CII-Bench具有多个突出特点。首先，为了确保中文背景的真实性，CII-Bench中的图像来源于中国互联网，并经过人工审查，相应的答案也由人工精心制作。此外，CII-Bench还纳入了代表中国传统文化的图像，如著名的中国传统绘画，这可以深入反映模型对中国传统文化的理解。通过在多个MLLMs上进行广泛的实验，我们得出了重要发现。最初，MLLMs在CII-Bench上的表现与人类存在显著差距。MLLMs的最高准确率为64.4%，而人类的平均准确率为78.2%，峰值达到令人印象深刻的81.0%。随后，MLLMs在处理中国传统文化图像时表现较差，这表明它们在理解高层次语义和缺乏对中国传统文化的深入了解方面存在局限性。最后，观察到大多数模型在图像情感提示被纳入提示时表现出更高的准确性。我们相信，CII-Bench将使MLLMs更好地理解中文语义和特定于中国的图像，从而推动向专家型通用人工智能（AGI）的发展。我们的项目可在https://cii-bench.github.io/公开访问。**|
|**2024-10-17**|**Retrospective Learning from Interactions**|Zizhao Chen et.al.|[2410.13852](http://arxiv.org/abs/2410.13852)|null|多回合交互中的大型语言模型（LLMs）和用户之间的互动自然包含了隐含的反馈信号。如果LLMs以出乎意料的方式回应用户的指令，用户很可能会通过重新表述请求、表达挫败感或转向替代任务来传达这一信号。这些信号与具体任务无关，并且占据相对受限的语言子空间，即使LLMs在实际任务上失败了，也能识别这些信号。这为LLMs通过互动持续学习提供了途径，而无需额外标注。我们引入了一种名为ReSpect的方法，通过回顾过去的交互来学习这些信号。我们在一个新的多模态交互场景中部署了ReSpect，在该场景中，人类指导LLMs解决具有组合解空间的抽象推理任务。通过与人类进行数千次交互，我们展示了ReSpect如何逐步提高任务完成率，从31%提升到82%，且无需任何外部标注。|
|**2024-10-17**|**SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction**|Xuan Zhang et.al.|[2410.13846](http://arxiv.org/abs/2410.13846)|**[link](https://github.com/sail-sg/simlayerkv)**|**近年来，大型语言模型（LLMs）的进展已经扩展了它们处理长上下文的能力。然而，增加模型层数和输入序列长度显著增加了存储键值（KV）缓存所需的内存，这对高效的推理构成了挑战。为了缓解这一问题，我们提出了SimLayerKV，这是一种简单而有效的方法，通过在识别为懒层的层中选择性地丢弃缓存来减少层间KV缓存的冗余。我们的方法基于这样的观察：在长上下文LLMs中，某些层表现出“懒惰”行为，与非懒层相比，对建模长距离依赖贡献较小。通过分析注意力权重模式，我们发现这些懒层在给定输入生成过程中对不同token的行为是一致的。这一见解启发了我们的SimLayerKV，该方法通过识别懒层并相应地减少其KV缓存。SimLayerKV无需训练，具有通用性，并且可以用仅七行代码实现。我们在三个代表性LLMs上进行了广泛的实验，例如LLaMA2-7B、LLaMA3-8B和Mistral-7B，在LongBench基准测试的16个任务上进行测试。结果显示，SimLayerKV实现了5倍的KV缓存压缩比，并且在结合4位量化时性能仅下降1.2%。我们的代码可在https://github.com/sail-sg/SimLayerKV获取。**|
|**2024-10-17**|**Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs**|Tianyu Guo et.al.|[2410.13835](http://arxiv.org/abs/2410.13835)|**[link](https://github.com/guotianyu2000/active-dormant-attention)**|实践者在变压器型大规模语言模型（LLMs）中观察到了三个令人困惑的现象：注意力汇点、值状态耗尽和残差状态峰值，这些现象统称为极端令牌现象。这些现象的特点是某些所谓的“汇点令牌”接收不成比例高的注意力权重，表现出明显较小的值状态，并且具有比其他令牌大得多的残差状态范数。这些极端令牌在LLM推理、量化和可解释性方面引发了许多挑战。我们阐明了极端令牌现象背后的机制。首先，我们在非常简单的架构——一到三层的变压器，在玩具模型Bigram-Backcopy（BB）任务上训练时展示了这些现象。在这种情况下，我们识别出一个活跃-休眠机制，其中注意力头对于特定输入域成为汇点，而对于其他输入则不是。我们对训练动态的理论分析揭示，这些现象是由一种相互增强机制驱动的。基于这些见解，我们提出了在预训练期间缓解极端令牌现象的策略，包括用ReLU替换softmax以及用SGD替换Adam。接下来，我们将分析扩展到预训练的LLM，包括Llama和OLMo，显示许多注意力头表现出与BB任务中类似的活跃-休眠机制，并且相互增强机制也支配着LLM预训练期间极端令牌现象的出现。我们的结果显示，许多由BB任务预测的静态和动态性质与预训练LLM中的观察结果一致。|
|**2024-10-17**|**AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents**|Ke Yang et.al.|[2410.13825](http://arxiv.org/abs/2410.13825)|null|通过使用大型语言模型（LLMs）的代理来实现自治，可以提升人类在个性化和标准化任务中的效率。自动化网络任务（如在预算内预订酒店）的需求日益增加。满足实际需求的同时，网络代理也作为一个重要的概念验证示例，展示了各种代理接地场景的重要性。其成功预示着许多未来应用的进步。先前的研究通常会手工设计网络代理策略（例如提示模板、多代理系统、搜索方法等），这些策略可能无法在所有现实世界场景中很好地推广。另一方面，关于网络代理的观察/动作表示与LLM预训练数据之间不匹配的研究非常有限。这种差异特别明显，因为LLM主要针对语言完成进行训练，而不是处理涉及具身导航动作和符号网络元素的任务。我们的研究通过简单地优化LLM网络代理的观察和动作空间，使其更好地与LLM的能力相匹配，从而提升了性能。这种方法使我们的基础代理在各种网络任务上显著优于以前的方法。具体来说，在WebArena基准测试中，该基准测试涵盖了通用网络交互任务，我们的代理AgentOccam比之前最先进的方法高出9.8分（+29.4%），比同时期的工作高出5.9分（+15.8%）。相比类似的基本网络代理，其观察和动作空间对齐后成功率为26.6分（+161%）。我们没有使用上下文示例、新的代理角色、在线反馈或搜索策略。AgentOccam的设计简单，突显了LLM在无样本情况下执行网络任务的强大性能，并强调了精心调整观察和动作空间对于基于LLM的代理至关重要。|
|**2024-10-18**|**Harnessing Webpage UIs for Text-Rich Visual Understanding**|Junpeng Liu et.al.|[2410.13824](http://arxiv.org/abs/2410.13824)|null|文本丰富的视觉理解——即处理密集文本内容与视觉元素相融合的环境的能力，对于多模态大型语言模型（MLLMs）在结构化环境中进行有效交互至关重要。为了增强这一能力，我们提出使用基于文本的大型语言模型（LLMs）从网页用户界面合成通用的多模态指令。尽管缺乏直接的视觉输入，基于文本的LLMs能够处理来自网页可访问性树的结构化文本表示。这些指令随后与UI截图配对以训练多模态模型。我们引入了MultiUI数据集，该数据集包含来自100万个网站的730万样本，涵盖了多种多模态任务和UI布局。在MultiUI上训练的模型不仅在网页UI任务中表现出色，在VisualWebBench上的提升高达48%，在Mind2Web的网页代理数据集中元素准确率提高了19.1%，而且在非网页UI任务以及甚至非UI领域（如文档理解、OCR和图表解释）中也表现出惊人的泛化能力。这些结果突显了网页UI数据在推动各种场景下文本丰富视觉理解的广泛应用性。|
|**2024-10-16**|**Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception**|Jihao Zhao et.al.|[2410.12788](http://arxiv.org/abs/2410.12788)|**[link](https://github.com/IAAR-Shanghai/Meta-Chunking)**|Retrieval-Augmented Generation（RAG）在作为大型语言模型（LLMs）的可行补充时，常常忽略了其管道中一个关键方面——文本分块，这影响了知识密集型任务的质量。本文介绍了一种称为元分块（Meta-Chunking）的概念，这是一种介于句子和段落之间的粒度，由段落内具有深层次语言逻辑联系的一组句子组成。为了实现元分块，我们基于LLMs设计了两种策略：边界采样分块和困惑度分块。前者利用LLMs对连续句子是否需要分割进行二分类决策，基于从边界采样获得的概率差做出决策。后者通过分析困惑度分布的特点来精确识别文本分块边界。此外，考虑到不同文本的固有复杂性，我们提出了一种结合元分块与动态合并的策略，以实现在细粒度和粗粒度文本分块之间取得平衡。实验在十一个数据集上进行，结果表明元分块可以更有效地提高基于RAG的单跳和多跳问答性能。例如，在2WikiMultihopQA数据集上，它比相似性分块提高了1.32的性能，同时仅消耗了45.8%的时间。我们的代码可在https://github.com/IAAR-Shanghai/Meta-Chunking 获取。|
|**2024-10-16**|**In-Context Learning Enables Robot Action Prediction in LLMs**|Yida Yin et.al.|[2410.12782](http://arxiv.org/abs/2410.12782)|null|最近，大型语言模型（LLMs）在语言领域通过上下文学习（ICL）取得了显著的成功。然而，利用LLMs的ICL能力直接预测机器人动作的研究还相对较少。在这篇论文中，我们介绍了一种名为RoboPrompt的框架，该框架使现成的纯文本LLMs能够在无需训练的情况下通过ICL直接预测机器人动作。我们的方法首先通过启发式方法识别出一个片段中的关键帧，这些关键帧捕捉了重要的时刻。接下来，我们从这些关键帧中提取末端执行器的动作以及估计的初始物体姿态，并将两者转换为文本描述。最后，我们构建了一个结构化的模板，从这些文本描述和任务指令中形成ICL演示。这使得LLM能够在测试时直接预测机器人动作。通过广泛的实验和分析，RoboPrompt在模拟和真实环境中均表现出比零样本和ICL基线更强的性能。|
|**2024-10-16**|**Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information**|Yingya Li et.al.|[2410.12774](http://arxiv.org/abs/2410.12774)|null|多任务学习的成功在很大程度上取决于任务的分组方式。简单地将所有任务或随机选择的任务组合在一起可能导致负迁移，从而使多任务模型的表现不如单任务模型。尽管已经做出了许多努力来识别任务分组并衡量不同任务之间的相关性，但定义一个指标以从众多潜在任务组合中确定最佳任务分组仍然是一个具有挑战性的研究课题。我们提出了一种基于点式V-可用信息（PVI）测量任务难度的任务相关性度量方法。PVI是一种新近提出的度量标准，用于估计给定模型时数据集包含多少可用信息。我们假设具有统计上不可区分的PVI估计值的任务足够相似，可以从联合学习过程中受益。我们在一般、生物医学和临床领域的15个NLP数据集上进行了全面实验，以评估该度量方法用于任务分组的可行性。我们将联合学习器的结果与单任务学习器、现有基线方法以及最近的大规模语言模型（包括Llama 2和GPT-4）进行了比较。结果显示，通过将具有相似PVI估计值的任务分组，联合学习器在较少总参数的情况下获得了具有竞争力的结果，并且在不同领域内表现一致。|
|**2024-10-16**|**StyleDistance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples**|Ajay Patel et.al.|[2410.12757](http://arxiv.org/abs/2410.12757)|null|风格表示旨在将具有相似写作风格的文本嵌入到接近的位置，并将具有不同风格的文本嵌入到远离的位置，而不考虑内容。然而，用于训练这些表示的对比三元组往往在风格和内容上都有所变化，导致表示中可能存在内容泄漏的问题。我们引入了一种名为StyleDistance的新方法来训练更强的独立于内容的风格嵌入。我们使用大型语言模型创建了一个合成数据集，其中包含受控风格变化的近似释义，并为精确的对比学习生成了跨越40个不同风格特征的正例和负例。我们通过人工和自动评估来评估合成数据和嵌入的质量。StyleDistance增强了风格嵌入的内容独立性，这种嵌入可以推广到现实世界的基准测试，并在下游应用中优于领先的风格表示。我们的模型可以在https://huggingface.co/StyleDistance/styledistance找到。|
|**2024-10-17**|**CREAM: Consistency Regularized Self-Rewarding Language Models**|Zhaoyang Wang et.al.|[2410.12735](http://arxiv.org/abs/2410.12735)|**[link](https://github.com/raibows/cream)**|近期的自我奖励大型语言模型（LLM）成功地应用了LLM作为裁判的方法，以迭代方式提升对齐性能，而无需人工标注的偏好数据。这些方法通常使用同一LLM作为策略模型（生成响应）和奖励模型（评分和排序这些响应）。然后，根据排名的响应作为偏好对来通过直接对齐技术（例如DPO）训练LLM。然而，值得注意的是，在这个过程中，奖励和排序的准确性没有保证，这对于确保准确的奖励和高质量的偏好数据至关重要。来自相对较小的LLM（例如7B参数）的经验结果也表明，在某些情况下，经过几次迭代后，自我奖励的改进可能会减弱，我们假设这是由于奖励系统中的累积偏差所致。这种偏差可能导致用于训练LLM的不可靠偏好数据。为了解决这个问题，我们首先制定了并分析了自我奖励语言模型的广义迭代偏好微调框架。然后，我们在这一广义框架中引入正则化，以减轻自我奖励过程中的过度自信偏好标记。基于这一理论洞察，我们提出了一种一致性正则化的自我奖励语言模型（CREAM），该模型利用不同迭代中的奖励一致性来正则化自我奖励训练，帮助模型从更可靠的偏好数据中学习。通过这种明确的正则化，我们的实证结果证明了CREAM在提高奖励一致性和对齐性能方面的优越性。代码可在https://github.com/Raibows/CREAM公开获取。|
|**2024-10-16**|**FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression**|Zhenheng Tang et.al.|[2410.12707](http://arxiv.org/abs/2410.12707)|null|为了缓解在训练大型深度神经网络（DNNs），特别是大型语言模型（LLMs）时的硬件短缺问题，我们提出了FusionLLM，这是一种去中心化的训练系统，旨在利用地理分布的GPU跨不同的计算集群或单个设备进行DNN训练。去中心化训练在系统设计和效率方面面临重大挑战，包括：1）需要远程自动微分（RAD），2）支持灵活的模型定义和异构软件，3）异构硬件导致资源利用率低或存在慢速节点问题，以及4）网络通信缓慢。为了解决这些挑战，在系统设计中，我们将模型表示为操作符（OP-DAG）的有向无环图。DAG中的每个节点代表DNN中的操作符，边则表示操作符之间的数据依赖关系。基于这种设计，1）用户可以自定义任何DNN而不必关心底层操作符实现；2）我们通过更细粒度的子任务进行任务调度，提供更多的优化空间；3）DAG运行时执行器可以在不依赖一致的低级机器学习框架版本的情况下实现RAD。  为了提高系统效率，我们实现了一个工作负载估计器，并设计了一种OP-Fence调度器，将具有相似带宽的设备分组在一起，并对DAG进行分区以增加吞吐量。此外，我们提出了一种AdaTopK压缩器，以自适应地压缩在最慢通信链路上的中间激活和梯度。为了评估我们的系统和算法的收敛性和效率，我们在三个现实测试平台上使用连接速度在8 Mbps到10 Gbps的48个GPU上训练了ResNet-101和GPT-2。实验结果表明，与基线方法相比，我们的系统和方法可以在确保收敛的同时实现1.45至9.39倍的速度提升。|
|**2024-10-16**|**Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via Lightweight Value Optimization**|Xingqi Wang et.al.|[2410.12700](http://arxiv.org/abs/2410.12700)|**[link](https://github.com/achernarwang/LiVO)**|**近年来，基于大规模数据训练的扩散模型已经能够生成与人类水平图像难以区分的图像，但它们常常产生有害内容，这些内容与人类价值观不符，例如社会偏见和冒犯性内容。尽管大型语言模型（LLM）领域进行了大量研究，但文本到图像（T2I）模型的对齐问题仍未得到充分探索。为了解决这一问题，我们提出了LiVO（轻量级价值优化），这是一种新颖的轻量级方法，用于将T2I模型与人类价值观对齐。LiVO仅优化一个即插即用的价值编码器，以将指定的价值原则整合到输入提示中，从而在控制生成图像的语义和价值观方面发挥作用。具体来说，我们设计了一种针对扩散模型的偏好优化损失函数，该函数在理论上逼近LLM对齐中使用的Bradley-Terry模型，但提供了图像质量和价值一致性之间的更灵活的权衡。为了优化价值编码器，我们还开发了一个框架来自动构建一个包含86k个样本（提示、对齐图像、违反图像、价值原则）的文本-图像偏好数据集。通过不更新大多数模型参数并通过从输入提示中进行自适应价值选择，LiVO显著减少了有害输出，并实现了更快的收敛，超越了几种强大的基线模型，迈出了向伦理对齐的T2I模型迈出的第一步。**|
|**2024-10-16**|**Automatic Mapping of Anatomical Landmarks from Free-Text Using Large Language Models: Insights from Llama-2**|Mohamad Abdi et.al.|[2410.12686](http://arxiv.org/abs/2410.12686)|null|解剖学标志在医学影像中对于导航和异常检测至关重要。现代大型语言模型（LLMs），如Llama-2，为将这些标志从自由文本的放射学报告映射到图像数据中的相应位置提供了希望。最近的研究表明，LLMs可能能够形成连贯的生成过程表示。受此启发，我们研究了LLMs是否准确地表示解剖学标志的空间位置。通过使用Llama-2模型进行实验，我们发现它们可以线性地表示空间中的解剖学标志，并且对不同提示具有相当强的鲁棒性。这些结果强调了LLMs增强医学影像工作流程效率和准确性的潜力。|
|**2024-10-16**|**Evaluating Morphological Compositional Generalization in Large Language Models**|Mete Ismayilzada et.al.|[2410.12656](http://arxiv.org/abs/2410.12656)|null|大型语言模型（LLMs）在各种自然语言生成和理解任务中已经取得了显著的进展。然而，它们的语言泛化能力仍然值得质疑，这引发了关于这些模型是否像人类一样学习语言的疑问。尽管人类在语言使用中表现出组合能力和语言创造性，但LLMs在这方面的表现，特别是在形态学方面的能力，仍需进一步探索。在这项工作中，我们通过组合性的视角系统地研究了LLMs在形态学泛化方面的能力。我们将词素定义为组合的基本单位，并设计了一套新的生成性和判别性任务来评估形态学的生产力和系统性。重点关注像土耳其语和芬兰语这样的黏着语，我们评估了几种最先进的指令微调多语言模型，包括GPT-4和Gemini。我们的分析表明，LLMs在处理形态学组合泛化时特别困难，尤其是在应用于新词根时，随着形态复杂性的增加，性能急剧下降。虽然模型能够比随机猜测更好地识别个别形态组合，但其表现缺乏系统性，导致与人类相比存在显著的准确率差距。|
|**2024-10-16**|**Explainable Moral Values: a neuro-symbolic approach to value classification**|Nicolas Lazzari et.al.|[2410.12631](http://arxiv.org/abs/2410.12631)|null|本文研究了基于本体的推理与机器学习技术在可解释价值分类中的整合。通过依赖道德基础理论中的道德价值观形式化以及DnS本体设计模式，使用sandra神经符号推理器来推断满足特定句子描述的价值。句子及其结构化表示是使用开源的大语言模型自动生成的。所推断的描述被用来自动检测句子所关联的价值。我们展示了仅依靠推理器的结果即可实现与更复杂方法相当的可解释分类。我们还展示了将推理器的推断结果与分布语义方法相结合可以大幅超越所有基线，包括基于神经网络架构的复杂模型。最后，我们构建了一个可视化工具来探索基于理论的值分类的潜力，该工具可在http://xmv.geomeaning.com/公开访问。|
|**2024-10-15**|**GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable Recommendation**|Fei Tang et.al.|[2410.11841](http://arxiv.org/abs/2410.11841)|**[link](https://github.com/sugarandgugu/GaVaMoE)**|基于大规模语言模型的可解释推荐（LLM-based ER）系统在生成类似人类的推荐解释方面显示出潜力。然而，它们面临着建模用户与项目之间的协同偏好、个性化解释以及处理稀疏用户-项目交互的挑战。为了解决这些问题，我们提出了一种名为GaVaMoE的新框架，即高斯变分门控专家混合模型，用于可解释推荐。GaVaMoE引入了两个关键组件：(1) 一个评分重构模块，采用带有高斯混合模型（GMM）的变分自编码器（VAE），以捕捉复杂的用户-项目协同偏好，作为预训练的多门机制；(2) 一组细粒度的专家模型，与多门机制耦合，用于生成高度个性化的解释。VAE组件对用户-项目交互中的潜在因素进行建模，而GMM则聚类具有相似行为的用户。每个聚类对应多门机制中的一个门，将用户-项目对路由到适当的专家模型。这种架构使GaVaMoE能够为特定类型的用户和偏好生成定制化解释，通过利用用户之间的相似性来缓解数据稀疏问题。在三个真实世界数据集上的广泛实验表明，GaVaMoE在解释质量、个性化和一致性方面显著优于现有方法。特别是，在稀疏用户-项目交互场景中，GaVaMoE表现出稳健的性能，即使对于历史数据有限的用户也能保持高质量的解释。|
|**2024-10-15**|**MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding**|Yue Cao et.al.|[2410.11829](http://arxiv.org/abs/2410.11829)|**[link](https://github.com/yuecao0119/MMFuser)**|**尽管在跨模态交互中理解复杂的人类意图方面，多模态大语言模型（MLLMs）取得了显著进展，但捕捉复杂的图像细节仍然具有挑战性。先前的方法通过整合多个视觉编码器来增强视觉细节，但这种方法引入了冗余和计算开销。我们观察到，大多数MLLMs仅使用视觉编码器的最后一层特征图来进行视觉表示，而忽略了浅层特征图中的丰富细粒度信息。为了解决这个问题，我们提出了\modelname，这是一种简单而有效的多层特征融合器，能够高效地整合来自视觉变换器（ViTs）的深层和浅层特征。具体来说，它利用语义对齐的深层特征作为查询，动态提取浅层特征中缺失的细节，从而在保持语义对齐的同时丰富了表示形式的细粒度信息。应用于LLaVA-1.5模型时，\modelname在视觉表示和基准性能上取得了显著提升，提供了一种比多编码器集成方法更灵活、更轻量化的解决方案。代码和模型已发布在https://github.com/yuecao0119/MMFuser。**|
|**2024-10-15**|**SGEdit: Bridging LLM with Text2Image Generative Model for Scene Graph-based Image Editing**|Zhiyuan Zhang et.al.|[2410.11815](http://arxiv.org/abs/2410.11815)|null|场景图以节点和边的形式提供了图像的结构化、分层表示，分别表示对象及其相互关系。它可以用作图像编辑的自然界面，显著提高精度和灵活性。利用这一优势，我们引入了一个新框架，该框架将大型语言模型（LLM）与文本到图像生成模型相结合，用于基于场景图的图像编辑。这种集成使得在对象级别进行精确修改以及对场景进行创造性重构成为可能，而不会损害整体图像的完整性。我们的方法分为两个主要阶段：1）利用LLM驱动的场景解析器，我们构建了图像的场景图，捕捉关键对象及其相互关系，并解析细粒度属性如对象掩码和描述。这些注释促进了概念学习，使用微调扩散模型来代表每个对象，用优化的标记和详细的描述提示表示。2）在图像编辑阶段，LLM编辑控制器指导特定区域的编辑。这些编辑通过注意力调节的扩散编辑器实现，利用微调模型执行对象添加、删除、替换和调整。通过广泛的实验，我们证明了我们的框架在编辑精度和场景美学方面显著优于现有图像编辑方法。|
|**2024-10-15**|**NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models**|Han Han et.al.|[2410.11805](http://arxiv.org/abs/2410.11805)|**[link](https://github.com/hhan1018/nestools)**|大型语言模型（LLMs）结合工具学习在现实应用中已经取得了显著的成果。在工具学习过程中，LLMs可能会按照嵌套顺序调用多个工具，其中后一个工具调用可能将其前一个工具的响应作为输入参数。然而，当前对嵌套工具学习能力的研究仍然不足，因为现有的基准测试缺乏相关数据实例。为了解决这个问题，我们引入了NesTools来填补全面评估嵌套工具学习能力的空白。NesTools包含一种新颖的自动数据生成方法，用于构建具有不同嵌套结构的大规模嵌套工具调用。通过人工审核和优化，该数据集质量高且与现实场景紧密相关。因此，NesTools可以作为一个新的基准来评估LLMs的嵌套工具学习能力。我们对22个LLMs进行了广泛的实验，并使用NesTools进行了深入分析，结果表明当前的LLMs在复杂的嵌套工具学习任务上仍然存在困难。|
|**2024-10-15**|**FoundTS: Comprehensive and Unified Benchmarking of Foundation Models for Time Series Forecasting**|Zhe Li et.al.|[2410.11802](http://arxiv.org/abs/2410.11802)|null|时间序列预测（TSF）在金融、气象服务和能源管理等多个领域都是关键功能。尽管近年来出现了许多TSF方法，但这些方法中的许多需要特定领域的数据收集和模型训练，并且在新领域上的泛化性能较差。基础模型旨在克服这一局限。它们通过大规模语言或时间序列数据预训练，表现出在新或未见过的数据上进行推理的潜力。这促使了新型TSF基础模型的涌现。我们提出了一种新的基准测试，即FoundTS，以实现对这些模型进行彻底而公平的评估和比较。FoundTS涵盖了各种基于大型语言模型和预训练时间序列的基础模型。此外，FoundTS支持不同的预测策略，包括零样本、少量样本和全样本，从而促进更全面的评估。最后，FoundTS提供了一个标准化的评估流程管道，包括数据集分割、加载、归一化和少量样本抽取，从而实现公平的评估。在此基础上，我们对广泛领域内具有不同统计特性的多种数据集上的TSF基础模型进行了广泛的评估。具体而言，我们识别了现有基础模型的优点、缺点及其内在限制，并确定了未来模型设计的方向。我们的代码和数据集可以在https://anonymous.4open.science/r/FoundTS-C2B0获取。|
|**2024-10-15**|**Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability**|Tsz Ting Chung et.al.|[2410.11786](http://arxiv.org/abs/2410.11786)|null|大型语言模型（LLMs）在广泛的自然语言处理任务中展示了令人印象深刻的性能，特别是在利用上下文学习时。然而，上下文学习带来了额外的计算和财务成本。为了缓解这一问题，一些提示压缩方法被提出以压缩上下文学习中的提示。尽管这些方法取得了成功，但它们面临着由于模型特定压缩而导致的迁移性差的问题，或者依赖外部训练数据，例如GPT-4。在这篇论文中，我们研究了LLMs开发统一压缩方法的能力，该方法通过离散化不具信息性的标记，采用自监督预训练技术。通过在持续预训练过程中引入少量参数，所提出的Selection-p为每个输入标记生成一个概率值，指示保留或丢弃该标记。实验表明，Selection-p在多个分类任务中达到了最先进的性能，在实现高达10倍的压缩率的同时，仅经历了微小的0.8%性能下降。此外，它相比先前的工作在不同模型上的迁移性更优。另外，我们进一步分析了Selection-p如何有助于在长上下文中保持上下文学习的性能。|
|**2024-10-15**|**G-Designer: Architecting Multi-agent Communication Topologies via Graph Neural Networks**|Guibin Zhang et.al.|[2410.11782](http://arxiv.org/abs/2410.11782)|null|近期在基于大规模语言模型（LLM）的代理技术方面取得了显著进展，证明集体智能可以显著超越单个代理的能力，这主要得益于精心设计的代理间通信拓扑。尽管有许多多样化且高性能的设计可供选择，但实践者在为特定任务选择最有效的管道时常常感到困惑：哪种拓扑最适合我的任务，同时避免不必要的通信令牌开销并确保高质量的解决方案？针对这一困境，我们介绍了G-Designer，这是一种自适应、高效且稳健的多代理部署解决方案，能够动态设计任务感知的定制化通信拓扑。具体来说，G-Designer将多代理系统建模为一个多代理网络，利用变分图自动编码器对节点（代理）和一个特定任务的虚拟节点进行编码，并解码出一个任务适应性强且性能高的通信拓扑。在六个基准测试中的广泛实验表明，G-Designer具有以下特点：\textbf{(1) 高性能}，在MMLU上的准确率达到84.50%，在HumanEval上的pass@1达到89.90%；\textbf{(2) 任务适应性}，根据任务难度构建定制化的通信协议，将令牌消耗减少了高达95.33%；并且\textbf{(3) 对抗鲁棒}，能够抵御代理对抗攻击，仅导致0.3%的准确率下降。|
|**2024-10-15**|**Language Models Encode Numbers Using Digit Representations in Base 10**|Amit Arnold Levy et.al.|[2410.11781](http://arxiv.org/abs/2410.11781)|**[link](https://github.com/amitlevy/base10)**|大型语言模型（LLMs）在处理即使是简单的数值问题时，如比较两个小数字，也经常出错。一个自然的假设是这些错误源于模型如何表示数字，特别是它们是否捕捉到了数字的实际数值。我们通过观察发现，LLM在数值任务上的错误通常分布在答案的“位数”上，而不是围绕其“数值”正常分布。通过一系列探针实验和因果干预，我们展示了LLM内部以十进制的每一位数字进行圆环式表示，而不是数值表示。这种基于位的表示方式，而非数值表示，揭示了模型在涉及数值推理的任务中的错误模式，并可作为未来研究分析LLM中数值机制的基础。|
|**2024-10-15**|**MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation**|Chenxi Wang et.al.|[2410.11779](http://arxiv.org/abs/2410.11779)|**[link](https://github.com/zjunlp/Deco)**|**多模态大语言模型（MLLMs）经常表现出幻觉现象，但其背后的原因尚未得到充分理解。在本文中，我们进行了实证分析并发现，尽管MLLMs在最终输出中错误地生成了对象，但在前一层它们实际上能够识别视觉对象。我们推测这可能是由于语言模型的强大知识先验抑制了视觉信息，从而导致幻觉。受此启发，我们提出了一种新颖的动态校正解码方法（DeCo），该方法自适应地选择合适的前一层，并按比例将知识整合到最终层以调整输出logits。值得注意的是，DeCo是与模型无关的，可以无缝地与各种经典解码策略结合，并应用于不同的MLLMs。我们在广泛使用的基准上评估了DeCo，结果表明它相比基线大幅降低了幻觉率，突显了其减轻幻觉的潜力。代码可在https://github.com/zjunlp/DeCo获取。**|
|**2024-10-15**|**Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models**|Kai Yao et.al.|[2410.11772](http://arxiv.org/abs/2410.11772)|**[link](https://github.com/kaiseem/ist)**|**参数高效微调（PEFT）方法因其在适应预训练大型语言模型（LLMs）到下游任务时显著减少内存和计算开销的潜力而广受欢迎。然而，大多数PEFT方法的一个常见限制是它们在整个层中应用统一的架构设计，这涉及相同的可训练模块，并忽略了每层的重要性差异，从而导致微调结果不佳。为了克服上述局限并获得更好的性能，我们开发了一种新颖的方法，称为重要性感知稀疏调优（IST），以充分利用固有的稀疏性，并通过有效的逐层重要性评分选择最重要的全层子集。所提出的IST是一种通用且即插即用的技术，与各种基于层的PEFT方法兼容。通过利用估计的重要性得分，IST在PEFT模块中动态更新这些选定的层，从而降低内存需求。我们进一步提供了收敛性的理论证明和优于均匀更新策略的实证证据，以证明IST相对于现有方法的优势。广泛的实验涵盖了各种LLMs、PEFT方法和下游任务，证实了我们提出方法的有效性，展示了IST增强现有基于层的PEFT方法的能力。我们的代码可在https://github.com/Kaiseem/IST获取。**|
|**2024-10-14**|**DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads**|Guangxuan Xiao et.al.|[2410.10819](http://arxiv.org/abs/2410.10819)|**[link](https://github.com/mit-han-lab/duo-attention)**|**部署长上下文的大规模语言模型（LLM）至关重要，但也带来了显著的计算和内存挑战。缓存所有注意力头中的Key和Value（KV）状态会消耗大量内存。现有的KV缓存剪枝方法要么损害了LLM的长上下文能力，要么只提供了有限的效率提升。本文发现，只有部分注意力头，即检索头，对于处理长上下文是至关重要的，并且需要对所有标记进行完整的注意力机制。相反，所有其他头部，主要关注最近的标记以及注意力汇点，称为流头部，不需要完整的注意力。基于这一见解，我们引入了DuoAttention框架，该框架仅对检索头应用完整的KV缓存，而对流头部使用轻量级、固定长度的KV缓存，从而在不损害长上下文能力的情况下减少LLM解码和预填充的内存和延迟。DuoAttention采用了一种基于优化的算法，使用合成数据准确识别检索头。我们的方法将长上下文推理内存最多减少了2.55倍（对于MHA模型）和1.67倍（对于GQA模型），同时解码速度提高了最多2.18倍（MHA模型）和1.50倍（GQA模型），并加速预填充最多1.73倍（MHA模型）和1.63倍（GQA模型），并且与全注意力相比，精度损失最小。值得注意的是，结合量化技术，DuoAttention使Llama-3-8B能够在单个A100 GPU上解码长达330万上下文长度的数据。代码可在https://github.com/mit-han-lab/duo-attention获取。**|
|**2024-10-14**|**Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free**|Ziyue Li et.al.|[2410.10814](http://arxiv.org/abs/2410.10814)|**[link](https://github.com/tianyi-lab/moe-embedding)**|尽管大型语言模型（LLMs）在生成任务上表现出色，但其解码器-only架构通常限制了它们作为嵌入模型的潜力，除非进行进一步的表示微调。这是否与它们作为通用模型的主张相矛盾？为了回答这个问题，我们更仔细地研究了混合专家（MoE）LLMs。我们的研究表明，MoE LLMs中的专家路由可以作为一个现成的嵌入模型，在各种嵌入重点任务上表现出色，而无需任何微调。此外，我们广泛的分析表明，MoE路由权重（RW）与LLMs广泛使用的隐藏状态（HS）互补。与HS相比，我们发现RW对提示的选择更具鲁棒性，并关注高层次语义。受此分析启发，我们提出了MoEE，结合了RW和HS，其性能优于单独使用任一方法。我们对它们的组合及其提示策略的探索揭示了若干新颖见解，例如，RW和HS相似度的加权和优于它们连接后的相似度。我们在来自大规模文本嵌入基准（MTEB）的6个嵌入任务中的20个数据集上进行了实验。结果表明，MoEE显著提升了基于LLM的嵌入效果，且无需进一步微调。|
|**2024-10-14**|**LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory**|Di Wu et.al.|[2410.10813](http://arxiv.org/abs/2410.10813)|**[link](https://github.com/xiaowu0162/longmemeval)**|**近期，大型语言模型（LLM）驱动的聊天助手系统已集成了记忆组件来跟踪用户与助手之间的聊天历史，从而实现更准确和个性化的响应。然而，它们在持续交互中的长期记忆能力仍需深入研究。本文介绍了一个名为LongMemEval的综合基准，用于评估聊天助手的五项核心长期记忆能力：信息提取、多会话推理、时间推理、知识更新和弃权。该基准包含500个精心策划的问题，并嵌入在自由扩展的用户与助手聊天历史中。LongMemEval对现有的长期记忆系统提出了重大挑战，在商业聊天助手和长上下文LLM上，跨持续交互的记忆信息保留率下降了30%。随后，我们提出了一种统一框架，将长期记忆设计分解为索引、检索和阅读阶段的四个设计选择。基于关键实验洞察，我们提出了几种内存设计，包括会话分解以优化值粒度、事实增强的关键扩展以增强索引结构以及时间感知查询扩展以细化搜索范围。实验结果表明，这些优化极大地提高了LongMemEval上的内存召回率和下游问题回答性能。总体而言，本研究为推进基于LLM的聊天助手的长期记忆能力提供了有价值的资源和指导，为更个性化和可靠的对话AI铺平了道路。**|
|**2024-10-14**|**Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning**|Aakanksha et.al.|[2410.10801](http://arxiv.org/abs/2410.10801)|null|大型语言模型（LLMs）已被全球广泛采用，应用于各种领域。然而，确保其安全使用仍然是一个重大挑战。偏好训练和安全措施往往过度拟合于西方中心数据集中的危害，而安全协议通常无法扩展到多语言环境。在这项工作中，我们在多样化的多任务设置中探索模型合并，在多语言背景下结合安全和通用任务。每种语言在不同任务中引入了独特的学习挑战。我们发现，基于目标的合并比混合数据更有效，总体性能和安全性分别提高了8%和10%。我们还发现，基于语言的合并非常有效——通过合并单语微调模型，我们实现了在相同可用数据下，相比混合数据方法，整体性能提高4%，所有语言上的危害减少7%。总的来说，我们对合并方法的综合研究提供了一个构建强大且安全的多语言模型的有用框架。|
|**2024-10-15**|**MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling**|Jian Yang et.al.|[2410.10798](http://arxiv.org/abs/2410.10798)|null|近年来，多模态大语言模型的发展推动了联合概率模型的进步，这些模型能够同时理解和生成图像。然而，我们发现最近的方法在理解任务过程中不可避免地会丢失图像信息，这主要是由于图像离散化或扩散去噪步骤造成的。为了解决这一问题，我们提出了一种新的多模态自回归（MMAR）概率建模框架。与离散化方法不同，MMAR采用连续值的图像标记来避免信息丢失。不同于基于扩散的方法，我们通过在每个自回归图像块嵌入顶部添加一个轻量级扩散头来解耦扩散过程和自回归主干模型。这样一来，当模型从图像生成过渡到通过文本生成进行理解时，主干模型对图像的隐藏表示不受限于最后的去噪步骤。为了成功训练我们的方法，我们还提出了一种理论上被证明可以解决数值稳定性问题的技术，并提出了一种平衡生成和理解任务目标的训练策略。通过在18个图像理解基准上进行广泛的评估，MMAR展示了比其他联合多模态模型更优越的性能，其性能可与采用预训练CLIP视觉编码器的方法相媲美，同时还能生成高质量的图像。我们还表明，该方法在更大数据集和更大模型规模下具有可扩展性。|
|**2024-10-14**|**Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance**|Sachin Goyal et.al.|[2410.10796](http://arxiv.org/abs/2410.10796)|**[link](https://github.com/locuslab/context-parametric-inversion)**|**大型语言模型通过指令微调来增强其遵循用户指令和处理输入上下文的能力。然而，即使是最先进的模型也常常难以遵循指令，尤其是在输入上下文与模型的参数知识不一致时。这会导致各种失败，例如幻觉，即响应内容过时、带有偏见或包含未经验证的事实。在这项工作中，我们试图理解这种不良上下文依赖性的根本原因，特别是在指令微调之后。我们观察到一个有趣的现象：在指令微调过程中，上下文依赖性最初如预期般增加，但随着指令微调的进行，这种依赖性逐渐减少。我们将这一现象称为上下文-参数反转，并发现在多个通用指令调优数据集（如TULU、Alpaca和Ultrachat）以及模型家族（如Llama、Mistral和Pythia）中都存在这种现象。在一个简单的理论设置中，我们沿着指令微调的梯度下降轨迹分离出上下文-参数反转发生的原因。我们将这一现象与指令微调数据混合中的示例联系起来，这些示例中输入上下文提供的信息已经存在于模型的参数知识中。我们的分析提出了某些有限的缓解策略，同时也验证了我们的理论见解。我们希望我们的工作能作为解决这一失败模式的一个起点，而这一模式是LLM训练中的一个标准部分。**|
|**2024-10-14**|**Focused ReAct: Improving ReAct through Reiterate and Early Stop**|Shuoqiu Li et.al.|[2410.10779](http://arxiv.org/abs/2410.10779)|null|大型语言模型（LLMs）在推理和决策能力方面有了显著的提升，这体现在ReAct等方法中。然而，尽管ReAct在处理复杂任务时非常有效，但它面临两个主要挑战：一是容易偏离原始问题，二是陷入行动循环。为了解决这些问题，我们引入了Focused ReAct，这是ReAct范式的一个增强版本，它结合了重申和早期停止机制。这些改进有助于模型保持对原始问题的关注并避免重复行为。实验结果表明，与原始的ReAct方法相比，Focused ReAct的准确率提高了18%到530%，运行时间减少了最多34%。|
|**2024-10-14**|**AFlow: Automating Agentic Workflow Generation**|Jiayi Zhang et.al.|[2410.10762](http://arxiv.org/abs/2410.10762)|**[link](https://github.com/geekan/metagpt)**|**大型语言模型（LLMs）在解决各种领域中的复杂任务方面展现出了显著的潜力，通常通过采用遵循详细指令和操作序列的代理工作流程来实现。然而，构建这些工作流程需要大量的人力，这限制了其可扩展性和通用性。最近的研究试图自动化生成和优化这些工作流程，但现有的方法仍然依赖于初始的手动设置，并且未能实现完全自动化和有效的流程生成。为了解决这一挑战，我们将工作流优化重新表述为一个代码表示的工作流空间搜索问题，在该空间中，由LLM调用的节点通过边连接。我们引入了AFlow，这是一个自动化的框架，使用蒙特卡洛树搜索有效地探索这个空间，通过代码修改、树结构的经验以及执行反馈迭代地改进工作流程。在六个基准数据集上的实证评估表明，AFlow的有效性，平均比最先进的基线提高了5.7%。此外，AFlow使得较小的模型在特定任务上能够超越GPT-4，同时其推理成本仅为GPT-4的4.55%。代码将在https://github.com/geekan/MetaGPT获取。**|
|**2024-10-14**|**Denial-of-Service Poisoning Attacks against Large Language Models**|Kuofeng Gao et.al.|[2410.10760](http://arxiv.org/abs/2410.10760)|**[link](https://github.com/sail-sg/p-dos)**|**近期的研究表明，大型语言模型（LLMs）容易受到拒绝服务（DoS）攻击，这种攻击通过恶意输入如拼写错误或无意义的提示词触发模型无限输出，而不会生成[EOS]结束符。这些攻击可能导致高延迟，并使LLM服务对其他用户或任务不可用。然而，在存在语音到文本接口的情况下（例如，对机器人的语音指令），执行此类DoS攻击变得具有挑战性，因为通过语音很难引入拼写错误或无意义的提示词。一种简单的DoS攻击方式是指示模型“不断重复‘Hello’”，但我们观察到依赖自然指令的方式会限制输出长度，该长度受限于预训练数据的最大长度。为了克服这一限制，我们提出了一种针对LLMs的基于投毒的DoS（P-DoS）攻击方法，证明通过注入一个精心设计的投毒样本可以突破输出长度的限制。例如，一个投毒样本能够以不到1美元的成本成功攻击GPT-4o和GPT-4o mini（通过OpenAI的微调API），导致重复输出直至达到最大推理长度（16K个标记，相比之下未投毒前为0.5K）。此外，我们还对开源LLMs进行了全面的消融研究，并将此方法扩展到LLM代理，其中攻击者可以控制微调数据集和算法。我们的发现强调了需要防御P-DoS攻击以确保LLMs的安全。我们的代码可以在https://github.com/sail-sg/P-DoS获取。**|
|**2024-10-14**|**SplitLLM: Collaborative Inference of LLMs for Model Placement and Throughput Optimization**|Akrit Mudvari et.al.|[2410.10759](http://arxiv.org/abs/2410.10759)|null|大型语言模型（LLMs）近年来成为一项颠覆性的创新，在我们的日常生活中扮演着重要角色，因为它们能够理解和生成类似人类的文本。它们的功能包括自然语言理解、信息检索和搜索、翻译、聊天机器人、虚拟助手等。然而，众所周知，LLMs在参数数量上非常庞大。此外，底层架构Transformer中的自注意力机制在计算和内存方面与输入序列长度呈二次复杂性关系。由于这些原因，LLM推理资源密集型高，因此LLM推理的吞吐量受到限制，尤其是在较长序列的情况下。在这份报告中，我们设计了一种服务器与其客户端之间的协作推理架构，以缓解吞吐量限制。在这个设计中，我们考虑了双方可用的资源，即计算和通信成本。我们开发了一种基于动态规划的算法，以最优方式分配服务器和客户端设备之间的计算，从而提高服务器吞吐量，同时不违反服务水平协议（SLA）。实验表明，我们能够高效地分配工作负载，使服务器的工作负载减少约三分之一，同时比贪心方法提高了19%。结果表明，在具有不同类型LLM推理请求的环境中，服务器的吞吐量得到了提升。|
|**2024-10-11**|**AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation**|Zijun Wang et.al.|[2410.09040](http://arxiv.org/abs/2410.09040)|**[link](https://github.com/ucsc-vlaa/attngcg-attack)**|**本文研究了基于转换器的大型语言模型（LLMs）受到囚禁攻击的脆弱性，特别关注基于优化的贪婪坐标梯度（GCG）策略。我们首先观察到攻击的有效性与模型内部行为之间存在正相关关系。例如，当模型对旨在确保LLM安全对齐的系统提示给予更多关注时，攻击往往效果较差。在此基础上，我们引入了一种增强方法，通过操纵模型的注意力分数来促进LLM的囚禁，我们将其命名为AttnGCG。实验上，AttnGCG在各种LLMs上表现出一致的改进，在Llama-2系列中平均提高了约7%，在Gemma系列中提高了约10%。我们的策略还展示了针对未见过的有害目标和黑盒LLMs（如GPT-3.5和GPT-4）的稳健攻击转移能力。此外，我们注意到我们的注意力分数可视化更易于解释，使我们能够更好地了解如何通过有针对性的注意力操纵实现更有效的囚禁。我们发布了代码，可在https://github.com/UCSC-VLAA/AttnGCG-attack中获取。**|
|**2024-10-11**|**Semi-Supervised Learning of Noisy Mixture of Experts Models**|Oh-Ran Kwon et.al.|[2410.09039](http://arxiv.org/abs/2410.09039)|null|混合专家（MoE）模型是一个灵活的预测建模框架，在大型语言模型的时代重新引起了人们的关注。一个由预测“专家”组成的集合与控制在预测时每个专家影响力的“门控函数”共同学习。这种结构允许相对简单的模型在复杂、异构的数据环境中表现出色。在当今许多应用场景中，未标记数据广泛可用而标注数据却难以获取。半监督学习方法旨在利用未标记数据。我们提出了一种用于MoE模型半监督学习的新方法。我们从海洋学家开发的一种假设强烈的半监督MoE模型开始，该模型假设未标注数据中的潜在聚类结构直接映射到监督任务中每个专家应给予的影响。我们放松了这一假设，设想两者之间存在噪声连接，并基于最小化剔除平方算法提出了一种算法，即使存在数据错位也能成功。我们的理论分析确定了该方法能够产生接近参数率收敛估计器的条件。模拟和真实数据示例证明了该方法的有效性。|
|**2024-10-11**|**SimpleStrat: Diversifying Language Model Generation with Stratification**|Justin Wong et.al.|[2410.09038](http://arxiv.org/abs/2410.09038)|null|生成大型语言模型（LLM）的多样化响应对于规划/搜索和合成数据生成等应用至关重要。这些应用需要在生成过程中提供多样化的答案，以便在每次生成时都能得到不同的结果。之前的方法通常依赖于增加温度来提高多样性。然而，与普遍认识相反，我们发现这种方法不仅会导致随着温度增加，个体生成的质量降低，而且其有效性还取决于模型的下一个词概率与真实答案分布的相似性。  我们提出了一种名为“SimpleStrat”的替代方法，该方法利用语言模型本身对空间进行分区。在推理阶段，随机选择一个分区并在其中抽取样本。为了衡量多样性，我们引入了CoverageQA数据集，它包含了具有多个同等可能答案的未指定问题。通过测量输出分布与有效地面真相答案的均匀分布之间的KL散度来评估多样性。由于计算专用模型每条响应/解决方案的概率通常是不可行的，因此我们使用召回率来评估地真理解。  我们的评估结果显示，使用SimpleStrat方法可以实现比GPT-4o高0.05的召回率，并且平均减少了0.36的KL散度与Llama 3相比。|
|**2024-10-11**|**Mentor-KD: Making Small Language Models Better Multi-step Reasoners**|Hojae Lee et.al.|[2410.09037](http://arxiv.org/abs/2410.09037)|**[link](https://github.com/2hojae/mentor-kd)**|**大型语言模型（LLMs）通过利用链式思维（CoT）提示在各种复杂任务中表现出非凡的性能。近期的研究提出了一种知识蒸馏（KD）方法——推理蒸馏，通过微调由LLM教师生成的多步推理语言模型，将LLM的推理能力转移到较小的模型上。然而，这些研究在以下两个方面考虑不足：从LLM教师模型获取的示例集质量低和软标签提供不足。本文提出了一种名为导师-KD的方法，该方法有效地将LLM的多步推理能力转移到较小的语言模型上，并解决了上述挑战。具体而言，我们利用一个导师——特定任务的中间大小的微调模型——来增加额外的CoT注释并为学生模型提供软标签，以在推理蒸馏过程中提供支持。我们进行了广泛的实验，并确认了导师-KD在不同模型和复杂推理任务上的有效性。**|
|**2024-10-11**|**PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents**|Xiangyu Yin et.al.|[2410.09034](http://arxiv.org/abs/2410.09034)|**[link](https://github.com/xyin-anl/Nodeology)**|Ptychography是一种在X射线和电子显微镜领域广泛应用的高级计算成像技术。它在物理学、化学、生物学和材料科学等研究领域以及半导体表征等工业应用中被广泛采用。实践过程中，获得高质量的ptychographic图像需要同时优化众多实验和算法参数。传统上，参数选择往往依赖于试错法，导致工作效率低下，并可能引入人为偏见。本工作开发了“ptychographic实验与分析机器人”（PEAR），这是一种利用大型语言模型（LLMs）自动处理ptychography数据分析的框架。为了确保高鲁棒性和准确性，PEAR采用了多个LLM代理进行知识检索、代码生成、参数推荐和图像推理任务。我们的研究表明，PEAR的多代理设计显著提高了工作流程的成功率，即使使用较小的开源权重模型如LLaMA 3.1 8B也是如此。PEAR还支持各种自动化级别，并设计有可自定义的本地知识库，以确保其在不同研究环境下的灵活性和适应性。|
|**2024-10-11**|**The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals**|Xiaofeng Wu et.al.|[2410.09013](http://arxiv.org/abs/2410.09013)|null|本文研究了大型语言模型（LLMs）和视觉语言模型（VLMs）在利用汉字中的视觉信息方面的潜力，尤其是关于部首、结构、笔画以及笔画数量的信息。我们构建了一个基准测试系统来评估这些模型对汉字中视觉元素的理解程度。实验结果表明，尽管提供字符图像，模型仍然展示了有限但部分理解视觉信息的能力。  为了激发模型利用部首进行中文理解任务的潜力，我们进一步尝试将部首信息融入到提示中。我们观察到，在提供关于部首的额外信息时，词性标注任务的表现得到了一致性的提升。这表明通过整合子字符信息，有可能增强语言处理能力。|
|**2024-10-11**|**Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models**|Hao Li et.al.|[2410.09012](http://arxiv.org/abs/2410.09012)|**[link](https://github.com/sailresearch/fmse-blogs)**|本文首次从实践者的视角分析了基础模型（FMs）在软件工程（SE）领域的应用。通过分析来自顶级科技公司的155篇FM4SE和997篇SE4FM博客文章，利用基于FM的调研方法系统地标记和总结了讨论的活动和任务。研究发现，虽然代码生成是FM4SE中最突出的任务，但FMs还被用于代码理解、总结和API推荐等众多其他SE活动。关于SE4FM的大多数博客文章关注于模型部署与操作以及系统架构与编排。尽管云部署占主导地位，但对FMs进行压缩并在边缘或移动设备上部署的兴趣正在增长。本文提出了八个未来研究方向，旨在弥合理论发现与实际应用之间的差距。我们的研究不仅丰富了FMs在SE领域实践应用的知识体系，还展示了FMs在技术与灰色文献领域进行文献调研的有效性。我们提供的数据集、结果、代码以及使用的提示可以在在线复制包https://github.com/SAILResearch/fmse-blogs中找到。|
|**2024-10-11**|**SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights**|Ling Yang et.al.|[2410.09008](http://arxiv.org/abs/2410.09008)|**[link](https://github.com/yangling0818/supercorrect-llm)**|**大型语言模型（LLMs）如GPT-4、PaLM和LLaMA在各种推理任务上表现出显著的改进。然而，较小的模型如Llama-3-8B和DeepSeekMath-Base仍然在复杂的数学推理方面存在挑战，因为它们无法有效地识别并纠正推理错误。近期的反思方法旨在通过使模型能够自我反思和自我校正来解决这些问题，但仍面临独立检测推理步骤中的错误的挑战。为了克服这些限制，我们提出了一种名为SuperCorrect的新型两阶段框架，它使用大型教师模型来监督和纠正较小学生模型的推理和反思过程。  在第一阶段，我们从教师模型中提取了层次化的高阶和详细的思想模板，以指导学生模型生成更细致的推理思想。在第二阶段，我们引入了跨模型协作直接偏好优化（DPO）来增强学生模型的自我校正能力，在训练过程中跟随教师的修正轨迹进行改进。这种跨模型DPO方法教会学生模型通过从教师模型获得的错误驱动的见解有效地定位并解决错误的思想，打破其思想的瓶颈，并通过学习新技能和知识来应对具有挑战性的问题。  广泛的实验一致证明了我们的优越性。值得注意的是，我们的SuperCorrect-7B模型在MATH/GSM8K基准测试中显著超越了强大的DeepSeekMath-7B和Qwen2.5-Math-7B，分别在MATH和GSM8K基准上提高了7.8%/5.3%和15.1%/6.3%，在所有7B模型中实现了新的最先进性能。代码：https://github.com/YangLing0818/SuperCorrect-llm**|
|**2024-10-11**|**From Interaction to Impact: Towards Safer AI Agents Through Understanding and Evaluating UI Operation Impacts**|Zhuohao Jerry Zhang et.al.|[2410.09006](http://arxiv.org/abs/2410.09006)|null|随着生成式人工智能的进步，人们在创建能够通过用户界面（UI）管理日常任务的自主代理方面取得了进展。尽管先前的研究已经探讨了AI代理如何导航UI以及理解UI结构的机制，但代理及其自主行为（特别是可能具有风险或不可逆性的行为）的影响和后果仍然缺乏深入研究。本工作中，我们探索了AI代理UI操作的实际世界影响和后果。  我们首先通过一系列与领域专家的工作坊开发了一种UI操作影响的分类系统。随后，我们进行了一项数据综合研究，收集了用户感知为具有影响力的UI屏幕轨迹和操作数据。然后，我们使用我们的影响类别对收集的数据和从现有UI导航数据集中重新利用的数据进行了注释。我们对不同大型语言模型（LLMs）及其变体的定量评估显示了这些LLM理解和预测AI代理可能采取的UI操作影响的能力。  我们的研究结果表明，我们的分类系统增强了这些LLM的推理能力，使它们能够更好地理解UI操作的影响。然而，我们也发现了他们在可靠地分类更微妙或复杂的影响力类别时存在显著差距的问题。|
|**2024-10-11**|**Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference**|Grace Proebsting et.al.|[2410.08996](http://arxiv.org/abs/2410.08996)|null|我们通过使用GPT-4、Llama-2和Mistral 7b等大型语言模型（LLM）来生成自然语言推理（NLI）假设，测试了用LLM替换众包工作者对产生注释偏见的影响。我们复现了斯坦福NLI语料库的部分数据，并训练了仅使用假设的分类器来确定LLM生成的假设是否包含注释偏见。在我们的由LLM生成的NLI数据集上，基于BERT的仅假设分类器达到了86%-96%的准确率，这表明这些数据集包含仅假设的偏见。我们还发现LLM生成的假设中存在频繁的“线索”，例如，“在泳池里游泳”这一短语在GPT-4生成的10000多个矛盾假设中出现。我们的分析提供了实证证据，证明NLI中已知的偏见可能在LLM生成的数据中持续存在。|
|**2024-10-10**|**Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training**|Gen Luo et.al.|[2410.08202](http://arxiv.org/abs/2410.08202)|null|随着大型语言模型（LLM）的迅速发展，对扩展其能力以处理多模态任务的关注日益增加。其中，对单体多模态大型语言模型（MLLM）的研究引起了广泛关注，这些模型整合了视觉编码和语言解码功能。尽管单体MLLM在结构上简洁且易于部署，但要实现具有竞争力性能的训练仍面临挑战。流行的策略采用连续预训练方法，将预训练的LLM扩展为单体MLLM，这会导致灾难性遗忘并导致性能退化。  本文旨在从增量学习的角度克服这一局限性。具体来说，我们的核心思想是在预训练的LLM中嵌入视觉参数，通过增量学习机制，即在优化视觉参数时冻结LLM，从大量数据中逐步学习视觉知识。基于这一原则，我们提出了一种名为Mono-InternVL的新型单体MLLM，它通过多模态混合专家结构无缝地融合了一系列视觉专家。此外，我们还提出了一种创新的预训练策略来最大化Mono-InternVL的视觉能力，即内生视觉预训练（EViP）。具体而言，EViP设计为一个视觉专家的渐进式学习过程，旨在充分利用从低质量数据到高质量数据的视觉知识。  为了验证我们的方法，我们在16个基准上进行了广泛实验。实验结果不仅证实了与当前最先进的单体MLLM相比，Mono-InternVL在6个多模态基准上的卓越性能，例如在OCRBench上的+113点优势，而且还确认了其更好的部署效率，首次令牌延迟降低了高达67%。|
|**2024-10-10**|**From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions**|Changle Qu et.al.|[2410.08197](http://arxiv.org/abs/2410.08197)|**[link](https://github.com/quchangle1/DRAFT)**|**本文专注于解决大型语言模型（LLM）与外部工具交互过程中存在的理解鸿沟问题，这一鸿沟源于现有人类导向的工具文档的不完善性和不准确性。我们提出了一种名为DRAFT的新框架，旨在动态优化工具文档，通过分析来自LLM与外部工具交互过程中的反馈和轨迹信息。该方法基于一种创新的试错学习流程，包括经验收集、从经验学习以及文档重写三个阶段，以迭代方式提升工具文档的质量。  为了确保探索的多样性并避免过拟合，DRAFT还采用了促进多样性的探索策略，并配备了一个工具适应性终止机制来提高效率。在多个数据集上的实验结果表明，DRAFT通过迭代反馈优化显著提高了文档质量，促进了LLM对工具的更深入理解和更有效利用。我们的分析进一步揭示了通过这种方法优化后的工具文档具有强大的跨模型通用能力。**|
|**2024-10-10**|**MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code**|Zimu Lu et.al.|[2410.08196](http://arxiv.org/abs/2410.08196)|**[link](https://github.com/mathllm/mathcoder2)**|**本文介绍了一种新颖的方法，用于生成伴随推理步骤的数学代码，以进行持续预训练。我们的方法首先通过整合数学相关网络数据、使用数学包的代码、数学教科书和合成数据来构建高质量的数学持续预训练数据集。接着，我们通过提取LaTeX表达式、表达式的条件以及结果来构造推理步骤。基于这些提取的信息，我们生成相应的代码，以准确捕捉数学推理过程。我们将生成的代码附加到每个推理步骤后，形成包含自然语言推理步骤及其对应代码的数据对。将此数据与原始数据集结合，得到一个包含19.2B个标记的高性能数学预训练语料库，我们将其命名为MathCode-Pile。使用此语料库对几种流行的基模进行训练，显著提高了它们的数学能力，从而产生了名为MathCoder2的模型家族。所有数据处理和训练代码均开源，确保了整个数据收集和训练流程的透明性和可复现性。代码在https://github.com/mathllm/MathCoder2上发布。**|
|**2024-10-10**|**GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment**|Yuancheng Xu et.al.|[2410.08193](http://arxiv.org/abs/2410.08193)|null|大型语言模型（LLM）展现出令人印象深刻的能力，但需要仔细对齐以满足人类的偏好。传统的训练时方法通过使用人类偏好数据集来微调LLM，但会带来显著的训练成本，并且需要重复训练以应对多样化的用户偏好。测试时对齐方法通过使用奖励模型（RM）来引导冻结的LLM，而无需重新训练，从而解决了这一问题。然而，现有的测试时方法依赖于轨迹级RM，它们旨在评估完整响应，这使得它们不适合用于需要从部分响应计算下一个词奖励的自回归文本生成。  为了应对这一挑战，我们引入了GenARM，一种测试时对齐方法，利用了自回归奖励模型——一种新型的奖励参数化方法，旨在预测自回归生成过程中的下一个词奖励，以实现高效和有效的自回归生成。理论上，我们证明了这种参数化可以在KL正则化强化学习框架内引导冻结的LLM接近任何由传统RM可实现的分布。实验结果表明，GenARM在性能上显著优于先前的测试时对齐基线，并且与训练时方法的性能相当。此外，GenARM支持弱到强的指导，允许在不需要训练更大模型的情况下，通过较小的RM对更大的LLM进行对齐，从而降低了成本。进一步地，GenARM还支持多目标对齐，允许实时平衡偏好维度，满足不同用户需求，而无需重新训练。|
|**2024-10-10**|**Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models**|Qingni Wang et.al.|[2410.08174](http://arxiv.org/abs/2410.08174)|null|本论文提出了一种名为TRON的两步框架，旨在对任何支持在开放和封闭场景下采样的大型多模态语言模型（MLLM）进行风险控制与评估。TRON由两个主要组件构成：（1）一种新颖的校准评分方法，用于以最小尺寸采样响应集；（2）基于自致性理论的非一致性评分，通过设定两种特定的风险水平来控制错误率。此外，本研究首次探讨了在开放场景下的预测集中的语义冗余问题，并据此提出了一个用于评价MLLM的新指标——平均集合大小。  通过在四个视频问答（VideoQA）数据集上使用八种MLLM进行全面实验，我们证明了TRON能够实现用户指定的风险水平范围内的期望错误率。同时，去重后的预测集在保持适应性的同时，展现出更高效、稳定的风险评估能力，在不同风险水平下均有出色表现。|
|**2024-10-10**|**On the Evaluation of Generative Robotic Simulations**|Feng Chen et.al.|[2410.08172](http://arxiv.org/abs/2410.08172)|null|由于获取真实世界数据的困难性，机器人模拟已成为并行训练和模拟到现实世界的转换的关键，这凸显了可扩展仿真机器人任务的重要性。基础模型已经展现出在自主生成可行机器人任务方面的惊人能力。然而，这一新范式强调了评估这些自主生成任务的挑战。为了解决这一问题，我们提出了一种针对生成模拟的全面评价框架。我们的框架将评估分为三个核心方面：质量、多样性和泛化。对于单任务质量，我们使用大型语言模型和视觉语言模型评估生成任务的真实性和生成轨迹的完整性。在多样性方面，我们通过任务描述的文本相似性和收集的任务轨迹训练的世界模型损失来测量任务和数据的多样性。对于任务级别的泛化，我们评估了使用多个生成任务训练的策略在未见过的任务上的零样本泛化能力。在三个代表性任务生成管道上进行的实验表明，我们的框架的评估结果与人类评估高度一致，确认了我们方法的可行性和有效性。研究发现，虽然可以通过某些方法实现质量和多样性的指标，但没有任何一种方法能够在所有指标上都表现出色，这表明需要更多地关注平衡这些不同指标。此外，我们的分析进一步突显了当前工作面临的共同挑战——低泛化能力。  匿名网站链接：https://sites.google.com/view/evaltasks|
|**2024-10-10**|**Agent S: An Open Agentic Framework that Uses Computers Like a Human**|Saaket Agashe et.al.|[2410.08164](http://arxiv.org/abs/2410.08164)|**[link](https://github.com/simular-ai/agent-s)**|**我们介绍了一种名为Agent S的开放性代理框架，它通过图形用户界面(GUI)与计算机进行自主交互，旨在通过自动化复杂、多步骤的任务来改变人机交互方式。Agent S旨在解决自动化计算机任务时遇到的三个关键挑战：获取特定领域的知识、在长任务周期内规划以及处理动态、非均匀的界面。为此，Agent S引入了经验增强的层次规划方法，该方法在多个级别上结合外部知识搜索和内部经验检索，从而实现高效的任务规划和子任务执行。此外，它采用了代理-计算机接口(ACI)，基于多模态大型语言模型(MLLMs)更好地揭示GUI代理的推理和控制能力。在OSWorld基准测试中的评估显示，与基线相比，Agent S的成功率提高了9.37%(相对提高了83.6%)，并达到了新的最高水平。全面分析强调了各个组件的有效性，并提供了未来改进的见解。此外，Agent S在新发布的WindowsAgentArena基准上展示了广泛的通用性，能够适应不同的操作系统。有关代码的更多信息，请参阅https://github.com/simular-ai/Agent-S。**|
|**2024-10-10**|**Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning**|Amrith Setlur et.al.|[2410.08146](http://arxiv.org/abs/2410.08146)|null|提高大型语言模型推理能力的一种有前景的方法是使用过程奖励模型（PRMs）。与仅在最终步骤提供反馈的结果奖励模型（ORMs）相比，PRMs在多步推理跟踪的每个步骤都提供反馈，可能有助于改进信用分配。然而，收集密集、每步骤的人类标签并不具有可扩展性，从自动标记数据训练PRMs迄今为止导致的增益有限。为了通过运行搜索来改进基策略或将其用作强化学习（RL）的密集奖励来优化基策略，我们提出的问题是：“我们应该如何设计过程奖励？”我们的关键洞察是，为了有效，步骤级奖励应该衡量进度：采取步骤前后产生正确响应的可能性变化，对应于RL中的步骤级优势的概念。关键在于，这种进展应该在与基策略不同的证明策略下进行测量。我们理论地描述了良好的证明者集合，并且我们的结果表明，通过这样的证明者优化过程奖励可以改善测试时搜索和在线RL期间的探索。实际上，我们的描述显示，弱证明者策略可以显着提高更强的基策略，这也是我们在实验上观察到的现象。我们通过训练过程优势验证器（PAVs）来预测在这些证明者下进行的进展，证明与ORMs相比，在线RL使用PAVs提供的密集奖励可以实现高达8％以上的准确性提高，以及1.5至5倍的计算效率提高。使用PAVs的在线RL首次实现了样本效率提升5-6倍，准确率提升超过6％的结果。|
|**2024-10-10**|**Insight Over Sight? Exploring the Vision-Knowledge Conflicts in Multimodal LLMs**|Xiaoyuan Liu et.al.|[2410.08145](http://arxiv.org/abs/2410.08145)|**[link](https://github.com/xyliu-cs/ConflictVIS)**|本文探讨了在多模态大型语言模型（MLLM）中视觉信息与模型内部常识知识冲突的问题。研究发现，在特定情况下，MLLMs可能基于文本查询而非视觉输入做出决策，导致常识级的视觉-知识矛盾。为解决这一问题，我们设计了一套自动化的评估流程，并辅以人工质量控制环节，构建了一个用于模拟和评估此类冲突的基准测试系统。  该基准测试包含了374张原创图片及1122个高质量的问题-答案对，覆盖了两种冲突目标类型和三个不同难度级别的问题，为全面评估模型提供了工具。通过这一基准，我们对九种代表性的MLLM进行了评估，发现这些模型在处理视觉与常识知识冲突时存在显著的文本依赖性问题。  基于此发现，我们提出了一种新的提示策略——“聚焦于视觉”（FoV），旨在增强模型在遇到冲突时优先考虑视觉输入的能力，从而减少对矛盾文本信息的依赖。我们的分析结果以及提出的策略对理解并缓解MLLM中的视觉-知识冲突具有重要意义。  此外，本文还提供了数据集和代码的公开访问权限，以促进社区进一步的研究和应用。|
|**2024-10-10**|**DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory**|Yutong Wang et.al.|[2410.08143](http://arxiv.org/abs/2410.08143)|**[link](https://github.com/yutongwang1216/docmtagent)**|**在机器翻译领域，大型语言模型（LLMs）已经取得了相当可观的质量提升。然而，大多数当前的MT-LLM研究仍然面临在处理整个文档时保持翻译一致性与准确性的挑战。本文介绍了一种名为DelTA的文档级翻译代理，旨在克服这些局限性。DelTA具有一种多层次记忆结构，能够存储不同粒度和跨度的信息，包括专有名词记录、双语摘要、长期记忆和短期记忆，这些信息由辅助的LLM组件连续检索和更新。实验结果显示，在四个开源/闭源LLM和两个代表性文档翻译数据集上，DelTA在翻译一致性与质量方面均显著优于强大的基线，平均一致性得分提高高达4.58个百分点，COMET得分提高高达3.16点。DelTA采用逐句翻译策略，确保无句子遗漏，并提供与主流方法相比更为内存高效的选择。此外，DelTA提高了代词翻译准确性，并且代理的摘要组件也显示出作为基于查询的摘要任务工具的潜力。我们已将代码和数据发布在https://github.com/YutongWang1216/DocMTAgent。**|
|**2024-10-09**|**Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models**|Fei Wang et.al.|[2410.07176](http://arxiv.org/abs/2410.07176)|null|在探索如何通过联合分析来理解不完美检索对生成型问答（RAG）行为的影响，以及如何在LLM内部知识与外部来源之间产生潜在冲突时，我们发现，不完美的检索增强可能是不可避免的，并且会对RAG系统造成严重影响。通过在现实条件下的控制性分析，我们发现了从检索到的不完整知识与LLM内部知识之间的知识冲突是RAG后处理阶段需要克服的关键瓶颈。  为了使LLM在面对不完美检索时具有鲁棒性，我们提出了“精明RAG”这一新颖的RAG方法。该方法能够适当地激发LLM内部知识中的关键信息，通过源意识地整合内部和外部知识，最终根据信息可靠性确定答案。我们的实验结果使用了Gemini和Claude两个模型验证了“精明RAG”的有效性，证明其显著优于现有的增强RAG鲁棒性的方法。值得注意的是，在最坏情况场景下，“精明RAG”是唯一能够达到或超过没有RAG的LLM性能的方法。  进一步的分析表明，“精明RAG”有效地解决了知识冲突问题，提高了RAG系统的可靠性和可信度。|
|**2024-10-09**|**Do better language models have crisper vision?**|Jona Ruthardt et.al.|[2410.07173](http://arxiv.org/abs/2410.07173)|null|本文探讨了文本仅依赖型大型语言模型（LLMs）在理解视觉世界方面的表现。随着LLMs在计算机视觉领域的应用日益广泛，这一问题变得既基础又关键。现有研究主要集中在有限的场景上，如生成视觉内容或对多模态数据进行聚类。因此，我们提出了一项名为“视觉文本表示基准”（ViTeRB）的任务，旨在识别出能够与视觉世界高度一致的关键属性。基于此任务的结果，我们发现解码器型大语言模型在视觉为中心的语境下作为文本表示的理想候选，这与当前使用文本编码器的做法形成了对比。  在此基础上，我们提出了“ShareLock”——一种超轻量级的类似CLIP的模型。通过利用从强大视觉和语言模型预计算的冻结特征，ShareLock在ImageNet上取得了51%的准确率，仅使用了563,000张图像-描述对。此外，训练所需的资源仅为1个GPU小时（或包括特征预计算的10个小时），远少于以往方法所需的时间数量级。我们将提供该代码。|
|**2024-10-09**|**Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate**|Qidong Huang et.al.|[2410.07167](http://arxiv.org/abs/2410.07167)|**[link](https://github.com/shikiw/modality-integration-rate)**|**我们提出了一种有效的、稳健的且通用的指标——模态整合率(MIR)，用于衡量大型视觉语言模型(LVLMs)的多模态预训练质量。大规模预训练在构建具备强大能力的LVLMs中扮演着关键角色，而如何在昂贵的监督微调阶段之前评估其训练质量则是一个未充分探索的领域。对于大型语言模型(LLLs)，常用的预训练指标包括损失、困惑度以及上下文内评估结果，但我们观察到这些指标在对良好训练的LLMs与新模态进行对齐时并不具有很好的指示性。由于缺乏合适的指标，LVLMs在关键的预训练阶段的研究受到了极大的阻碍，包括训练数据选择、高效模块设计等。本文提出从跨模态分布距离的角度来评估预训练质量，并引入了模态整合率(MIR)，该指标具有以下特点：1）**有效**地代表预训练质量，并与经过监督微调后的基准性能呈现正相关；2）**稳健**于不同的训练/评估数据；3）**泛化**于多种训练配置和架构选择。我们进行了一系列预训练实验以探索MIR的有效性，并观察到令人满意的结果，即MIR能够指示训练数据选择、训练策略调度以及模型架构设计以获得更好的预训练结果。我们希望MIR能够成为构建具备强大能力的LVLMs的有用指标，并激发不同领域关于模态对齐的后续研究。我们的代码已开源在：https://github.com/shikiw/Modality-Integration-Rate。**|
|**2024-10-09**|**Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making**|Manling Li et.al.|[2410.07166](http://arxiv.org/abs/2410.07166)|**[link](https://github.com/embodied-agent-interface/embodied-agent-interface)**|**为了系统地评估大型语言模型（LLMs）在实体化决策中的表现，虽然已有大量研究利用LLMs处理实体化环境中的决策问题，但我们仍缺乏对其性能的全面理解。现有工作通常在不同领域、针对不同目的、基于不同输入和输出构建LLMs，这使得难以统一评价它们。现有评估方法往往仅依赖最终的成功率，这使得难以识别LLMs缺失的能力以及问题所在，进而阻碍了实体化智能体有效且选择性地利用LLMs。  为此，我们提出了一种通用接口（实体化智能体接口），旨在支持各种任务类型与LLM模块输入-输出规范的统一化。具体而言，该接口允许：  1. 统一多种涉及状态与时间延伸目标的实体化决策任务。 2. 统一四种常用的用于决策的LLM模块：目标解释、子目标分解、动作序列规划和过渡建模。 3. 提供一系列精细粒度的度量标准，将评估细分为各种错误类型，如幻觉错误、可用性错误、不同类型规划错误等。  整体而言，我们的基准提供了对LLMs在不同子任务上的全面评估，揭示了LLM驱动的实体化人工智能系统的强项与弱点，并为有效和选择性地利用LLMs在实体化决策中提供了见解。**|
|**2024-10-09**|**Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning**|Chongyu Fan et.al.|[2410.07163](http://arxiv.org/abs/2410.07163)|**[link](https://github.com/OPTML-Group/Unlearn-Simple)**|本文旨在解决大型语言模型（LLM）的去学习问题，即在不重新从头训练的情况下，消除不需要的数据影响以及相关模型能力（如版权数据或有害内容生成），同时保留必要的模型功能。尽管对LLM去学习的需求日益增长，但尚未形成一种原理性的优化框架。  为此，我们回顾了当前最先进的方法——负偏好优化（NPO），并发现了参考模型偏见的问题，这可能削弱NPO的有效性，特别是在去学习不同难度数据时。鉴于此，我们提出了一种简单而有效的去学习优化框架——SimNPO，表明通过简单的偏好优化减少对参考模型的依赖（从简化视角来看）有助于去学习过程。此外，我们还提供了深入的SimNPO优势分析，通过混合马尔可夫链的分析方法支持这一观点。  我们通过在TOFU和MUSE等基准测试中的大量实验验证了SimNPO相对于现有去学习基线的优越性，并展示了其对重新学习攻击的鲁棒性。所有代码均可在GitHub上的https://github.com/OPTML-Group/Unlearn-Simple获取。|
|**2024-10-09**|**Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis**|Bohan Zeng et.al.|[2410.07155](http://arxiv.org/abs/2410.07155)|**[link](https://github.com/yangling0818/trans4d)**|**近期在扩散模型领域的进展展示了其在图像和视频生成方面的卓越能力，进一步提升了4D合成的有效性。现有的4D生成方法能够根据用户友好的条件生成高质量的4D对象或场景，对游戏和视频行业大有裨益。然而，这些方法在合成复杂4D过渡和场景内对象交互的显著变形方面仍存在挑战。为解决这一问题，我们提出了一种名为Trans4D的创新文本到4D合成框架，旨在实现真实可信的场景级复杂过渡。具体而言，我们首先利用多模态大型语言模型（MLLMs）生成物理意识的场景描述以进行4D场景初始化以及有效过渡时间规划。随后，我们提出了一种几何感知的4D过渡网络，基于计划实现复杂的场景级4D过渡，涉及表现力强的对象几何变形。广泛实验结果表明，Trans4D在生成具有准确性和高质量过渡的4D场景方面始终超越现有最先进的方法，验证了其有效性。代码：https://github.com/YangLing0818/Trans4D**|
|**2024-10-09**|**Mental Disorders Detection in the Era of Large Language Models**|Gleb Kuzmin et.al.|[2410.07129](http://arxiv.org/abs/2410.07129)|null|本文比较了传统机器学习方法、编码器基模型以及大型语言模型（LLM）在抑郁症和焦虑症检测任务上的效果。考虑了五个不同格式的数据库，每个数据库都采用了不同的方法来定义目标病理学类别。我们测试了基于语言特征的AutoML模型、多种变体的Transformer编码器，如BERT，以及最先进的LLM作为病理分类模型。结果表明，LLM在噪声大且训练样本在文本长度和类型上差异显著的小数据集上表现出色。然而，当在确诊为抑郁症个体的文本上进行训练时，语言模型的性能优于传统的心理语言学特征和编码器基模型，这凸显了它们在特定临床应用中的潜力。|
|**2024-10-09**|**Personalized Visual Instruction Tuning**|Renjie Pi et.al.|[2410.07113](http://arxiv.org/abs/2410.07113)|**[link](https://github.com/sterzhang/pvit)**|近期，多模态大型语言模型（MLLMs）的进展展现了显著的进步，然而，这些模型存在一个明显的局限性——“面部盲症”。具体来说，它们能够进行一般性的对话，但却无法针对特定个体进行个性化对话。这一缺陷阻碍了MLLMs在个性化场景中的应用，如定制化的移动设备视觉助手或需要识别家庭成员的家用机器人。为此，本文提出了一种名为个性化视觉指令调整（PVIT）的新颖数据整理与训练框架，旨在使MLLMs能够识别图像中的目标个体，并展开个性化且连贯的对话。我们的方法涉及开发一个复杂的管道，该管道能够自主生成包含个性化对话的训练数据。这个管道利用了各种视觉专家、图像生成模型和（多模态）大型语言模型的能力。为了评估MLLMs的个性化潜力，我们设计了一个名为P-Bench的基准测试，其中包括不同难度级别的多种问题类型。实验结果表明，在使用我们整理的数据集进行微调后，个性化性能得到了显著提升。|
|**2024-10-09**|**I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy**|Gian Maria Campedelli et.al.|[2410.07109](http://arxiv.org/abs/2410.07109)|**[link](https://github.com/mobs-fbk/llm_interaction_simulator)**|随着大型语言模型（LLM）驱动的智能体变得越来越自主，并且在彼此间自由互动时，研究它们之间的交互模式变得至关重要。这有助于我们预见可能产生的新现象以及潜在风险。本文受斯坦福监狱实验启发，专注于研究具有严格社会等级背景的多智能体环境中的LLM交互模式。  研究聚焦于两类主要现象：说服力和反社会行为，在涉及看守和试图达成特定目标（如获得额外的户外活动时间或逃狱）的囚犯智能体之间的模拟场景中进行探讨。通过使用200个实验场景，共计2000次机器间的对话，研究了五种流行的LLM，获得了以下显著发现：  1. 一些模型在多智能体设置中持续失败，无法进行有意义的对话。 2. 对于能够成功互动的模型，目标对智能体的说服力有显著影响，而对反社会行为的影响则微乎其微。 3. 智能体的角色，特别是看守的人格特质，对囚犯的说服成功几率和反社会行为的出现有着直接推动作用。 4. 即使没有明确提示特定的人格特质，仅通过赋予角色，也观察到了反社会行为的自然产生。  这些结果对LLM交互智能体的发展以及对其社会影响的讨论具有重要启示。|
|**2024-10-09**|**Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context**|Sangwon Yu et.al.|[2410.07103](http://arxiv.org/abs/2410.07103)|null|在多跳推理领域，大型语言模型（LLM）面临着基于给定上下文内的支持文档进行多步骤推理的挑战。LLM往往难以筛选出不相关的文档，并且其性能对上下文中支持文档的位置非常敏感。在这篇论文中，我们识别出了一个额外的挑战：LLM的性能也对呈现支持文档的顺序非常敏感。我们将此问题称为“错序上下文问题”。为了应对这一问题，我们提出了一种简单而有效的解决方法——上下文重复（CoRe），该方法通过多次提示模型以确保支持文档以最佳顺序呈现来解决这个问题。  通过应用CoRe，我们在多跳问答任务上的F1得分提高了高达30%，在合成任务上的准确率提高了高达70%。此外，CoRe有助于缓解LLM普遍存在的“中间迷失”问题，并可以与利用链式思考（CoT）推理的检索方法有效结合使用。|
|**2024-10-07**|**Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models**|Fei Wang et.al.|[2410.05269](http://arxiv.org/abs/2410.05269)|**[link](https://github.com/feiwang96/Data-Advisor)**|大型语言模型（LLM）中的数据是关键要素。近期研究探索了利用LLM进行高效数据收集的方法。然而，由LLM生成的数据往往存在质量参差不齐、某些方面被低估或缺失以及数据点质量低下的问题。为了应对这些问题，我们提出了一种名为“数据顾问”的增强型LLM数据生成方法，该方法能够考虑目标数据集的特性，从预定义的原则出发，监控生成数据的状态，识别当前数据集的弱点，并据此指导数据生成的下一轮迭代。数据顾问可以轻松地集成到现有的数据生成方法中，以提高数据质量和覆盖面。  在对三个代表性LLM（即Mistral、Llama2和Falcon）的安全对齐进行的实验中，数据顾问证明了其在不牺牲模型实用性的情况下，有效提升模型对各种精细粒度安全问题的适应性的能力。|
|**2024-10-07**|**PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs**|Mengzhao Chen et.al.|[2410.05265](http://arxiv.org/abs/2410.05265)|**[link](https://github.com/chenmnz/prefixquant)**|**量化对于大规模语言模型（LLMs）的部署至关重要，它能显著提升内存效率与推理速度。现有的激活量化方法主要针对通道级异常值进行处理，往往忽略了令牌级的异常值，这导致了对成本高昂的逐令牌动态量化依赖。本文提出了一种名为PrefixQuant的新颖技术，该技术在不重新训练的情况下离线识别出高频异常令牌，并将其作为前缀放入KV缓存中，以防止推理过程中生成异常令牌，并简化了量化过程。据我们所知，PrefixQuant是首个能够实现高效逐张量静态量化并超越昂贵的逐令牌动态量化的方法。例如，在W4A4KV4（权重4位、激活4位、KV缓存4位）的Llama-3-8B模型中，使用PrefixQuant和逐张量静态量化后，WikiText2的困惑度降低了7.43个点，平均准确率在5个常识推理任务上提高了71.08%，相较于之前的逐令牌动态量化方法QuaRot，分别在困惑度上提升了0.98个点，在准确率上提升了5.98个点。此外，使用PrefixQuant量化后的模型的推理速度相较于FP16模型提升了1.60倍到2.81倍，且超过了QuaRot模型1.2倍到1.3倍。我们的代码已开源于\url{https://github.com/ChenMnZ/PrefixQuant}。**|
|**2024-10-07**|**TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles**|Qingchen Yu et.al.|[2410.05262](http://arxiv.org/abs/2410.05262)|**[link](https://github.com/mazzzystar/TurtleBench)**|**随着大型语言模型（LLM）的应用范围不断扩大，对可靠评估的需求也在增加。现有的LLM评估基准主要依赖静态数据集，这使得评估模型在与用户动态交互时的表现变得具有挑战性。此外，这些基准往往需要特定背景知识，从而复杂化了衡量模型逻辑推理能力的测量。基于强大模型或人工努力的其他动态评估方法可能会引入偏见，并且成本和时间需求高，这阻碍了大规模应用。  为了解决这些问题，我们提出了TurtleBench。TurtleBench从我们开发的在线Turtle Soup Puzzle平台收集真实的用户猜测，这种方法允许生成相对动态的评估数据集，可以降低模型作弊的风险，同时使评估更贴近实际用户的推理需求，从而提高评估的可靠性。TurtleBench包含了1,532个用户猜测及其正确性的注释信息。利用这个数据集，我们全面评估了当前最先进的九个LLM模型。值得注意的是，OpenAI o1系列模型在这些评估中并未取得领先地位。  我们提出了一些进一步研究的假设，例如“o1的潜在推理使用了简单的链式思考（CoT）技术”和“增加CoT长度不仅提供了推理益处，同时也带来了噪音成本”。**|
|**2024-10-07**|**Differential Transformer**|Tianzhu Ye et.al.|[2410.05258](http://arxiv.org/abs/2410.05258)|**[link](https://github.com/microsoft/unilm/blob/master/Diff-Transformer/)**|在本文中，我们引入了差异变换器（Diff Transformer），它能够增强对相关上下文的注意力同时消除噪音。具体来说，差异注意力机制通过计算两个独立的softmax注意力映射之间的差值来确定注意力分数。这种减法操作可以消除噪音并促进稀疏注意力模式的产生。在语言建模任务上的实验结果表明，与标准的变换器相比，差异变换器在模型大小和训练样本量的扩展上均表现出色。更令人兴奋的是，在实际应用中，如长上下文建模、关键信息检索、幻觉抑制、上下文内学习以及激活异常减少等方面，差异变换器都展现出显著优势。由于对无关上下文的关注较少，差异变换器能够有效缓解问答和文本摘要中的幻觉问题。在上下文内学习方面，差异变换器不仅提高了准确率，而且对于顺序排列更为鲁棒，这被认为是长期的稳健性问题。这些结果确立了差异变换器作为推动大型语言模型发展的高效且有前景架构的地位。|
|**2024-10-07**|**GLEE: A Unified Framework and Benchmark for Language-based Economic Environments**|Eilam Shapira et.al.|[2410.05254](http://arxiv.org/abs/2410.05254)|**[link](https://github.com/eilamshapira/GLEE)**|**大型语言模型（LLMs）在经济与战略互动领域展现出巨大潜力，因为这些领域通常以自然语言沟通为主。这引发了一系列关键问题：LLMs是否表现出理性行为？它们能否模仿人类行为？它们是否倾向于达到高效和公平的结果？自然语言在策略互动中的角色是什么？经济环境的特性如何影响这些动态？这些问题对于将基于LLM的代理集成到现实世界的数据驱动系统（如在线零售平台和推荐系统）中时的经济和社会影响至关重要。尽管机器学习社区一直在探索LLMs在多代理设置中的潜力，但不同研究之间的假设、设计选择和评估标准差异使得很难得出稳健且有意义的结论。为解决这一问题，我们提出了一种标准化研究基于双人、序列、语言驱动游戏的标准框架。受经济学文献启发，我们定义了三个基本游戏家族，具有一致的参数化、自由度和用于评估代理性能（自我收益）以及游戏结果（效率和公平性）的经济指标。  我们开发了一个开源框架来模拟交互和分析，并利用它收集了LMM对LMM交互的大量数据集以及额外的人类对LMM交互数据集。通过广泛的实验，我们展示了我们的框架和数据集如何被用来：  (i) 比较基于LLM的代理与人类玩家在各种经济背景下的行为； (ii) 从个体和集体层面评估代理的性能； (iii) 定量分析经济环境特性对代理行为的影响。**|
|**2024-10-07**|**Causal Micro-Narratives**|Mourad Heddaya et.al.|[2410.05252](http://arxiv.org/abs/2410.05252)|null|我们提出了一种新颖的方法来对文本中的因果微叙事进行分类。这些叙事是关于目标主体的因果解释的句子级描述。该方法仅需要针对特定主题的因果和效果的本体，我们通过应用到通货膨胀叙事中进行了示范。利用覆盖美国历史和当代新闻文章的人工标注数据集进行训练，我们在多标签分类任务上评估了几种大型语言模型（LLMs）。表现最好的模型——微调后的Llama 3.1 8B，在叙事检测上达到F1得分为0.87，在叙事分类上达到F1得分为0.71。全面的错误分析揭示了语义歧义带来的挑战，并指出模型错误往往反映了人工注释者的分歧。这项研究建立了一个从实际数据中提取因果微叙事的框架，具有广泛的社会科学研究应用前景。|
|**2024-10-07**|**SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe**|Yuxin Xiao et.al.|[2410.05248](http://arxiv.org/abs/2410.05248)|null|为了在交互驱动任务中诱导大型语言模型（LLM）展现出期望的行为，通常采用指令-调优阶段，通过下一个词预测（NTP）损失训练LLM于指令响应对。先前的工作旨在提升调优性能，常着重于高质量的监督微调（SFT）数据集的构建，这通常需要昂贵的数据过滤过程或人力密集型的人工注释。然而，这些方法并未充分利用数据集的内在特性，导致了高昂的计算和劳动成本，限制了可扩展性和性能提升。本文提出了一种名为SFTMix的新颖方法，它超越了传统NTP范式，无需精心设计的SFT数据集即可提升调优性能。  观察到LLM在语义表示空间中表现出不均匀的置信度分布，我们提出，不同置信度级别的示例在调优过程中应扮演不同的角色。基于这一见解，SFTMix利用训练动态来识别具有不同置信度级别的示例，然后应用基于Mixup的正则化来减少对高置信度示例的过拟合，同时传播监督信号以改善相对低置信度示例的学习效果。这种方法使得SFTMix能够在广泛的操作指令遵循和医疗保健领域的特定SFT任务中显著超越NTP，证明了其对不同LLM家族和任意大小数据集的适应性和可扩展性。全面的消融研究进一步验证了SFTMix设计选择的稳健性，强调了其在不同LLM和数据集上的一致性能提升能力，适用于更广泛的自然语言处理应用。|
|**2024-10-07**|**Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents**|Boyu Gou et.al.|[2410.05243](http://arxiv.org/abs/2410.05243)|**[link](https://github.com/OSU-NLP-Group/UGround)**|本论文探讨了多模态大型语言模型（MLLMs）如何重塑图形用户界面（GUI）代理的能力，使其从受控模拟向跨平台的复杂现实世界应用过渡。然而，这些代理的有效性在很大程度上取决于其固有性的稳健性。当前的GUI代理主要依赖于基于文本的表示，如HTML或可访问性树，尽管它们具有实用性，但往往引入噪声、不完整性以及增加计算开销。  我们的观点是，为GUI代理构建一种类似人类的体现，能够完全通过视觉感知环境，并直接对GUI执行像素级操作。关键在于视觉定位模型，它们能够准确地将GUI元素的各种引用表达映射到其在不同平台上的GUI坐标上。我们表明，一个简单的配方——包括基于网络的合成数据和对LLaVA架构的轻微调整——对于训练这样的视觉定位模型是出奇有效的。  我们收集了迄今为止最大的GUI视觉定位数据集，包含10M个GUI元素及其引用表达，覆盖了1.3M张截图，以此来训练UGround，这是用于GUI代理的强大通用视觉定位模型。在六个跨三个类别（定位、离线代理和在线代理）的基准测试上，实验结果显示出以下两点：  1）UGround显著优于现有GUI代理的视觉定位模型，绝对性能提升高达20%。 2）使用UGround的代理在性能上超越了最先进的代理，尽管现有的代理使用额外的基于文本的输入，而我们的代理仅依赖于视觉感知。  这些结果强有力地支持了这样一种设想：即像人类一样在数字世界中导航的GUI代理是可行的，并且充满了潜力。|
|**2024-10-07**|**GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models**|Iman Mirzadeh et.al.|[2410.05229](http://arxiv.org/abs/2410.05229)|null|大型语言模型（LLM）的最新进展引发了对它们在数学推理能力上的关注，特别是针对小学水平问题。GSM8K基准测试广泛用于评估模型在这一领域的表现。尽管LLM在GSM8K上的成绩近年来显著提高，但其数学推理能力是否真正有所提升仍然存在疑问，这使得现有评估指标的可靠性受到质疑。为了应对这些问题，我们进行了一项大规模研究，涵盖了当前最前沿的开放和封闭模型。为了克服现有评估方法的局限性，我们引入了GSM-Symbolic改进版基准，该基准基于符号模板生成了多样化的题目。GSM-Symbolic使得评估更加可控，提供了关键洞察和更可靠的指标来衡量模型的推理能力。  我们的发现揭示了LLM在回答不同版本同题时表现出明显的差异性。具体而言，在GSM-Symbolic基准中，仅改变问题中的数值后，所有模型的表现都会下降。此外，我们研究了这些模型在数学推理方面的脆弱性，并表明随着问题中条目数量的增加，其性能会显著降低。我们推测，这是因为当前的LLM无法执行真正的逻辑推理；它们只是复制了训练数据中的推理步骤。即使添加一个看似与问题相关的单个条目，所有最先进的模型的表现也会大幅下降（高达65%），尽管这个条目实际上并不贡献于完成答案所需的关键推理链。总之，我们的工作为理解LLM在数学推理上的能力和局限性提供了一个更为细致的视角。|
|**2024-10-07**|**Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates**|Avanika Narayan et.al.|[2410.05224](http://arxiv.org/abs/2410.05224)|null|本文介绍了一种名为Cookbook的框架，该框架通过编程方式生成训练数据，数据主要由随机标记的简单模式组成。这种方法在规模和成本方面具有优势，且避免了与人类或大型语言模型（LLM）生成数据相关的法律和隐私问题。首先，Cookbook利用数据生成Python函数模板来产生鼓励模型学习与特定任务相匹配的显式规则的训练数据。研究发现，使用Cookbook生成的数据进行微调能够显著提高模型在对应任务上的表现，最高可达52.7个准确性点。其次，由于指令数据集能够同时改善多个下游任务的表现，Cookbook算法自动学习如何混合来自不同模板的数据以优化多个任务的性能。在标准的多任务GPT4ALL评估套件上，使用Cookbook生成的数据集进行微调的Mistral-7B模型在平均准确性和三个任务中的三个上均取得最佳成绩。最后，分析了Cookbook为何能提高性能以及其背后的原理，并提出了一项指标来验证改进的主要原因是模型生成的结果更好地遵循了模板规则。|
|**2024-10-04**|**Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models**|Zhuochun Li et.al.|[2410.03663](http://arxiv.org/abs/2410.03663)|null|本文介绍了一种名为“Mistake-Aware Peer-Review Distillation”（MAPD）的创新方法。该方法旨在通过改进开源小型模型的知识提炼（KD）过程来提高它们的性能，这些过程通常依赖于大型商业语言模型作为教师。与以往研究仅使用单一教师生成的黄金理据进行训练不同，MAPD方法采取了更为细致的策略：  1. **个性化错误反馈**：MAPD不仅要求教师提供学生答案的正确理据，更进一步地，它让教师指出学生的错误并解释原因，从而生成定制化的教学数据。  2. **模拟同行评审**：通过设计一个教师间的模拟同行评审过程，MAPD筛选出那些达到一定接受标准的生成理据。这一机制减少了教师因猜测而给出错误理据的可能性，从而提高了教学数据的质量。  本文在数学、常识和逻辑推理任务上进行了全面的实验和分析，验证了MAPD方法的有效性。|
|**2024-10-04**|**RAFT: Realistic Attacks to Fool Text Detectors**|James Wang et.al.|[2410.03658](http://arxiv.org/abs/2410.03658)|**[link](https://github.com/jameslwang/raft)**|本文提出了一种针对现有大型语言模型检测器的语法无误的黑盒攻击方法，称为RAFT。与之前针对语言模型的攻击不同，RAFT方法利用了词级上的LLM嵌入的可迁移性，同时保持原始文本质量不变。通过利用辅助嵌入，RAFT贪婪地选择需要扰动的目标单词，以对抗特定的检测器。实验结果表明，RAFT攻击能够有效地使所有研究中的检测器在各种领域中失效高达99%，并且具有跨源模型的可移植性。手动的人类评估研究表明，RAFT生成的攻击实例既真实又难以与原创人类编写文本区分开来。此外，我们还展示了RAFT生成的例子可以用来训练鲁棒性更强的检测器。我们的工作揭示了当前的LLM检测器并非具有鲁棒性，强调了迫切需要更强大的检测机制的必要性。|
|**2024-10-04**|**Aligning LLMs with Individual Preferences via Interaction**|Shujin Wu et.al.|[2410.03642](http://arxiv.org/abs/2410.03642)|**[link](https://github.com/shujinwu-0814/aloe)**|**随着大型语言模型（LLMs）展现出日益先进的能力，确保它们的行为与人类价值观和偏好保持一致对于广泛采用这些模型变得至关重要。尽管先前的研究主要集中在遵循诸如帮助性、无害性和诚实性等一般原则上，但忽视了考虑到个人和多样性偏好的需求，这可能削弱了个性化的人类体验。为了填补这一空白，我们训练了一种能够“交互以对齐”的LLMs，即让LLMs发展出一种隐式推断当前用户未明确表达的个性化偏好的元技能，并据此动态调整后续行为和响应以适应这些推断的偏好。我们的方法包括建立一个由3,310个不同用户人设组成的多样化池，通过初始示例创建，然后通过迭代自我生成和筛选进行扩展。在不同用户人设的指导下，我们利用多LLM协作开发了一个包含3K+多轮对话的树形结构多轮偏好数据集。最后，我们使用监督微调和强化学习对数据集进行了增强，以提高LLMs的能力。为了评估，我们建立了ALOE（ALign With CustOmized PrEferences）基准，包含100个精心挑选的例子以及用于衡量对话中个性化对齐性能的适当度量标准。实验结果表明，我们的方法在通过互动实现动态、个性化的对齐方面非常有效。**|
|**2024-10-04**|**Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation**|Jie Xiao et.al.|[2410.03613](http://arxiv.org/abs/2410.03613)|null|随着大型语言模型（LLM）在我们工作和日常生活的各个方面日益普及，对用户隐私的关注推动了这些模型本地部署的趋势。存在一些轻量级LLM（例如Gemini Nano，LLAMA2 7B），它们可以在智能手机上本地运行，为用户提供对其个人数据的更大控制权。作为一项迅速发展的应用，我们关注它们在商用移动设备上的性能。  为了全面了解LLM在移动平台上的部署现状，我们进行了一项全面的测量研究。我们评估了影响用户体验的指标，包括令牌吞吐量、延迟和电池消耗，以及对开发者至关重要的因素，如资源利用、动态电压频率缩放策略和推理引擎。此外，我们详细分析了硬件能力和系统动力学如何影响本地设备上的LLM性能，这可能有助于开发者识别并解决移动LLM应用程序中的瓶颈。我们还提供了针对主要供应商的移动系统级芯片（SoC）的全面比较，突出了它们在处理LLM工作负载时的性能差异。我们希望这项研究能够为本地设备LLM的开发和未来移动系统架构的设计提供洞察。|
|**2024-10-04**|**TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation**|Jonathan Cook et.al.|[2410.03608](http://arxiv.org/abs/2410.03608)|null|在大型语言模型（LLM）的广泛应用背景下，构建灵活且可解释的评估其遵循指令能力的方法至关重要。目前，偏好判断成为了评估标准的默认选择，尽管这种做法简化了复杂、多维偏好的提炼，将其归结为单一排名。然而，随着人工注释的缓慢和成本高昂，LLM被越来越多地用于做出这些判断，这牺牲了可靠性和可解释性。为此，我们提出了TICK（针对特定指令的结构化评估与核查清单），这是一种全自动化、可解释的评估方案，通过LLM生成的、针对指令的核查清单结构化评估。  首先，我们展示了，在给定指令的情况下，LLM能够可靠地产生高质量、定制化的评估核查清单，将指令分解为一系列是/否问题。每个问题询问候选回应是否满足指令的具体要求。我们证明使用TICK能够显著提高LLM判断与人类偏好之间精确一致性的频率，相比直接由LLM评分输出，这一比例从46.4%提升至52.2%。  接着，我们展示了STICK（自我TICK）可以利用自我细化和最佳中的N选择来改善多个基准的生成质量。对LiveBench推理任务进行STICK自我细化，实现了绝对增益+7.8%，而使用STICK进行最佳中的N选择在真实世界指令数据集WildBench上获得了+6.3%的绝对改进。这表明，结构化的、多维度的自我改进是进一步提升LLM能力的一个有前景的方向。  最后，通过向直接为WildBench指令评估LLM响应的人类评估者提供LLM生成的核查清单，我们显著提高了评估者之间的共识度（从0.194提升至0.256）。|
|**2024-10-04**|**Efficiently Identifying Watermarked Segments in Mixed-Source Texts**|Xuandong Zhao et.al.|[2410.03600](http://arxiv.org/abs/2410.03600)|null|文本水印在大型语言模型（LLM）中的应用日益增长，用于检测合成文本，以缓解虚假新闻和学术不诚实等滥用情况。现有水印检测技术主要关注于对整个文档进行分类，判断其是否被水印标记，但往往忽略了在更长的混合来源文档中识别单独水印段落的常见场景。受到抄袭检测系统的启发，我们提出了两种新型方法进行部分水印检测。首先，我们开发了一种几何覆盖检测框架，旨在确定长文本中是否存在水印段落。其次，我们引入了一个自适应在线学习算法，以准确定位文本中的水印段落位置。在三种流行的水印技术（KGW-Watermark、Unigram-Watermark 和 Gumbel-Watermark）上进行了评估，我们的方法取得了高精度，并显著优于基线方法。此外，我们的框架具有适应其他水印技术的能力，提供了精确水印检测的新见解。|
|**2024-10-04**|**Understanding Reasoning in Chain-of-Thought from the Hopfieldian View**|Lijie Hu et.al.|[2410.03595](http://arxiv.org/abs/2410.03595)|null|大型语言模型在各类任务中展现出非凡能力，链式思考（Chain-of-Thought, CoT）提示作为一种提升推理能力的关键技术逐渐受到关注。然而，现有研究主要集中在提高性能方面，缺乏对CoT成功背后根本因素的全面解释框架。为了填补这一空白，我们提出了一种基于认知神经科学中的霍普菲尔德认知观的新视角。我们建立了一个链接CoT推理与刺激、动作、神经群体和表示空间等关键认知元素之间的关系框架。从这一视角出发，我们可以理解推理过程实质上是这些表示空间之间的移动。  基于此洞察，我们开发了一种方法来定位CoT响应中的推理错误。此外，我们提出了一个名为“思考的表示”（Representation-of-Thought, RoT）的框架，利用低维表示空间的鲁棒性来增强CoT推理过程的鲁棒性和可解释性，并提供了对推理过程进行精细控制的能力。实验结果表明，RoT不仅提高了CoT推理的鲁棒性和可解释性，而且提供了对推理过程进行精细化控制的可能性。|
|**2024-10-04**|**Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models**|Xin Zou et.al.|[2410.03577](http://arxiv.org/abs/2410.03577)|**[link](https://github.com/1zhou-Wang/MemVR)**|尽管大型多模态语言模型（MLLMs）具有令人印象深刻的性能，但它们容易出现幻觉，特别是在视觉输入中不存在关键细节时，会夸张地编造内容。为了解决这一挑战，我们遵循了人类认知过程中的一个常见步骤——当对现场关键细节的记忆逐渐模糊时，直观的做法是再次查看这些细节以寻求准确和真实的信息。因此，我们引入了一种名为“记忆空间视觉重读”（MemVR）的新型幻觉缓解范式，它无需外部知识检索或额外的微调。具体而言，我们将视觉提示作为补充证据，通过前馈网络（FFN）注入到MLLMs中作为键值记忆，当模型对问题相关的视觉记忆不确定甚至遗忘时。全面的实验评估表明，MemVR在各种MLLMs上显著缓解了幻觉问题，并且在不增加时间开销的情况下，在通用基准测试中表现出色，从而突显出其广泛适用性的潜力。|
|**2024-10-04**|**Towards Linguistically-Aware and Language-Independent Tokenization for Large Language Models (LLMs)**|Abrar Rahman et.al.|[2410.03568](http://arxiv.org/abs/2410.03568)|null|本文对当前顶级大型语言模型（LLMs）采用的分词技术进行了全面研究，并探讨了这些技术在不同语言尤其是资源匮乏语言服务成本与可用性方面的潜在影响。研究考虑了多种LLMs，包括使用cl100k_base嵌入的GPT-4、使用p50k_base嵌入的GPT-3以及使用r50k_base嵌入的DaVinci，同时对比了广泛使用的BERT基础分词器。研究分析了这些模型之间的分词差异，并深入探究了子词分词在语言表示上的挑战。  研究强调了培养语言意识开发实践的重要性，特别是针对那些传统上资源不足的语言。此外，本文还通过案例研究展示了分词选择在实际应用中的影响，特别是在电子健康记录（EHR）系统中的应用。研究旨在促进AI服务领域，特别是跨语言环境中的通用化国际化（I18N）实践，特别关注被现有AI应用严重忽视的语言的包容性发展。|
|**2024-10-04**|**Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding**|Wei Wu et.al.|[2410.03553](http://arxiv.org/abs/2410.03553)|null|蛋白质作为生物分子的核心，在生物过程中扮演着关键角色，包括代谢反应和DNA复制。准确预测它们的性质和功能对生物应用至关重要。最近开发的蛋白质语言模型（pLMs）通过监督微调提供了解决问题的有希望的方法。然而，微调的模型仅针对特定下游预测任务进行定制，实现通用的蛋白质理解仍然是一个挑战。为此，我们引入了结构增强的蛋白质指令调谐（SEPIT）框架来填补这一空白。我们的方法在pLMs中集成了一个新颖的结构感知模块，以提供有关结构的知识，并将这些增强的pLMs与大型语言模型（LLMs）连接起来，以生成蛋白质的理解。在这个框架中，我们提出了一个新颖的两阶段指令调谐管道，首先通过基于图标的指令建立蛋白质的基本理解，然后使用专家混合（MoEs）学习更复杂属性和功能信息，同时保持激活参数的数量相同。此外，我们构建了迄今为止最大的最全面的蛋白质指令数据集，这使我们能够训练和评估通用的蛋白质理解模型。广泛的经验结果在开放式生成和封闭集合答案任务上显示了SEPIT相对于闭源通用LLM和使用蛋白质知识训练的开源LLM的优越性能。|
|**2024-10-03**|**FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models**|Zhipei Xu et.al.|[2410.02761](http://arxiv.org/abs/2410.02761)|**[link](https://github.com/zhipeixu/fakeshield)**|生成式AI的快速发展犹如一把双刃剑，既促进了内容创作，也使得图像编辑和难以辨识变得更加便捷。当前的图像伪造检测与定位（IFDL）方法虽然在一定程度上有效，但仍然面临两个主要挑战：1）黑盒性质，即无法知晓其检测原理；2）对不同伪造技术（如Photoshop、DeepFake、AIGC-Editing等）的泛化能力有限。为了应对这些问题，我们提出了可解释的IFDL任务，并设计了具有多模态能力的框架——FakeShield。该框架旨在评估图像的真实性，生成篡改区域的掩模，并基于像素级和图像级的篡改线索提供判断依据。此外，我们利用GPT-4o增强了现有的IFDL数据集，创建了多模态篡改描述数据集（MMTD-Set），用于训练FakeShield的篡改分析能力。同时，我们引入了域标签引导的可解释伪造检测模块（DTE-FDM）和多模态伪造定位模块（MFLM），以应对各种伪造检测解释和实现由详细文本描述指导的伪造定位。  通过广泛的实验验证，FakeShield有效地检测和定位了各种篡改技术，提供了比以往IFDL方法更可解释且性能更优的解决方案。|
|**2024-10-03**|**Loong: Generating Minute-level Long Videos with Autoregressive Language Models**|Yuqing Wang et.al.|[2410.02757](http://arxiv.org/abs/2410.02757)|null|在生成时长达到数分钟的丰富内容视频方面，尽管具有挑战性但前景广阔。自回归大型语言模型（LLMs）在自然语言处理领域生成连贯且长度较长的令牌序列方面取得了巨大成功，而在探索使用自回归LLMs进行视频生成时，主要局限于生成几秒钟的短视频。本文对阻止基于自回归LLM的视频生成器生成长时间视频的挑战进行了深入分析。基于观察和分析结果，我们提出了一种新的基于自回归LLM的视频生成器“Loong”，能够生成长达数分钟的视频。具体而言，我们将文本令牌和视频令牌统一为自回归LLM可以进行自回归建模的序列，并从零开始训练模型。我们提出了渐进式短至长训练和损失重新加权方案，以缓解长期视频训练中的损失不平衡问题。此外，我们还研究了推理策略，包括视频令牌重编码和采样策略，以减少推理过程中累积的误差。我们的提出的“Loong”可以从10秒的视频进行训练，并扩展到根据文本提示生成数分钟级别的长视频，如结果所示。更多示例请访问：https://epiphqny.github.io/Loong-video。|
|**2024-10-03**|**SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at 1% the Cost**|Jifan Zhang et.al.|[2410.02755](http://arxiv.org/abs/2410.02755)|null|本文提出了一种名为SIEVE的轻量级替代方案，该方案在成本仅为GPT-4o单次过滤调用的十分之一的情况下，仍能与GPT-4o的准确性相匹配。SIEVE的核心在于将GPT-4o和轻量级T5模型无缝集成，并使用主动学习方法在少量GPT-4o调用的支持下对T5进行微调。一旦训练完成，SIEVE的表现与GPT-4o相当，但成本却低得多（仅为现有技术的1%）。我们在OpenWebText数据集上进行了实验，针对高质量和领域特定内容的五个高度定制化的过滤任务验证了SIEVE的有效性和效率。  进一步验证SIEVE的效果显示，SIEVE和GPT-4o在准确性方面达到相似水平，而人类评估者更倾向于SIEVE的过滤结果而非GPT-4o的结果。|
|**2024-10-03**|**Training Language Models on Synthetic Edit Sequences Improves Code Synthesis**|Ulyana Piterbarg et.al.|[2410.02749](http://arxiv.org/abs/2410.02749)|**[link](https://github.com/upiterbarg/lintseq)**|本文开发了一种名为LintSeq的合成数据生成算法。该算法通过使用代码检查器来程序化地在不引入错误的情况下随机选取插入操作序列，从而对现有代码进行重构，生成一系列代码编辑序列。这些序列以连续的程序差异形式输出。  为了测试LintSeq，我们将其应用于将指令+程序对重新格式化为指令+程序差异序列对的代码库。然后，我们对参数从2.6B到14B的多个较小的语言模型进行了基于指令的微调，比较了在原始版本和重新格式化版本数据集上的零次射击性能在代码合成基准上的表现。结果显示，在多次采样期间，经过代码差异微调的模型产生的程序多样性高于基线。这导致了在给定尝试次数“k”时，针对基准覆盖率的推理时间扩展性更好，即解决任何问题的概率“pass@k”。例如，在HumanEval pass@50上，较小模型在经过合成代码编辑序列微调后与GPT-4相比具有竞争力，并且优于基于基线数据集微调的模型，绝对得分高出20%（±3%）。  最后，我们还预训练了自己的小型模型用于代码理解。结果表明，对小型模型进行基于合成代码编辑的微调可以达到类设备模型的最高代码合成性能。我们的1.5亿参数编辑序列模型在性能上匹配或超越了参数量翻倍的代码模型，无论是否进行多次采样，包括Codex和AlphaCode。|
|**2024-10-03**|**CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation**|Han He et.al.|[2410.02748](http://arxiv.org/abs/2410.02748)|**[link](https://github.com/amazon-science/crispo)**|本文探讨了利用从源文档中提取的显著信息增强总结提示的方法。我们证明，在提示中加入关键短语可以提高ROUGE F1和召回率，使生成的摘要与参考摘要更相似且更完整。关键短语的数量可以控制精确度和召回率之间的权衡。进一步的分析显示，融入短语级别的显著信息优于基于单词或句子级别的信息。然而，这种方法对幻觉的影响并非在所有大型语言模型（LLM）上都是积极的。为了进行这项分析，我们引入了轻量级模型Keyphrase Signal Extractor（CriSPO），该模型可以微调以提取显著的关键短语。通过使用CriSPO，我们在数据集、开源和专有LLM上实现了对ROUGE改进的一致性，无需对LLM进行定制。我们的发现为构建基于提示的总结系统时利用显著信息提供了见解。|
|**2024-10-03**|**Contrastive Localized Language-Image Pre-Training**|Hong-You Chen et.al.|[2410.02746](http://arxiv.org/abs/2410.02746)|null|本文针对对比语言-图像预训练（CLIP）作为视觉语言基础模型的成功，重点在于通过在图像级别上对齐网络文本注释来优化视觉编码器。然而，这种策略在需要细粒度视觉表示的下游任务中可能变得不够充分，尤其是当多模态大型语言模型（MLLMs）需要进行区域级理解时。本文提出了一种名为对比定位语言-图像预训练（CLOC）的方法，通过补充CLIP以增加区域文本对比损失和模块来提升其定位能力。我们引入了一个新的概念，即可提示嵌入，其允许编码器生成易于通过空间提示转换为区域表示的图像嵌入。为了支持大规模预训练，设计了一个视觉增强且空间局部化的描述符生成框架，能够有效生成大规模的区域文本伪标签。通过扩展到数十亿标注图像，CLOC使得图像区域识别和检索任务中的高质量区域嵌入成为可能，并可以作为CLIP的直接替代品，用于增强MLLMs，特别是在指代和上下文理解任务中。|
|**2024-10-03**|**Neutral residues: revisiting adapters for model extension**|Franck Signe Talla et.al.|[2410.02744](http://arxiv.org/abs/2410.02744)|null|我们解决了一个新的问题：如何将预训练的大规模语言模型扩展到在训练时未曾见过的领域，例如添加一种原始模型未见过或见过很少训练数据的语言。流行的解决方案如微调或低秩适应在领域适应方面取得成功，但它们实际上并未增加额外的能力，并且降低了原始领域的性能。本文从三个角度分析了这个问题：数据、架构和训练过程，这些都被有利地联合考虑。特别是，我们改进了适配器，并使其有可能学习一个全新的语言，同时确保神经网络在原始领域的输出几乎不变。为此，我们修改了新的残差块的方式，使得每个新的残差块在原始领域输出接近零的结果。  这种被称为“中性残差”的解决方案借鉴了混合专家架构的组件，效果显著：与仅用英语训练的原始模型相比，只需要额外20%的学习权重，我们的方法在学习新语言和不忘记英语之间的权衡上取得了显著优于同时进行的其他方法（微调、低秩或常规适配器）的结果。|
|**2024-10-03**|**MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions**|Yekun Chai et.al.|[2410.02743](http://arxiv.org/abs/2410.02743)|**[link](https://github.com/ernie-research/ma-rlhf)**|强化学习从人类反馈（RLHF）已经证明了在使大型语言模型（LLMs）与人类偏好保持一致方面具有有效性。然而，基于token的RLHF面临着长期序列中的责任归因问题，其中延迟奖励使得模型难以确定哪些操作导致了成功的结果，这阻碍了学习效率并减慢了收敛速度。在这篇论文中，我们提出了一种名为MA-RLHF的简单而有效的RLHF框架，它将宏动作——一系列token或更高层次的语言构造——融入到学习过程中。通过在更高抽象级别上操作，我们的方法减少了行动和奖励之间的时序距离，从而促进了更快且更准确的责任归因。这导致了更稳定的策略梯度估计，并提高了每个episode内的学习效率，所有这些都无需在训练或推理期间增加计算复杂性。我们通过在文本摘要、对话生成、问题回答和程序合成等各个模型大小和任务上进行的大量实验验证了我们的方法。我们的方法在文本摘要和代码生成任务中实现了高达30%的性能提升，在对话任务中实现了18%，在问题回答任务中实现了8%。值得注意的是，我们的方法比标准的RLHF快1.7至2倍的训练时间达到与之相匹敌的性能水平，并且随着进一步的训练，继续超越它。我们将提供我们的代码和数据，供公众访问，网址为https://github.com/ernie-research/MA-RLHF 。|
|**2024-10-03**|**Grounding Large Language Models In Embodied Environment With Imperfect World Models**|Haolan Liu et.al.|[2410.02742](http://arxiv.org/abs/2410.02742)|null|尽管大型语言模型（LLMs）在各种应用中取得了广泛的成功，但它们在处理基本物理推理或执行机器人任务时经常遇到困难，这主要是由于它们缺乏对现实世界物理细节的直接经验。为了解决这些问题，我们提出了一种名为Grounding Large language model with Imperfect world MOdel (GLIMO)的方法，该方法利用代理世界模型，如模拟器，来收集和合成训练数据。GLIMO整合了一个基于LLM的自动数据生成器，用于创建高质量且多样化的指令数据集。生成器包括一个用于时间一致性体验采样的迭代自我精炼模块、一组多样化的问答指令种子，以及一个反思先前经验的检索增强生成模块。  全面的实验结果显示，我们的方法能够显著提升强开源LLMs（如LLaMA-3）的表现，分别在三个不同基准上实现了2.04倍、1.54倍和1.82倍的性能提升。其性能能够与或超越其更大的同辈模型，如GPT-4。|
|**2024-10-03**|**Salient Information Prompting to Steer Content in Prompt-based Abstractive Summarization**|Lei Xu et.al.|[2410.02741](http://arxiv.org/abs/2410.02741)|**[link](https://github.com/amazon-science/SigExt)**|本文探讨了利用源文档中提取的显著信息来增强生成提示以改进大型语言模型（LLM）的摘要能力。我们发现，在提示中加入关键短语能提升ROUGE F1和召回率，使得生成的摘要与参考摘要更加相似且更完整。通过调整关键短语的数量，可以控制精确度和召回率之间的权衡。进一步分析显示，将短语级的显著信息融入提示优于基于单词或句子的策略。然而，这并不意味着对所有LLM都普遍有益，特别是在减少幻觉方面。为了进行这一分析，我们引入了轻量级的Keyphrase Signal Extractor（SigExt）模型，该模型可进行微调以提取关键短语。通过使用SigExt，我们在多个数据集、公开权重和专有LLM上实现了不依赖于LLM定制的ROUGE指标改善效果。我们的研究结果为构建基于提示的摘要系统时利用显著信息提供了见解。|
|**2024-10-02**|**Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads**|Yuxiang Huang et.al.|[2410.01805](http://arxiv.org/abs/2410.01805)|**[link](https://github.com/huangyuxiang03/Locret)**|**大型语言模型（LLMs）在支持长期上下文理解和处理任务方面取得了显著进步。然而，将LLMs的生成推理扩展到如此长的上下文会增加大量的计算负载，并要求在维持基于转换器的LLMs的关键值对（KV）缓存时使用大量GPU内存。现有的KV缓存压缩方法，如量化，随着上下文长度的增加而遇到内存瓶颈；而固定大小的缓存，如淘汰策略，则由于不高效的策略而导致效率低下。这些限制限制了在单个Nvidia 4090 GPU等消费者级设备上的部署。  为了克服这一挑战，我们提出了Locret框架，这是一种用于长上下文LLM推理的方法，通过引入保留头部来评估KV缓存单元的因果重要性，从而允许在固定缓存大小内进行更准确的淘汰。Locret在冻结的主干LLM基础上进行了微调，使用标准长时间上下文SFT数据集的少量数据。在推理过程中，我们以分块预填充模式淘汰低重要性的缓存单元，显著减少了峰值GPU内存使用量。  我们进行了广泛的实证研究来评估Locret，实验结果表明，与最近的竞争方法（包括InfLLM、量化、SirLLM和MInference）相比，Locret在内存效率和生成内容质量方面均表现出色——Locret实现了与Phi-3-mini-128K和Llama-3.1-8B-instruct全KV缓存相比超过20倍和8倍的KV缓存压缩比率。此外，Locret还可以与其他方法（如量化和令牌合并）结合使用。据我们所知，Locret是第一个能够将Llama-3.1-8B或类似模型部署到单个Nvidia 4090 GPU上，同时在不牺牲生成质量的情况下实现128K长上下文推理的框架，且仅需要少量额外的系统优化。**|
|**2024-10-02**|**Efficient $1$-bit tensor approximations**|Alex W. Neal Riasanovsky et.al.|[2410.01799](http://arxiv.org/abs/2410.01799)|null|我们提出了一种空间效率高的矩阵和任意阶张量分解方法，作为线性组合的张量积形式，其中向量值为$\{-1, 1\}$。对于任一矩阵$A \in \mathbb{R}^{m \times n}$，其表达式为：$$A - R_w = S_w C_w T_w^\top = \sum_{j=1}^w c_j \cdot \mathbf{s}_j \mathbf{t}_j^\top$$ 这是一个关于$A$的“宽度为$w$的符号切分解”。这里$C_w = "diag"(\mathbf{c}_w)$，且$S_w, T_w$和向量$\mathbf{s}_j, \mathbf{t}_j$均为$\{-1, 1\}$值。用于存储$(S_w, T_w, C_w)$所需的空间是$w \cdot (m + n)$位，并仅需$w$个浮点数。当应用于具有i.i.d. $\mathcal N (0, 1)$分布元素的#f32矩阵时，$\,R_w\,_F$呈现出指数衰减。选择合适的$w$，使$(S_w, T_w, C_w)$的内存占用与\textit{f16}或\textit{bf16}矩阵相同，相对误差相当。我们的算法在20行伪代码中实现了高效的符号切分解。它源自1999年Frieze和Kannan的一篇著名论文的简单修改。  作为第一个应用，我们对开放源码大型语言模型\textit{Mistral-7B-v0.1}中的权重矩阵进行了$50\%$的空间压缩。令人惊讶的是，所有$226$个余矩阵的相对误差均小于$6\%$，且扩展模型在huggingface排行榜上与\textit{Mistral-7B-v0.1}模型表现相近。随着空间压缩率从$50\%$降低至$25\%$ ，基准性能缓慢下降。我们优化了开源的\textit{rust}实现，使用了\textit{avx2}和\textit{avx512}架构下的\textit{simd}指令进行加速。此外，我们还将该算法扩展到了任意阶张量，并利用它压缩了一张作者猫Angus的照片。  请注意，这里的文本并未包含任何特殊字符或特定格式标记，而是以纯文本形式呈现了摘要内容。|
|**2024-10-02**|**Knowledge-Driven Feature Selection and Engineering for Genotype Data with Large Language Models**|Joseph Lee et.al.|[2410.01795](http://arxiv.org/abs/2410.01795)|**[link](https://github.com/pennshenlab/freeform)**|**基于复杂遗传基础预测表型，利用小而可解释的变异特征仍然是一项具有挑战性的任务。传统上，使用数据驱动的方法进行此任务，但基因型数据的高维特性使得分析和预测变得困难。受到预训练大型语言模型（LLM）中编码的丰富知识及其在处理复杂生物医学概念上的成功启发，我们旨在探索LLM在表格基因型数据特征选择与工程方面的能力，并引入一种基于知识的框架。我们开发了FREEFORM，一种自由流动推理与集成增强特征输出和稳健建模的框架，该框架结合了链式思考与集成原则，利用LLM的内在知识来选择和工程特征。在两个不同的人类基因型-表型数据集上进行评估，包括遗传血统和遗传性听力损失，我们发现这个框架在低样本量情况下优于几种数据驱动方法。FREEFORM作为一个开源框架，可以在GitHub上获取：https://github.com/PennShenLab/FREEFORM。**|
|**2024-10-02**|**When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1**|R. Thomas McCoy et.al.|[2410.01792](http://arxiv.org/abs/2410.01792)|null|在“自动回归余烬”（McCoy等人，2023年）中，我们展示了几个大型语言模型（LLMs）在起源上存在一些重要限制，这归因于它们的下一个单词预测特性。这里我们探讨了OpenAI的新系统o1是否依然存在这些问题，与之前的LLMs相比，o1在推理优化方面有所不同。研究发现，o1在许多情况下显著优于之前模型，在某些常见任务的罕见变体上（例如，从列表中的每个词的第二个字母形成缩写，而不是第一个字母）表现尤其出色。尽管这些定量改进令人印象深刻，但o1依然显示出了与之前系统相同的基本趋势：对于概率较高的示例和任务，o1的表现更好且需要的“思考令牌”数量较少；而在概率较低的情况下则表现不佳。  这些结果表明，优化语言模型以进行推理可以减轻但可能无法完全克服语言模型的概率敏感性问题。|
|**2024-10-02**|**Investigating on RLHF methodology**|Alexey Kutalev et.al.|[2410.01789](http://arxiv.org/abs/2410.01789)|null|本文研究了大型语言模型根据人类偏好的对齐问题。我们讨论了训练偏好模型的特性，该模型模拟人类偏好，并介绍了实现最佳结果所需的方法和细节。此外，我们还探讨了使用强化学习微调大型语言模型的方法，描述了遇到的挑战以及克服这些挑战的方式。我们还提出了直接偏好优化方法的经验，这种方法允许我们将大型语言模型与人类偏好对齐，而无需创建单独的偏好模型。作为我们的贡献，我们引入了一种通过困惑度筛选收集偏好数据集的方法，这使得为特定语言模型创建这样的数据集的过程更加简便且成本效益更高。|
|**2024-10-02**|**OmniGenBench: Automating Large-scale in-silico Benchmarking for Genomic Foundation Models**|Heng Yang et.al.|[2410.01784](http://arxiv.org/abs/2410.01784)|**[link](https://github.com/yangheng95/OmniGenomeBench)**|**近年来，人工智能领域的进步，特别是大型语言模型（LLMs），激发了对基因组基础模型（GFMs）突破性进展的期待。自生命进化之初就隐藏在多样化的基因组中的“自然之码”，蕴含着巨大潜力，能够通过基因组建模对人类和生态系统产生深远影响。近期GFM领域的重要突破，如Evo，吸引了大量投资与关注，它们解决了长期存在的挑战，并将基因组研究从手动、不可靠和低效的传统模式转变为自动化、可靠和高效的新范式。在基因组学连续技术革命的背景下，GFM研究面临两大挑战：缺乏GFM基准测试工具以及多维基因组学的开源软件缺失。这些挑战阻碍了GFM快速演进及其广泛应用于理解与合成基因组等数十年来存在的问题的能力。为了应对这些挑战，我们引入了GFMBench框架，一个专注于GFM导向基准测试的平台。GFMBench标准化了基准套件，并实现了对大量开源GFMs的自动化基准测试。它集成了来自四大大型基准的数百万个基因序列，覆盖数百种基因组任务，使GFMs民主化，适用于广泛的虚拟基因组应用。此外，GFMBench作为开源软件发布，提供用户友好界面和多样化教程，适用于自动测试以及RNA设计和结构预测等复杂任务。为了促进基因组建模领域的进一步发展，我们启动了一个公共排行榜，展示由AutoBench生成的基准性能。GFMBench代表了标准化GFM基准测试和民主化GFM应用的一大步。**|
|**2024-10-02**|**Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models**|Shayekh Bin Islam et.al.|[2410.01782](http://arxiv.org/abs/2410.01782)|**[link](https://github.com/ShayekhBinIslam/openrag)**|为了提升大型语言模型（LLM）在事实准确性上的表现，检索增强生成（RAG）方法已经得到了广泛研究。然而，现有的方法往往在利用检索到的证据进行推理的能力上存在局限性，尤其是在使用开源LLM时。为了填补这一差距，我们提出了一种新颖的框架——Open-RAG，旨在增强开源LLM在RAG中的推理能力。我们的框架将任意密集型LLM转换成一个参数高效的稀疏混合专家（MoE）模型，能够处理包括单跳和多跳查询在内的复杂推理任务。  Open-RAG的独特之处在于，它通过训练模型来应对看似相关但具有误导性的干扰项，从而有效地导航复杂场景。通过利用潜学习，Open-RAG动态选择相关专家并整合外部知识，以提供更准确、更具上下文的相关响应。此外，我们还提出了一种混合自适应检索方法，用于判断检索的必要性，并平衡性能增益与推理速度之间的权衡。  实验结果显示，基于Llama2-7B的Open-RAG在各种知识密集型任务中，相较于ChatGPT、Self-RAG和Command R+等最先进的LLM和RAG模型，表现出更优的表现。我们已将代码和模型开源在https://openragmoe.github.io/。|
|**2024-10-02**|**Quantifying Generalization Complexity for Large Language Models**|Zhenting Qi et.al.|[2410.01769](http://arxiv.org/abs/2410.01769)|**[link](https://github.com/zhentingqi/scylla)**|在大型语言模型（LLMs）展现出理解复杂查询和执行高级任务的非凡能力的同时，它们的泛化能力往往与记忆深度交织在一起，这要求我们进行更精确的评估。为了应对这一挑战，我们引入了Scylla，这是一个动态评估框架，定量衡量LLMs的泛化能力。Scylla通过在分布内（ID）和分布外（OOD）数据上评估模型性能来分离泛化与记忆，涉及20个任务，覆盖5个复杂度级别。通过广泛的实验，我们揭示了任务复杂度与ID和OOD数据之间的性能差距之间非单调的关系，我们将其称为泛化山谷。具体来说，这一现象揭示了一个关键阈值——称为关键复杂性——在该阈值处，非泛化行为的依赖达到峰值，表明了LLMs泛化能力的上限。随着模型大小的增加，关键复杂性向更高层次的任务复杂度移动，表明更大的模型可以在依赖于记忆之前处理更复杂的推理任务。利用Scylla和关键复杂性的概念，我们对包括开源模型如LLaMA和Qwen家族、以及闭源模型如Claude和GPT在内的28个LLMs进行了基准测试，提供了更稳健的评估，并对LLMs的泛化能力有了更清晰的理解。|
|**2024-10-02**|**LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks**|Mengzhao Jia et.al.|[2410.01744](http://arxiv.org/abs/2410.01744)|**[link](https://github.com/jill0001/leopard)**|文本丰富的图像在实际应用中普遍存在，如幻灯片演示、扫描文档和网页快照等，其中文本作为核心视觉元素引导整体理解。多图像文本丰富的任务尤其具有挑战性，因为它们不仅需要理解单个图像的内容，还需要在多个视觉输入之间推理关系和逻辑流程。尽管这些场景的重要性，当前的多模态大型语言模型（MLLMs）在处理此类任务时遇到两个关键挑战：（1）缺乏适合于多图像文本丰富场景的高质量指令调优数据集；（2）难以平衡图像分辨率与视觉特征序列长度。为了应对这些挑战，我们提出了\OurMethod，一个专门设计用于处理涉及多文本丰富图像的视语言任务的MLLM。首先，我们收集了约一百万条针对多文本丰富、多图像场景的高质量多模态指令调优数据。其次，我们开发了一种适应性的高分辨率多图像编码模块，根据输入图像的原始纵横比和分辨率动态优化视觉序列长度的分配。在一系列广泛的基准测试中，我们的模型在多文本丰富、多图像评估中表现出优越的能力，并在通用领域评估中展现出竞争力。|
|**2024-10-02**|**VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch Diffusion Models**|Kailai Feng et.al.|[2410.01738](http://arxiv.org/abs/2410.01738)|**[link](https://github.com/carlofkl/vitaglyph)**|**本文引入了一种双分支、无需训练的新型艺术字体生成方法——VitaGlyph。该方法旨在通过灵活地表达输入字符的核心概念以及丰富相关的背景信息，实现艺术字体与可控制的几何变化之间的平衡，从而保持字体的可读性。VitaGlyph的核心理念是将输入字符视为由主体和周围环境组成的场景，并在不同几何变换程度下进行渲染。  具体来说，VitaGlyph通过以下三个阶段框架实现其功能：(i) 知识获取阶段利用大型语言模型设计主体和周围环境的文本描述；(ii) 区域分解阶段识别最匹配主体描述的部分，并将输入的字符图像分为主体和周围区域；(iii) 字体风格化阶段首先通过语义字体优化主体区域的结构，然后分别使用可控组合生成技术渲染主体和周围区域的纹理。  实验结果表明，VitaGlyph不仅在艺术性和可读性方面表现出色，还能够描绘多种定制概念，从而促进更富有创意和愉悦的艺术字体生成。项目代码将在https://github.com/Carlofkl/VitaGlyph公开提供。**|
|**2024-09-30**|**MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning**|Haotian Zhang et.al.|[2409.20566](http://arxiv.org/abs/2409.20566)|null|我们提出了一种新的多模态大型语言模型家族MM1.5，旨在增强文本丰富图像理解、视觉引用与定位以及多图像推理的能力。在MM1架构的基础上，MM1.5采用数据驱动的方法进行模型训练，系统性地探索在整个模型训练生命周期内不同数据混合的影响。这包括高质量的OCR数据和合成描述符用于持续预训练，以及优化的视觉指令调参数据混合用于监督微调。我们的模型涵盖了从1亿到30亿参数的范围，包括密集型和混合专家（MoE）变体，并证明了即使在较小规模（1亿和3亿参数）下，精心的数据整理和训练策略也能产生强大的性能。此外，我们引入了两个专门的变体：MM1.5-Video，用于视频理解；MM1.5-UI，用于移动用户界面理解。通过广泛的实证研究和消融分析，我们提供了关于训练过程和决策的详细见解，这些见解对于未来多模态大型语言模型的发展具有宝贵的指导意义。|
|**2024-09-30**|**Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in Instructional Videos**|Md Mohaiminul Islam et.al.|[2409.20557](http://arxiv.org/abs/2409.20557)|null|本文提出了VidAssist，一个用于从教学视频中进行零样本或少量样本的目标导向规划的集成框架。VidAssist利用大型语言模型（LLM）作为知识库和评估工具，生成并评估行动计划，以此克服从小规模、低多样性数据集获取过程知识的挑战。此外，VidAssist采用广度优先搜索算法进行最优计划生成，并使用专为目标导向规划设计的价值函数，在每一步评估预测动作。广泛实验表明，VidAssist提供了一个适用于不同目标导向规划设置的统一框架，如视觉辅助规划（VPA）和程序规划（PP），在零样本和少量样本设置下表现出卓越性能。具体而言，我们的少量样本模型在COIN数据集上的VPA任务和PP任务上分别比全监督的前导方法高出+7.7%和+4.81%，同时预测4个未来动作。所有代码和模型都在https://sites.google.com/view/vidassist公开提供。|
|**2024-09-30**|**LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation**|Ziyao Zhang et.al.|[2409.20550](http://arxiv.org/abs/2409.20550)|null|本文提出了一项针对大型语言模型（LLM）在代码生成任务中的幻觉现象的实证研究。尽管LLM在代码生成任务上的表现令人鼓舞，但它们在处理实际开发过程中复杂的上下文依赖关系时，往往会产生错误或不准确的结果。以往的研究主要关注于基于LLM的代码生成在单一功能生成场景下的幻觉分析，但本文将研究范围扩展至更实际且复杂的仓库级生成情景。  首先，通过人工检查六种主流LLM的代码生成结果，本文建立了LLM生成代码的幻觉分类体系。接下来，详细阐述了幻觉现象，并分析了不同模型间幻觉分布的情况。进一步地，本文探讨了幻觉产生的原因，并识别了四个可能导致幻觉的因素。  最后，提出了一种基于记忆网络（RAG）的缓解方法，该方法在所有研究的LLM上均表现出一致的有效性。提供了一个包括代码、数据和实验结果的可复制包，供学术界和工业界参考和验证。此研究有助于提高LLM在代码生成任务中的可靠性与准确性，对软件工程领域具有重要意义。|
|**2024-09-30**|**Robi Butler: Remote Multimodal Interactions with Household Robot Assistant**|Anxing Xiao et.al.|[2409.20548](http://arxiv.org/abs/2409.20548)|null|在这篇论文中，我们引入了Robi Butler，一种新型的家庭机器人系统，它能够与远程用户进行多模态交互。基于先进的通信接口，Robi Butler允许用户监控机器人的状态、发送文本或语音指令，并通过手势选择目标对象。我们的系统的核心是一个由大型语言模型（LLMs）驱动的高级行为模块，该模块能够解释多模态指令并生成行动计划。这些计划由支持文本和点击查询的视觉语言模型（VLMs）处理的开放词汇集组成。整合以上组件使得Robi Butler能够在零样本的情况下将远程多模态指令转化为现实世界家庭环境中的实际操作。我们通过演示各种日常家务任务的有效性和效率，展示了该系统的应用，这些任务涉及到远程用户给出多模态指令。此外，我们还进行了用户研究，分析了多模态交互对远程人机交互的效率和用户体验的影响，并讨论了可能的改进措施。|
|**2024-09-30**|**Uncertainty-Informed Screening for Safer Solvents Used in the Synthesis of Perovskite via Language Models**|Arpan Mukherjee et.al.|[2409.20512](http://arxiv.org/abs/2409.20512)|null|本文提出了一种创新框架，旨在解决准确预测工业合成中所用钙钛矿溶剂毒性这一挑战。由于缺乏针对性和结构化的毒性数据，这一任务面临局限性。该框架结合了语言模型的自动化数据提取与具有不确定性信息的预测模型，以填补数据空白并提高预测的置信度。  首先，我们采用了两种方法从涉及钙钛矿合成溶剂的科学文献语料库中自动提取相关数据：较小的双向语言模型（如BERT和ELMo）因其重复性和确定性输出而被使用；而自回归大型语言模型（LLM）如GPT-3.5则利用其庞大的训练语料库和更好的响应生成能力。我们的“提示和验证”技术集成到LLM中，旨在实现有针对性的提取和优化，从而减少LLM的幻觉现象，提升提取数据的质量。  接下来，提取的数据被输入到预训练的多任务二元分类深度学习模型，用于预测提取溶剂的ED性质。我们利用从分类模型获得的类别概率进行香农熵为基础的不确定性量化，以此来量化不确定性并识别预测中的数据缺口。这种方法导致构建了一个结构化的用于钙钛矿合成溶剂及其基于不确定性虚拟毒性的评估数据集。  此外，我们使用了和弦图来可视化溶剂之间的相互作用，并优先考虑那些可能存在危险的溶剂，结果发现70%的溶剂相互作用主要与特定的两种钙钛矿相关联。|
|**2024-09-30**|**COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models**|Divyanshu Daiya et.al.|[2409.20502](http://arxiv.org/abs/2409.20502)|null|我们提出了一种名为COLLAGE的新型框架，用于通过利用大型语言模型（LLM）和层次化的运动特异性向量量化变分自编码器（VQ-VAE）来生成协作式代理-对象-代理交互。我们的模型解决了这一领域数据稀缺的问题，通过整合LLM的知识和推理能力来指导生成性扩散模型。层次化的VQ-VAE架构在多个抽象级别捕获了不同的运动特异性特征，避免了冗余概念，并实现了高效的多分辨率表示。我们引入了一个在隐空间中操作的扩散模型，并结合了由LLM生成的运动规划提示来引导去噪过程，从而实现了针对特定提示的运动生成，具有更高的控制性和多样性。在CORE-4D和InterHuman数据集上的实验结果证明了我们的方法在生成真实且多样化的协作人类-物体-人类交互方面的有效性，超越了现有最佳方法。我们的工作为机器人学、图形学和计算机视觉等领域建模复杂交互提供了新的可能性。|
|**2024-10-01**|**Instance-adaptive Zero-shot Chain-of-Thought Prompting**|Xiaosong Yuan et.al.|[2409.20441](http://arxiv.org/abs/2409.20441)|null|零射链思考（CoT）提示策略在增强大型语言模型（LLM）解决现实世界推理任务的性能方面展现出简单而有效的方法。然而，单一任务级提示在整个实例上的应用存在局限性，因为一个提示无法与所有实例都成为最佳搭档。因此，更恰当的做法是精心考虑提示与每个实例之间的互动。本文提出了一种实例自适应提示算法作为零射CoT推理的一种替代策略，旨在通过适当地区分出好的和坏的提示来提升性能。  具体来说，我们首先通过信息流的角度对LLM进行分析，以揭示零射CoT推理机制，发现信息从问题到提示以及问题到推理的双向流动对推理结果影响最大。我们注意到，更优秀的零射CoT推理需要提示从问题中获取语义信息，然后推理从问题直接或通过提示间接地聚合足够信息。相反，缺失这些任何一项可能都会导致一个不理想的提示。基于此发现，我们进一步提出了一个适用于零射CoT推理的实例自适应提示策略（IAP）。  在LLaMA-2、LLaMA-3和Qwen上对数学、逻辑和常识推理任务（如GSM8K、MMLU、因果判断）进行的实验表明，实例自适应零射CoT提示策略在某些定制提示或复杂程序的基础上表现出更好的性能，这证明了我们在零射CoT推理机制研究中的发现具有重要意义。|
|**2024-09-30**|**Wait, but Tylenol is Acetaminophen... Investigating and Improving Language Models' Ability to Resist Requests for Misinformation**|Shan Chen et.al.|[2409.20385](http://arxiv.org/abs/2409.20385)|null|背景：大型语言模型（LLMs）被训练成遵循指令，但这种设计使其容易在生成错误信息时盲目遵从用户请求。在医学领域，这可能会加速错误信息的传播，从而影响人类健康。研究目标/方法：我们分析了模型在知道请求不合理的情况下，生成与药物有关误导性内容的倾向。我们探讨了通过上下文提示和调整参数，使LLMs优先考虑逻辑推理而非遵从性，以降低医疗信息误导风险的可能性。  结果：所有前沿LLMs都遵守了生成误导性内容的不合理请求。然而，基于提示的方法和参数调整策略可以提升检测请求逻辑错误的能力，并防止医疗信息的误传。  结论：将LLMs的设计重心从遵从性转向逻辑推理，有助于降低其被利用于传播医疗信息误导的风险。|
|**2024-09-30**|**The Perfect Blend: Redefining RLHF with Mixture of Judges**|Tengyu Xu et.al.|[2409.20370](http://arxiv.org/abs/2409.20370)|null|本文介绍了一种新的后训练范式，称为约束生成策略优化（CGPO）。CGPO的核心是“裁判混合”（MoJ），它以成本效益的方式对策略进行分层约束优化，从而在原理上识别RLHF中的完美融合。此方法在理论上有保证，不需要大量的超参数调整，并且可以在常见的后训练管道中无缝集成。这有助于检测和缓解奖励作弊行为，并在大量目标的场景下达到帕累托最优点。  我们的实验评估表明，CGPO在各种任务上显著优于标准的RLHF算法，如PPO和DPO，包括通用聊天、STEM问题、指令遵循和编程等。具体而言，CGPO在AlpacaEval-2（通用聊天）上提高了7.4%，在Arena-Hard（STEM与推理）上提高了12.5%，并在数学和其他领域如编程等任务上保持一致的改进。值得注意的是，虽然PPO经常被使用，但在流行的编程基准测试中，它容易遭受严重的奖励作弊，而CGPO成功地解决了这个问题。  这一突破在RLHF领域不仅解决了奖励作弊和极端多目标优化的挑战，而且推进了通用语言模型在多种应用中的对齐技术。|
|**2024-09-30**|**VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs**|Ruotong Liao et.al.|[2409.20365](http://arxiv.org/abs/2409.20365)|**[link](https://github.com/mayhugotong/videoinsta)**|在视频语言领域，利用零样本大型语言模型（LLM）推理进行视频理解的最新工作已成为挑战传统端到端模型的有力竞争者。然而，长视频的理解面临着独特的挑战，尤其是在处理持续时间较长的时间跨度时，即使是零样本LLM方法也是如此。长视频中的信息冗余问题促使我们思考哪些信息对于大型语言模型至关重要，以及如何利用它们进行复杂的空间-时间推理，以实现对长视频分析的理解。  为此，我们提出了一种名为VideoINSTA（INformative Spatial-TemporAl Reasoning）的框架，用于零样本长视频理解。VideoINSTA的主要贡献包括：（1）利用LLM进行长视频理解的零样本框架；（2）事件驱动的时间推理和基于内容的空间推理方法，使LLM能够对视频中的空间-时间信息进行推理；（3）一种自我反思的信息推理方案，通过信息充分性和预测置信度的平衡来调整时间因素。  我们的模型在三个长视频问答基准测试上显著提高了现有最佳性能：EgoSchema、NextQA和IntentQA，以及开放问答数据集ActivityNetQA。代码已在此处发布：https://github.com/mayhugotong/VideoINSTA。|
|**2024-09-27**|**LML: Language Model Learning a Dataset for Data-Augmented Prediction**|Praneeth Vadlapati et.al.|[2409.18957](http://arxiv.org/abs/2409.18957)|**[link](https://github.com/pro-genai/lml-dap)**|**本文提出了一种利用大型语言模型（LLM）解决分类任务的新方法，这通常由机器学习（ML）模型处理。与依赖大量数据清洗和特征工程的ML模型不同，此方法通过简化流程，使用LLM来优化过程。本文引入了一个名为“语言模型学习（LML）”的概念，借助一种称为“数据增强预测（DAP）”的新方法。分类任务由LLM执行，类似于人类手动探索和理解数据，并利用数据作为参考来做出分类决策。  训练数据被总结和评估，以确定导致每个标签分类的主要特征。在DAP过程中，系统使用数据概要自动生成查询，用于从数据集中检索相关行。通过使用数据概要和相关数据，LLM基于数据概要和相关行生成分类，即使面对复杂数据也能确保满意的准确性。数据概要和类似数据在DAP中的应用确保了决策的上下文意识。该方法在提示中使用了“以可解释的机器学习模型身份行事”的语句，增强了预测的可解释性，允许用户审查每条预测背后的逻辑。在某些测试案例中，系统的准确率超过90%，证明了系统的有效性及其在各种场景下超越传统ML模型的潜力。代码已发布于https://github.com/Pro-GenAI/LML-DAP。**|
|**2024-09-27**|**Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models**|Jiaming Li et.al.|[2409.18943](http://arxiv.org/abs/2409.18943)|**[link](https://github.com/geaming2002/ruler)**|**大型语言模型的遵循指令能力使得人类能够以自然的方式与AI代理互动。然而，在需要生成特定长度响应时，大型语言模型往往难以满足用户需求，这主要是由于它们在准确感知数值限制方面存在的固有困难。为了探索大型语言模型在遵循特定长度指令时控制生成响应长度的能力，我们提出了目标长度生成任务（TLG）并设计了两个度量标准，精确匹配（PM）和灵活匹配（FM），以评估模型在遵守指定响应长度方面的性能。此外，我们引入了一种新颖的、模型无关的方法Ruler，通过使用元长度标记（MLTs）增强大型语言模型在长度受限指令下的指令遵循能力。具体而言，Ruler使LLMs能够在指令中包含长度约束的情况下生成指定长度的响应。而且，当长度约束没有明确提供时，Ruler还能自动生成适当的MLT，表现出出色的通用性和泛化能力。全面的实验表明，Ruler在目标长度生成任务上对不同的LLMs都显示出有效性，例如在PM上的平均增益为27.97，在FM上的平均增益为29.57。此外，我们还进行了广泛的消融实验进一步验证了Ruler的有效性及其泛化能力。我们的代码和数据可在https://github.com/Geaming2002/Ruler获取。**|
|**2024-09-27**|**From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding**|Heqing Zou et.al.|[2409.18938](http://arxiv.org/abs/2409.18938)|**[link](https://github.com/Vincent-ZHQ/LV-LLMs)**|本文综述了大型语言模型（LLMs）与视觉编码器集成在视觉理解任务中的最新进展，利用其固有优势来理解和生成类似人类的文本以进行视觉推理。由于视觉数据的多样性，多模态大型语言模型（MM-LLMs）在设计和训练上针对理解图像、短视频和长视频时表现出不同的特征和挑战。我们的研究聚焦于长视频理解与静态图像及短视频理解之间的显著差异及其独特挑战。  不同于静态图像，短视频包含了序列帧的时空信息以及事件内部的时间信息；而长视频则包含了多个事件的时空信息以及事件间的长期时间依赖性。本文旨在追溯并总结MM-LLMs从图像理解到长视频理解的发展历程，详细对比各种视觉理解任务之间的差异，并突出长视频理解所面临的挑战，如更细致的时空细节、动态事件和长期依赖性。  接着，本文对MM-LLMs在模型设计和训练方法上的发展进行了详尽的概述，特别关注于如何有效理解长视频。最后，通过比较现有MM-LLMs在不同长度的视频理解基准测试上的表现，本文讨论了多模态大型语言模型在长视频理解领域可能的未来发展方向。|
|**2024-09-27**|**AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow**|Huizi Yu et.al.|[2409.18924](http://arxiv.org/abs/2409.18924)|null|在现代医学教育与研究领域，模拟患者系统发挥着至关重要的作用，它们提供了一个安全、综合的学习环境，并允许进行临床决策模拟。大型语言模型（LLM）有望通过高保真度和低成本地复制医疗状况和医患互动，进一步提升模拟患者系统的能力。然而，确保这些系统的有效性和可信性仍是一个挑战，因为它们需要一个规模大、多样且精确的患者知识库，同时具备强大的稳定知识传播能力。  在此背景下，我们开发了AIPatient，这是一个高级的模拟患者系统，它以AIPatient知识图谱（AIPatient KG）作为输入，并采用基于推理检索增强生成（Reasoning RAG）的代理工作流程作为生成基础。AIPatient KG从Medical Information Mart for Intensive Care（MIMIC-III）数据库中的电子健康记录（EHRs）抽取数据，生成了一个在知识库有效性方面表现出色（F1得分为0.89）、临床多样性和相关性高的1,495名患者的群体。  Reasoning RAG利用了六个由LLM驱动的代理，覆盖了包括检索、KG查询生成、抽象、检查、重写和总结在内的任务。这个代理框架在基于EHR的医疗问答（QA）任务上达到了94.15%的整体准确性，显著优于仅使用无代理或部分代理集成的基准。  我们的系统还展示了高可读性（中位数Flesch阅读轻松度77.23；中位数Flesch-Kincaid年级5.6）、稳健性（ANOVA F值0.6126，p<0.1）和稳定性（ANOVA F值0.782，p<0.1）。AIPatient系统的出色性能预示着其在医学教育、模型评估和系统集成等多个应用领域的巨大潜力。|
|**2024-09-27**|**Soft Measures for Extracting Causal Collective Intelligence**|Maryam Berijanian et.al.|[2409.18911](http://arxiv.org/abs/2409.18911)|**[link](https://github.com/kuldeep7688/soft-measures-causal-intelligence)**|**理解与模拟集体智慧对于处理复杂社会系统至关重要。模糊认知地图（FCMs）作为表示因果心理模型的强大工具，通过定向图进行编码，但直接从文本提取高可信度的FCMs具有挑战性。本研究提出了一种利用大型语言模型（LLMs）自动提取FCMs的方法。我们引入了新颖的基于图的相似性度量，并通过使用Elo评分系统关联输出与人类判断来评估这些度量。结果显示，这些度量与人类评价之间存在正相关，尽管表现最好的度量仍然在捕捉FCM细微差别方面存在局限性。对LLMs进行微调可以提高性能，但现有的度量仍然不足以满足需求。本研究强调了需要针对FCMs提取设计的软相似性度量，从而推动了使用NLP模拟集体智慧的发展。**|
|**2024-09-27**|**IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation**|Fan Lin et.al.|[2409.18892](http://arxiv.org/abs/2409.18892)|**[link](https://github.com/DUTlf/IDGen)**|随着大型语言模型（LLMs）在处理复杂任务方面的能力日益增强，评估集必须与时俱进，以确保其持续保持足够的区分能力。受教育评估中广泛使用的项目鉴别（Item Discrimination, ID）理论启发，我们提出了一种基于ID的提示合成框架，用于评估LLMs，确保评估集能够根据模型的能力不断更新和优化。我们的数据合成框架注重广度与精确性并重。它能生成既能全面评估LLMs能力，又能揭示不同模型之间有意义性能差异的提示，从而实现对它们在各种任务和领域中的相对强项和弱点的有效区分。  为了产生高质量的数据，我们在通用化框架中融入了一个自我校正机制，并开发了两个模型来预测提示的鉴别能力和难度评分，以此推动我们的数据合成框架。这些工具对评估数据合成研究具有重要价值。我们将生成的数据应用于评估五款最先进的模型。该数据平均得分为51.92，方差为10.06。相比之下，先前的工作（如SELF-INSTRUCT和WizardLM）的平均得分超过67，方差低于3.2。结果表明，我们框架生成的数据在挑战性和区分能力上比之前的工作更具优势。我们计划发布包含超过3000个精心设计的提示的数据库，以促进LLMs评估研究的发展。|
|**2024-09-27**|**Predicting and analyzing memorization within fine-tuned Large Language Models**|Jérémie Dentan et.al.|[2409.18858](http://arxiv.org/abs/2409.18858)|null|大型语言模型因其在解决复杂任务方面的能力而受到广泛关注。然而，这些模型在训练数据中记忆了相当大的比例，这在推理时构成了严重的威胁。为了缓解这种无意的记忆问题，理解哪些元素被记忆以及原因至关重要。目前大多数现有工作提供的是事后解释，这在实践中兴趣有限。为填补这一缺口，我们提出了一种新的方法，基于切片互信息，在分类场景中预先检测记忆样本。该方法从训练的早期阶段就具有高效性，并且易于适应实际场景。我们的方法得到了新的理论结果的支持，我们通过实验展示了这一点，并且需要较低的计算预算。我们获得了强大的实证结果，为在记忆发生之前系统地检查和保护这些易受影响的样本铺平了道路。|
|**2024-09-27**|**Mitigating Selection Bias with Node Pruning and Auxiliary Options**|Hyeong Kyu Choi et.al.|[2409.18857](http://arxiv.org/abs/2409.18857)|null|大型语言模型（LLM）在回答多选题时往往表现出对某些选项的不适当偏好，这在LLM自动化系统中引发了显著的可靠性问题。以往的解决方案主要通过调整模型的输入和/或输出来应对偏见问题。而我们的工作则采取了不同的路径，旨在探究模型内部偏见的形成机制。具体而言，我们提出了一种名为偏差节点修剪（BNP）的新颖去偏方法，该方法旨在删除那些导致偏见的线性层参数。此外，我们还引入了一种名为辅助选项注入（AOI）的简单而有效的输入修改技术，适用于黑盒模型的去偏。为了提供一个更系统的方法来评估选择偏见，我们回顾了现有指标，并提出了选择Kullback-Leibler散度（CKLD），以解决常用指标对标签不平衡不敏感的问题。实验结果表明，我们的方法在应用到三种不同的LLM时表现出了鲁棒性和适应性。|
|**2024-09-27**|**LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis**|Hamed Babaei Giglou et.al.|[2409.18812](http://arxiv.org/abs/2409.18812)|**[link](https://github.com/HamedBabaei/LLMs4Synthesis)**|面对科学文献日益增长的复杂性和数量，本文提出了LLMs4Synthesis框架，旨在增强大型语言模型（LLMs）在生成高质量科学综合分析的能力。该框架针对快速、连贯和语境丰富的科学见解集成需求，利用开源和专有LLMs，以解决当前定量指标在评估这些综合分析时存在的不足。通过开发一种处理科学论文的新方法、定义新的综合类型以及建立九项详细的质量评估标准，我们的研究对这一领域做出了贡献。我们还提议将LLMs与强化学习和AI反馈相结合，以优化综合质量，并确保其与既定标准保持一致。LLMs4Synthesis框架及其组成部分的可用性，有望提升科学研究综合过程的生成和评价能力。|
|**2024-09-27**|**Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs**|Yanyuan Qiao et.al.|[2409.18794](http://arxiv.org/abs/2409.18794)|null|本文介绍了一项名为Open-Nav的创新研究，旨在探索开源大型语言模型（LLMs）在连续环境中的零样本视觉与语言导航（VLN）任务应用。Open-Nav采用了空间时间链式思维（CoT）推理方法，将任务分解为指令理解、进度估计和决策制定三个部分，以提高模型在导航场景中的感知能力并增强对细粒度物体和空间知识的理解。实验结果在模拟环境和真实世界环境中均显示，Open-Nav能够与使用闭源LLMs实现相当的竞争性性能。|
|**2024-09-26**|**EgoLM: Multi-Modal Language Model of Egocentric Motions**|Fangzhou Hong et.al.|[2409.18127](http://arxiv.org/abs/2409.18127)|null|在穿戴设备的普及背景下，理解主观视角的动作变得至关重要，以发展具有情境意识的人工智能。本文提出了一种名为EgoLM的通用框架，用于从多模态输入（如主观视频和运动传感器）中跟踪和理解主观动作。EgoLM通过利用丰富的上下文来解决单模态条件下的主体运动跟踪和理解难题。为了促进这一通用且多模态的框架，我们的核心洞察是使用大型语言模型（LLM）来建模主体动作和自然语言的联合分布。多模态传感器输入被编码并投影到语言模型的联合潜在空间中，并用于触发动作生成或文本生成，分别用于主体运动跟踪或理解。大规模多模态人体动作数据集上的广泛实验验证了EgoLM作为通用模型在普遍主观学习中的有效性。|
|**2024-09-26**|**Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography**|Yuexi Du et.al.|[2409.18119](http://arxiv.org/abs/2409.18119)|**[link](https://github.com/xypb/mama)**|在医疗图像分析领域，对比语言-图像预训练（CLIP）显示出巨大潜力，但其需要大量的数据和计算资源。因此，现有的CLIP应用主要集中在如胸片这类拥有丰富图像报告数据的模态上，而忽略了诸如乳腺X光等许多重要模态的研究。本文首次提出将完整的CLIP模型应用于乳腺X光图像分析，这一任务面临着标记数据稀缺、高分辨率图像中的小感兴趣区域以及数据不平衡的挑战。  我们首先开发了一种针对乳腺X光的专用监督框架，利用其多视图特性。此外，设计了对齐模块以更好地聚焦于高分辨率图像中的详细特征。最后，引入了一种参数高效微调方法，用于大规模语言模型，这些模型预先使用医学知识进行训练，以应对数据限制问题。  我们的多视图和多尺度对齐（MaMA）方法，在两个大型真实世界乳腺X光数据集EMBED和RSNA-Mammo上，对于三种不同的任务，相较于最先进的基线方法取得了显著性能提升，同时相比最大的基线模型，仅使用了52%的模型大小。|
|**2024-09-26**|**E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding**|Ye Liu et.al.|[2409.18111](http://arxiv.org/abs/2409.18111)|**[link](https://github.com/PolyU-ChenLab/ETBench)**|**为了验证视频大语言模型（Video Large Language Models, Video-LLMs）在通用视频理解中的巨大潜力，已提出了一系列基准测试来诊断模型在不同场景下的能力。然而，现有的基准测试仅通过视频级问题回答进行评估，缺乏对事件级别的精细评估和任务多样性。为了填补这一空白，我们引入了E.T. Bench（事件级别与时间敏感的视频理解基准），这是一个针对开放式的事件级别视频理解的大规模、高质量基准测试。  E.T. Bench按照三层任务分类体系进行组织，包含了涵盖12个任务的7300个样本，以及8个领域的2514小时总时长的7000个视频，提供了全面的评估。我们广泛地对8个图像大语言模型和12个视频大语言模型进行了评估，并且结果显示，用于粗粒度（视频级）理解的最先进的模型在解决我们的精细粒度任务时表现不佳，例如在视频中定位感兴趣的事件，主要原因是视频上下文长度短、时间表示不当以及缺乏多事件训练数据。针对这些问题，我们进一步提出了一个强大的基线模型——E.T. Chat，以及专门为精细粒度事件理解设计的指令调优数据集E.T. Instruct 164K。我们的简单但有效的解决方案在多个场景中表现出优越的性能。**|
|**2024-09-26**|**Infering Alt-text For UI Icons With Large Language Models During App Development**|Sabrina Haque et.al.|[2409.18060](http://arxiv.org/abs/2409.18060)|null|确保移动应用的无障碍性仍然是一个重大挑战，尤其是对于依赖屏幕阅读器的视障用户。界面图标对于导航和互动至关重要，但往往缺乏有意义的替代文本，从而形成使用障碍。传统的深度学习方法在生成替代文本时需要大量数据集，并且在图标类型多样性与不平衡性方面存在困难。更近期的视觉语言模型（VLMs）则要求完整的UI屏幕，这在应用程序开发的迭代阶段可能不切实际。为了应对这些问题，我们引入了一种新的方法，使用大型语言模型（LLMs）通过部分UI数据自主生成移动UI图标的描述性替代文本。通过整合包括类别、资源ID、边界、OCR检测到的文字以及父节点和同级节点的上下文信息在内的图标上下文，我们对大约1400个图标的小型数据集进行离线微调，从而生成了IconDesc。在实证评估和用户研究中，IconDesc显著提高了生成相关替代文本的能力。这一能力使得IconDesc成为开发者的重要工具，帮助他们快速迭代和提升UI的无障碍性。|
|**2024-09-26**|**DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving**|Dingrui Wang et.al.|[2409.18053](http://arxiv.org/abs/2409.18053)|**[link](https://github.com/TUM-AVS/DualAD)**|我们提出了一种新型自主驾驶框架DualAD，旨在模仿人类在驾驶过程中的决策逻辑。DualAD由两层构成：底层为基于规则的运动规划器，负责处理需要较少决策的常规驾驶任务；上层则配备了一个基于规则的文字编码器，将绝对状态下的驾驶场景转化为文本描述。此文本随后由大型语言模型（LLM）进行决策。当检测到潜在危险时，上层会介入底层的决策过程，以模仿人类在关键情况下的决策逻辑。闭合环路实验显示，使用零训练预训练模型的DualAD显著优于缺乏决策能力的基于规则的运动规划器。我们的实验还强调了文字编码器的有效性，它极大地增强了模型对场景的理解能力。此外，集成的DualAD模型随着更强大的LLM的使用而得到改善，这表明该框架具有进一步增强的潜力。我们提供代码和基准测试供公众访问。|
|**2024-09-26**|**EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions**|Kai Chen et.al.|[2409.18042](http://arxiv.org/abs/2409.18042)|null|在开放源代码社区中，让大型语言模型能够以公开数据进行端到端的图像、文本和语音生成仍然具有挑战性。现有的视语模型依赖于外部工具进行语音处理，而语音语模型仍缺乏视觉理解能力。为了填补这一缺口，我们提出了EMOVA（情绪化的全模式语音助手），以使大型语言模型具备端到端的语音能力，同时保持领先的视语表现。通过语义-声学分离的语音编码器，我们意外地发现，全模态对齐可以进一步增强视语和语音能力，与相应的双模态对齐模型相比。此外，我们还提出了一种轻量级风格模块，用于灵活控制语音风格（例如情感和音调）。首次，EMOVA在视语和语音基准测试中均达到了最先进的性能，并同时支持带有生动情感的全模态对话。|
|**2024-09-26**|**Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective**|Yotam Wolf et.al.|[2409.18028](http://arxiv.org/abs/2409.18028)|null|在进行复杂分析任务（如代码生成）的大型语言模型（LLM）使用中，通常会将整个任务的解决方案在模型的上下文窗口内进行采样。先前的研究表明，在模型的上下文中分解任务（即链式思维）对于解决这类任务是有益的。本文指出了一种限制，即LLM在同一个上下文窗口内执行多个子任务的能力——一种“复合难度”。这表明在LLM组成的多智能体系统中将分解后的问题分发处理具有优势。我们通过生成复杂度指标来量化这种复合难度，即在采样到至少一个正确解所需的LLM生成次数。我们发现，相对于在相同上下文内解决组合问题，将问题分散给多个智能体的生成复杂度之间存在差距，并且随着解长度的增加，这个差距呈指数增长。我们通过理论证明和实验证明了这一结果。|
|**2024-09-26**|**An Adversarial Perspective on Machine Unlearning for AI Safety**|Jakub Łucki et.al.|[2409.18025](http://arxiv.org/abs/2409.18025)|**[link](https://github.com/ethz-spylab/unlearning-vs-safety)**|本文探讨了大型语言模型在拒绝危险知识相关问题方面的微调方式，但这些防护措施往往容易被绕过。去学习方法旨在彻底消除模型的危险能力并使其对攻击者不可访问。本文从对抗性视角挑战了去学习与传统安全后训练之间的基本差异。我们证明了之前被认为无效的现有逃脱方法，在精心应用时可以成功应对去学习。此外，我们开发了一系列适应性方法来恢复大部分被认为是无法学习的能力。例如，我们展示了使用RMU（当前最先进的去学习方法）编辑模型后，通过在无关示例上进行微调或在激活空间中移除特定方向，可以恢复大部分危险能力。我们的发现质疑了当前去学习方法的稳健性，并对它们相对于安全训练的优势提出了疑问。|
|**2024-09-26**|**DARE: Diverse Visual Question Answering with Robustness Evaluation**|Hannah Sterz et.al.|[2409.18023](http://arxiv.org/abs/2409.18023)|null|《DARE：多样化的视觉问答与鲁棒性评估》论文摘要翻译如下：  本文引入了DARE（Diverse Visual Question Answering with Robustness Evaluation），一个精心设计并收集的多选型视觉问答基准。DARE旨在评估大型语言模型在视觉语言推理任务中的表现，特别是在五个不同类别的视觉问题上，并包括基于提示变化、答案选项子集、输出格式和正确答案数量等四个鲁棒性导向评估的全面评估。  研究发现，当前最先进的视觉语言模型在大多数类别中仍然面临挑战，且无法在测试的所有鲁棒性评估中保持一致的高性能。在不同答案选项子集的情况下，最差情况下的性能下降可达标准情况下的34%。开源模型如LLaVA 1.6和Idefics在鲁棒性方面无法与闭源模型GPT-4和Gemini相匹敌，而后者在不同变体下仍表现出明显的脆弱性。  总之，该研究揭示了视觉语言模型在处理视觉推理任务时所面临的局限性，并强调了在设计更鲁棒的模型时需要考虑的问题。|
|**2024-09-26**|**Role-RL: Online Long-Context Processing with Role Reinforcement Learning for Distinct LLMs in Their Optimal Roles**|Lewei He et.al.|[2409.18014](http://arxiv.org/abs/2409.18014)|null|针对长文本上下文处理的大型语言模型（LLM）仍然存在实现复杂性、训练效率和数据稀疏性等挑战。为此，我们提出了一个新范式——在线长期上下文处理（OLP），适用于处理无限长度的文档，常见于自动化新闻报道、直播电商和病毒短视频等多样化的流媒体信息接收与组织场景。同时，在选择众多性能优异、价格适中且响应延迟短的LLM时，往往遇到难以抉择的问题。鉴于此，我们开发了角色强化学习（Role-RL）框架，自动部署不同角色的LLM在OLP管道中，根据其实际性能进行合理分配。  我们进行了大量的实验，并在我们的OLP-MINI数据集上发现，结合Role-RL框架的OLP系统平均召回率为93.2%，实现了OLP基准，并节省了79.4%的LLM成本。相关代码和数据集已公开发布：https://anonymous.4open.science/r/Role-RL。|
|**2024-09-25**|**Attention Prompting on Image for Large Vision-Language Models**|Runpeng Yu et.al.|[2409.17143](http://arxiv.org/abs/2409.17143)|**[link](https://github.com/yu-rp/apiprompting)**|**与大型语言模型（LLM）相比，大型视觉-语言模型（LVLM）还能接受图像作为输入，因此展示了更多有趣的现象级能力，并在各种视觉-语言任务上表现出令人印象深刻的表现。受LLM中文本提示的启发，探索了增强LVLM对视觉信息感知能力的视觉提示技术。然而，以往的视觉提示技术仅处理视觉输入而不考虑文本查询，限制了模型遵循文本指令完成任务的能力。为了填补这一空白，本工作提出了一个名为“注意力映射上的图像提示”的新提示技术，该技术简单地在原始输入图像上叠加了一个由辅助模型（如CLIP）生成的、依赖于文本查询的注意力热图，并有效地增强了LVLM在各种任务上的表现。具体来说，我们通过一个辅助模型（如CLIP）为输入图像生成一个依赖于文本查询的注意力热图。然后，热图简单地乘以原始图像的像素值来获得实际输入图像供LVLM使用。在各种视觉-语言基准测试上的广泛实验验证了我们技术的有效性。例如，“注意力映射上的图像提示”分别提高了LLaVA-1.5在MM-Vet和LLaVA-Wild基准上的性能3.8%和2.9%。**|
|**2024-09-25**|**FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression**|Fazal Mittu et.al.|[2409.17141](http://arxiv.org/abs/2409.17141)|**[link](https://github.com/fazalmittu/finezip)**|**本文深入分析了基于神经网络与Transformer的文本压缩技术，并将其与传统文本压缩系统进行对比。尽管基于大型语言模型（LLM）的系统在压缩比上显著优于传统方法，但它们在实用性方面却极为有限。以Llama3-8B为基础的LLM压缩系统——LLMZip，在压缩仅10MB文本时需要9.5天的时间，尽管压缩效果有所提升。  为解决这一问题，我们提出了FineZip——一种结合在线记忆与动态上下文概念的新型LLM文本压缩系统。FineZip相较于LLMZip，将压缩时间大幅缩短至约4小时，性能提升了54倍，且与传统算法压缩方法相比，其压缩效率提高了大约50%。通过本研究，我们迈出了让基于LLM的无损文本压缩成为现实的第一步。尽管FineZip已取得显著进展，但LLM仍不适用于大规模文本压缩。我们期待本文的研究和创新能为未来解决这一问题铺平道路。**|
|**2024-09-25**|**Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents**|Junting Lu et.al.|[2409.17140](http://arxiv.org/abs/2409.17140)|null|本文提出了一种名为AXIS的新型基于语言模型的代理框架，该框架通过应用程序编程接口（API）优先处理操作而非用户界面（UI）操作，以解决大型语言模型（LLM）驱动的代理在复杂任务中的高延迟和低可靠性问题。此外，AXIS框架还通过自动化探索应用程序的方式促进了API的创建与扩展。  在Office Word应用上的实验结果表明，与人类相比，AXIS在任务完成时间上缩短了65%-70%，认知负荷降低了38%-53%，同时保持了97%-98%的准确性。这项工作为人类、代理和计算机交互（HACI）框架以及应用程序提供者在LLM时代的新UI设计原则做出了贡献。它也探讨了将每个应用程序转化为代理的可能性，为代理为中心的操作系统（Agent OS）铺平了道路。|
|**2024-09-25**|**Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale**|Fan Zhou et.al.|[2409.17115](http://arxiv.org/abs/2409.17115)|**[link](https://github.com/gair-nlp/prox)**|**在大型语言模型预训练领域，人们长期以来依赖于人类专家制定提升数据质量的启发式规则，至今已发展出众多规则。然而，这些规则缺乏灵活性，无法有效针对每个实例的独特特性进行调整。同时，为每个实例应用定制规则对于人类专家而言是不切实际的。本文展示了即使是参数数量仅有0.3B的语言模型，也能展现出与人类专家相当的数据优化能力。我们引入了“编程每例”（ProX）框架，该框架将数据优化视为编程任务，允许模型通过生成并执行精细粒度的操作（如字符串规范化）对每个个体实例进行大规模优化。  实验结果表明，使用ProX筛选后的数据预训练的模型，在各种下游基准测试中均优于原始数据或由其他筛选方法处理的数据，性能提升超过2%。该框架的有效性适用于不同规模的模型和预训练数据集，包括C4、RedPajama-V2和FineWeb。此外，ProX在特定领域的连续预训练中表现出巨大潜力：在无需特定领域设计的情况下，使用ProX优化的OpenWebMath数据预训练的模型，在准确性上分别比Mistral-7B、Llama-2-7B和CodeLlama-7B提高了7.6%、14.6%和20.3%，仅使用约10B令牌即可达到类似于使用200B令牌预训练的Llama-7B模型的水平。进一步的分析显示，ProX显著节省了训练FLOPs，为高效LLM预训练开辟了有前景的道路。  我们公开发布了ProX，包括>100B的语料库、模型以及所有训练和实现细节，以促进可复制研究和未来创新。代码：https://github.com/GAIR-NLP/ProX**|
|**2024-09-25**|**Accumulator-Aware Post-Training Quantization**|Ian Colbert et.al.|[2409.17092](http://arxiv.org/abs/2409.17092)|null|近年来的研究已经探索了低精度累加，报告了在不同平台上的吞吐量、功率和面积的改进。然而，这些提议仅考虑了量化感知训练（QAT）范式，在该范式中，模型在量化循环中进行微调或从头开始训练。随着模型继续增大，QAT技术的成本变得越来越高，这激发了最近对后量化量化（PTQ）研究的热潮。据我们所知，这是首次正式研究PTQ背景下的积算器感知量化。为了填补这一空白，我们引入了AXE，一个旨在赋予现有层式PTQ算法溢出避免保证的实用框架的扩展。我们通过在两个最先进的PTQ算法：GPFQ和OPTQ之上实现AXE来理论地推动AXE，并证明其灵活性。进一步地，我们通过首次支持多阶段积累来一般化AXE，为全数据路径优化和大型语言模型（LLMs）的扩展打开大门。我们在图像分类和语言生成模型上评估了AXE，并观察到与基线方法相比，在积算器位宽与模型准确性的权衡上取得了显著改进。|
|**2024-09-25**|**VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models**|Yifei Liu et.al.|[2409.17066](http://arxiv.org/abs/2409.17066)|**[link](https://github.com/microsoft/vptq)**|**本文介绍了一种名为Vector Post-Training Quantization（VPTQ）的低比特量化方法，针对大型语言模型（LLMs）。通过使用二次优化来定义LLM向量量化问题，并通过解决优化问题来指导量化算法设计。进一步地，引入了通道独立的二次优化以实现精细化量化。同时，通过分解优化问题，提出了简明有效的代码本初始化算法。此外，VPTQ还扩展了残差和异常值量化支持，这不仅提高了模型精度，还能进一步压缩模型。  实验结果表明，与SOTA相比，在2比特量化时，VPTQ将模型量化困惑度降低0.01-0.34，Mistral-7B上为0.38-0.68，LLaMA-3上为4.41-7.34。在问答任务上的平均准确度提升范围为LLaMA-2上的0.79%-1.5%，Mistral-7B上的1%，以及LLaMA-3上的11%-22%。量化算法执行时间仅占10.4%-18.6%，导致推理吞吐量提高1.6-1.8倍。**|
|**2024-09-25**|**Using LLM for Real-Time Transcription and Summarization of Doctor-Patient Interactions into ePuskesmas in Indonesia**|Azmul Asmar Irfan et.al.|[2409.17054](http://arxiv.org/abs/2409.17054)|null|本文提出了一种解决方案，利用本地化大型语言模型（LLM）来转录、翻译和总结医生与患者的对话。我们使用Whisper模型进行转录，GPT-3进行总结，并将其格式化为ePuskemas医疗记录。此系统作为现有网络浏览器扩展的附加组件实现，允许医生在说话时填写患者表格。通过利用实时转录、翻译和总结功能，医生可以提高患者护理的周转时间，同时增强记录的质量，使得记录更加详细且富有洞察力，以供未来的访问参考。这一创新旨在解决印尼医疗机构拥挤以及医护人员行政负担重的问题。我们相信，这种解决方案将帮助医生节省时间、提供更好的护理并产生更准确的医疗记录，代表了向现代化医疗保健迈进的重要一步，确保即使在资源有限的环境中，患者也能获得及时、高质量的护理。|
|**2024-09-25**|**How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not**|Francesco Verdini et.al.|[2409.17044](http://arxiv.org/abs/2409.17044)|null|大型语言模型（LLM）的惊人表现推动了研究努力，使其能够应用于一系列任务和输入模态。在语音转文本（S2T）任务中，新兴的解决方案是通过适配器模块将语音基础模型（SFM）的输出投影到LLM嵌入空间。然而，目前还没有工作探讨下游任务性能在多大程度上依赖于每个组件（SFM、适配器、LLM），或者选择适配器的最佳设计是否取决于所选的SFM和LLM。为了填补这一空白，我们评估了5个适配器模块、2个LLM（Mistral和Llama）以及2个SFM（Whisper和SeamlessM4T）在自动语音识别和语音翻译两个广泛使用的S2T任务上的组合效果。我们的结果表明，SFM在下游性能中扮演着至关重要的角色，而适配器的选择具有适度的影响，并且取决于所选的SFM和LLM。|
|**2024-09-25**|**Counterfactual Token Generation in Large Language Models**|Ivi Chatzi et.al.|[2409.17027](http://arxiv.org/abs/2409.17027)|**[link](https://github.com/networks-learning/counterfactual-llms)**|本文旨在提升大型语言模型的功能，使其能够推理过去生成的令牌所呈现的可能替代情况。我们开发了一种基于Gumbel-Max结构因果模型的因果模型，以增强大型语言模型的这一功能。我们的模型能够在几乎不增加与基础令牌生成成本的情况下，进行反事实令牌生成，实现过程简单且无需任何微调或提示工程。我们在此基础上在Llama 3 8B-instruct上实现了该模型，并对生成的反事实文本进行了定性和定量分析。  此外，我们还探讨了反事实令牌生成在偏见检测方面的应用，揭示了大型语言模型构建的世界模型中的一些有趣见解。|
|**2024-09-25**|**LLM-CARD: Towards a Description and Landscape of Large Language Models**|Shengwei Tian et.al.|[2409.17011](http://arxiv.org/abs/2409.17011)|**[link](https://github.com/shengwei-tian/dependency-parser-visualization)**|随着自然语言处理（NLP）领域的迅速发展，大型语言模型（LLMs）在各种NLP任务中不断涌现。随着发表的论文数量不断增加，研究人员和开发者面临信息过载的挑战。因此，开发一个能够自动从学术论文中提取并组织LLM关键信息的系统变得尤为重要。本工作旨在通过使用命名实体识别（NER）和关系抽取（RE）方法来实现这一目标，这些方法可以自动从论文中提取关于大型语言模型的关键信息，帮助研究人员高效地获取关于LLMs的信息。这些特性包括模型的“许可”、“名称”和“应用”。借助这些特性，我们可以为每篇论文形成一个模型卡片。在数据贡献方面，对106篇学术论文进行了处理，定义了三个字典——LLMs名称、许可和应用。通过字典查找提取了11051个句子，并通过人工审查最终选择了129个句子，其中包含名称与许可之间的链接，以及106个句子，其中包含模型名称与应用之间的链接。|
|**2024-09-20**|**Gender Representation and Bias in Indian Civil Service Mock Interviews**|Somonnoy Banerjee et.al.|[2409.12194](http://arxiv.org/abs/2409.12194)|null|本文提出了三个关键贡献。首先，通过收集自888个印度公务员候选人面试模拟的YouTube视频中的51,278个问题样本，我们展示了对男性和女性候选人提问的性别偏见在广泛性质上的显著存在。第二，我们的大型语言模型实验揭示了在性别推断任务中，这些模型提供的解释中存在强烈的性别偏见。最后，我们提供了一个包含51,278个面试问题的新型数据集，这可以为未来的人文社会科学研究提供信息。|
|**2024-09-18**|**To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning**|Zayne Sprague et.al.|[2409.12183](http://arxiv.org/abs/2409.12183)|**[link](https://github.com/zayne-sprague/to-cot-or-not-to-cot)**|为了分析链式思考（CoT）在哪些任务中真正有益，我们进行了一项量化元分析，覆盖了超过100篇使用CoT的论文，并对20个数据集进行了14种模型的自我评估。结果表明，CoT主要在数学或逻辑任务上提供显著性能优势，而在其他类型任务上的增益较小。在MMLU上，直接生成答案而无需CoT几乎与CoT具有相同的准确性，除非问题或模型的回答包含等号，这表明符号操作和推理。  基于这一发现，我们分析了CoT在这些问题中的行为，通过分离规划和执行，并与增强工具的大型语言模型进行比较。CoT大部分收益来自改进的符号执行，但相较于使用符号求解器，它在性能上表现不佳。我们的结果表明，可以根据需要应用CoT，同时保持性能并节省推理成本。此外，这些结果还表明，需要超越基于提示的CoT，转向新的范式，更好地利用整个范围内的大型语言模型应用中的中间计算。|
|**2024-09-18**|**Finetuning Language Models to Emit Linguistic Expressions of Uncertainty**|Arslan Chaudhry et.al.|[2409.12180](http://arxiv.org/abs/2409.12180)|null|本文研究了大型语言模型（LLM）在信息检索与决策任务中的应用。尽管LLM具有广泛的应用价值，但它们倾向于生成与现实世界事实相冲突的信息，并以说服性的方式表达，使得这些不准确性看起来自信且令人信服。这导致最终用户难以一致地将LLM的自信度与预测的准确性对齐，常常导致对所有输出的盲目信任或完全忽视其可靠性。为此，我们探索了在不确定性增强的预测基础上进行监督微调的方法，以此来开发能够生成语言不确定性表述的模型。具体而言，我们衡量预训练模型的校准程度，然后通过基于模型自身信心的微调，使语言模型产生校准的不确定性表述。通过对各种问答数据集的实验，我们证明了LLM在评估预测时具有良好的校准能力，并基于模型本身的信心进行监督微调，可获得特别适用于单个声明答案的良好校准的不确定性表述。|
|**2024-09-18**|**Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference**|Najmeh Forouzandehmehr et.al.|[2409.12150](http://arxiv.org/abs/2409.12150)|null|本文提出了一种新颖的框架，利用大型语言模型（LLM）的强大表达能力来解决个性化服装推荐这一复杂挑战。通过细调和直接反馈集成，我们试图克服LLM的“黑盒”特性和静态性。我们通过在人类编目的时尚图像上使用多模态大型语言模型（MLLM）进行图像描述，来弥合项目视觉与文本之间的差距。这使得LLM能够从人类编目的时尚图像中提取风格和色彩特征，从而形成个性化的推荐基础。我们使用开源的Polyvore数据集对LLM进行高效细调，优化其推荐时尚搭配的能力。采用直接偏好机制并结合负例，以增强LLM的决策过程。这创建了一个自我增强的人工智能反馈循环，持续地根据季节性时尚趋势优化推荐。我们的框架在Polyvore数据集上进行了评估，针对两个关键任务：补全空白和辅助项目检索。这些评估结果强调了框架生成时尚、与潮流一致的服装建议的能力，并通过直接反馈持续改进。评估结果显示，我们的提议框架在这些任务上的表现显著优于基于原始LLM的服装生成，创造了更加协调的服装。改进的表现证明了该框架增强购物体验、提供准确建议的潜力，证明了它相对于基于原始LLM的服装生成方法的有效性。|
|**2024-09-18**|**MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning**|Justin Chih-Yao Chen et.al.|[2409.12147](http://arxiv.org/abs/2409.12147)|**[link](https://github.com/dinobby/magicore)**|**大型语言模型（LLM）的推理能力可以通过在测试时采用聚合策略进行提升，即生成多个样本并基于生成样本进行投票。虽然这些策略能够提高性能，但它们往往存在饱和点。改进方法引入了一种名为“Refinement”的策略，通过利用LLM生成的反馈来提升解决方案的质量。然而，Refinement也带来了三个关键挑战：（1）过度细化：对所有实例进行统一细化可能导致过度修正，从而降低整体性能。（2）难以定位和纠正错误：LLM具有有限的自我纠正能力，很难识别并纠正自己的错误。（3）细化不足：决定需要多少迭代的细化并不容易，过早停止可能会让错误未得到解决。  为了应对这些问题，我们提出了一种名为MAgICoRe的方法，它通过将问题难度分为简单或困难，并使用粗粒度聚合解决简单问题，使用细粒度和多轮迭代细化解决困难问题，以避免过度细化。为了改善错误定位，我们引入了基于步骤级奖励模型（RM）分数的外部评分。此外，我们采用了一个由三个代理组成的多代理循环：求解者、审查者（根据步骤级RM分数生成针对性反馈）以及细化者（整合反馈），以确保有效细化。为了确保足够的细化，我们重新评估更新后的解决方案，并在必要时启动进一步的细化轮次。我们使用Llama-3-8B和GPT-3.5在5个数学数据集上评估了MAgICoRe，并展示了其有效性。即使只进行一次迭代，MAgICoRe也能在使用不到基线样本一半的情况下，分别超过Self-Consistency、Best-of-k和Self-Refine算法3.4%、3.2%和4.0%。与迭代细化的基线相比，MAgICoRe随着迭代次数的增加持续提高性能。最后，我们的消融实验强调了MAgICoRe中RMs和多代理通信的重要性。**|
|**2024-09-18**|**MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion**|Kalakonda Sai Shashank et.al.|[2409.12140](http://arxiv.org/abs/2409.12140)|**[link](https://github.com/Motion-RAG/MoRAG)**|我们提出了一种名为MoRAG的创新多部分融合检索增强生成策略，用于基于文本的人体动作生成。此方法通过利用增强的运动检索过程获得的额外知识来提升运动扩散模型。通过有效激发大型语言模型（LLM），我们解决了运动检索中的拼写错误和重述问题。我们的方法采用多部分检索策略以提高运动检索在语言空间上的泛化能力。我们通过空间组合检索到的动作来生成多样化的样本。此外，利用低级、特定部分的运动信息，我们可以构建针对未见过文本描述的运动样本。我们的实验结果表明，我们的框架可以作为插件模块使用，以提高运动扩散模型的性能。代码、预训练模型和视频示例将在以下网址提供：https://motion-rag.github.io/|
|**2024-09-24**|**Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models**|Sijing Chen et.al.|[2409.12139](http://arxiv.org/abs/2409.12139)|null|随着大数据和大型语言模型时代的到来，零样本个性化快速定制已成为一个显著趋势。本报告介绍了Takin AudioLLM系列技术与模型，主要包括Takin TTS、Takin VC和Takin Morphing，专门用于有声读物制作。这些模型具备零样本语音生成能力，能产生几乎与真人声音难以区分的高质量语音，使得个人可以根据自身需求定制语音内容。  首先，我们介绍Takin TTS，这是一种基于增强神经语音编解码器和多任务训练框架的神经编解码语言模型，能够以零样本方式生成高保真自然语音。对于Takin VC，我们提出了一种有效的内容与音色联合建模方法来提高说话人相似度，并倡导基于条件流匹配的解码器进一步提升其自然性和表达力。最后，我们提出了Takin Morphing系统，该系统采用高度解耦且先进的音色与节奏建模方法，使个体能够以精确可控的方式根据自己的偏好定制语音生产。广泛实验验证了我们Takin AudioLLM系列模型的有效性和鲁棒性。有关详细演示，请参阅<https://everest-ai.github.io/takinaudiollm/>。|
|**2024-09-18**|**Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement**|An Yang et.al.|[2409.12122](http://arxiv.org/abs/2409.12122)|null|在本报告中，我们介绍了系列数学专用大型语言模型：Qwen2.5-Math 和 Qwen2.5-Math-Instruct-1.5B/7B/72B。Qwen2.5 系列的核心创新在于在整个管道中融入自我提升的哲学，包括预训练、后处理和推理阶段：（1）在预训练阶段，使用 Qwen2-Math-Instruct 来生成大规模高质量的数学数据。（2）在后处理阶段，我们通过从 Qwen2-Math-Instruct 进行大量采样来开发奖励模型（RM）。然后，我们将此 RM 应用于监督微调（SFT）的迭代进化。通过增强的 SFT 模型，有可能进行迭代训练并更新 RM，进而指导 SFT 数据的下一轮迭代。在最终的 SFT 模型上，我们采用终极 RM 进行强化学习，从而产生 Qwen2.5-Math-Instruct 模型。（3）此外，在推理阶段，使用 RM 来引导采样，优化模型性能。  Qwen2.5-Math-Instruct 支持中文和英文，并具有高级数学推理能力，包括链式思考（CoT）和工具集成推理（TIR）。我们在英语和中文的 10 个数学数据集上评估了我们的模型，如 GSM8K、MATH、GaoKao、AMC23 和 AIME24，涵盖从小学水平到数学竞赛问题的广泛难度。|
|**2024-09-18**|**Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference**|Edresson Casanova et.al.|[2409.12117](http://arxiv.org/abs/2409.12117)|null|大型语言模型（LLM）在通过将音频转换为离散令牌的音频编解码器方面显著推动了音频处理，这使得可以将语言建模技术应用于音频数据。然而，音频编解码器通常以高帧率运行，导致训练和推理速度缓慢，特别是在自回归模型中。为了应对这一挑战，我们提出了低帧率语音编解码器（LFSC）：一种神经音频编解码器，它利用有限标量量化和与大型语音语言模型的对抗性训练，以1.89 kbps的比特率和21.5帧/秒实现高质量的音频压缩。我们证明，我们的新型编解码器可以使基于LLM的文本到语音模型的推理速度加快约三倍，同时提高可懂度并产生与以往模型相当的质量。|
|**2024-09-18**|**Measuring Human and AI Values based on Generative Psychometrics with Large Language Models**|Haoran Ye et.al.|[2409.12106](http://arxiv.org/abs/2409.12106)|**[link](https://github.com/value4ai/gpv)**|**本文引入了基于大型语言模型（LLM）的生成心理测度（GPV），这是一种数据驱动的价值测量范式，理论基础在于文本揭示的选择性感知。首先，我们对LLM进行微调以实现精确的感知层级价值测量，并验证LLM解析文本形成感知的核心能力，从而构建GPV管道的基础。然后，我们将GPV应用于人类撰写的博客，证明其稳定性和有效性，并且优于先前的心理学工具。接着，我们将GPV扩展到LLM价值测量，通过以下方式推动当前技术：1）提出了一种基于LLM可扩展和自由形式输出的量化方法，使价值测量能够针对特定情境；2）比较了不同测量方法，揭示了前人方法的回应偏差；3）尝试将LLM价值与安全性联系起来，发现不同价值体系的预测力，并分析各种价值对LLM安全性的影响。通过跨学科努力，本文旨在利用AI推动下一代心理测度的发展，并利用心理测度促进价值导向的AI。**|
|**2024-09-17**|**AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs**|Basel Mousi et.al.|[2409.11404](http://arxiv.org/abs/2409.11404)|null|阿拉伯语，以其丰富的方言多样性，仍然在大型语言模型中显著被低估，尤其是在方言变体方面。我们通过使用机器翻译结合人工后编辑创建的七个人工合成数据集来填补这一空白，这些数据集涵盖了现代标准阿拉伯语（MSA）以及阿拉伯各地区的方言。我们提出了AraDiCE基准，用于评估阿拉伯方言和文化理解与生成能力。我们的研究侧重于低资源阿拉伯方言，并对其进行了评价。  此外，我们首次引入了一个细粒度基准，专门用于评估阿拉伯半岛、埃及和黎凡特地区之间的文化意识，为LLM评估提供了新的维度。我们的发现表明，尽管针对特定阿拉伯语模型如Jais和AceGPT在方言任务上优于多语言模型，但在方言识别、生成和翻译方面仍存在重大挑战。这项工作贡献了约4.5万个经过人工后编辑的样本、一个文化基准，并强调了根据特定训练来改善大型语言模型捕捉不同阿拉伯方言和文化背景细微差异的重要性。我们将发布在本研究中构建的方言翻译模型和基准。|
|**2024-09-17**|**NVLM: Open Frontier-Class Multimodal LLMs**|Wenliang Dai et.al.|[2409.11402](http://arxiv.org/abs/2409.11402)|null|我们引入了NVLM 1.0，这是一个在视觉语言任务上达到前沿水平的多模态大型语言模型家族，其性能与顶级专有模型（如GPT-4o）和开源模型（如Llama 3-V 405B和InternVL 2）相匹敌。令人惊讶的是，NVLM 1.0在多模态训练后，在仅文本任务上的表现甚至超过了其背后的语言模型基础架构。  在模型设计方面，我们对解码器型多模态语言模型（如LLaVA）和交叉注意力型模型（如Flamingo）进行了全面比较。基于这两种方法的优势和劣势，我们提出了一种新型架构，以提高训练效率和多模态推理能力。此外，我们引入了一种用于动态高分辨率图像的1-D瓷砖标记设计，这显著提高了多模态推理和OCR相关任务的性能。  关于训练数据，我们精心收集并提供了所有架构的预训练和监督微调数据集的详细信息。我们的发现表明，在预训练阶段，数据质量和任务多样性比规模更为重要。值得注意的是，我们为NVLM-1.0模型开发了生产级多模态功能，使它们在视觉语言任务中不仅保持甚至超越了基础语言模型的性能。为了实现这一目标，我们在多模态训练中巧妙地整合了一个高质量的纯文本数据集，以及大量的多模态数学和推理数据，从而在所有模态下提高了数学和编码能力。  为了推动领域研究，我们将发布模型权重并开源代码供社区使用：https://nvlm-project.github.io/。|
|**2024-09-17**|**Says Who? Effective Zero-Shot Annotation of Focalization**|Rebecca M. M. Hicke et.al.|[2409.11390](http://arxiv.org/abs/2409.11390)|null|在这篇论文中，我们通过实验测试了当前大型语言模型（LLMs）在为文学文本标注焦点模式时的表现。尽管任务具有挑战性，但我们的实验结果表明，LLMs在这一任务上的表现与受过训练的人类注释者相当。我们以斯蒂芬·金的小说为例进行案例研究，展示了这种方法在计算文学研究中的实用性，说明了如何大规模地研究焦点模式。|
|**2024-09-17**|**Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement**|Simon Yu et.al.|[2409.11378](http://arxiv.org/abs/2409.11378)|**[link](https://github.com/for-ai/iterative-data-selection)**|细调大规模语言模型在指令数据上的能力对于增强预训练知识和提升指令遵循能力至关重要。随着指令数据集的不断增多，选择有效的数据进行有效训练变得越来越重要。本文探讨了如何确定有效训练的最佳数据子集。现有研究往往侧重于实例质量等局部标准进行子集选择，但我们认为全局视角关注数据多样性更为关键。我们采用k均值聚类方法确保所选子集充分代表整个数据集。  我们提出了一种启发自主动学习技术的迭代优化方法，用于从各个聚类中重新采样实例，并在每一次训练迭代中重新评估每个聚类的重要性和采样权重。这种方法能够降低异常值的影响并自动筛选出包含低质量数据的聚类。通过在自然语言推理、一般世界知识、代码和数学推理任务上进行广泛评估，并对各种模型家族进行微调，我们观察到一致性改进，相比于随机选择提高了7%，相较于最先进的采样方法提高了3.8%。我们的工作强调了在微调大型语言模型以增强广泛的评估任务性能时，优先考虑多样性的采样方法的重要性。  我们的代码已开源在https://github.com/for-ai/iterative-data-selection。|
|**2024-09-17**|**Towards Time Series Reasoning with LLMs**|Winnie Chow et.al.|[2409.11376](http://arxiv.org/abs/2409.11376)|null|多模态大型语言模型（MLLMs）在视觉等领域的理解和推理方面取得了重大进展，但时间序列领域尚未看到这种广泛的成功。尽管先前的时间序列MLLM研究在时间序列预测中显示出有希望的表现，但很少有工作展示了如何使用大语言模型进行自然语言的时间序列推理。我们提出了一种新颖的多模态时间序列LLM方法，该方法能够跨各种领域学习通用信息，并具有强大的零样本性能。  首先，我们在LLM顶部训练一个轻量级时间序列编码器，直接提取时间序列信息。然后，我们通过增强的时间序列任务对模型进行微调，以鼓励模型生成推理路径。我们的研究表明，模型学习到的潜在表示反映了特定的时间序列特征（例如斜率、频率），并且在多种领域的一系列零样本推理任务上均优于GPT-4o。|
|**2024-09-17**|**Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**|Fatema-E- Jannat et.al.|[2409.11375](http://arxiv.org/abs/2409.11375)|null|在医疗领域中，获取大量数据面临着显著的挑战，主要是由于隐私问题。然而，为了训练用于视网膜疾病诊断的深度学习模型，需要大量的数据集。在较小数据集上有效泛化的能力仍然是一个持续的挑战。数据稀缺性构成了实施可扩展医疗AI解决方案的实际障碍。  为了解决这个问题，我们结合了多种数据源，以提高性能并增强对新数据的泛化能力，通过赋予模型从多模态数据集中更深入理解数据表示的能力。我们基于大型语言模型（LLMs）和SwinV2框架开发了一个自监督框架，以增强模型对多模态数据集表示的理解，从而提高使用光学相干断层成像（OCT）图像检测眼病的能力。  我们采用了两阶段训练方法，即自监督预训练和下游监督分类器的微调。针对三种不同数据集进行的消融研究，在未融合数据、数据量有限设置和无自监督预训练场景下采用不同的编码器架构，强调了我们方法的稳健性。我们的发现表明，即使在这些多样化的条件下，也表现出一致的性能，并且与基线模型ResNet-50相比，具有更强的泛化能力。|
|**2024-09-17**|**CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration**|Jiahui Gao et.al.|[2409.11365](http://arxiv.org/abs/2409.11365)|null|本文探讨了多模态大型语言模型（MLLM）在面对恶意视觉输入时的安全意识问题。MLLM通常基于大型语言模型构建，并配以图像编码器将图像转换为与人类价值观相一致的文本数据集中的令牌嵌入空间。然而，这种视觉模态的整合引入了一种独特的脆弱性：MLLM对恶意图像输入变得敏感，并倾向于生成可能引发安全或有害响应的输出。  研究发现，通过在MLLM的输入中加入一个原则，以明确定义安全性要求，其安全意识得到了增强。这证实了MLLM在处理图像输入时具有一定的安全意识，但这一能力受到模态差距的影响而减弱。  为此，本文提出了一种简单而有效的技术——CoCA（Calibration of Conditional Awareness），旨在通过调整输出分布来增强MLLM的安全意识。该策略有助于模型恢复其原始的安全意识，同时不牺牲其原有能力。通过在多模态安全性和理解基准上验证了这种方法的有效性。|
|**2024-09-17**|**AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances**|Dhruv Agarwal et.al.|[2409.11360](http://arxiv.org/abs/2409.11360)|null|本文探讨了当西方导向的AI模型向来自不同文化背景的用户提供写作建议时会发生什么情况。我们进行了一个跨文化的受控实验，共有来自印度和美国的118名参与者完成了具有文化基础的写作任务，并在有无AI建议的情况下完成。我们的分析显示，AI为美国人提供了更高的效率增益，相比之下，印度参与者则在采用西方写作风格方面受到影响，不仅改变了所写的内容，也改变了其写作风格。这些发现表明，以西方为中心的AI模型会将写作方式同质化，使之趋向于西方规范，从而削弱了能够体现文化差异的细微之处。|
|**2024-09-17**|**THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models**|Mengfei Liang et.al.|[2409.11353](http://arxiv.org/abs/2409.11353)|**[link](https://github.com/holistic-ai/THaMES)**|本文介绍了一种名为THaMES（工具用于幻觉缓解与评估）的集成框架和库，旨在解决大型语言模型（LLMs）中存在的幻觉生成这一日益增长的挑战。现有的检测和缓解方法往往孤立且无法满足特定领域的需要，缺乏标准化流程。THaMES提供了一个端到端解决方案，涵盖评估和缓解LLMs中幻觉问题的各个环节，包括自动化测试集生成、多维度基准测试以及灵活的缓解策略。它通过批量处理、加权抽样和反事实验证等技术自动创建高质量、多样性和成本效益高的测试集。THaMES评估了模型在文本生成和二分类任务中的幻觉检测与减少能力，并应用了最佳缓解策略，如上下文学习（ICL）、检索增强生成（RAG）和参数高效微调（PEFT）。使用学术论文、政治新闻和维基百科的知识库对前沿LLMs进行评估发现，商业模型如GPT-4o在受益于RAG方面比ICL更多，而开源模型如Llama-3.1-8B-Instruct和Mistral-Nemo则从ICL中获得更大益处。此外，PEFT显著提高了Llama-3.1-8B-Instruct在评估任务中的性能。|
|**2024-09-17**|**Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5**|Marcel Lamott et.al.|[2409.11282](http://arxiv.org/abs/2409.11282)|null|随着各类数字文档格式的激增，尤其是那些非标准化的文档如商业报告和环境评估报告，文档理解变得愈发重要。大型语言模型（LLMs）在多种自然语言处理任务上展现出强大的能力，但在文档理解领域的直接应用仍面临挑战。以往的研究表明LLMs在这一领域具有潜力，然而它们巨大的计算需求使其难以有效地部署。此外，专有的“黑盒”LLMs往往优于开源版本，这构成了广泛可访问性的障碍。本文深入探讨了文档理解的领域，利用了从LLM ChatGPT到FLAN-T5的提炼方法来平衡大模型的强大功能与计算限制。我们提出了一种创新的方法，通过整合标记和课程学习机制来促进知识的有效转移。这项工作对文档理解方法的进展做出了贡献，提供了一个可扩展的解决方案，以弥合资源密集型LLMs与实际应用之间的差距。我们的发现强调了提炼技术在使复杂语言模型在现实世界场景中得到广泛应用的潜力，从而推动自然语言处理和文档理解领域的发展。|
|**2024-09-16**|**RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval**|Di Liu et.al.|[2409.10516](http://arxiv.org/abs/2409.10516)|**[link](https://github.com/jzbjyb/reatt)**|基于转换器的大型语言模型（LLMs）在各个领域变得越来越重要。然而，注意力操作的二次时间复杂度对扩展到更长上下文带来了重大挑战，导致了极高的推理延迟和GPU内存消耗以缓存键值（KV）向量。本文提出了一种无需训练的方法——检索注意力（RetrievalAttention），以加速注意力计算。通过利用注意力操作的动态稀疏特性，RetrievalAttention在CPU内存上构建了近似最近邻搜索（ANNS）索引，并在生成过程中通过向量搜索检索最相关的部分。  由于查询向量与键向量之间的分布外（OOD）问题，现成的ANNS索引仍需要扫描O(N)（通常为所有键的30%）的数据进行精确检索，这无法充分利用高稀疏性。RetrievalAttention首先识别了ANNS基注意力中的OOD挑战，并通过一个适应查询的注意力感知向量搜索算法来解决这一问题，该算法仅访问1-3%的数据，从而实现了亚线性时间复杂度。  RetrievalAttention大幅降低了长上下文LLMs的推理成本，同时显著减少了GPU内存需求，而保持了模型准确性。尤其值得注意的是，RetrievalAttention仅需要16GB的GPU内存即可为具有8B参数的LLM提供服务，支持处理128K个令牌，能够在单个NVIDIA RTX4090（24GB）上生成一个令牌耗时0.188秒。|
|**2024-09-16**|**Context-aware Code Segmentation for C-to-Rust Translation using Large Language Models**|Momoko Shiraishi et.al.|[2409.10506](http://arxiv.org/abs/2409.10506)|null|由于现有C程序中的内存安全性漏洞持续威胁以及Rust语言作为C语言替代品所受到的广泛关注，将C代码转换为Rust代码存在强烈的动机。大型语言模型（LLM）在通过生成比基于规则方法更自然、更安全的代码来自动化这一翻译过程方面显示出潜力。然而，先前的研究表明，LLM生成的Rust代码往往无法编译，即使是相对较小的C程序，这主要归因于两种语言之间的显著差异和上下文窗口限制。  我们提出了一种基于LLM的翻译方案，以提高大规模C代码成功转化为可编译的Rust代码的概率。我们的方法包括三个关键技术：（1）预处理C代码，使其结构和表达式更好地与Rust对齐；（2）将代码分割为最佳大小的翻译单元，以避免超出LLM的上下文窗口限制；（3）通过使用上下文补充提示，迭代编译并修复错误，同时保持不同翻译单元之间的一致性。成功编译是实现功能等效性的首要步骤，因为只有可编译的代码才能进一步进行测试。  在20个基准C程序的实验中，包括那些超过4千行代码的程序，我们成功地将所有程序转化为可编译的Rust代码，没有丢失原始代码的对应部分。|
|**2024-09-16**|**DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction**|John Wu et.al.|[2409.10504](http://arxiv.org/abs/2409.10504)|null|在医学编码等高维或多标签预测任务中，既需要预测的准确性也需要解释的可读性。现有研究往往依赖于局部解释方法，无法提供整个多标签集内每个标签预测背后的全面机制解释。我们提出了一种名为DIctionary Label Attention（简称\method）的模块化解释方法，用于将不可解释的密集嵌入分解到稀疏嵌入空间中。在该空间中，非零元素（字典特征）代表了全局学习的医疗概念。  通过人工评估，我们发现我们的稀疏嵌入比其密集对应物在人类理解上至少提高了50%。我们的自动字典特征识别管道，利用大型语言模型（LLMs），通过检查并总结每个字典特征激活的最高级词汇，揭示了数千个学习到的医疗概念。我们通过一个稀疏的可解释矩阵表示字典特征与医疗代码之间的关系，这不仅增强了模型预测的机制性和全局理解能力，而且在不需要大量人工注释的情况下，保持了竞争力和可扩展性。|
|**2024-09-16**|**Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles**|Kulin Shah et.al.|[2409.10502](http://arxiv.org/abs/2409.10502)|**[link](https://github.com/kulinshah98/llm-reasoning-logic-puzzles)**|近年来，基于Transformer架构的因果语言建模在大型语言模型（LLMs）方面取得了显著的进步。然而，这些模型是否真正发展出了基本的搜索和推理能力，仍是一个持续讨论的话题。本研究旨在探讨因果语言建模能否学会解决复杂的数独谜题这一任务。解决数独谜题需要模型首先在所有空白单元格中进行搜索以决定填充哪个单元格，然后应用适当的策略来填充选定的单元格。有时，策略的应用仅导致单元格可能值的减少，而非确定确切值。在这种情况下，需要对单个单元格应用多个策略。我们发现，经过逻辑步骤序列训练的Transformer模型确实能够学会解决数独谜题（我们的模型正确解决了94.21%的谜题）。我们还对Zebra谜题（又称爱因斯坦谜题）进行了扩展分析，并证明模型能够正确解决92.04%的谜题。此外，我们还研究了训练后的Transformer内部表示，并通过线性探查发现，可以从它们中解码出给定单元格的所有可能值信息，这表明Transformer权重中隐含着强大的推理引擎。|
|**2024-09-16**|**Code Vulnerability Detection: A Comparative Analysis of Emerging Large Language Models**|Shaznin Sultana et.al.|[2409.10490](http://arxiv.org/abs/2409.10490)|null|近年来，软件开发领域对开源项目依赖的增加导致了漏洞问题的显著增长，这一现象引起了广泛关注。本文旨在探讨大型语言模型（LLMs）在识别代码库中的漏洞方面的能力与效果，特别关注了新兴LLM技术的最新进展。通过对比分析，我们评估了包括Llama、CodeLlama、Gemma和CodeGemma在内的最近加入的大型语言模型，以及BERT、RoBERTa和GPT-3等现有最先进的模型在检测软件安全漏洞方面的性能。我们的研究目标是揭示LLM在漏洞检测领域的能力，从而促进不同开源仓库的安全实践提升。结果显示，CodeGemma在检测软件安全漏洞方面取得了最高的F1分数（58%）和召回率（87%）。|
|**2024-09-16**|**XLM for Autonomous Driving Systems: A Comprehensive Review**|Sonda Fourati et.al.|[2409.10484](http://arxiv.org/abs/2409.10484)|null|大型语言模型（LLMs）在各种信息处理任务中展现出了惊人的能力。这些任务涵盖了从数据提取和文献总结到内容生成、预测建模、决策制定以及系统控制等多个方面。此外，视觉大型模型（VLMs）和多模态大型语言模型（MLLMs），即XLMs，能够结合多种数据模态，并利用语言理解的强大力量，从而推动了诸如自动驾驶系统（ADS）等基于信息系统的进步。通过将语言通信与多模式感官输入（如全景图像和激光雷达或雷达数据）相结合，可以采取准确的驾驶行动。在此背景下，本文综述了XLMs在实现自动驾驶方面的潜力。具体而言，我们回顾了ADS和XLMs的相关文献，包括它们的架构、工具和框架。然后，我们详细阐述了部署XLMs以实现自动驾驶解决方案的方法。最后，我们指出了XLM部署在ADS中的相关挑战，并提出了未来研究方向，旨在促进XLM在未来ADS框架中的应用。|
|**2024-09-17**|**Schrodinger's Memory: Large Language Models**|Wei Wang et.al.|[2409.10482](http://arxiv.org/abs/2409.10482)|null|记忆是人类活动的基础；没有记忆，几乎不可能执行日常生活中的任何任务。随着大型语言模型（LLMs）的发展，它们的语言能力正变得越来越接近人类。但LLMs有记忆吗？根据当前的表现，LLMs确实显示出具有记忆的迹象。那么，这种记忆机制背后是什么原理呢？目前的研究缺乏对LLMs记忆能力和底层理论的深入探讨。在本文中，我们利用泛逼近定理（UAT）来解释LLMs的记忆机制。我们还进行了实验来验证各种LLMs的记忆能力，并提出了一种基于这些记忆能力的新方法来评估它们的能力。我们认为，LLMs的记忆工作方式类似于薛定谔的记忆，即只有在查询特定记忆时才会显现出来。我们只能通过响应查询的输出来确定模型是否保留了记忆；否则，它仍然是不确定的。最后，我们扩展了这一概念，通过比较人脑和LLMs的记忆能力，强调了它们在操作机制上的相似性和差异性。|
|**2024-09-16**|**LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot Task Planning**|Jicong Ao et.al.|[2409.10444](http://arxiv.org/abs/2409.10444)|**[link](https://github.com/proneverfake/kios)**|本文提出了一种名为“LLM作为行为树规划器”的新框架，旨在利用大型语言模型（LLMs）在机器人装配任务规划与执行中的行为树（BT）生成。我们引入了四种基于上下文学习的方法，利用LLMs的自然语言处理和推理能力，以BT格式产生任务计划，从而减少人工努力并确保其稳健性和可理解性。此外，我们还评估了对同一任务进行微调的参数较少的LLMs的表现。在模拟和实际世界设置下的实验结果表明，我们的框架提高了LLMs在BT生成方面的性能，通过基于上下文的学习和监督微调，在BT生成方面显著提高了成功率。|
|**2024-09-16**|**A Large-Scale Privacy Assessment of Android Third-Party SDKs**|Mark Huasong Meng et.al.|[2409.10411](http://arxiv.org/abs/2409.10411)|null|本文研究对Android平台上的第三方软件开发工具包（SDK）进行了针对性分析，旨在填补Android软件供应链中的关键空白，关注于用户隐私保护问题。研究主要从两个关键的SDK发布平台，官方平台与大型替代平台，对广泛使用的158个SDK进行了调查。  在隐私泄露方面，我们发现了338个实例，表明这些SDK在未经授权的情况下，非法传输了用户的敏感信息。这可能被用于非法目的，如用户追踪或牟利。  在隐私合规性方面，我们的研究表明，超过30%的被检查SDK并未提供隐私政策，以披露其数据处理实践。对于那些提供了隐私政策的SDK，有37%过度收集了用户数据，而88%则错误地声称拥有访问敏感数据的权利。  我们在一年后重新审视了SDK的最新版本，结果显示，这些令人担忧的趋势并没有得到改善。  基于我们的发现，我们提出了三项行动建议，旨在降低隐私泄露风险并增强Android用户的隐私保护。这项研究不仅对行业提出了紧迫的关注呼吁，也为未来的监管干预提供了关键见解。|
|**2024-09-17**|**Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot**|Bhuvan Sachdeva et.al.|[2409.10354](http://arxiv.org/abs/2409.10354)|null|本文探讨了大型语言模型（LLMs）在医疗保健领域的应用及其面临的挑战，如幻觉、信息不完整和偏见，这影响了它们的可靠性。为了克服这些问题，研究者发布了一个名为“构建你自己的专家机器人”（BYOeB）的平台，允许开发人员创建集成专家验证的LLM驱动的聊天机器人。CataractBot是该平台的第一个实现，它专注于提供有关白内障手术的专家验证回答。初步评估显示了其潜力，但该研究样本量较小且主要为定性分析。本工作中，我们对CataractBot进行了为期24周的大规模部署，涉及318名患者及其陪同人员发送的1992条消息，其中91.71%的回答经过了七位专家的验证。通过分析交互日志，我们发现医疗问题远多于物流问题，幻觉现象可以忽略不计，并且专家评定84.52%的医疗回答准确无误。随着知识库通过专家更正不断扩展，系统的性能得到了19.02%的提升，减少了专家的工作负担。这些发现指导未来LLM驱动的聊天机器人设计的发展方向。|
|**2024-09-13**|**Agents in Software Engineering: Survey, Landscape, and Vision**|Yanxian Huang et.al.|[2409.09030](http://arxiv.org/abs/2409.09030)|**[link](https://github.com/deepsoftwareanalytics/awesome-agent4se)**|**近年来，大型语言模型（LLMs）在各种下游任务中取得了显著成功，尤其是在软件工程（SE）领域中的任务。我们注意到，许多将LLMs与SE结合的研究工作明确或隐含地采用了代理的概念。然而，缺乏对现有工作发展背景的深入综述、分析它们如何结合基于LLM的代理技术优化各种任务以及澄清SE中基于LLM的代理框架。本文旨在进行首次关于结合LLMs与SE的研究综述，并提出SE中基于LLM的代理框架，包括三个关键模块：感知、记忆和行动。同时，我们总结了这两个领域结合时面临的当前挑战，并针对这些挑战提出了未来的机遇。我们维护了一个相关的论文GitHub仓库，地址为：https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE。**|
|**2024-09-13**|**Contri(e)ve: Context + Retrieve for Scholarly Question Answering**|Kanchan Shivashankar et.al.|[2409.09010](http://arxiv.org/abs/2409.09010)|null|### 摘要翻译  学者交流是一个快速发展的领域，蕴含着丰富的知识。然而，由于其非结构化的文档格式，传统的文档检索方法难以从中提取有用信息。学者知识图谱通过构建一个语义网络来解决这一问题，提供了隐藏的洞察、摘要和易于通过查询获取的访问性。自然地，对学者图谱进行问答扩展了更广泛受众的可访问性。但在这一领域的某些知识仍然以非结构化文本形式呈现，因此需要结合解决方案来为问答系统提供支持。本文提出了一种两步解决方案，使用开源大型语言模型（LLM）：Llama3.1对学者-QALD数据集进行处理。  首先，我们从不同的结构化和非结构化数据源中提取与问题相关的内容：DBLP、SemOpenAlex知识图谱以及维基百科文本。  其次，我们实施了提示工程，以提高大型语言模型的信息检索性能。  我们的方法在F1分数上取得了40%的成绩，并观察到一些来自LLM的异常响应，这些响应在论文的最后部分进行了讨论。|
|**2024-09-13**|**Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance**|Lucio La Cava et.al.|[2409.08963](http://arxiv.org/abs/2409.08963)|null|确保内容符合社区准则对于维护健康的在线社交环境至关重要。然而，传统的基于人类的合规性检查在处理用户生成内容的不断增长量和有限的管理员数量时面临着扩展难题。大型语言模型在自然语言理解方面的新进展，为自动化内容合规性验证开辟了新的可能性。本文评估了六个人工智能代理，这些代理基于Open-LLMs，在去中心化社交网络中对规则合规性进行自动验证，这是一个具有挑战性的环境，因为社区的范围和规则各不相同。通过对来自数百个Mastodon服务器的超过50,000条帖子的分析，我们发现人工智能代理能够有效地检测非合规内容、掌握语言上的细微差别，并适应不同的社区上下文。大多数代理还显示出高的一致性和一致性，在评分解释和合规建议上与人工评价者相匹配。通过领域专家的人工评估，确认了代理的可靠性和实用性，这表明它们是半自动化或人机协作内容管理系统的有前景的工具。|
|**2024-09-13**|**Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data Quality Assessment, and Cognitive Forcing Functions**|Zahra Ashktorab et.al.|[2409.08937](http://arxiv.org/abs/2409.08937)|null|本文研究了在人类与人工智能合作进行文本生成任务时，幻觉和认知驱动因素的影响，特别是利用大型语言模型（LLMs）协助生成高质量对话数据。对于这些模型而言，需要数据进行微调，这是提升其性能的关键步骤。在客户服务对话上下文中，数据以人与客服代理之间的对话形式存在，并可借助AI助手生成。在我们的研究中，共招募了11位用户，每位用户完成8项任务，总共完成了88项任务。结果发现，幻觉的存在对数据质量产生了负面影响。我们还发现，尽管认知驱动因素并非总能抵消幻觉对数据质量的不利影响，但幻觉和认知驱动因素共同作用于数据质量，并影响用户如何利用呈现给他们的AI响应。通过分析用户行为，我们揭示了对AI生成响应依赖的明显模式，这强调了在对话AI情境下管理幻觉在AI生成内容中的重要性。|
|**2024-09-13**|**SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**|Paloma Rabaey et.al.|[2409.08936](http://arxiv.org/abs/2409.08936)|**[link](https://github.com/prabaey/synsum)**|**我们提出了SynSUM基准数据集，这是一个合成数据集，将非结构化的临床记录与结构化背景变量联系起来。该数据集由10,000个虚构的患者记录组成，包含表格变量（如症状、诊断和基础条件）以及与之相关的描述虚构患者就诊情况的临床笔记，领域为呼吸疾病。表格部分的数据通过贝叶斯网络生成，其中因果结构和条件概率由专家基于领域知识提出。然后，我们使用大型语言模型（GPT-4o）生成与患者就诊相关的临床笔记，描述患者的症状和额外的上下文信息。  SynSUM数据集主要旨在促进在存在表格背景变量的情况下对临床信息提取的研究，可以通过领域知识将这些变量链接到从文本中提取的概念兴趣点——在SynSUM的情况下是症状。次要用途包括研究表格数据和文本的自动化临床推理、在存在表格和/或文本混杂因素情况下的因果效应估计以及多模态合成数据生成。  该数据集可以从以下链接下载：<https://github.com/prabaey/SynSUM>**|
|**2024-09-13**|**LLM-based Weak Supervision Framework for Query Intent Classification in Video Search**|Farnoosh Javadi et.al.|[2409.08931](http://arxiv.org/abs/2409.08931)|null|流媒体服务已经彻底改变了我们发现和参与数字娱乐的方式。尽管如此，有效理解用户搜索查询的广泛范围仍然面临重大挑战。构建一个能够处理代表不同用户意图的各种实体的准确查询理解系统对于提供增强的用户体验至关重要。通过训练自然语言理解（NLU）模型可以实现这一目标，然而，在这个专门领域的高质量标注数据获取是一个巨大的障碍。手动注释成本高昂且在捕捉用户词汇变异性方面不切实际。为了解决这个问题，我们提出了一种新颖的方法，通过弱监督利用大型语言模型（LLM）自动标注大量用户搜索查询。通过使用提示工程和多样化的LLM角色，我们生成了与人工注释者期望相匹配的训练数据。通过引入领域知识，利用链式思考和上下文学习，我们的方法利用标记数据训练优化用于实时推理的低延迟模型。广泛的评估显示，我们的方法在召回率上优于基线平均提高了113%。此外，我们提出的新型提示工程框架产生用于弱监督的高质量LLM生成数据；与人类注释的F1得分加权分布相比，我们观察到预测和人类注解之间的一致性提高了47.60%。我们的角色选择路由机制进一步增加了3.67%的加权F1得分，这是在新型提示工程框架基础上的额外收益。|
|**2024-09-13**|**AnyBipe: An End-to-End Framework for Training and Deploying Bipedal Robots Guided by Large Language Models**|Yifei Yao et.al.|[2409.08904](http://arxiv.org/abs/2409.08904)|**[link](https://github.com/sjtu-mvasl-robotics/AnyBipe)**|本文提出了一种端到端的框架，用于训练和部署机器人强化学习（RL）策略，该框架利用大型语言模型（LLM）进行引导。该框架由三个相互连接的模块组成：一个通过LLM设计奖励函数的模块、一个利用现有工作的RL训练模块以及一个模拟到现实（sim-to-real）同态评估模块。这种方法显著减少了对人工干预的需求，仅需要基本的模拟和部署平台，并且提供了人工工程策略和历史数据的整合选项。我们详细介绍了这些模块的构建、它们相对于传统方法的优势，以及展示该框架在双足机器人步态控制自主开发和改进能力的实例，证明其在不需要人类干预的情况下操作的可能性。|
|**2024-09-13**|**A Market for Lemons? Strategic Directions for a Vigilant Application of Artificial Intelligence in Entrepreneurship Research**|Martin Obschonka et.al.|[2409.08890](http://arxiv.org/abs/2409.08890)|null|在人工智能（AI）采用的迅速增长以及大数据可用性的背景下，创业学领域可能迎来有史以来最重大的转变。本文通过强调AI革命期间创业研究中潜在的无成效知识交流风险，做出了紧迫的元贡献。它提供了缓解这一风险的策略，并为未来基于AI的研究提供了指导，以增强其集体影响力和相关性。  借鉴Akerlof著名的“劣质商品市场”概念，我们识别了由于领域演进到当前环境而可能出现的重大知识不对称性，如构造有效性、理论构建和研究相关性方面的复杂性。这些不对称性特别深植于所谓的双重黑箱困境中，即AI方法的广泛认可的黑箱性质与由内在不确定性驱动的创业现象的黑箱性质的交汇点。结果，这些不对称可能导致不可检测的次优研究产品增加，从而形成一个损害领域福祉、声誉和影响力的劣质商品市场。  然而，重要的是，如果能够缓解这些风险，AI革命有可能预示着创业研究的新黄金时代。我们讨论了提升领域至更高水平的AI韧性所需采取的行动，同时坚定地保持其基础原则和核心价值观。|
|**2024-09-13**|**Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies**|Zhiqiang Zhong et.al.|[2409.08864](http://arxiv.org/abs/2409.08864)|null|大型语言模型（LLM）在处理各种数据结构时展现了惊人的能力，包括图。尽管先前的研究集中在开发用于图表示的文本编码方法上，但多模态LLM的出现为理解图提供了一个新的前沿。这些先进的模型能够同时处理文本和图像，通过结合视觉表示与传统的文本数据，可能在提高对图结构的理解方面带来改进。这项研究探讨了可视化图在不同级别（节点、边和图级别）上对LLM性能的影响。我们的实验对比了多模态方法与纯文本图表示的有效性。结果提供了关于利用视觉图模态增强LLM对图结构理解能力的潜力和限制的宝贵见解。|
|**2024-09-13**|**FP-VEC: Fingerprinting Large Language Models via Efficient Vector Addition**|Zhenhua Xu et.al.|[2409.08846](http://arxiv.org/abs/2409.08846)|null|训练大型语言模型（LLMs）需要巨大的计算能力和大量的数据。因此，通过指纹保护这些模型的知识产权对于所有权认证至关重要。尽管尝试通过微调向LLMs添加指纹，但这仍成本高昂且难以扩展。为此，我们提出了FP-VEC，一种使用指纹向量作为高效LLM指纹方法的试点研究。我们的方法生成一个代表嵌入在模型中的保密签名的指纹向量，允许通过向量相加无缝地将相同的指纹整合到无限数量的LLMs中。在多个LLMs上的结果表明，FP-VEC轻量级，可以在仅使用CPU的设备上运行以进行指纹识别；可扩展，只需要一次训练即可实现无限次的指纹生成过程，并且能够保持模型的正常行为。项目页面位于https://fingerprintvector.github.io 。|
|**2024-09-12**|**Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale**|Rogerio Bonatti et.al.|[2409.08264](http://arxiv.org/abs/2409.08264)|**[link](https://github.com/microsoft/windowsagentarena)**|**大型语言模型（LLM）展现出在需要规划和推理的多模态任务中作为计算机代理的强大潜力，能显著提升人类生产力和软件可访问性。然而，衡量这些代理在真实环境中的性能仍存在挑战：（i）大多数基准测试仅限于特定模态或领域（例如纯文本、网页导航、问题回答、编程），（ii）完整基准评估耗时长（通常需数天时间），因为任务具有多步骤的序列性质。  为解决这些挑战，我们引入了“Windows Agent Arena”：一个可复现的通用环境，专注于Windows操作系统，允许代理自由操作并使用与人类用户在解决任务时相同的广泛应用程序、工具和网络浏览器。我们根据OSWorld框架（Xie等人，2024年）创建了150多个跨代表领域的多样化Windows任务，这些任务涵盖了规划、屏幕理解及工具使用的代理能力要求。  我们的基准具有可扩展性，并能够无缝地在Azure上并行化，从而在短短20分钟内完成全面基准评估。为了展示Windows Agent Arena的能力，我们还引入了一个新的多模态代理Navi。Navi在Windows领域内的成功率达到了19.5%，相比之下，未经辅助的人类表现则为74.5%。此外，Navi在另一个流行的基于网络的基准测试Mind2Web中也表现出色。  我们提供了对Navi性能的详细定量和定性分析，并提供了利用Windows Agent Arena进行未来研究的代理开发和数据生成机会的见解。网页：https://microsoft.github.io/WindowsAgentArena  代码：https://github.com/microsoft/WindowsAgentArena**|
|**2024-09-12**|**OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering**|Jiahao Nick Li et.al.|[2409.08250](http://arxiv.org/abs/2409.08250)|null|人们常通过照片、屏幕截图和视频来捕捉记忆。现有的基于AI的工具能够使用自然语言检索这些数据，但主要局限于检索像照片中的特定物体这样的单一信息，难以处理涉及理解相互关联记忆（如事件序列）的更复杂查询。我们进行了一项为期一个月的日志研究，收集了现实用户查询，并生成了一个集成与捕获记忆相关必要上下文信息的分类体系。随后，我们引入了OmniQuery，这是一种能够回答需要提取和推断多层上下文信息以整合相互关联记忆的复杂个人记忆相关问题的新型系统。OmniQuery通过从多个相互关联的记忆中集成分散的上下文信息来增强单个捕获的记忆，检索相关记忆，并利用大型语言模型（LLM）提供全面的答案。在人类评估中，我们展示了OmniQuery的有效性，准确率达到71.5%，并且它在74.5%的时间里超越了传统的RAG系统，在某些任务上甚至取得了胜利或并列第一的成绩。|
|**2024-09-12**|**Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources**|Alisia Lupidi et.al.|[2409.08239](http://arxiv.org/abs/2409.08239)|null|在面对依赖结构化数据、复杂推理或工具使用的挑战性场景时，大型语言模型仍然存在困难。为此，我们提出了一种名为Source2Synth的新方法，它无需昂贵的人类标注即可用于教授LLMs新技能。Source2Synth接受自定义数据源作为输入，并生成具有基于现实世界来源的中间推理步骤的合成数据点。该方法通过根据其可回答性丢弃低质量生成来提高数据集质量。我们通过在两个具有挑战性的领域中应用此方法来展示其通用性：在多跳问题回答（MHQA）中测试推理能力，在表格型问题回答（TQA）中测试工具使用。与经过微调的基本模型相比，我们的方法在WikiSQL上的TQA上提高了25.51%，在HotPotQA上的MHQA上提高了22.57%的性能。|
|**2024-09-12**|**LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems**|Hakan T. Otal et.al.|[2409.08234](http://arxiv.org/abs/2409.08234)|**[link](https://github.com/ai-in-complex-systems-lab/llm-honeypot)**|**本文介绍了一种创新方法，使用大型语言模型（LLMs）构建真实且互动的蜜罐系统。通过在包含攻击者生成命令和响应的多样化数据集上对开源预训练语言模型进行微调，我们开发出一种能够与攻击者进行高级交互的蜜罐。我们的方法涉及关键步骤：数据收集与处理、提示工程、模型选择以及监督式微调，以优化模型性能。通过相似性指标评估与现场部署，结果显示我们的方法能够生成准确且信息丰富的响应。研究结果强调了LLMs在重塑蜜罐技术方面的潜力，为网络安全专业人员提供了一个强大的工具来检测和分析恶意活动，从而增强整体安全架构。**|
|**2024-09-12**|**What Makes a Maze Look Like a Maze?**|Joy Hsu et.al.|[2409.08202](http://arxiv.org/abs/2409.08202)|null|人类视觉理解的独特之处在于能够灵活地解释抽象概念的能力：获取提升规则来解释它们所象征的含义，在熟悉和不熟悉的上下文中锚定它们，并对它们进行预测或推理。尽管现成的视觉语言模型在识别图像中的具体对象类别（如树枝）方面表现出色，但它们仍然难以理解这样的视觉抽象（例如，一组树枝如何形成迷宫的墙壁）。为了应对这一挑战，我们引入了深度架构接地（DSG），这是一个利用明确的结构化表示法来锚定和推理视觉抽象的框架。DSG的核心是架构——分解抽象概念的依赖图形描述，将其分解为更基本的符号。DSG使用大型语言模型提取架构，然后通过视觉语言模型分层地将架构中的具体到抽象组件锚定到图像上。锚定后的架构用于增强对视觉抽象的理解。我们系统地评估了DSG及其不同的方法在我们新创建的视觉抽象数据集上的推理性能，该数据集由人类标注的真实世界图像和相应的问答对组成。我们展示了DSG显著提高了视觉语言模型在抽象视觉推理方面的表现，并朝着与人类一致的视觉抽象理解迈进了一步。|
|**2024-09-12**|**Fine-tuning Large Language Models for Entity Matching**|Aaron Steiner et.al.|[2409.08185](http://arxiv.org/abs/2409.08185)|**[link](https://github.com/wbsg-uni-mannheim/tailormatch)**|**本文探讨了利用大型语言模型（LLM）进行实体匹配的潜力，特别是通过微调。已有研究主要集中在提示工程和基于上下文的学习上。本文从两个维度分析了微调的可行性：1）训练示例的表示方式，实验涉及在训练集中添加不同类型的LLM生成解释；2）使用LLM选择和生成训练示例。我们不仅关注源数据集上的匹配性能，还研究了微调对模型在同域数据集以及跨领域数据集上的泛化能力的影响。  实验结果显示，微调显著提升了小型模型的性能，而大型模型的表现则参差不齐。微调在提升同域数据集的泛化能力的同时，也影响了跨域迁移的能力。我们发现，向训练集添加结构化的解释对四种LLM中的三种有正面影响，而提出的示例选择和生成方法仅提升了Llama 3.1 8B的性能，同时降低了GPT-4o Mini的性能。**|
|**2024-09-12**|**Faster Speech-LLaMA Inference with Multi-token Prediction**|Desh Raj et.al.|[2409.08148](http://arxiv.org/abs/2409.08148)|null|大型语言模型（LLMs）在解决各种任务上变得极为熟练，包括涉及多模态输入的任务。具体来说，通过使用语音编码器实例化LLM（例如LLaMA）并利用配对数据对其进行训练，可以赋予只解码的模型语音识别（ASR）能力，因此称之为Speech-LLaMA。然而，由于自回归推理的顺序性质以及相对较大的解码器，Speech-LLaMA模型的推理时间相对较高。本工作中，我们提出通过在同一解码步骤中预测多个令牌来加速Speech-LLaMA的推理。我们探索了几个能够实现这一目标的模型架构，并通过阈值推理和验证推理策略来评估它们的性能。此外，我们还提出了一个基于前缀的束搜索解码方法，允许此类模型进行高效的最小词错误率（MWER）训练。我们在多种公共基准上评估了我们的模型，结果显示它们将解码调用的数量减少了约3.2倍，同时保持或提高了WER性能。|
|**2024-09-12**|**LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models**|Zhengliang Liu et.al.|[2409.08147](http://arxiv.org/abs/2409.08147)|null|本文提出了一种利用大型语言模型（LLM）来评估总统辩论表现的新方法，旨在解决长期存在的客观评估辩论结果的挑战。我们构建了一个框架，从“政策、个性与视角”（3P）和“兴趣、意识形态与身份认同”（3I）的角度分析四位关键受众群体：选民、企业、捐赠者及政客对候选人的共鸣。该方法通过生成“LLM-POTUS评分”，即基于3P与3I之间一致性度量的量化指标，来评价辩论表现。我们应用此框架对近期美国总统辩论的文本进行分析，揭示了不同辩论策略的有效性及其对不同受众群体的影响。研究不仅提供了一个新的政治分析工具，还探索了在复杂社会背景下使用LLM作为公正评判者的潜力与局限性。此外，该框架为个人公民提供了一个独立的工具，用于评估总统辩论的表现，从而增强民主参与度，减少对可能偏见的媒体解读和机构影响力的依赖，进而加强知情公民参与的基础。|
|**2024-09-12**|**The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal**|Huiyuan Xie et.al.|[2409.08098](http://arxiv.org/abs/2409.08098)|null|本文研究了技术革新与获取公正之间的交汇点，通过在英国就业法庭（UKET）构建预测案例结果的基准。为了应对大量人工注释的挑战，该研究利用大型语言模型（LLM）进行自动注释，从而创建了CLC-UKET数据集。该数据集包含约19,000个UKET案例及其元数据。全面的法律注释涵盖了事实、主张、先例引用、法规引用、案例结果、理由和管辖权代码。借助CLC-UKET数据，我们对UKET的多类案例结果预测任务进行了研究。收集了人类预测以建立模型比较的性能参考。从基础模型的实证结果来看，微调的转换器模型在UKET预测任务上优于零次和少量样本的LLM。零次LLM的性能可以通过整合与任务相关的信息来增强，融入少量样本示例中。我们希望CLC-UKET数据集、人类注释以及实证发现能够作为就业相关纠纷解决的宝贵基准。|
|**2024-09-12**|**Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks**|Benji Peng et.al.|[2409.08087](http://arxiv.org/abs/2409.08087)|null|本文综述了近年来有关大型语言模型（LLM）安全性的关键问题的研究文献，重点是准确性、偏见、内容检测以及对抗攻击的脆弱性。文章详细讨论了LLM输出可能不准确或误导性的问题，并强调了通过事实核查方法增强响应可靠性的实施策略。文章深入探讨了内嵌于LLM中的固有偏见，通过多样化的评估技术，如控制输入研究和红队演练，对其进行批判性审视。提出了全面的偏见缓解策略分析，包括从预处理干预到训练期间调整和后处理改进的各种方法。此外，文章还探究了区分LLM生成内容与人类创作文本的复杂性，引入了诸如DetectGPT的检测机制以及水印技术，同时指出在复杂情况下基于机器学习的分类器存在局限性。文章还分析了LLM的漏洞，包括逃逸攻击和提示注入攻击，通过案例研究和大规模竞赛HackAPrompt等进行了深入探讨。最后，文章回顾了保护LLM的防御措施，强调了需要对LLM安全性领域进行更深入研究的重要性。|
|**2024-09-11**|**"My Grade is Wrong!": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays**|Shengxin Hong et.al.|[2409.07453](http://arxiv.org/abs/2409.07453)|null|交互式反馈在教师与学生之间双向流动，相较于传统的单向反馈更为有效。然而，这种反馈方式往往耗时过多，难以在教育实践中广泛应用。虽然大型语言模型（LLM）具有自动化反馈的潜力，但它们在互动情境下的推理和交互方面存在困难。本文提出了一种名为CAELF（Contestable AI Empowered LLM框架），旨在通过集成多代理系统与计算论辩来自动化交互式反馈。首先，学生的作文由多个教学助理代理（TA代理）进行评估，随后，教师代理通过形式化推理整合这些评价，生成反馈和评分。学生可以进一步与反馈互动，以深化理解。通过对500篇批判性思维作文的案例研究，并结合用户研究，结果表明，CAELF显著提高了交互式反馈的质量，增强了LLM的推理和互动能力。这一方法提供了一个克服影响教育领域广泛应用交互式反馈的时间和资源障碍的有前景解决方案。|
|**2024-09-11**|**SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories**|Ben Bogin et.al.|[2409.07440](http://arxiv.org/abs/2409.07440)|**[link](https://github.com/allenai/super-benchmark)**|**给定大型语言模型（LLM）在编写代码方面取得的重大进展，它们现在是否能够自主重现研究仓库中的结果？这样的能力将对研究社区产生巨大益处，帮助研究人员验证、理解并扩展先前的工作。为了向这一目标迈进，我们引入了SUPER，这是首个旨在评估LLM在从研究仓库设置和执行任务方面的能力的基准。SUPER旨在捕捉研究人员在机器学习（ML）和自然语言处理（NLP）研究仓库工作时所面临的真实挑战。我们的基准由三个不同的问题集组成：45个端到端问题，附有专家解决方案的注释，152个专注于特定挑战（例如配置训练器）的子问题，以及602个用于更大规模开发的自动生成问题。我们引入了各种评估指标来评估任务成功和进度，当有黄金解决方案可用时使用黄金解决方案，否则使用近似值。我们展示了最先进的方法在解决这些问题时遇到了困难，最好的模型（GPT-4o）仅解决了16.3%的端到端集和46.1%的场景。这表明了这项任务的挑战性，并表明SUPER可以作为社区衡量和推动进步的宝贵资源。**|
|**2024-09-11**|**CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification**|Zeqing Qin et.al.|[2409.07407](http://arxiv.org/abs/2409.07407)|null|大型语言模型（LLM）在漏洞识别领域展现出了巨大的潜力。由于C/C++在过去十年中占据了开源软件（OSS）漏洞的一半，并且主要通过提交进行更新，因此增强LLM在识别C/C++漏洞贡献提交（VCC）方面的能力变得至关重要。然而，当前的研究主要集中在对大规模代码集进一步预训练LLM上，这既耗费资源又存在效率挑战。本文提出了一种轻量级方法来提升基于BERT的LLM识别C/C++ VCC的能力。我们提出了CodeLinguaNexus（CLNX），作为连接C/C++程序与LLM的桥梁。CLNX通过在保留关键细节的同时，以更自然的方式高效地将源代码转换为更适合LLM处理的表示。具体来说，CLNX首先应用结构级自然化来分解复杂的程序，然后应用符号级自然化来解释复杂的符号。我们在包含25,872个C/C++函数及其提交的公开数据集上评估了CLNX。结果表明，CLNX显著提升了LLM识别C/C++ VCC的能力。此外，配备CLNX的CodeBERT达到了新的最优性能，并在真实世界中识别了38个OSS漏洞。|
|**2024-09-11**|**AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge**|Han Wang et.al.|[2409.07394](http://arxiv.org/abs/2409.07394)|**[link](https://github.com/hannight/adacad)**|**在大语言模型（LLM）的上下文与模型参数存储的知识之间存在知识冲突，这会导致使用标准解码技术时性能受损，因为这些技术往往忽视了上下文。现有的测试时间对比方法试图通过比较带有和不带有上下文的LLM输出分布之间的对比，并根据它们之间的对比调整模型来解决这个问题。然而，我们发现这些方法经常错误地判断冲突的程度，并且难以处理不同冲突程度的实例，静态方法在冲突不存在时过度调整。为此，我们提出了一种基于实例的精细粒度方法AdaCAD，它动态地根据Jensen-Shannon散度测量的上下文和参数知识分布之间的冲突程度来推断调整权重。我们在四个模型上对六个多样化的问答（QA）数据集和三个摘要任务进行的实验显示，我们的无需训练的自适应方法始终在问答任务上优于其他解码方法，平均准确率提高了14.21%（绝对值），并且提高了摘要的真实性，AlignScore提高了5.59分。此外，我们的分析表明，与冲突的对比基线相比，当冲突不存在时，解码会损害性能，而AdaCAD能够缓解这些损失，使其更适用于现实世界的数据集，在这些数据集中，有些示例存在冲突，而其他示例则不存在冲突。**|
|**2024-09-11**|**Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**|Khiem Ton et.al.|[2409.07368](http://arxiv.org/abs/2409.07368)|null|本文介绍了一种名为SGCode的灵活提示优化系统，用于通过大型语言模型（LLM）生成安全代码。SGCode将最近的提示优化方法与LLM结合在一个统一的系统中，通过前端和后端API提供服务，使用户能够：1）生成无漏洞的安全代码；2）查看和共享安全性分析；以及3）轻松在不同的提示优化方法之间切换，并提供有关模型和系统性能的见解。我们使用AWS服务器上的PromSec填充SGCode，这是一种方法，通过将LLM、安全工具与轻量级生成对抗图神经网络相结合，来检测并修复生成代码中的安全漏洞，从而优化提示。广泛的实验表明，SGCode作为公共工具，能够揭示模型实用性、安全代码生成和系统成本之间的权衡，具有相对较低的成本。SGCode已上线于：<http://3.131.141.63:8501/>。|
|**2024-09-11**|**Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation**|SeongYeub Chu et.al.|[2409.07355](http://arxiv.org/abs/2409.07355)|**[link](https://github.com/BBeeChu/InteractEval)**|**本文介绍了一种名为“InteractEval”的框架，该框架采用“Think-Aloud”方法结合大型语言模型（LLM）与人类专家意见，以生成基于检查清单的文本评估的属性。通过融合人类的灵活性和推理能力以及LLM的一致性，InteractEval在一致性、流畅性、相关性和连贯性四个维度上均超越了传统的非LLM基线和LLM基线模型。实验还探讨了“Think-Aloud”方法的有效性，表明它能促进人类和LLM的发散思维，从而产生更广泛的相关属性，并提高文本评估性能。比较分析显示，人类在识别与内部质量相关的属性（如连贯性和流畅性）方面表现优异，而LLM在与外部对齐相关的属性（如一致性和相关性）上表现更好。因此，结合人类和LLM共同产生的评估结果最佳。换句话说，本文强调了在自动化基于检查清单的文本评估框架中有效整合人类和LLM的必要性。代码已开源于\textbf{\url{https://github.com/BBeeChu/InteractEval.git}}}。**|
|**2024-09-11**|**Learning to Compress Contexts for Efficient Knowledge-based Visual Question Answering**|Weixi Weng et.al.|[2409.07331](http://arxiv.org/abs/2409.07331)|null|多模态大型语言模型（MLLMs）在视觉问答（VQA）任务上展示了出色的零样本性能。然而，在知识基视觉问答（KB-VQA）任务中，MLLMs可能缺乏人类常识或特定领域的专业知识，从而需要从外部知识源获取所需信息以回答此类问题。先前的工作，如检索增强的VQA-v2（RAVQA-v2），侧重于充分利用输入信息，例如图像文本描述和检索的知识，以提高性能，但它们都忽视了一个问题：随着输入令牌数量的增加，推理效率显著降低，这与实际应用的需求相矛盾。为了解决这一问题，我们提出了检索增强的多模态大语言模型（RACC）。RACC学习压缩并聚合检索上下文，并生成紧凑的键值（KV）缓存形式的调节。然后，使用这种调节来适应下游冻结的MLLM，从而实现有效且高效的推理。RACC在OK-VQA上实现了当前最佳的62.9%性能。此外，它将RAVQA-v2的推理延迟显著降低了22.0%-59.7%。大量的实验表明了RACC的广泛适用性。它与各种现成的MLLM兼容，并可以处理包括文本和多模态文档在内的不同知识源。|
|**2024-09-11**|**MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**|Praveen K Kanithi et.al.|[2409.07314](http://arxiv.org/abs/2409.07314)|null|大型语言模型（LLM）在医疗健康领域的快速开发引发了对超越如USMLE等常用基准评估的全面评估需求，以更好地反映实际应用表现。虽然现实世界的评估是实用性的重要指标，但它们往往落后于LLM演进的速度，可能导致研究结果在部署时变得过时。这种时间上的脱节需要一种全面的前期评估方法，以指导特定临床应用中的模型选择。  我们引入了MEDIC框架，它从五个关键的临床能力维度评估LLM：医学推理、伦理与偏见、数据和语言理解、上下文学习以及临床安全性。MEDIC采用了一种新颖的交叉审查框架，量化了LLM在覆盖范围和幻觉检测等领域的性能，而无需参考输出。我们使用MEDIC对医疗问答、安全、总结、笔记生成以及其他任务进行了评估。  我们的结果显示不同模型大小之间、基线模型与医学微调模型之间的性能差异，并对需要特定模型优势的应用（如低幻觉或较低推理成本）的模型选择具有启示意义。MEDIC的多维度评估揭示了理论能力和实际实施之间的性能权衡，弥合了在医疗保健环境中识别和适应最有前景模型的差距，确保了适合多种医疗保健应用的模型得到识别和适应。|
|**2024-09-11**|**STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM**|Qijiong Liu et.al.|[2409.07276](http://arxiv.org/abs/2409.07276)|null|传统推荐模型通常依赖于独特的项目标识符（ID）来区分项目，这可能限制了它们利用项目内容信息和推广长尾或冷启动项目的能 力。近期，已提出语义分词作为解决这一问题的有希望的方法，旨在将每个项目的语义表示分词为一系列离散的令牌。通过这种方式，它保 留了项目在这些令牌内的语义，并确保具有相似语义的项目由相似的令牌表示。这些语义令牌成为训练生成推荐模型的基础。然而，现有 的生成推荐方法通常涉及多个子模型进行嵌入、量化和推荐，导致系统过于复杂。在这篇论文中，我们提出了一种统一框架，称为STORE， 利用单一大型语言模型（LLM）同时执行这两项任务。具体而言，我们将语义分词表述为文本到令牌的任务，而生成推荐则表述为令牌到 令牌的任务，通过补充令牌到文本重构任务和文本到令牌辅助任务，所有这些任务均以生成方式表述并使用单一LLM骨干进行训练。 我们进行了大量实验，以验证我们的STORE框架在各种推荐任务和数据集上的有效性。我们将发布源代码和配置，以便进行可复现的研究。|
|**2024-09-11**|**MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving**|Enming Zhang et.al.|[2409.07267](http://arxiv.org/abs/2409.07267)|**[link](https://github.com/emzucas/minidrive)**|本文提出了一种名为MiniDrive的新型框架，旨在解决视觉语言模型（VLM）在自动驾驶场景中的应用难题。现有的VLM方法通常依赖于计算密集型的视觉编码器和大型语言模型（LLMs），这使得它们难以在实际世界和实时应用中部署。此外，大多数现有VLM缺乏处理多张图片的能力，这使得它们难以适应自动驾驶中的多摄像头感知需求。  为了解决这些问题，我们引入了两个关键模块：特征工程混合专家（FE-MoE）和动态指令适配器（DI-Adapter）。FE-MoE有效地将二维特征映射到视觉令牌嵌入，然后作为输入传递给语言模型。DI-Adapter允许视觉令牌嵌入根据指令文本嵌入动态变化，解决了以往方法中同一图片下静态视觉令牌嵌入的问题。  与之前的成果相比，MiniDrive在参数大小、浮点运算量和响应效率方面均达到了最优性能，最小版本仅包含83M参数。|
|**2024-09-10**|**E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning**|Zihan Liao et.al.|[2409.06679](http://arxiv.org/abs/2409.06679)|null|在大型语言模型（LLMs）领域，处理长文本上下文的能力对于多轮对话、代码生成和文档摘要等任务愈发重要。本文探讨了增强长文本上下文性能、降低计算复杂性以及充分利用预训练模型所面临的挑战——即所谓的“不可能三角”。我们提出了一种名为E2LLM（编码器扩展大型语言模型）的创新方法，旨在有效解决这一悖论。  该方法的核心思想是将长文本上下文划分为多个片段，并通过预训练的文本编码器将每个片段压缩为嵌入向量。然后利用适配器将这些表示与解码器型LLM对齐，以促进对软提示的理解。本文提出了两个训练目标：一是重建编码器输出，二是针对长文本指令进行微调，以帮助LLM理解软提示。  实验结果表明，E2LLM在长文本上下文场景中取得了显著的性能提升，同时保持了效率、性能和与预训练模型的兼容性。因此，我们的框架代表了领域内的重大进展，为有效的大文本建模做出了贡献。|
|**2024-09-10**|**LLaMA-Omni: Seamless Speech Interaction with Large Language Models**|Qingkai Fang et.al.|[2409.06666](http://arxiv.org/abs/2409.06666)|**[link](https://github.com/ictnlp/llama-omni)**|**针对大型语言模型（LLM）通过语音实现实时交互的能力提升，相较于传统的文本交互方式，模型如GPT-4显著增强了用户体验。然而，当前在基于开源LLM构建语音交互模型方面仍缺乏深入探索。为了填补这一空白，我们提出了一种新型模型架构——LLaMA-Omni，旨在实现低延迟与高质量的语音与LLM交互。该架构融合了预训练的语音编码器、语音适配器、LLM和流式语音解码器，无需进行语音转录，即可直接从语音指令生成文本和语音响应，响应速度极快。  我们的模型基于最新的Llama-3.1-8B-Instruct模型构建，并针对语音交互场景构建了一个名为InstructS2S-200K的数据集，其中包含了20万条语音指令及其对应的语音回应。实验结果显示，与以往的语音语言模型相比，LLaMA-Omni在内容与风格上提供了更好的响应，响应延迟低至226毫秒。此外，训练LLaMA-Omni仅需不到3天的时间，在4块GPU上即可完成，这为未来高效开发语音语言模型铺平了道路。**|
|**2024-09-10**|**Human Perception of LLM-generated Text Content in Social Media Environments**|Kristina Radivojevic et.al.|[2409.06653](http://arxiv.org/abs/2409.06653)|null|新兴技术，尤其是人工智能（AI）和大型语言模型（LLM），为恶意行为者提供了操纵数字对话的强大工具。LLM有可能影响传统形式的民主参与，例如选民选择、政府调查或与监管机构的在线交流，因为机器人能够生成大量可信文本。为了研究人类对LLM生成内容的感知，我们招募了超过1000名参与者，然后让他们尝试在社交媒体讨论线程中区分机器人与人类帖子。我们发现人类在识别社交媒体上的真实用户帖子方面表现不佳。我们也发现了人类在社交媒体对话中识别LLM生成文本内容的模式。最后，我们观察到了“怪异谷”效应在文本对话中的存在，无论是在感知还是识别过程中。这表明尽管人类在识别过程中的表现不佳，但当阅读LLM生成的内容时，他们仍能感受到不适。|
|**2024-09-10**|**Optimal Workload Placement on Multi-Instance GPUs**|Bekir Turkkan et.al.|[2409.06646](http://arxiv.org/abs/2409.06646)|null|本文旨在探讨如何优化大型语言模型（LLM）为基础的AI推理工作负载在GPU上的部署。我们首先识别并阐述了实践中遇到的一些需要高效分配或迁移工作负载到其他GPU以腾出空间供新工作负载使用的情况。目标是尽可能减少使用的GPU数量，并进一步降低被利用GPU中的内存和计算浪费。  为了实现这一目标，我们提出了两种方法：一种是优化方法，另一种是启发式方法。我们使用两种工作负载调度启发式算法对多种用例进行了基准测试。结果显示，在与基线启发式相比的情况下，我们能够节省高达2.85倍的GPU使用量，以及高达70%的GPU浪费。  我们计划让SRE（系统可靠性工程）社区能够在生产环境中利用我们的提议方法。|
|**2024-09-10**|**MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders**|Wenyu Zhang et.al.|[2409.06635](http://arxiv.org/abs/2409.06635)|null|快速发展的大型语言模型（LLM）显著提高了自然语言处理能力，促进了音频LLM的发展，这些模型能够理解语音和音频输入。现有的音频LLM通常结合预训练的音频编码器与文本预训练的LLM，并在特定的音频任务上进行微调。然而，预训练的音频编码器的容量有限，无法捕获新任务和数据集中的特征。为了应对这一问题，我们提出将“弱”编码器混合（MoWE）融入音频LLM框架。MoWE通过在基本编码器基础上补充一组相对较轻量级的编码器，根据音频输入动态激活以增强特征提取，同时避免显著增加模型大小。我们的实验结果表明，MoWE有效提高了多任务性能，使音频LLM能够应用于更多样化的音频任务。|
|**2024-09-10**|**A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio**|Ningyuan Xi et.al.|[2409.06624](http://arxiv.org/abs/2409.06624)|null|本文研究了大规模语言模型（LLM）在持续预训练（CPT）过程中，如何通过额外语言混合比（ALMR）和学习率（LR）之间的最优相关性，提升模型在中文及其他特定领域的性能。针对8B大小的Llama-3模型，我们进行了深入研究，确定了实验设置中的关键超参数，并通过精细调整，显著提升了模型在中文相关的基准测试以及数学、编程和情绪智能等特定领域的能力。最终，我们将70B大小的LLM部署到实际聊天系统中，并取得了令人满意的效果。|
|**2024-09-10**|**Alleviating Hallucinations in Large Language Models with Scepticism Modeling**|Yetao Wu et.al.|[2409.06601](http://arxiv.org/abs/2409.06601)|null|大型语言模型（LLM）面临的主要挑战是幻觉现象，这阻碍了其在多个领域的应用。不确定性估计可以被用于缓解幻觉带来的损害。人类的怀疑情绪被认为能增强自我评估的能力。基于这一观察，我们提出了一种名为“质疑建模”（SM）的新方法。这一方法通过结合词元和logits信息来进行自我评估而得到形式化。我们构建了包含怀疑情绪意识的数据集，并进行连续预训练，然后对LLM进行微调，从而提升它们自我评估的能力。实验结果证明了这种方法有效增强了模型估算不确定性的能力，并通过跨领域实验验证了其在其他任务中的泛化能力。|
|**2024-09-10**|**GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering**|Sacha Muller et.al.|[2409.06595](http://arxiv.org/abs/2409.06595)|**[link](https://github.com/illuin-tech/grouse)**|本文探讨了使用大型语言模型（LLMs）与私有且更新至最新的知识库相结合的检索增强生成（RAG）范式时面临的挑战。我们特别关注评估由RAG系统生成的基于现实的答案时，作为裁判的LLM所遇到的问题。为了评估裁判模型的校准和区分能力，我们识别了7种生成器失败模式，并引入了GroUSE（基于问题解答的元评估基准），这是一个包含144个单元测试的元评估基准。这个基准揭示了现有的自动化RAG评估框架往往忽视了重要失败模式，即使在使用GPT-4作为裁判的情况下也是如此。  为了改进当前自动化RAG评估框架的设计，我们提出了一种新的管道，并发现封闭模型在GroUSE上表现良好，而最先进的开源裁判模型在我们的提议标准下并未表现出良好的泛化能力，尽管它们与GPT-4的判断高度相关。我们的研究结果表明，与GPT-4的相关性是一个不完整的代理指标，用于衡量裁判模型的实际性能，并应该通过对参考情况的精确失败模式检测进行补充评估。  进一步的研究显示，通过在GPT-4的推理痕迹上对Llama-3进行微调，显著提升了其评估能力，不仅提高了与GPT-4评价的相关性和参考情况的校准度。|
|**2024-09-10**|**MAPS: Energy-Reliability Tradeoff Management in Autonomous Vehicles Through LLMs Penetrated Science**|Mahdieh Aliazam et.al.|[2409.06558](http://arxiv.org/abs/2409.06558)|null|随着自动驾驶车辆的日益普及，对高度精确和高效的系统的需求也在不断增长，以提升安全性能、操作效率和能源消耗。在管理能源与可靠性之间的权衡时，预测车辆运行期间的各种条件变得尤为重要。近年来，大型语言模型（LLMs）的改进以及知名模型如ChatGPT的出现，为自动驾驶相关预测提供了独特的机会。本文提出了一种名为MAPS的方法，利用LLMs作为地图阅读辅助驾驶员，预测在自动驾驶车辆操作过程中设置的关键参数，以平衡能源与可靠性之间的权衡。MAPS方法在导航精度方面相较于最佳基线方法提高了20%。此外，MAPS还显示了在计算单元上节省了11%的能源，并在机械和计算单元上最高节省了54%。|
|**2024-09-10**|**Questioning Internal Knowledge Structure of Large Language Models Through the Lens of the Olympic Games**|Juhwan Choi et.al.|[2409.06518](http://arxiv.org/abs/2409.06518)|**[link](https://github.com/c-juhwan/olympics_analysis)**|大型语言模型（LLM）在自然语言处理领域已经成为主导性方法，然而它们的内部知识结构仍然未被充分探索。本文通过分析奥林匹克运动会的历史奖牌统计情况，研究了LLM的内部知识结构。我们要求模型提供各队的奖牌数量，并确定哪些队伍获得了特定排名。我们的结果表明，尽管最先进的LLM在报告单个队伍的奖牌数量方面表现得非常出色，但在回答关于特定排名的问题时却遇到显著困难。这暗示了LLM的内部知识结构与人类的根本不同，人类能够轻松地从已知的奖牌数量推断出排名。为了支持进一步的研究，我们公开发布了代码、数据集和模型输出。|
|**2024-09-09**|**MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct**|Run Luo et.al.|[2409.05840](http://arxiv.org/abs/2409.05840)|null|在多模态大型语言模型（MLLMs）的发展过程中，我们已经取得了显著的进步。然而，在数据量和数据质量方面仍然存在关键瓶颈。手动创建多模态指令数据既耗时又低效，尤其是在生成高复杂性的指令时。此外，从“黑盒”商业模型（例如GPT-4o、GPT-4V）中提取指令数据往往导致生成的指令数据过于简单，这限制了模型性能仅与其自身水平相当。构建多样性和复杂性指令数据的挑战依然巨大。  为解决这一问题，我们提出了一种名为MMEvol的新颖多模态指令数据进化框架，该框架结合了精细感知演化、认知推理演化以及互动演化。这一迭代方法突破了数据质量瓶颈，生成了一个复杂且多样化的图像-文本指令数据集，从而增强了MLLMs的能力。我们以初始指令集合SEED-163K为基础，利用MMEvol系统地扩展了指令类型的多样性，融入了增强认知能力的推理步骤，并从图像中提取了详细信息以提高视觉理解和鲁棒性。  为了全面评估我们数据的有效性，我们使用进化的数据训练了LLaVA-NeXT，并在13个视觉语言任务上进行了实验。与基于原始数据训练的基线相比，我们的方法平均提高了3.1点准确率，并在9个任务上达到了最先进的性能水平。|
|**2024-09-09**|**Are Large Language Models a Threat to Programming Platforms? An Exploratory Study**|Md Mustakim Billah et.al.|[2409.05824](http://arxiv.org/abs/2409.05824)|null|本文研究了大型语言模型（LLM）如ChatGPT、Gemini和Meta AI在LeetCode、Codeforces和HackerRank等竞赛编程平台上的问题解决能力。这些平台常被招聘人员用来筛选编程技能。随着LLM能力的提升，对其在不同难度级别、各类别的编程挑战中的表现进行评估变得尤为重要。  研究团队从LeetCode选取了98个问题，从Codeforces选取了126个问题，覆盖了15个类别。通过九场在线Codeforces和LeetCode竞赛以及HackerRank的两项认证测试，对LLM的实时性能进行了评估。研究过程中使用了提示和反馈机制来引导LLM，并探索了不同场景之间的相关性。  结果显示，ChatGPT等LLM在LeetCode和HackerRank的认证测试中表现出色（成功率为71.43%），但在虚拟竞赛中，特别是在Codeforces的高难度比赛中，它们的表现不尽如人意。尽管在LeetCode档案库中的用户中表现优于部分用户，但LLM在时间效率和内存效率上表现突出，而在更困难的Codeforces竞赛中则处于劣势。  尽管当前情况并未立即构成威胁，但LLM在这些平台上的表现令人担忧，未来需要改进以提高其性能。|
|**2024-09-09**|**Benchmarking Chinese Knowledge Rectification in Large Language Models**|Tianhe Lu et.al.|[2409.05806](http://arxiv.org/abs/2409.05806)|**[link](https://github.com/zjunlp/easyedit)**|**大型语言模型（LLM）展现出惊人的生成能力，但它们并非没有缺陷，特别是存在幻觉的问题。当LLM应用于特定语言和领域时，这一问题尤为突出。例如，在处理中国古代诗歌、谚语或成语时，LLM可能会生成毫无意义的信息，这是由于缺乏特定知识造成的。为此，本文提出了一种针对LLM的基准，通过知识编辑来纠正中文知识。具体来说，我们通过从各种来源收集七种类型的知识，包括古典文本、成语以及来自百度贴吧“求诸家”的内容，构建了一个新的中文数据集CKnowEdit，以应对中文语言特有的复调性、反讽性和逻辑结构。通过对这个数据集的分析，我们揭示了当前LLM在掌握中文方面的挑战。此外，我们在该数据集上对现有的知识编辑技术进行评估，发现对中文知识的修正仍存在巨大的提升空间。代码和数据集可访问：https://github.com/zjunlp/EasyEdit。**|
|**2024-09-09**|**Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models**|Emily Cheng et.al.|[2409.05771](http://arxiv.org/abs/2409.05771)|null|研究已反复证明，从大型语言模型中提取的中间隐藏状态能够预测对自然语言刺激的测量大脑反应。然而，关于使这一高预测性能成为可能的表示特性的了解非常有限。为什么是中间层而不是输出层在这一独特且高度通用的转移任务中最为有效？在这项工作中，我们展示了功能性磁共振成像中的语言编码模型证据支持大型语言模型内存在两个阶段抽象过程的存在。我们使用流形学习方法表明，这种抽象过程自然地在语言模型训练过程中产生，并且随着训练继续进行，这个抽象过程的第一个“组合”阶段被压缩到更少的层中。最后，我们证明了层次编码性能与大型语言模型表示的内在维度之间存在强烈的对应关系。我们初步证据表明，这种对应关系主要来源于大型语言模型的内在组合性，而非其下一个单词预测属性。|
|**2024-09-09**|**Model Input Verification of Large Scale Simulations**|Rumyana Neykova et.al.|[2409.05768](http://arxiv.org/abs/2409.05768)|null|本文提出了一种用于验证模拟输入数据有效性的方法论，我们将其称为模型输入验证（MIV）。我们通过设计特定于模拟建模需求的数据模式和验证工具在名为FabGuard的工具集中实现了这一方法。本文引入了MIV模式的正式分类，并提供了一个集成到现有模拟工作流程中的简化验证管道。FabGuard在三个不同领域——冲突驱动的人口迁移、灾害疏散以及疾病传播模型——的应用得到了展示。我们还探讨了大型语言模型（LLMs）在自动化约束生成和推理方面的应用。在对一个移民模拟案例的研究中，LLMs不仅正确推断出了23个开发者定义的约束中的22个，而且还发现了现有约束中的错误，并提出了新的有效约束。我们的评估表明，对于大型数据集，MIV是可行的，FabGuard能够在140秒内高效处理12,000个输入文件，并且其性能在不同文件大小下保持一致。|
|**2024-09-09**|**A Novel Idea Generation Tool using a Structured Conversational AI (CAI) System**|B. Sankar et.al.|[2409.05747](http://arxiv.org/abs/2409.05747)|null|本文提出了一种新型的、基于对话的人工智能激活创新界面，作为创意生成工具，旨在帮助初学者设计者缓解通常存在的初始延迟和创新瓶颈问题。这是一个动态、互动且上下文响应式的解决方案，积极地利用人工智能领域自然语言处理（NLP）中的大型语言模型（LLM），以生成针对不同设计问题的多个潜在想法表述。将此类AI模型与创新过程结合，我们称之为“激活创新”情景，旨在促进基于对话的连续互动、上下文相关的对话以及大量的想法生成。  为了验证这一工具的有效性，我们对30名初学者设计师进行了试点研究，让他们使用传统方法和新的基于CAI的界面来为给定问题生成想法。通过专家小组对结果进行的定性比较，我们采用了流畅度、新颖性和多样性作为关键参数。研究发现，所提出的工具能够有效地产生大量、多样且新颖的想法。  为了提高界面的可用性，我们引入了结构化的对话模式，为每个创新阶段设计了提示工程化结构，使其更加统一和方便设计师操作。采用这种结构化的CAI界面后，得到的响应更加简洁，并且与随后的设计阶段，即概念化阶段，更加紧密相关。  综上所述，本文证明了生成式人工智能（Gen-AI）在创意产品设计过程的早期、结构不明确阶段的应用潜力。|
|**2024-09-09**|**LLMs Will Always Hallucinate, and We Need to Live With This**|Sourav Banerjee et.al.|[2409.05746](http://arxiv.org/abs/2409.05746)|null|随着大型语言模型在各个领域的广泛应用，深入探讨它们内在局限性变得至关重要。本文提出，语言模型中的幻觉并非偶然错误，而是这些系统固有的特征。我们通过计算理论和哥德尔第一不完全性定理的引用（涉及Halting、Emptiness和Acceptance问题的不可判定性），展示了幻觉源于LLM的基本数学和逻辑结构。因此，通过架构改进、数据集增强或事实核查机制消除幻觉是不可能的。  我们的分析表明，从训练数据编译到事实检索、意图分类和文本生成的每个阶段，都存在产生幻觉的非零概率。由此，我们引入了结构性幻觉的概念，作为这些系统的固有特性。通过建立幻觉的数学确定性，本文挑战了幻觉可以完全避免的传统观点。|
|**2024-09-09**|**A System and Benchmark for LLM-based Q\&A on Heterogeneous Data**|Achille Fokoue et.al.|[2409.05735](http://arxiv.org/abs/2409.05735)|null|在许多工业环境中，用户希望以自然语言形式提出问题，并从结构化数据源（如电子表格、数据库、API或它们的组合）中获取答案。通常情况下，用户并不知道如何识别或访问正确的数据源。如果需要组装多个（甚至可能是隔离的）数据源来得出答案，这个问题会变得更加复杂。最近，一些依赖大型语言模型（LLMs）的文本到SQL应用已解决了一些这些问题，通过使用户能够用自然语言提出问题。然而，在现实的工业场景中，这些应用仍然不实用，因为它们无法应对典型环境中数据源的异质性。本文旨在通过引入siwarex平台解决异质性问题，该平台允许无缝地使用自然语言访问数据库和API。  为了展示siwarex的有效性，我们扩展了流行的Spider数据集并进行基准测试，通过替换其中的一些表格为数据检索API。我们发现siwarex很好地应对了数据源异质性的问题。我们修改后的Spider基准很快将对研究社区开放。|
|**2024-09-09**|**Towards Democratizing Multilingual Large Language Models For Medicine Through A Two-Stage Instruction Fine-tuning Approach**|Meng Zhou et.al.|[2409.05732](http://arxiv.org/abs/2409.05732)|null|## 上文背景 多语言开源医疗大型语言模型（LLMs）具有服务于不同地区语言多样性的潜力。将通用LLMs适应于医疗领域通常需要持续预训练，但这在计算上成本高昂且有时不可行。仅通过指令微调特定任务可能无法保证最佳性能，因为缺乏广泛领域知识使得模型难以在各种场景下理解和推理。为解决这些挑战，我们引入了两个多语言指令微调数据集：MMed-IFT和MMed-IFT-MC，这两个数据集分别包含了超过20万条高质量的多语种医疗样本，在六种语言中。我们提出了一种两阶段训练范式：第一阶段利用MMed-IFT注入通用医学知识，第二阶段则使用MMed-IFT-MC微调针对特定任务的多项选择题。我们的方法在英语和多语言基准测试中均取得了竞争力的结果，实现了高效性和性能之间的平衡。我们计划在未来将我们的数据集和模型权重公开在\url{https://github.com/SpassMed/Med-Llama3}。  ## 任务 请将上述论文摘要翻译为中文，避免输出其他任何无关内容，并确保输出内容中不包含","字符。|
|**2024-09-09**|**The Influence of Task and Group Disparities over Users' Attitudes Toward Using Large Language Models for Psychotherapy**|Qihang He et.al.|[2409.05703](http://arxiv.org/abs/2409.05703)|null|近年来，心理健康障碍患者的数量持续增长，而大型语言模型（LLM）在不同领域的进步也使得基于LLM的心理治疗引起了越来越多的关注。然而，影响用户对基于LLM心理治疗工具态度的因素鲜有探讨。本文作为首次尝试，旨在研究任务差异和群体差异对用户对基于LLM心理治疗工具的态度的影响。通过运用技术接受模型（TAM）和自动化接受模型（AAM），结合在线问卷调查，我们收集并分析了来自中国大陆222名基于LLM心理治疗工具用户的反馈。研究结果表明，群体差异（即心理健康状况）可以影响用户对LLM工具的态度。进一步地，作为典型任务差异之一的隐私顾虑，并未发现对信任度和使用意图产生显著影响。这些发现可指导未来基于LLM心理治疗服务的设计工作。|
|**2024-09-06**|**RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs**|Jiaxing Wu et.al.|[2409.04421](http://arxiv.org/abs/2409.04421)|null|本文引入了一种名为“基于预测反馈的强化学习（Reinforcement Learning from Prediction Feedback，RLPF）”的方法，旨在解决大型语言模型（Large Language Models，LLMs）在个人化系统中应用时面临的问题。具体而言，当LLMs从用户的过往活动预测行为时，它们的有效性往往取决于能否有效地利用大量、长篇的用户历史数据，而这些数据通常含有噪音且长度过长。现有预训练的LLMs可能生成的摘要虽短小精悍，但缺乏对下游任务至关重要的上下文信息，从而限制了其在个人化系统中的应用。  为了克服这一挑战，RLPF方法通过微调LLMs来生成精炼、人类可读的用户概要，这些概要能够优化下游任务的表现。通过最大化生成概要的有用性，RLPF能够有效提取大量用户历史数据的关键信息，同时保持对下游任务至关重要的信息。实验结果表明，与基线方法相比，RLPF在下游任务性能上显著提升了22%，在事实性、抽象性和可读性等指标上的表现分别达到了84.59%的胜率，同时实现了74%的上下文长度减少，且在16个未见的任务和/或数据集上均有性能提升，这表明其具有良好的泛化能力。  总之，RLPF提供了一种增强LLMs在个人化领域应用的有前景的解决方案，通过将长篇、噪音丰富的用户历史转化为信息丰富、易于理解的表示，从而提高LLMs的个人化能力。|
|**2024-09-06**|**Question-Answering Dense Video Events**|Hangyu Qin et.al.|[2409.04388](http://arxiv.org/abs/2409.04388)|null|在本文中，我们提出了一项新的任务——针对长视频中的密集事件进行问题回答与定位，这要求模型能够准确理解并推理持续时间较长的多个事件。为了支持这一研究，我们构建了一个名为DeVE-QA的数据集，其中包含关于10600个长视频中26000个事件的78000个问题。  现有在单事件问答上表现出色的大型多模态语言模型（MLLMs）在面对DeVE-QA时遇到挑战，这表明它们在处理长时间段内发生的多个事件的理解和推理方面存在局限性。为此，我们提出了一种名为DeVi的新方法，这是一种无需训练即可提升MLLM性能的方法。DeVi通过引入三个关键模块来改进现有的MLLMs：层级描述模块、时间事件记忆模块和自我一致性检查模块。这三个模块分别用于检测、上下文化和记忆长视频中的密集事件，以及定位相关视频片段以进行问题回答。  实验结果表明，与现有MLLMs相比，DeVi在回答密集事件问题和定位相关视频片段方面表现更优。具体而言，在DeVE-QA数据集上，DeVi的G(round)QA准确率提高了4.1%，在NExT-GQA数据集上的准确率提高了3.7%。|
|**2024-09-06**|**Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs**|Aliakbar Nafar et.al.|[2409.04318](http://arxiv.org/abs/2409.04318)|**[link](https://github.com/HLR/LvsR-LLM)**|本文提出了一种评估生成大型语言模型（LLMs）内在学习机制的框架。我们声称，这些机制是通过检索内部知识和通过关注回归任务从上下文中的示例进行学习的组合。首先，我们展示了LLMs在真实世界数据集上执行回归的能力，并设计实验来衡量模型在多大程度上通过检索其内部知识而不是从上下文示例中学习来进行内在学习。我们认为这个过程位于这两个极端之间的连续体上。我们深入分析了根据各种因素（如任务的先验知识以及提供给上下文示例的信息类型和丰富度）这些机制被触发的程度。我们使用三种LLMs并利用多个数据集来验证我们的发现的稳健性。我们的结果揭示了如何根据所解决的问题利用上下文示例中的元学习和促进知识检索的方法。|
|**2024-09-06**|**An optically accelerated extreme learning machine using hot atomic vapors**|Pierre Azam et.al.|[2409.04312](http://arxiv.org/abs/2409.04312)|null|机器学习正逐渐成为一种广泛应用的技术，其增长速度令人印象深刻，原因在于它能够提供解决社会关注问题的实用解决方案的多样性。然而，随着应用和所需资源的增加，当前的硬件技术开始受限。特别是对于大型语言模型或高分辨率图像识别等新型机器学习领域，计算时间与能源成本成为了关键问题。在此背景下，多年来已经设计出了光学平台，旨在开发更高效的机器学习硬件。  其中，自由空间传播平台具有多种优势：并行性、低能耗与计算速度。本文介绍了一种结合光束在热原子蒸气中传播的强烈且可调非线性特性的新设计，并与极端学习机模型相结合。通过数值模拟与实验验证，我们展示了在MNIST图像分类任务中使用此类自由空间非线性传播增强训练的效果。此外，我们指出了实验中的多个超参数，这些参数进一步优化后可以提高平台的准确性。|
|**2024-09-06**|**Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**|Desiree Heim et.al.|[2409.04286](http://arxiv.org/abs/2409.04286)|null|当前公开的知识工作数据集在多样性、详尽注释以及用户和文档的上下文信息方面存在不足，这阻碍了对知识工作辅助系统进行客观和可比较的数据驱动评估与优化。由于在真实环境中收集此类数据所需的资源巨大，以及数据审查的必要性，因此构建这样的数据集几乎不可能实现。鉴于此，我们提出了一种可配置的多代理知识工作数据集生成器。该系统模拟了由生成大型语言模型的文档并相互协作的代理之间的知识工作，并记录了伴随的数据轨迹。此外，生成器在其配置中捕获或在模拟过程中创建的所有背景信息，并以知识图谱的形式存储。最后，产生的数据集可以用于利用和共享，而无需涉及隐私或机密问题。  本文介绍了我们方法的设计愿景，并专注于使用大型语言模型生成真实的知识工作文档。我们的研究中，人类评估者评估了生成文档的53%和真实文档的74%，认为它们具有真实性，这表明了我们方法的潜力。此外，我们分析了参与者评论中提到的真实性标准，并对已识别的常见问题进行了详细说明，提出了改进措施。|
|**2024-09-06**|**Advancing Automated Knowledge Transfer in Evolutionary Multitasking via Large Language Models**|Yuxiao Huang et.al.|[2409.04270](http://arxiv.org/abs/2409.04270)|null|本文引入了一种基于大型语言模型（LLM）的优化范式，以建立一个自主模型工厂，用于生成适用于不同优化任务的知识转移模型。这一方法旨在通过自动化设计过程，实现高效且有效的知识转移。为了评估所提出方法的性能，我们进行了全面的实验研究，将生成的知识转移模型与现有的最佳知识转移方法进行了比较。结果表明，生成的模型在效率和有效性方面均表现出优于或与手工设计的知识转移模型相当的性能。|
|**2024-09-06**|**GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**|Ziyin Zhang et.al.|[2409.04183](http://arxiv.org/abs/2409.04183)|null|在本工作中，我们提出了GALLa - 图形对齐大型语言模型。GALLa 利用图神经网络和跨模态对齐技术，在微调过程中向LLM注入代码的结构信息作为辅助任务。这种框架既无模型依赖性也无任务依赖性，它可以应用于任何代码LLM用于任何代码下游任务，并仅在训练时从与微调数据无关的语料库中获取结构化图形数据，而在推理阶段无需额外成本。通过四种不同基线LLM（参数量从3.5亿到80亿不等）在五个代码任务上的实验验证了GALLa的有效性，即使对于强大的模型如LLaMA3，也证明了其一致性改进。|
|**2024-09-06**|**Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**|Larissa Pusch et.al.|[2409.04181](http://arxiv.org/abs/2409.04181)|null|自然语言处理领域的进步极大地改变了我们与数据库等信息系统的交互方式，使其变得更加便捷。然而，在关键准确性领域，如生物医学领域，仍存在挑战。其中一个重要问题是幻觉问题，即模型生成了数据支持之外的信息，这可能导致危险的错误信息。本文提出了一种新颖的方法，旨在通过结合大型语言模型（LLM）和知识图谱（KG）来改善问答系统的准确性和可靠性，以生物医学KG为例。该方法基于LangChain框架构建，通过引入查询检查器确保LLM生成的查询在语法和语义上的有效性，然后使用这些查询从知识图谱中提取信息，大幅减少了错误如幻觉的发生。  我们使用了一个包含50个生物医学问题的新基准数据集对整体性能进行了评估，测试了包括GPT-4 Turbo和llama3:70b在内的几种LLM。结果显示，虽然GPT-4 Turbo在生成准确查询方面表现出色，但开源模型如llama3:70b在适当的问题提示工程下也显示出潜力。为了使这种方法易于访问，我们开发了一个用户友好的Web界面，允许用户输入自然语言查询，查看生成和修正的Cypher查询，并验证结果路径的准确性。  总体而言，这种混合方法有效地解决了数据缺口和幻觉等常见问题，提供了一个可靠且直观的解决方案来改进问答系统。生成本文结果和用户界面所需源代码的Git仓库链接如下：https://git.zib.de/lpusch/cyphergenkg-gui|
|**2024-09-06**|**From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks**|Andreas Stephan et.al.|[2409.04168](http://arxiv.org/abs/2409.04168)|null|为了减少对人工标注的需求，提出了大型语言模型（LLM）作为候选模型质量的评判者。这些LLM评判者通常通过在摘要或机器翻译等生成任务上与人类判断的相关性来评估。相比之下，我们研究了在数学推理任务上的LLM评判者。这类任务需要多步推理，其解答的正确性可以验证，从而提供了一种更客观的评估方式。我们进行了详细的表现分析，并发现使用的评判者大多无法提高任务性能，但能够选择更好的模型。我们的分析揭示了评判表现与候选模型任务表现之间的强相关性。观察到评判者倾向于选择更高质量的模型，即使其答案是错误的。进一步地，我们展示了可以通过统计措施，如候选模型的任务性能，来预测评判表现。在消融实验中，我们交换或屏蔽候选答案，并观察到评判者经常保持原始判断，这提供了证据表明评判者在判断中融入了写作风格。总之，我们发现使用统计指标量化判断中的规律性，并提供了利用它们的各种角度。|
|**2024-09-06**|**Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language Models for Text-to-Code Generation**|Luis Mayer et.al.|[2409.04164](http://arxiv.org/abs/2409.04164)|null|近年来，大型语言模型（LLMs）作为一种强大的工具，在多个领域展现出潜力，包括软件工程。在本研究中，我们评估了五款最先进的LLM——Bard、BingChat、ChatGPT、Llama2和Code Llama——在文本到代码生成任务上的能力。我们通过向模型提供来自编程网站LeetCode的编码问题描述文本提示，要求它们用Python编写解决方案。随后，我们使用LeetCode的测试功能来评估生成输出的质量。  研究结果表明，这些模型在性能上存在显著差异。ChatGPT在处理这类编程挑战方面表现最为有效，甚至超过了专门针对代码的模型，如Code Llama。为了进一步了解情况，我们测量了生成代码的运行时间和内存使用情况，并将其与LeetCode上的其他代码提交进行了比较。详细错误分析包括比较生成代码中的正确缩进和形式差异，以及将未解决的任务归类到特定错误类别，有助于我们更深入地理解结果并找到改进空间。研究结果还显示，当模型面临大量上下文信息时，即较长提示时，生成的代码越来越不准确。|
|**2024-09-05**|**Attention Heads of Large Language Models: A Survey**|Zifan Zheng et.al.|[2409.03752](http://arxiv.org/abs/2409.03752)|**[link](https://github.com/iaar-shanghai/awesome-attention-heads)**|**自ChatGPT问世以来，大型语言模型在各种任务上表现出色，但它们仍然作为黑盒系统存在。因此，其发展主要依赖于数据驱动的方法，限制了通过改变内部架构和推理路径来提升性能的可能性。许多研究者开始探索大型语言模型的内部机制，旨在识别推理瓶颈的本质，大多数研究集中在注意力头部上。我们的综述旨在通过聚焦于大型语言模型的可解释性和注意力头部的内在机制，揭示其内部推理过程。首先，我们将人类思考过程提炼为四个阶段框架：知识回忆、情境内识别、潜在推理和表达准备。利用这一框架，我们系统地回顾现有研究，识别并分类特定注意力头部的功能。此外，我们总结了发现这些特殊头部所使用的实验方法，分为无模型方法和有模型方法两大类。我们也概述了相关评估方法和基准。最后，我们讨论当前研究的局限性，并提出几个潜在的发展方向。我们的参考文献列表开源于<https://github.com/IAAR-Shanghai/Awesome-Attention-Heads>。**|
|**2024-09-05**|**LLM-CI: Assessing Contextual Integrity Norms in Language Models**|Yan Shvartzshnaider et.al.|[2409.03735](http://arxiv.org/abs/2409.03735)|null|大型语言模型（LLM）在从互联网上收集的数据中记忆部分训练数据的同时，也可能无意中编码了社会偏好和规范。随着这些模型被整合到社会技术系统中，确保它们编码的规范符合社会期望至关重要。这些规范可能因模型、超参数、优化技术以及数据集的不同而不同。由于提示敏感性的问题——微小的提示变化会导致不同的响应，现有的评估方法变得不可靠。需要一个全面的框架来涵盖各种模型、优化和数据集，并提供可靠的方法来评估编码的规范。  我们提出了LLM-CI，这是第一个用于评估LLM中编码隐私规范的开源框架。LLM-CI使用基于上下文完整性因素的情境叙述方法来评估不同上下文中和不同LLM中的编码规范。我们提出了一种多提示评估方法来解决提示敏感性问题，通过仅从导致多个变体一致响应的提示中评估规范，以全面评估使用先前工作中的IoT和COPPA情景数据集的LLM。  通过使用LLM-CI和我们提出的这种方法，我们全面地评估了LLM，研究了模型属性（如超参数、容量）和优化策略（如对齐、量化）的影响。|
|**2024-09-05**|**Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to Market Entry**|Meena Jagadeesan et.al.|[2409.03734](http://arxiv.org/abs/2409.03734)|null|本文从经济和算法两个角度研究大型语言模型等大规模机器学习（ML）模型市场中的集中问题，以及是否存在进入此类市场的不可克服障碍。我们通过正式定义一个多目标高维回归框架来探讨降低进入壁垒的问题，该框架捕捉到了声誉损害的特征，并分析了新公司进入市场所需的样本数量。我们的结果表明，多目标考虑能够从根本上降低进入壁垒——所需样本数量可能远小于现有公司的数据集大小。在证明这些结果的过程中，我们还发展了多目标环境中高维线性回归的缩放定律，展示了当数据集规模较大时，缩放率会变得较慢，这一发现可能具有独立的研究价值。|
|**2024-09-05**|**Planning In Natural Language Improves LLM Search For Code Generation**|Evan Wang et.al.|[2409.03733](http://arxiv.org/abs/2409.03733)|**[link](https://github.com/scaleapi/plansearch)**|在大规模提升训练计算能力的同时，推理计算的规模扩展并未带来类似的进步。我们假设，这一领域缺乏关键性的突破在于生成模型的输出多样性不足，导致搜索效率低下，因为模型不断产生高度相似但错误的结果。通过实证研究，我们发现提高输出多样性可以有效缓解这一问题。  基于这一发现，我们提出了一种名为PLANSEARCH的新颖搜索算法，它在人类评价、MBPP+和LiveCodeBench（一个用于竞争性编程的无污染基准）等任务上表现出色。该算法通过生成关于问题的多样观察，并利用这些观察构建解决策略，来探索比传统方法更广泛的潜在解决方案空间。在使用PLANSEARCH结合Claude 3.5 Sonnet进行优化后，我们实现了LiveCodeBench上77.0%的通过率（pass@200），这不仅超越了不使用搜索方法（pass@1=41.4%）的结果，也优于仅依赖重复采样的方法（pass@200=60.6%）。此外，我们还展示了能够准确预测搜索带来的性能提升，其关键因素是生成想法的多样性。|
|**2024-09-06**|**RAG based Question-Answering for Contextual Response Prediction System**|Sriram Veturi et.al.|[2409.03708](http://arxiv.org/abs/2409.03708)|null|本文介绍了一种端到端的框架，利用大型语言模型（LLMs）的检索增强生成（RAG）能力，针对实际工业应用中的问题回答场景。给定客户查询，该系统会检索相关知识文档，并结合之前的聊天历史，为零售公司的客服中心提供客户服务代表生成响应建议。通过全面的自动化和人工评估，结果显示，这种解决方案在准确性和相关性上优于当前基于BERT的算法。我们的研究结果表明，基于RAG的LLMs可以作为人类客户服务代表的优秀辅助工具，减轻他们的工作负担。|
|**2024-09-05**|**TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course Scheduling Problems**|Stylianos Loukas Vasileiou et.al.|[2409.03671](http://arxiv.org/abs/2409.03671)|**[link](https://github.com/yoda-lab/trace-cs)**|我们提出了一种名为TRACE-cs的新型混合系统，它结合了符号推理与大型语言模型（LLM），以解决排程问题中的对比查询。TRACE-cs利用SAT求解技术编码排程约束，并生成用户查询的解释，同时通过大型语言模型将用户的查询转换为逻辑条目，并细化符号求解器生成的解释为自然语言句子。通过整合这些组件，我们的方法展示了将符号方法与LLM相结合，创建具有正确性保证的可解释AI代理的潜力。|
|**2024-09-05**|**A Fused Large Language Model for Predicting Startup Success**|Abdurahman Maarouf et.al.|[2409.03668](http://arxiv.org/abs/2409.03668)|null|为了帮助投资者做出有效的决策并持续寻找盈利的创业投资机会，需要预测初创公司的成功率。如今，投资者不仅可以利用有关初创公司的各种基本面信息（如公司的成立时间、创始人数量以及所处行业），还可以通过在线风险投资（VC）平台获取关于公司创新和业务模式的文本描述信息，例如Crunchbase。为了支持投资者的决策，我们开发了一种机器学习方法，旨在在VC平台上定位成功的初创公司。具体而言，我们开发、训练并评估了一个专门的融合大型语言模型，用于预测初创公司的成功率。我们的工作旨在评估VC平台上公司的自我描述在多大程度上能够预测其成功性。使用来自Crunchbase的20,172个在线资料档案，我们发现我们的融合大型语言模型可以预测初创公司的成功率，其中文本自我描述对预测能力贡献了显著部分。我们的工作提供了一个决策支持工具，帮助投资者找到盈利的投资机会。|
|**2024-09-05**|**The representation landscape of few-shot learning and fine-tuning in large language models**|Diego Doimo et.al.|[2409.03662](http://arxiv.org/abs/2409.03662)|**[link](https://github.com/diegodoimo/geometry_icl_finetuning)**|**本文探讨了在特定任务上改进现代大型语言模型（LLM）性能的两种常见策略：上下文学习（ICL）和监督微调（SFT）。尽管这两种方法的本质不同，但它们往往能产生相似的性能提升。然而，我们对它们是否在LLM内部诱导出相似的表示结构知之甚少。我们通过分析这两种情况下隐藏表示的概率景观来解决这个问题。具体来说，我们在相同的问答任务上比较了LLM的表现，发现ICL和SFT产生了非常不同的内部结构，两者都在网络的中间部分经历了一个明显的转变。在模型的前半部分，ICL塑造了分层组织的可解释表示，按照其语义内容进行排序。相比之下，SFT得到的概率景观更加模糊且语义混杂。在网络的后半部分，微调后的表示发展出了更有利于编码答案身份的概率模式，而ICL表示的概率峰则不太明确。我们的方法揭示了LLM在不同条件下解决相同任务时所采用的多样化计算策略，这有助于我们朝着设计出从语言模型中提取信息的最佳方法迈进。**|
|**2024-09-06**|**LLM-based multi-agent poetry generation in non-cooperative environments**|Ran Zhang et.al.|[2409.03659](http://arxiv.org/abs/2409.03659)|**[link](https://github.com/zhangr2021/Multiagent_poetry)**|**尽管大型语言模型（LLM）在自动诗歌生成领域取得了显著进展，但生成的诗歌在多样性方面存在不足，且训练过程与人类学习方式大相径庭。基于这样的考虑，我们提出了一种基于社会学习的框架，在此框架下，我们强调非合作互动，以鼓励多样性，同时除了合作互动外还强调非合作互动。我们的实验是首次尝试在非合作环境中使用基于训练的多智能体系统（GPT-2）和基于提示的系统（GPT-3 和 GPT-4）进行诗歌生成。  根据对生成的96,000首诗歌的评估，我们的框架对基于训练的智能体的诗歌生成过程产生了积极影响，导致以下结果：1）多样性增加了3.0-3.7个百分点（pp），新颖性增加了5.6-11.3个百分点，根据独特和新颖的n-grams评估。生成的诗歌在词汇、风格和语义方面也表现出群体差异。基于提示的智能体在我们的框架中也从非合作环境中获益，具有非同质智能体的多样化的模型组合有可能进一步提高多样性，实验结果显示多样性增加了7.0-17.5个百分点。然而，基于提示的智能体显示了随着时间推移词汇多样性的下降，并没有展现出旨在在社交网络中实现的群体间分化。  本文认为，在诸如自动诗歌生成等创意任务中，需要进行范式转变，引入类似于人类交互的社会学习过程（通过基于LLM的智能体建模），以促进更加多样性和创新的生成。**|
|**2024-09-05**|**From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents**|Jifan Yu et.al.|[2409.03512](http://arxiv.org/abs/2409.03512)|null|自最早的在线教育实例出现，课程被上传至可访问并共享的在线平台以来，这种扩大知识传播范围、触及更广泛受众的形式引发了广泛讨论和普遍采纳。认识到个性化学习仍存在改进空间，人工智能技术不断融入这一学习模式，由此产生了多种教育AI应用，如教育推荐和智能辅导。大型语言模型（LLMs）智能的涌现，使得这些教育增强功能得以基于统一的基础模型构建，实现更深层面的整合。在此背景下，我们提出MAIC（大规模AI赋能课程），这是一种新的在线教育形式，利用LLM驱动的多代理系统构建AI辅助课堂，平衡了规模性和适应性。除了探索概念框架和技术创新外，我们在清华大学——中国顶尖大学之一——进行了初步实验。通过超过10万条学习记录和500多名学生的数据，我们获得了宝贵观察和初步分析。这个项目将持续发展，最终目标是建立一个全面开放的平台，支持和统一研究、技术和应用，在大模型AI时代探索在线教育的可能性。我们设想这个平台是一个合作枢纽，汇集教育者、研究人员和创新者共同探索AI驱动在线教育的未来。|
|**2024-09-04**|**RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)**|Yao Mu et.al.|[2409.02920](http://arxiv.org/abs/2409.02920)|null|本篇论文介绍了一种名为RoboTwin的新型基准数据集，它结合了现实世界中的遥控数据与通过数字孪生生成的合成数据。RoboTwin旨在为双臂机器人场景提供支持，特别关注工具使用能力和人机交互能力。我们利用COBOT Magic平台收集了丰富的数据，涵盖工具操作和人机互动的多样性。  论文提出了一种创新的方法来创建数字孪生体，利用AI生成的内容将二维图像转换为详细的三维模型。同时，我们借助大型语言模型生成专家级训练数据和面向功能性的任务特定姿态序列。  我们的主要贡献包括： 1. RoboTwin基准数据集， 2. 高效的现实到模拟管道，以及 3. 利用语言模型进行自动专家级数据生成。  这些进展旨在解决机器人训练数据稀缺的问题，有望加速开发更多功能强大、适应性广泛的机器人系统，应用于广泛的现实世界场景。项目页面可访问：https://robotwin-benchmark.github.io/early-version/|
|**2024-09-05**|**LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA**|Jiajie Zhang et.al.|[2409.02897](http://arxiv.org/abs/2409.02897)|**[link](https://github.com/THUDM/LongCite)**|尽管当前的长文本大语言模型在基于大量文本回答用户问题方面表现出令人印象深刻的性能，但它们缺乏引用使得用户难以验证答案的准确性，从而引发了对其可靠性的担忧，因为它们可能产生错误的信息。我们的工作旨在使这些长文本大语言模型能够生成包含精细句级引用的响应，以提高它们的忠实度和可验证性。  我们首先引入了LongBench-Cite，一个自动评估当前大语言模型在长文本上下文问题回答中的表现的基准，揭示了在句级引用方面存在巨大的改进空间。为了实现这一目标，我们提出了CoF（粗到细）这一新颖的管道，利用现成的大语言模型自动生成包含精确句级引用的长文本问答实例，并以此管道构建了LongCite-45k，一个用于句级引用问题的大型自监督训练数据集。最后，我们使用LongCite-45k数据集训练了LongCite-8B和LongCite-9B模型，成功地使它们能够在单个输出中生成准确的响应和精细的句级引用。在LongBench-Cite上的评估结果显示，我们的训练模型在引用质量方面达到了最先进的水平，超越了包括GPT-4在内的高级专有模型。|
|**2024-09-04**|**LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture**|Xidong Wang et.al.|[2409.02889](http://arxiv.org/abs/2409.02889)|**[link](https://github.com/freedomintelligence/longllava)**|**扩展多模态大语言模型（MLLMs）的长期上下文能力对于视频理解、高分辨率图像理解和多模态代理至关重要。这涉及到一系列系统优化，包括模型架构、数据构造和训练策略，尤其是解决随着更多图像引入而出现的性能下降以及高昂计算成本等问题。本文通过将模型架构调整为Mamba和Transformer块的混合体、采用既能考虑多个图像间时间依赖性又能考虑空间依赖性的数据构造方法，并实施渐进式训练策略，对这些挑战进行了应对。发布的模型“LongLLaVA”（长期语言与视觉助手）是首个混合型MLLM，实现了效率与效果之间的良好平衡。LongLLaVA不仅在各种基准测试中取得了竞争力的结果，而且保持了高吞吐量和低内存消耗的特点。特别地，它能够在单个A100 80GB GPU上处理近一千张图片，展示了广泛任务应用前景的潜力。**|
|**2024-09-04**|**Historical German Text Normalization Using Type- and Token-Based Language Modeling**|Anton Ehrmanntraut et.al.|[2409.02841](http://arxiv.org/abs/2409.02841)|null|本文提出了一种针对1700年至1900年德国文学文本的正词法规范化系统，该系统基于平行语料库训练。所提出的系统利用机器学习方法和Transformer语言模型，结合编码器-解码器模型对单个词汇类型进行规范化，并通过预训练的因果语言模型在上下文中调整这些规范化结果。广泛评估表明，该提出的系统提供了最先进的准确性，与完全端到端的句子级规范化系统相当，该系统是通过对预训练的Transformer大型语言模型进行微调而实现的。然而，由于模型难以泛化以及缺乏大量高质量平行数据，历史文本的规范化仍是一个挑战。|
|**2024-09-04**|**Exploring Sentiment Dynamics and Predictive Behaviors in Cryptocurrency Discussions by Few-Shot Learning with Large Language Models**|Moein Shahiki Tash et.al.|[2409.02836](http://arxiv.org/abs/2409.02836)|null|本文通过运用高级自然语言处理技术，对加密货币相关讨论中的预测陈述、希望演讲及悔恨检测行为进行分析。我们提出了一种新的分类方法——“预测陈述”，将其细分为预测增加、预测减少、预测中立或非预测类别。利用GPT-4o这一前沿大规模语言模型，我们在五大主流加密货币（Cardano、Binance、Matic、Fantom、Ripple）的讨论中探索了情绪动态。研究发现，Matic在乐观预测方面显示出特别高的倾向性。此外，我们还探讨了希望与悔恨情绪之间的相互作用，揭示了这些情感与预测行为之间复杂的互动模式。尽管面临数据量和资源可用性方面的限制，我们的研究仍揭示了加密货币市场投资者行为和情绪趋势的重要发现，为战略决策和未来研究提供了信息。|
|**2024-09-04**|**CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models**|Wentao Liu et.al.|[2409.02834](http://arxiv.org/abs/2409.02834)|**[link](https://github.com/ecnu-icalk/educhat-math)**|本文发布了一个名为CMM-Math的中文多模态数学数据集，包含基准和训练部分，旨在评估和增强大型多模态模型（LMM）在数学推理方面的表现。CMM-Math包含了超过28,000个高质量样本，涵盖了从小学到高中的中国12个年级的多种问题类型（例如选择题、填空题等），并提供了详细的解决方案。特别地，问题或观点中可能包含视觉上下文，使得这个数据集更具挑战性。通过全面分析，我们发现当前最先进的LMM在CMM-Math数据集上面临挑战，这强调了在LMM开发方面进一步改进的必要性。为此，我们提出了一种名为Multimodal Mathematical LMM（Math-LMM）的模型来处理混合输入的多个图像和文本段落的问题。我们采用三个阶段进行模型训练：基础预训练、基础微调和数学微调。广泛的实验表明，我们的模型在与三个多模态数学数据集上的SOTA LMM进行比较时，有效地提高了数学推理性能。|
|**2024-09-04**|**ExpLLM: Towards Chain of Thought for Facial Expression Recognition**|Xing Lan et.al.|[2409.02828](http://arxiv.org/abs/2409.02828)|null|面部表情识别（FER）在多媒体领域至关重要，对各种应用具有重大影响。然而，理解面部表情的原因对于准确识别表情至关重要。目前的方法，如基于面部动作单位（AUs）的方法，通常提供AU名称和强度，但缺乏关于AU之间的互动以及整体表情之间关系的洞察。本文提出了一种名为ExpLLM的新方法，利用大型语言模型生成面部表情识别的准确思维链（CoT）。我们从三个关键视角设计了CoT机制：关键观察、总体情感解释和结论。关键观察描述了AU的名称、强度及其相关情感。总体情感解释基于多个AU及其互动进行分析，确定主导情感及其关系。最后，结论基于前一分析得出最终的表情标签。此外，我们还引入了Exp-CoT引擎，用于构建此表情CoT并生成指令描述数据以训练我们的ExpLLM。在RAF-DB和AffectNet数据集上的大量实验表明，ExpLLM优于当前最先进的面部表情识别方法。在微表情识别方面，ExpLLM也超越了最新的GPT-4o，尤其是在GPT-4o经常失败的情况下。|
|**2024-09-04**|**Design Contradictions: Help or Hindrance?**|Aron E. Owen et.al.|[2409.02823](http://arxiv.org/abs/2409.02823)|null|在数据可视化领域，创新思维的迫切需求促使我们探索新的创意方法。通过组合两个或更多具有对立性质的创造性词汇，能够激发新型想法与设计，对创意过程产生积极影响。随着人工智能驱动设计的发展，一个关键问题浮出水面：这些设计矛盾是否能与AI工具协同工作？目前答案是否定的。AI系统，尤其是大型语言模型（LLMs），依赖于产生相似性的算法，而创造力往往需要差异性和新颖性。这份海报开启了关于如何引导AI系统变得更具创造性和生成新想法的对话。这项研究邀请我们重新考虑传统设计方法，并探索AI驱动世界中的新方法。我们能否应用传统的设计方法，如双钻石模型，或者是否需要新的设计工程方法？如何利用生成式AI快速设计可视化并构思新想法？这篇论文旨在开启这一重要对话，并提供有关AI在推动数据可视化创意方面的潜力的实用见解。|
|**2024-09-04**|**Language Understanding as a Constraint on Consensus Size in LLM Societies**|Giordano De Marzo et.al.|[2409.02822](http://arxiv.org/abs/2409.02822)|null|在大型语言模型（LLM）的应用朝着协作任务发展的情况下，多个代理相互作用，如同一个LLM社会。在这种背景下，大量的LLM能够通过自我组织方式达成关于任意规范的共识，这些规范在信息支持某一选项优于另一选项的情况下不存在。为了理解LLM是否与人类社会一样，在没有机构的情况下能够达到共识，我们应用了复杂科学的方法和行为科学的原则，开创了一种AI人类学的新方法。研究发现，LLM能够在群体中达成共识，并且LLM的意见动态可以用一个由多数力量系数参数化的函数来理解，该系数决定了共识是否可能。对于具有更高语言理解能力的模型而言，这种多数力量更强，而对于较大的群体而言则会减弱，导致存在一个临界群体大小，超过这个大小，对于给定的LLM，达成共识变得不可能。这一临界群体大小随着模型的语言理解能力的增长呈指数级增长，对于最先进的模型而言，其可以达到远超非正式人类群体典型规模的数量级。|
|**2024-09-04**|**Towards a Unified View of Preference Learning for Large Language Models: A Survey**|Bofei Gao et.al.|[2409.02795](http://arxiv.org/abs/2409.02795)|**[link](https://github.com/kbsdjames/awesome-llm-preference-learning)**|大型语言模型（LLM）展现了惊人的能力。实现成功的关键因素之一是使LLM的输出与人类偏好保持一致。这一过程通常需要少量数据就能高效提升LLM的表现。尽管有效，但在这一领域的研究覆盖了多个领域，相关方法相对复杂难以理解。不同方法之间的关系尚未得到充分探索，限制了偏好调整策略的发展。鉴于此，我们分解了现有流行调整策略的四个组成部分，并提供了一个统一框架来研究当前的调整策略，以此建立它们之间的联系。在本文综述中，我们将所有偏好学习策略分解为四个部分：模型、数据、反馈和算法。这种统一视角为现有调整算法提供了深入理解，并且也开启了整合不同策略优势的可能性。此外，我们详细介绍了现有主流算法的工作示例，以帮助读者全面了解。最后，基于我们的统一视角，我们探讨了调整大型语言模型与人类偏好之间的挑战以及未来研究方向。|
|**2024-08-30**|**SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists**|Raoyuan Zhao et.al.|[2408.17437](http://arxiv.org/abs/2408.17437)|**[link](https://github.com/loreley99/syntheval_checklist)**|**在自然语言处理（NLP）领域，传统的基准测试通常使用静态预留测试集。然而，这种方法往往会导致性能过估计，并缺乏提供全面、可解释和动态评估NLP模型的能力。近期，如DynaBench（Kiela等，2021年）和CheckList（Ribeiro等，2020年）等作品通过多步骤人工注释管道生成测试类型来解决这些问题，以对NLP模型进行行为测试。不幸的是，手动创建各种测试类型需要大量的人力劳动，成本高昂。本研究提出了一种名为SYNTHEVAL的混合行为测试框架，利用大型语言模型（LLMs）生成大量测试类型，为NLP模型进行全面评估。SYNTHEVAL首先通过LLMs进行受控生成生成句子，然后通过比较LLMs与特定任务的NLP模型的预测结果来识别挑战性示例。最后阶段，由人类专家调查这些挑战性示例，手动设计模板，并确定特定任务模型一致表现的失败类型。我们将SYNTHEVAL应用于情感分析和有毒语言检测两个分类任务上，并展示了我们的框架在识别这些任务中强大模型的弱点方面的有效性。我们分享了代码于https://github.com/Loreley99/SynthEval_CheckList。**|
|**2024-08-30**|**Advancing Multi-talker ASR Performance with Large Language Models**|Mohan Shi et.al.|[2408.17431](http://arxiv.org/abs/2408.17431)|null|在自动语音识别（ASR）领域，识别对话场景中的重叠语音是极具挑战性的问题。传统的处理方法通过序列输出训练（SOT），即将多个说话者的声音排放时间按照其发言顺序进行拼接，来解决多说话者ASR问题。然而，这种从对话中拼接相关话语的转录依赖于构建长上下文的能力。相比之下，基于大型语言模型（LLM）的新方法可能更适合处理这类复杂且具有挑战性的场景，因为它利用了预训练解码器的强大能力。本文提出了一种基于LLM的SOT方法用于多说话者ASR，该方法利用预训练的语音编码器和LLM，并通过适当的策略对多说话者数据集进行微调。实验结果表明，我们的方法在模拟数据集LibriMix上优于传统的方法，并在真实世界数据集AMI的评估集上达到了最先进的性能，显著超越了之前使用1000倍更多监督数据训练的AED模型。|
|**2024-08-30**|**Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach**|Jialiang Wei et.al.|[2408.17404](http://arxiv.org/abs/2408.17404)|**[link](https://github.com/jl-wei/feature-inspiration)**|在过去十年中，借鉴应用商店（AppStore）的规范获取方法被证明非常有益。开发者经常研究竞争对手的应用程序以收集新功能的灵感。随着生成式人工智能的进步，最近的研究表明大型语言模型（LLM）启发的规范获取具有潜力。LLMs可以在这一过程中提供新功能想法的灵感。尽管这两种方法在实践中越来越受欢迎，但它们之间的差异缺乏深入理解。我们进行了一项比较研究，对比了应用商店和LLM启发的方法在细化功能为子功能时的表现。通过手动分析从两种方法推荐的1200个子功能，我们识别出了它们的优点、挑战以及关键差异。尽管两种方法都推荐了高度相关且描述清晰的子功能，但LLMs在特别涉及未见应用范围的新颖性方面似乎更为强大。此外，一些推荐的功能是虚构的，其可行性不明确，这强调了人类分析师在获取过程中的重要性。|
|**2024-08-30**|**NDP: Next Distribution Prediction as a More Broad Target**|Junhao Ruan et.al.|[2408.17377](http://arxiv.org/abs/2408.17377)|null|大型语言模型（LLM）通过下一个词预测（NTP）范式进行训练，展示了强大的能力。然而，现有的NTP范式存在几个限制，特别是在计划任务复杂性和推理阶段的错误传播方面。我们的工作扩展了对NTP的批评，指出其限制还源于训练目标狭窄：预测一个次优的一热分布。为了支持这一批评，我们进行了一项预实验，将强大的LLM的输出分布视为高效的世界数据压缩。通过评估n-gram分布与LLM输出分布之间的相似性，我们发现n-gram分布与LLM输出分布更为一致。基于这一洞察，我们引入了下一个分布预测（NDP），使用n-gram分布来替换一热目标，从而增强学习过程而无需额外的在线训练时间。我们在翻译、通用任务、语言迁移和医疗领域适应等四个领域进行了实验。与NTP相比，NDP在翻译任务上可达到+2.97 COMET改进，在通用任务上平均改善+0.61，在医疗领域上平均改善+10.75。这表明解决目标狭窄问题的具体益处，并指出了未来改进NTP的一个新方向。|
|**2024-08-30**|**Assessing Generative Language Models in Classification Tasks: Performance and Self-Evaluation Capabilities in the Environmental and Climate Change Domain**|Francesca Grasso et.al.|[2408.17362](http://arxiv.org/abs/2408.17362)|**[link](https://github.com/stefanolocci/LLMClassification)**|**本文探讨了两种大型语言模型（LLMs）GPT3.5和Llama2以及一种小型语言模型（SLM）Gemma在气候变化（CC）和环境领域内的三种不同分类任务中的性能。通过使用基于BERT的模型作为基准，我们将这些转换器基模型与它们进行比较。此外，我们还评估了模型的自我评估能力，通过分析这些文本分类任务中的口头信心分数的校准情况。我们的发现表明，尽管基于BERT的模型通常在所有模型中表现最佳，但大生成模型的性能仍然值得注意。进一步地，我们的校准分析显示，Gemma在初期任务中表现出良好的校准性，随后产生不一致的结果；Llama具有合理的校准性，而GPT始终表现出强大的校准性。通过这项研究，我们旨在为讨论大型生成型LM在解决地球最紧迫问题方面的适用性和有效性做出贡献，特别是在生态学和CC背景下突出其优势和限制。**|
|**2024-08-30**|**Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage**|Md Rafi Ur Rashid et.al.|[2408.17354](http://arxiv.org/abs/2408.17354)|null|针对私有数据进行下游应用的大型语言模型微调存在重大隐私风险，可能泄露敏感信息。当前社区平台提供了方便的大规模预训练模型分发，任何人都可以发布而无需严格的验证。这种情境下，隐私威胁显著增加，因为预训练模型可能被故意篡改以在微调过程中泄露私人数据。本研究引入了一种新颖的中毒技术，使用模型卸载作为攻击工具。这种方法通过调整预训练语言模型来提高微调过程中的私人数据泄露程度。我们的方法在保持模型实用性的同时，增强了成员归属性和数据提取攻击的效果。实验结果在不同模型、数据集和微调设置下显示，我们的攻击显著超越了基准性能。这项工作向下载未经过严格验证来源预训练模型的用户发出了警告，突显了潜在的风险。|
|**2024-08-30**|**Bridging Domain Knowledge and Process Discovery Using Large Language Models**|Ali Norouzifar et.al.|[2408.17316](http://arxiv.org/abs/2408.17316)|**[link](https://github.com/alinorouzifar/imr-llm)**|**发现优质流程模型对于执行不同的流程分析任务至关重要，如一致性检查和流程改进。自动化流程发现方法往往忽视了有价值的专业领域知识。这些知识，包括来自专业领域专家的见解和详细流程文档，通常在流程发现过程中未得到充分利用。本文通过利用大型语言模型（LLMs）直接将此类知识整合到流程发现中来解决这一问题。我们使用从LLMs中提取的规则来指导模型构建过程，确保其与领域知识和实际流程执行保持一致。通过整合LLMs，我们建立了一座连接以自然语言表达的流程知识与发现稳健流程模型之间的桥梁，显著推进了流程发现方法论。为了展示我们框架的实用性，我们进行了一个案例研究，对象是UWV员工保险公司，这证明了其实际优势和有效性。**|
|**2024-08-30**|**Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts**|Rhui Dih Lee et.al.|[2408.17280](http://arxiv.org/abs/2408.17280)|null|我们提出了一种工具包，用于从已训练的模型创建低成本的领域专家混合（MOE）。该工具包可以用于从模型或适配器创建混合。我们进行了广泛的测试，并提供了关于使用工具包定义结果MOE架构的指导。公开了一个可用的存储库。|
|**2024-08-30**|**Joint Estimation and Prediction of City-wide Delivery Demand: A Large Language Model Empowered Graph-based Learning Approach**|Tong Nie et.al.|[2408.17258](http://arxiv.org/abs/2408.17258)|null|电子商务和城市化的蓬勃发展，极大地增强了城市区域的配送活动，导致了需求量的增加与复杂性的提升。为了应对这些挑战，数据驱动的预测方法，特别是基于机器学习的技术，开始在城市配送需求管理问题中发挥关键作用。然而，一个尚未得到充分研究的问题是全城范围内的配送需求联合估计与预测。针对这一问题，我们将其建模为一个基于图的时空学习任务。  首先，我们定义了一个消息传递神经网络模型来捕捉相关区域之间需求模式的交互。其次，通过利用大型语言模型的最新进展，我们从未结构化的地理位置数据中提取通用的地理空间知识编码，并将其整合到需求预测器中。最后，为了促进模型在不同城市的迁移能力，我们设计了一种端到端的归纳训练方案。  我们在两个真实的配送数据集上进行了广泛的实验验证，包括中国的八个城市和美国的城市，结果表明我们的模型在这些具有挑战性的任务中显著优于现有的基准方法。|
|**2024-08-30**|**VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters**|Mouxiang Chen et.al.|[2408.17253](http://arxiv.org/abs/2408.17253)|**[link](https://github.com/keytoyze/visionts)**|**本文探讨了从丰富且高质量的自然图像出发构建时间序列预测（TSF）基础模型的新路径。现有的方法要么通过微调大型语言模型（LLM），要么建立大规模时间序列数据集来开发TSF基础模型，但这些方法面临跨域差距或领域内异质性的严峻挑战。我们基于图像与时间序列之间内在相似性，探索了一种新的TSF任务表示，将其重新表述为图像重建任务，并利用在ImageNet数据集上进行自我监督预训练的视觉掩码自动编码器（MAE）进行处理。  令人惊讶的是，在无需进一步在时间序列领域进行适应的情况下，所提出的VisionTS就能实现优于现有TSF基础模型的零样本预测性能。通过最小程度的微调，VisionTS能够进一步提升预测性能，并在大多数情况下达到最先进的水平。这些发现表明，视觉模型可能为TSF提供免费午餐，并强调了计算机视觉与TSF领域未来交叉研究的潜力。我们的代码已公开在https://github.com/Keytoyze/VisionTS上。**|
|**2024-08-29**|**How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models**|Jiyue Jiang et.al.|[2408.16756](http://arxiv.org/abs/2408.16756)|**[link](https://github.com/jiangjyjy/yue-benchmark)**|快速发展的大型语言模型（LLMs）已经改变了自然语言处理（NLP）的竞赛环境，特别是在英语和其他数据丰富的语言中。然而，在诸如粤语这样的代表性不足的语言领域，开发差距仍然显著存在，这尤其令人担忧，考虑到广深港澳大湾区的经济重要性，以及在新加坡和北美地区大量粤语使用者的情况。尽管粤语广泛使用，但在NLP研究中对粤语的代表却少之又少，尤其是与其他同样发达地区的语言相比。为了填补这些空白，我们概述了当前的粤语NLP方法，并引入了旨在评估LLM在事实生成、数学逻辑、复杂推理和粤语中的通用知识等方面的性能的新基准，旨在推动开源粤语LLM技术的发展。我们也提出了未来的研究方向和推荐的模型，以增强粤语LLM的开发。|
|**2024-08-29**|**Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models**|Alec Solway et.al.|[2408.16753](http://arxiv.org/abs/2408.16753)|null|强化学习在预训练模型后，通过最大化似然性来预测大型文本语料库中的下一个文本令牌，用于将语言模型与人类偏好信号对齐。在部署到特定领域之前，通常会对模型进行进一步的微调以适应任务相关的数据。由于人类偏好信号在最后阶段往往不可用，因此通常使用最大化似然性进行微调，这是默认方法。然而，强化学习除了能够促进与人类定义奖励函数的对齐之外，还有其他优势。相比于最大化似然性，即模仿学习模型在理想条件下应执行的操作，强化学习不限于仅展示达到最优状态时的操作，而是在探索策略空间的过程中训练模型在各种情况下的操作。此外，它还训练模型避免执行竞争但效果不佳的操作。本文开发了一种使用强化学习进行最后一阶段微调的框架，并测试了该方法是否能带来性能提升。实验集中在抽象概括上，但框架具有普遍适用性。采用该流程产生的结果显著优于仅使用最大似然性输出的结果。对于特定的数据集，通过后处理最大似然输出可以缩小性能差距。然而，该框架提供了一种优化模型的新途径，在后处理可能不那么直接有效或有效的场景中尤为有用，并且它可以扩展以包括更多类别的需要惩罚并训练反对的不适当输出，如幻觉。|
|**2024-08-29**|**Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge**|Beidi Dong et.al.|[2408.16749](http://arxiv.org/abs/2408.16749)|null|本文探讨了在检测和限制网络上极端主义思想传播方面，自动工具的重要性。研究比较了双向编码表示的Transformer（BERT）和生成预训练Transformer（GPT）模型，在“右翼”和“左翼”意识形态关键词的社交媒体帖子中进行检测与分类的能力。我们收集了含有上述关键词的帖子，并人工标记为极端主义或非极端主义。进一步地，我们将极端主义帖子分为五个构成要素之一，基于工作定义框架。  BERT模型的性能评估基于训练数据规模和类别间的知识转移。此外，我们对比了使用不同提示的GPT 3.5和GPT 4模型的性能：原始提示、一般定义、角色扮演和专业定义。结果表明，最佳表现的GPT模型优于最佳表现的BERT模型，更详细的提示通常能带来更好的结果。然而，过于复杂的提示可能会影响性能。不同的GPT版本对被认定为极端主义的敏感度各不相同。GPT 3.5在识别左翼极端主义帖子方面表现更好，而GPT 4则在识别右翼极端主义帖子方面表现更好。  大型语言模型（GPT模型）在在线极端主义分类任务中展现出显著潜力，超越了传统的BERT模型，在零样本设置下表现出色。未来研究应探索人类与计算机交互在优化GPT模型以进行极端主义检测与分类任务中的作用，以开发更高效（例如，更快捷、更少努力）且更有效的识别极端主义内容方法。|
|**2024-08-29**|**Theoretical and Methodological Framework for Studying Texts Produced by Large Language Models**|Jiří Milička et.al.|[2408.16740](http://arxiv.org/abs/2408.16740)|null|本文从定量语言学的角度探讨了研究大型语言模型（LLM）及其生成文本所面临的概念、方法论和技术挑战。本文基于一个理论框架，该框架区分了作为载体的LLM与模拟的实体。本文倡导对模型采取严格非拟人化的方法，同时谨慎地应用用于研究人类语言行为的方法来分析模拟实体。虽然自然语言处理研究者关注模型本身、其架构、评估以及提高性能的方法，作为定量语言学家，我们的目标是构建关于LLM生成文本特性的理论体系，它们与人类生成的文本有何不同，以及模拟实体的属性。此外，我们还应探索LLM作为研究人类文化工具的可能性，而语言是这一文化不可或缺的一部分。|
|**2024-08-29**|**GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative Models**|Moreno D'Incà et.al.|[2408.16700](http://arxiv.org/abs/2408.16700)|**[link](https://github.com/moreno98/gradbias)**|**近期在文本到图像（T2I）生成模型领域取得的进展使得高质量图像生成成为可能。随着性能和可访问性的提高，这些模型正受到越来越多的关注和欢迎，确保它们的公平性和安全性是防止偏见传播和延续的关键。现有研究主要集中在预定义偏见（如性别、种族）的封闭集合上进行偏见检测。然而，在开放集设置下，即无需预先设定的情况下，检测和量化偏见是一个挑战。  本文提出了一种通用框架，用于识别、量化和解释开放集设置下的偏见。该管道利用大型语言模型（LLM）从一组描述中提出偏见。随后，使用目标生成模型生成一系列图像。最后，通过视觉问答（VQA）进行偏见评估。我们展示了两种基于此框架的方法：OpenBias 和 GradBias。OpenBias 能够检测并量化与人、物体和动物相关的已知和新型偏见，并与现有的封闭集偏见检测方法以及人类判断高度一致。GradBias 显示出中性词汇对偏见的影响显著，并且在多项基线中表现最佳，包括最先进的基础模型。  代码已在此处提供：https://github.com/Moreno98/GradBias。**|
|**2024-08-29**|**Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity**|Ziniu Li et.al.|[2408.16673](http://arxiv.org/abs/2408.16673)|null|本文旨在解决大型语言模型在下游任务的精调（Supervised Fine-Tuning，SFT）过程中遇到的过拟合和输出多样性受限的问题。传统上，交叉熵（Cross Entropy，CE）损失函数被广泛用于SFT，然而它可能导致模型对数据分布进行过于激进的更新，从而引发过拟合和降低输出的多样性。  为了解决这些问题，本文引入了最大熵原则，该原则倾向于促进模型生成更平滑的概率分布，同时仍能有效捕捉数据特征。具体地，我们提出了一种名为GEM的新方法，它通过解决反向Kullback-Leibler散度最小化问题，并加入熵正则化器，来匹配目标分布。  在对Llama-3-8B模型进行SFT时，GEM在多个方面优于CE。首先，在使用UltraFeedback数据集训练以增强模型的指令遵循能力时，GEM表现出较低的过拟合迹象，表现为更低的困惑度和在IFEval基准测试上的更好性能。此外，GEM还提高了输出的多样性，即使在没有特定领域数据的情况下，仅通过最佳n采样，数学推理和代码生成任务的性能也得到了最高7分的提升。  进一步地，当使用特定领域的数据集对数学推理和代码生成任务进行微调时，GEM同样表现出较低的过拟合和与CE相比高达10分的性能提升。|
|**2024-08-29**|**Examination of Code generated by Large Language Models**|Robin Beer et.al.|[2408.16601](http://arxiv.org/abs/2408.16601)|**[link](https://github.com/t-muras/ai-code-analysis)**|**大型语言模型（LLM），例如ChatGPT和Copilot，正在通过自动化代码生成彻底改变软件开发，这在一定程度上促进了快速原型设计、教育支持以及生产力的提升。因此，LLM生成的代码正确性和质量应与人工编写的代码相当。为了评估当前LLM在生成Java和Python语言中的简单算法及其对应的单元测试时的正确性和质量（覆盖率）的能力，我们进行了受控实验。实验包括让LLM生成代码并评估其正确性与质量。我们观察到LLM之间、不同编程语言之间、算法与测试代码之间以及时间上的显著差异。本文报告了这些结果及实验方法，以便进行重复和可比的评估，以涵盖更多的算法、语言和LLM随时间的变化情况。**|
|**2024-08-29**|**Enhancing Dialogue Generation in Werewolf Game Through Situation Analysis and Persuasion Strategies**|Zhiyang Qi et.al.|[2408.16586](http://arxiv.org/abs/2408.16586)|null|近期自然语言处理领域的进步，尤其是大型语言模型（LLM）如GPT-4的发展，显著提升了对话系统的性能，使得它们能够生成更为自然流畅的对话。然而，这些系统仍面临着诸如持续对话管理、记忆保留和减少幻觉等挑战。AIWolfDial2024这一项目通过采用“狼人杀”这一不完全信息游戏来测试LLM在复杂互动环境中的能力，以应对上述挑战。该项目引入了一种基于LLM的“狼人杀”游戏AI，其中每个角色都通过情境分析来辅助回应生成。对于“狼人”这一角色，项目采用了包括逻辑吸引力、可信度吸引力和情感吸引力在内的多种说服策略，以有效地引导其他玩家与自己的行动保持一致。|
|**2024-08-29**|**CNIMA: A Universal Evaluation Framework and Automated Approach for Assessing Second Language Dialogues**|Rena Gao et.al.|[2408.16518](http://arxiv.org/abs/2408.16518)|**[link](https://github.com/renagao/csl2024)**|我们开发了CNIMA（一种中文作为第二语言的非母语互动测量与自动化数据集），包含10,000个对话。我们使用了一个评估框架来注释CNIMA，该框架最初用于英语作为第二语言的对话，它评估了微观层面特征（如回话）和宏观层面互动标签（如主题管理）。我们测试了该框架从英语到中文的可移植性。发现该框架在不同语言之间具有鲁棒性，并揭示了普遍性和特定于语言的微观层面和宏观层面特征之间的关系。接下来，我们提出了一种自动化评估的方法，并找到了强大的性能，创建了一个新的自动化第二语言评估工具。我们的系统易于适应其他语言，因为它使用大型语言模型，因此不需要大规模标注训练数据。|
|**2024-08-29**|**LLMs vs Established Text Augmentation Techniques for Classification: When do the Benefits Outweight the Costs?**|Jan Cegin et.al.|[2408.16502](http://arxiv.org/abs/2408.16502)|null|生成式大型语言模型（LLMs）在数据增强任务中的应用越来越广泛，文本样本通过LLM进行同义替换后用于分类模型的微调。然而，关于LLM数据增强方法相较于现有成熟方法是否具有明显优势的研究证据相对缺乏。为了探讨在何种情况下使用LLM数据增强方法更为有利，本研究在6个数据集、3个分类器和2种微调方法上进行了对比实验。我们还调整了种子数量和收集样本的数量，以便更全面地探索下游模型准确度空间。此外，我们还进行了成本效益分析，结果表明，在使用非常少量种子的情况下，LLM数据增强方法值得部署。在许多情况下，现有方法能够达到或超过类似甚至更好的模型准确度。|
|**2024-08-28**|**Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders**|Min Shi et.al.|[2408.15998](http://arxiv.org/abs/2408.15998)|**[link](https://github.com/nvlabs/eagle)**|**《大规模语言模型在多模态任务中的视觉理解能力：混合视觉编码器的设计空间探索》一文探讨了准确解析复杂视觉信息对于多模态大型语言模型（MLLMs）的重要性。近期研究显示，增强的视觉感知能显著降低幻觉现象，并在光学字符识别、文档分析等分辨率敏感任务上提升性能。许多先进MLLMs通过集成多种视觉编码器来实现这一目标。然而，当前缺乏对关键方面系统的比较和详细的拆解研究，比如专家选择和多视觉专家融合策略。本文对使用混合视觉编码器的MLLM设计空间进行了广泛探索。研究发现，多个互补视觉编码器的视觉令牌简单拼接即可达到与更复杂的混合架构或策略相当的效果。此外，引入预对齐（Pre-Alignment）机制，以弥合专注于视觉的编码器与语言令牌之间的差距，从而提升模型一致性。由此产生的MLLM家族——Eagle，在主要的MLLM基准测试中超越了其他领先开源模型。相关代码及模型已开源发布：https://github.com/NVlabs/Eagle**|
|**2024-08-28**|**BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems**|Wei Wang et.al.|[2408.15971](http://arxiv.org/abs/2408.15971)|null|大型语言模型（LLM）正在变得越来越强大，能够处理复杂任务，例如构建单一代理和多代理系统。相较于单一代理，多代理系统对语言模型的协作能力提出了更高的要求。已有的评估基准主要关注于多代理系统的协作能力，但在细粒度评估方面存在不足，并且忽略了多代理系统的协作与竞争场景。  为了填补这一空白，我们提出了一种新的基准测试——BattleAgentBench。该基准定义了三个不同难度级别的七个子阶段，旨在从单一代理场景导航能力、配对代理任务执行能力以及多代理合作与竞争能力等多个维度，对语言模型进行细致的评估。我们对四大闭源模型和七大开源模型进行了广泛评估。  实验结果表明，基于API的模型在简单任务上表现出色，而开源小型模型在简单任务上则面临挑战。对于需要合作与竞争能力的困难任务，尽管基于API的模型展示了一定的协作能力，但仍有巨大的改进空间。|
|**2024-08-28**|**More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding**|Yuan Tang et.al.|[2408.15966](http://arxiv.org/abs/2408.15966)|**[link](https://github.com/tangyuan96/greenplm)**|在本论文中，我们重新审视了让大型语言模型（LLM）理解三维物理世界这一挑战。由于缺乏大规模的三维点云与文本配对数据集，LLM 在三维理解上的成功尚未实现复制。为此，我们提出了一项新任务：3D 数据高效点云-语言理解。目标是使LLM 能够利用最少的三维点云和文本数据对实现稳健的三维对象理解。  为了应对这一任务，我们引入了GreenPLM，通过利用更多的文本数据来弥补缺少的三维数据。首先，借鉴使用CLIP对图像和文本进行对齐的方式，我们利用预训练的点云-文本编码器将三维点云空间映射到文本空间。这一映射使得我们可以无缝地连接文本空间与LLM。一旦建立了点云-文本-LLM的连接，我们进一步通过扩展中间文本空间增强文本-LLM的对齐，从而减少对三维点云数据的依赖。  具体而言，我们生成了600万个关于三维物体的自由文本描述，并设计了三阶段训练策略，帮助LLM更好地探索不同模态之间的内在联系。为了实现高效的模态对齐，我们设计了一个零参数交叉注意力模块用于令牌聚合。  广泛的实验结果表明，GreenPLM仅需要现有最先进的模型所用3D训练数据的12%，就能达到更优的三维理解性能。令人惊讶的是，GreenPLM仅使用文本数据也能实现竞争力的表现。相关代码和权重可在以下链接获取：https://github.com/TangYuan96/GreenPLM。|
|**2024-08-28**|**Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games**|Nicholas R. Waytowich et.al.|[2408.15950](http://arxiv.org/abs/2408.15950)|null|近期，大型语言模型（LLMs）的进展使其能力超越了传统的文本任务，扩展到了多模态领域，整合了视觉、听觉和文本数据。虽然在机器人学和游戏等高阶规划领域对多模态LLM的研究已经相当广泛，但在低级控制任务中的应用潜力却鲜有探索。本文探讨了多模态LLM在 Atari 视频游戏领域的应用，引入了 Atari 游戏性能作为评估多模态LLM执行低级控制任务能力的新基准。与传统强化学习（RL）和模仿学习（IL）方法相比，这些LLM无需大量的计算资源和奖励函数定义，而是利用现有的多模态知识直接与游戏环境交互。  我们的研究评估了多个多模态LLM的表现，与传统RL代理、人类玩家和随机代理进行了比较，重点关注它们理解复杂视觉场景并制定战略响应的能力。此外，我们还通过引入人类演示的游戏玩法轨迹来研究上下文学习（ICL）的影响，以增强模型的上下文理解能力。  通过这一研究，我们旨在确定多模态LLM能否利用其广泛的训练来有效地充当低级控制器，从而重新定义动态和视觉复杂环境中的潜在应用。有关额外结果和视频的更多信息，请访问我们的项目网页：https://sites.google.com/view/atari-gpt/。|
|**2024-08-28**|**Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models**|Yuncheng Yang et.al.|[2408.15915](http://arxiv.org/abs/2408.15915)|**[link](https://github.com/yaphabates/rocket)**|在特定领域培养大型语言模型（LLM）以解决任务所需的专长往往需要针对稳定预期输出进行专门调整。避免手动准备指令数据集和训练资源带来的巨大成本，利用开放知识包括低秩适应（LoRA）模型和指令数据集作为起点是合理的选择。然而，现有方法在模型和数据选择上侧重于通用能力的性能，而忽视了在特定领域部署时暴露的知识差距。本研究提出了一种通过引入少量人工标注样本（即K-shot）来弥合此类差距的方法，以促进LLM在开放知识上的任务专长。  具体来说，我们开发了一个高效且可扩展的管道，以成本效益方式生成任务专家，其中K-shot数据参与选择最具潜力的专家候选者和任务相关的指令。构建了一个混合专家（MoE）系统，充分利用多个专家之间独特但互补的知识。我们揭示了MoE系统成功的关键因素：  1. 遵循K-shot原则：确保真正具备解决K-shot问题能力的模型被选中，而非盲猜者。 2. 强调多样性：不仅专家本身具有多样性，而且在整个模型和数据选择过程中，细调指令也体现出多样性。  广泛的实验结果证实了我们的方法在各种任务上对开放知识利用的优越性。后续将发布代码和模型。|
|**2024-08-28**|**Decentralized LLM Inference over Edge Networks with Energy Harvesting**|Aria Khoshsirat et.al.|[2408.15907](http://arxiv.org/abs/2408.15907)|null|大型语言模型在自然语言任务上表现出的卓越性能已经极大地改变了多个领域，但在资源受限环境如边缘网络中的部署仍面临挑战。分布式推理技术的出现通过在多台设备间分配模型块来提升灵活性和成本效益，但仍存在能源限制问题，尤其是针对电池供电的边缘设备。我们提出了一种基于互联、使用能量收集的电池供电边缘设备的协作推理可持续模型。通过建立半马尔可夫模型描述设备状态，考虑处理参数和平均绿色能源到达情况，以指导设计旨在减少设备停机时间和最大化网络吞吐量的调度算法。通过实证评估和模拟运行，验证了我们的方法的有效性，为边缘网络上的节能分布式推理铺平了道路。|
|**2024-08-28**|**LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**|Ruirui Chen et.al.|[2408.15903](http://arxiv.org/abs/2408.15903)|null|快速过时的信息使得大型语言模型（LLMs）在整合新知识方面面临挑战。现有方法在处理需要准确事实识别和序列逻辑推理的多跳问题时仍存在困难，尤其是在面对大量事实更新的情况下。为解决这些问题，本文提出了Graph Memory-based Editing for Large Language Models（GMeLLo），一种简单而有效的方法，它结合了知识图谱（KGs）的明确知识表示与LLMs的语言灵活性。GMeLLo不仅利用LLMs进行问答，还运用这些模型将自然语言转换为结构化查询和事实三元组，从而实现与KGs的无缝交互，用于快速更新和精确的多跳推理。实验结果表明，GMeLLo在多跳问答基准MQuAKE中显著超越当前最先进的知识编辑方法，特别是在涉及大量知识更新的场景中。|
|**2024-08-28**|**Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts**|Nikolas Gritsch et.al.|[2408.15901](http://arxiv.org/abs/2408.15901)|null|当前大型语言模型在效率、专业化和对新数据分布的适应性方面难以同时具备这些优秀品质。混合专家（MoE）架构因其条件计算的内在特性，成为研究的重点领域，旨在提升这些品质。本工作专注于“升级”密集型专家模型至MoE架构，旨在增强专业化的同时，也增加对新任务的灵活适应性。  我们引入了Nexus，一种增强的MoE架构，其具有自适应路由机制，允许模型学习将专家嵌入从领域表示进行投影。这种策略使得Nexus能够通过单独训练的密集模型灵活地添加新的专家，无需对未见数据域进行大规模MoE训练。实验结果显示，与基线相比，Nexus在初始升级阶段实现了高达2.1%的相对增益，在使用有限的微调数据扩展MoE时实现了18.8%的相对增益。Nexus的灵活性对于建立一个开源生态系统至关重要，该生态系统允许每个用户根据自己的需求不断组装自己的MoE混合模型。|
|**2024-08-28**|**Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models**|Sebastian Vallejo Vera et.al.|[2408.15895](http://arxiv.org/abs/2408.15895)|null|人类编码员存在偏见。我们通过复制Ennser-Jedenastik和Meyer（2018）的实验，发现大型语言模型（LLMs）在评估政治声明时使用政治信息，特别是政党线索。LLMs不仅根据政党线索上下文化判断陈述是正面、负面还是中性，还反映出它们在训练过程中生成的人类数据所具有的偏见。我们还发现，与人类不同的是，人类仅在面对极端政党声明时表现出偏见，而LLMs即使在被提示来自中间左翼和中间右翼政党的声明时也显示出显著偏见。最后部分讨论了这些发现的意义。|
|**2024-08-28**|**Persuasion Games using Large Language Models**|Ganesh Prasath Ramani et.al.|[2408.15879](http://arxiv.org/abs/2408.15879)|null|大型语言模型（LLM）已经发展成为一种强大的工具，能够理解和生成类似人类的文本。本文研究了LLM在塑造人类观点并进而影响他们在特定任务上的决策方面的潜力。这些能力在投资、信用卡和保险等多个领域找到了应用，帮助用户选择合适的保险政策、投资计划、信用卡以及零售产品，甚至在行为改变支持系统（BCSS）中也有应用。  我们提出了一种复杂多代理框架，其中一组代理以协作方式操作。主要代理直接与用户进行有说服力的对话，而辅助代理执行诸如信息检索、响应分析、制定说服策略和事实验证等任务。我们的实验证据表明，这种协作方法显著提高了LLM的说服效果。我们持续分析用户的抵抗性，并通过结合规则基于和LLM基于的抵抗-说服映射技术来应对这一挑战。  我们使用模拟的人格形象，并在保险、银行和零售领域生成对话，以评估大型语言模型（LLM）在识别、适应和影响不同人格类型方面的熟练程度。同时，我们也检查了LLM模拟人格所采用的抵抗机制。说服效果通过交互前后的可衡量调查、LLM生成的对话评分以及用户决策（购买或不购买）进行量化。|
|**2024-08-27**|**Generative Verifiers: Reward Modeling as Next-Token Prediction**|Lunjun Zhang et.al.|[2408.15240](http://arxiv.org/abs/2408.15240)|null|验证器或奖励模型常用于增强大型语言模型（LLM）的推理性能。一种常见的方法是Best-of-N策略，其中从LLM生成的N个候选解决方案中由验证器进行排名，选择最佳一个。传统上，验证器是作为判别分类器进行训练以对解决方案打分的，但它们并未充分利用预训练LLM的文本生成能力。为了克服这一限制，我们提议通过在验证和解决方案生成上使用通用的下一个词预测目标联合训练验证器。与标准验证器相比，这样的生成型验证器（GenRM）可以从LLM的几个优势中获益：它们可以无缝地与指令调谐相结合，支持链式思考推理，并且可以通过增加推理时的计算量来利用多数投票，从而进行更好的验证。我们展示了，在算法问题和小学数学推理任务上使用Gemma为基础的验证器时，GenRM优于判别型验证器和LLM作为裁判，表现出16%-64%的问题解决率提升。此外，我们证明了GenRM在数据集规模、模型容量和推理时计算量增加方面具有良好的可扩展性。|
|**2024-08-27**|**LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet**|Nathaniel Li et.al.|[2408.15221](http://arxiv.org/abs/2408.15221)|null|近期的大规模语言模型（LLM）防御措施显著提升了模型对有害查询的拒绝能力，即使在遭受有组织攻击的情况下也不例外。然而，这些防御措施主要是在单轮对话中针对自动化攻击进行评估，这种威胁模型不足以反映真实世界中恶意行为的复杂性。  我们通过实验展示了多轮对话的人工智能“越狱”（即攻击者利用模型的漏洞来绕过防御机制）能够揭露防御系统中的重大漏洞。在使用HarmBench这一评估平台，对抗那些在单轮对话中仅报告低百分比攻击成功率（ASR）的防御系统时，我们发现多轮对话的人工智能“越狱”的成功率超过了70%。这表明当前的防御机制在面对更复杂的、多步骤的攻击策略时存在不足。  此外，多轮对话的人工智能“越狱”还揭示了机器遗忘防御系统的漏洞。攻击者成功地从未被删除的模型中恢复了可用于生物安全双重用途的知识，这进一步证明了现有防御措施在保护敏感信息方面存在的弱点。  为了总结和共享这些发现，我们构建了一个名为“多轮对话人工智能越狱”（Multi-Turn Human Jailbreaks，简称MHJ）的数据集，包含了来自537个不同多轮对话场景的2912个触发指令，共计2,912个触发指令涉及2,912个不同的多轮对话“越狱”案例。同时，我们还公开发布了这个数据集以及在多种商业红队测试中发展出的一系列“越狱”策略的综述，旨在为研究更强大的LLM防御系统提供资源和支持。|
|**2024-08-27**|**Investigating Coverage Criteria in Large Language Models: An In-Depth Study Through Jailbreak Attacks**|Shide Zhou et.al.|[2408.15207](http://arxiv.org/abs/2408.15207)|null|大型语言模型（LLM）的迅速发展极大地改变了人工智能的格局，然而在敏感领域部署时，它们的脆弱性引发了一系列严重关切，尤其是对于恶意利用的风险。这种情况凸显了预部署测试不足的问题，强调了需要更加严格和全面评估方法的紧迫性。本研究通过全面的实证分析，评估了传统覆盖标准在识别这些漏洞方面的有效性，特别关注了关键问题——“越狱”攻击。研究首先对LLM中的隐藏状态进行了聚类分析，结果显示这些状态的内在特性能够明显区分不同类型的查询。随后，我们从三个关键维度——标准级别、层级别和词级别——评估了这些标准的性能。我们的发现揭示了正常查询与“越狱”查询在神经元激活模式上的显著差异，从而验证了聚类结果。基于这些发现，我们提出了一种创新的方法，用于实时检测“越狱”攻击，利用神经激活特征。我们的分类器表现出了极高的准确率，平均达到96.33%，成功识别出包括可能导致对抗性攻击的“越狱”查询。这项研究的重要性在于其对LLM安全性测试复杂挑战的全面应对。通过使系统能够在生成第一个词时立即检测到攻击，我们的方法为集成LLM的未来系统提供了强大的实时检测能力。这一研究深化了我们对LLM安全性的理解，并为开发更稳健的人工智能系统奠定了基础。|
|**2024-08-27**|**Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation**|Jian Hu et.al.|[2408.15205](http://arxiv.org/abs/2408.15205)|**[link](https://github.com/lwpyh/ProMaC_code)**|本文提出了一种任务通用的提示可分割方法，旨在减少对每种所需对象的实例特定手动提示的需求。通过使用单个任务通用提示来指导同一任务下不同对象的不同图像的分割，引入了任务通用提示分割。当前的方法利用多模态大型语言模型（MLLMs）从通用提示推理出详细的实例特定提示，以提高分割准确性。这种方法的有效性在很大程度上取决于生成提示的精确度。然而，MLLMs在推理过程中经常出现幻觉，导致提示不准确。现有方法专注于消除幻觉以提高模型性能，本文认为MLLM幻觉在正确利用时可以揭示有价值的任务相关信息，因为它们代表了超越单张图像的预训练大规模知识。因此，本文利用幻觉从图像中挖掘任务相关信息，并验证其准确性以增强生成提示的精确度。  具体而言，我们引入了一个迭代的提示-掩码循环生成框架（ProMaC），该框架包括一个提示生成器和一个掩码生成器。提示生成器使用多尺度链式思考提示，最初探索幻觉以提取测试图像上的扩展上下文知识。然后，将这些幻觉降低到形成精确的实例特定提示，从而引导掩码生成器通过掩码语义对齐产生与任务语义一致的掩码。生成的掩码通过迭代引导提示生成器更关注任务相关的图像区域并减少无关的幻觉，最终共同提高了提示和掩码的质量。  实验结果在5个基准数据集上证明了ProMaC的有效性。详细代码见https://lwpyh.github.io/ProMaC/。|
|**2024-08-27**|**Can Unconfident LLM Annotations Be Used for Confident Conclusions?**|Kristina Gligorić et.al.|[2408.15204](http://arxiv.org/abs/2408.15204)|**[link](https://github.com/kristinagligoric/confidence-driven-inference)**|大型语言模型（LLM）在各种任务中与人类评估者高度一致，显示出减轻人类数据收集挑战的潜力。在计算社会科学（CSS）领域，研究人员越来越多地利用LLM注释来补充缓慢且昂贵的人类注释。然而，对于如何收集和使用LLM注释而不损害下游结论的有效性，仍缺乏明确的指南。我们引入了“置信驱动推理”方法，该方法结合了LLM注释和LLM置信度指示器，以战略方式选择应收集哪些人类注释，旨在生产准确的统计估计和可验证的置信区间，同时减少所需的人类注释数量。我们的方法具有防止LLM注释质量差的保障措施，确保得出的结论既有效又不比仅依赖人类注释更不准确。我们在三个CSS场景——礼貌文本、立场和偏见——中的统计估计任务中，通过与基线比较，证明了置信驱动推理的有效性，每种场景下所需的人类注释数量减少了超过25%。尽管我们使用CSS场景进行演示，但置信驱动推理可以用于广泛NLP问题中的大多数标准量估计。|
|**2024-08-27**|**Unlocking Potential in Pre-Trained Music Language Models for Versatile Multi-Track Music Arrangement**|Longshen Ou et.al.|[2408.15176](http://arxiv.org/abs/2408.15176)|null|大型语言模型在多个领域展示了显著的能力，包括符号音乐生成。然而，利用这些预训练的模型进行可控音乐编排任务的挑战仍然新颖，每个任务都需要不同的音乐信息作为控制。本文提出了一种统一的序列到序列框架，它允许对符号音乐语言模型进行微调，以执行四个不同的多轨编排任务：乐队编排、钢琴缩减、鼓编排和声音分离。我们的实验结果表明，所提出的策略在所有四个任务上均实现了更高音乐质量的结果，与专门针对特定任务的基线相比。此外，通过额外的探查分析实验，我们展示了预训练阶段赋予模型理解音乐条件的基本知识，这在仅通过特定任务的微调难以获得的情况下尤为重要。|
|**2024-08-27**|**X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation**|Hanjia Lyu et.al.|[2408.15172](http://arxiv.org/abs/2408.15172)|null|大型语言模型（LLM）和大型多模态模型（LMM）已被证明能显著提升丰富项目描述的效果，进而增强推荐系统的准确性。然而，现有方法往往仅依赖于纯文本提示，或者采用基本的多模态策略，未能充分利用文本与视觉模态之间互补的信息。本文提出了一种名为Cross-Reflection Prompting（X-Reflect）的新框架，旨在通过引导LMM明确识别并调和文本与图像之间的支持性与冲突信息来解决这些问题。通过捕捉两种模态的细微洞察，此方法生成了更为全面且语境丰富的项目表示。在两个广泛使用的基准上进行的大量实验表明，我们的方法在下游推荐准确度上优于现有的提示基线。此外，我们评估了框架在不同LMM架构下的泛化能力以及提示策略的鲁棒性，提供了优化的见解。这项工作强调了整合多模态信息的重要性，并提出了改善多模态推荐系统中项目理解的新型解决方案。|
|**2024-08-27**|**Measuring text summarization factuality using atomic facts entailment metrics in the context of retrieval augmented generation**|N. E. Kriman et.al.|[2408.15171](http://arxiv.org/abs/2408.15171)|null|自2022年ChatGPT的发布以来，大型语言模型（LLMs）的应用范围显著扩大，显示出其在各种场景中的价值。然而，对于企业级和商业应用而言，LLMs生成不准确信息的趋势，即所谓的“幻觉”现象，成为了一个主要挑战。本项目提出了一种方法，用于在与原始文本进行比较时评估LLM生成概要的准确性。我们的方法利用朴素贝叶斯分类来判断生成内容的真实性。  通过这种方法，我们可以估计生成文本与实际信息之间的匹配度，从而提高LLM应用的质量和可靠性。这不仅有助于识别可能存在的错误或不准确之处，还能增强用户对LLM生成内容的信任，促进其在更广泛领域的有效使用。此外，该方法还能为LLM的持续改进提供有价值的反馈，推动技术进步，最终实现更高质量、更可靠的人工智能辅助内容生成。|
|**2024-08-27**|**BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline**|Guosheng Dong et.al.|[2408.15079](http://arxiv.org/abs/2408.15079)|null|大型语言模型（LLM）的核心能力高度依赖于广泛预训练数据集的组成和选择，这些数据集被多个机构视为商业秘密。为了缓解这一问题，我们开源了一个通用适用的数据处理管道，并通过引入一个竞争性的LLM基线来验证其有效性和潜力。具体来说，数据处理管道包括广域收集以扩大规模和重新加权以提高质量。然后，我们使用我们的管道对3万亿个令牌进行预训练，而无需任何明确的下游任务优化，接着进行一个简单但有效的监督微调阶段。BaichuanSEED在整个训练过程中表现出一致性与预测性，并在综合基准测试中与几个先进的商业大型语言模型，如Qwen1.5和Llama3，实现了可比性能。我们还进行了几个启发式实验，讨论了在数学和编程等下游任务进一步优化的可能性。|
|**2024-08-27**|**Constraining Participation: Affordances of Feedback Features in Interfaces to Large Language Models**|Ned Cooper et.al.|[2408.15066](http://arxiv.org/abs/2408.15066)|null|本文探讨了交互反馈功能在ChatGPT界面中的可用性，分析了这些功能如何塑造用户输入以及大型语言模型迭代过程中的参与度。通过调研ChatGPT用户并应用了可操作性框架，我们展示了这类功能鼓励简单、频繁且侧重于性能的反馈，同时限制了集体输入和用户间的讨论。我们主张，这种反馈格式极大地限制了用户的参与，强化了用户、公众与开发大型语言模型的公司之间的权力不平等。我们的分析为现有参与式人工智能文献提供了新的视角，着重于现有反馈流程的局限性，并提出了重新设计的方向。  为了使公众在人工智能发展中能够更具有意义地参与，我们提倡转向关注模型输出与特定用户偏好的一致性的过程。相反，我们强调需要促进公司与不同“公众”之间关于大型语言模型的目的和应用进行对话的过程。这一方法要求对持续的社会基础设施建设的关注，即创建和维持解决AI开发和部署影响群体关切所需的社会、技术和机构结构。|
|**2024-08-27**|**Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models**|Aradhye Agarwal et.al.|[2408.14470](http://arxiv.org/abs/2408.14470)|**[link](https://github.com/Aradhye2002/selective-peft-toolkit)**|**细调大型语言模型（LLMs）在下游任务上需要大量计算资源。参数高效细调（PEFT）类方法旨在通过仅微调模型参数的小部分来缓解这些计算挑战。虽然从计算效率方面考虑，这些技术通常无法与完全微调的模型性能相匹敌，主要原因是参数选择过程中固有的偏见。传统的选择性PEFT技术基于预先定义的预算（也称为去遮罩）使用固定参数集，未能动态捕捉参数的重要性，并经常超出预算。我们引入了 $\text{ID}^3$，这是一种新颖的选择性PEFT方法，它连续计算参数的重要性，并通过平衡参数选择过程中的探索与利用来动态地去遮罩参数。我们在15个任务上进行的实验覆盖了自然语言理解与生成任务，显示了与基于固定去遮罩的PEFT技术相比，我们的方法的有效性。我们通过理论分析证明，$\text{ID}^3$将梯度更新的数量减少了一倍，从而提高了计算效率。$\text{ID}^3$ 对神经元的随机初始化具有鲁棒性，因此可以无缝集成到现有添加式和重新参数化基PEFT模块，如适配器和LoRA中，用于动态稀疏化。**|
|**2024-08-26**|**Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos**|Qirui Chen et.al.|[2408.14469](http://arxiv.org/abs/2408.14469)|null|本文探讨了长形式第一人称视角视频中的多跳视频问答（Multi-Hop Video Question Answering，MH-VidQA）问题。这项任务不仅需要回答视觉问题，还需要在视频中定位多个相关的时间段作为视觉证据。我们开发了一个自动化流程来创建带有关联时间证据的多跳问题解答配对，从而构建了一个用于指令调整的大规模数据集。为了监测这一新任务的进展，我们进一步整理了一个高质量的基准——MultiHop-EgoQA，通过仔细的手动验证和细化进行构建。  实验结果揭示了现有跨模态系统在多跳定位和推理能力方面存在不足，导致性能不佳。随后，我们提出了一种名为“Grounding Scattered Evidence with Large Language Model”（GeLM）的新架构，该架构通过引入一个地理解码模块增强了大型语言模型（Large Language Models，LLMs），该模块使用灵活的地理解码令牌从视频中检索时间证据。在我们的视觉指令数据上进行训练后，GeLM展示了增强的多跳定位和推理能力，为这一具有挑战性的任务设定了新的基准。此外，当在第三人称视角视频上进行训练时，相同的架构在单跳视频问答基准（ActivityNet-RTL）上也达到了最先进的性能，证明了其有效性。|
|**2024-08-26**|**Explicit Inductive Inference using Large Language Models**|Tianyang Liu et.al.|[2408.14467](http://arxiv.org/abs/2408.14467)|null|在本论文中，我们提出了一种管道方法，利用大型语言模型（LLM）的这一偏差进行明确的归纳推理。该管道使用LLM将前提转换为一组已验证的替代方案，并通过聚合衍生的新蕴含询问的答案来支持原始推理预测。在方向性谓词蕴含基准测试上，我们展示了通过应用此简单管道，可以提高LLM在推理上的整体性能，并显著减轻它们的证实偏差影响。|
|**2024-08-26**|**Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study**|Liuchang Xu Shuo Zhao et.al.|[2408.14438](http://arxiv.org/abs/2408.14438)|null|随着大型语言模型如ChatGPT、Gemini等的问世，评估它们在自然语言理解、代码生成等多方面能力的重要性日益凸显。然而，这些模型在空间任务方面的表现并未得到全面评估。本研究填补了这一空白，通过引入一个新颖的多任务空间评价数据集，系统性地探索和比较几种先进模型在空间任务上的性能。该数据集涵盖了十二种不同的任务类型，包括空间理解和路径规划，并且每项任务都有经过验证的准确答案。  我们采用双阶段测试方法对多个模型进行了评估，包括OpenAI的gpt-3.5-turbo、gpt-4o以及ZhipuAI的glm-4。首先进行零样本测试，随后根据难度对数据集进行分类，并执行了提示调优测试。结果显示，在第一阶段的测试中，gpt-4o的整体准确性最高，平均达到了71.3%。尽管moonshot-v1-8k在总体上略逊一筹，但在地名识别任务上却超越了gpt-4o。研究还揭示了特定任务中提示策略对模型性能的影响。例如，链式思考（COT）策略使gpt-4o在路径规划任务上的准确率从12.4%提升至87.5%，而一次射击策略则使moonshot-v1-8k在地图绘制任务上的准确率从10.1%提高到76.3%。|
|**2024-08-26**|**CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models**|Shubham Bharti et.al.|[2408.14419](http://arxiv.org/abs/2408.14419)|null|我们提出了一种名为CHARTOM的视觉理论理解基准，针对多模态大型语言模型。CHARTOM由专门设计的数据可视化图表组成。给定一个图表，语言模型不仅需要正确理解图表（事实问题），还需要判断该图表是否会让人类读者产生误导（思维问题）。这两个问题都具有重要的社会价值。我们将详细介绍构建CHARTOM基准的过程，包括其对人类表现的校准。|
|**2024-08-26**|**MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**|Kuluhan Binici et.al.|[2408.14418](http://arxiv.org/abs/2408.14418)|null|自动语音识别(ASR)系统在将语音转换为文本方面至关重要，然而，它们引入的错误会严重降低下游任务如摘要生成的表现。这个问题在临床对话摘要领域尤为突出，这是一个数据资源有限的领域，用于微调的监督数据稀缺，因此需要将ASR模型作为黑盒解决方案使用。传统的数据增强方法也不适用于提高摘要模型对噪音的鲁棒性，原因是缺乏足够的医疗对话音频记录及其对应的ASR转录文本。为了应对这一挑战，我们提出了一种名为MEDSAGE的方法，用于通过大型语言模型(LLMs)生成合成样本进行数据增强。具体来说，我们利用LLMs的上下文学习能力，并指导它们基于少量可用的医疗对话示例和音频记录，生成类似ASR的错误。实验结果表明，LLMs能够有效地建模ASR噪音，将这种含噪数据融入训练过程显著提高了医疗对话摘要系统的鲁棒性和准确性。这种方法解决了关键应用中ASR输出噪音的问题，提供了一个增强临床对话摘要可靠性的稳健解决方案。|
|**2024-08-26**|**Language-specific Calibration for Pruning Multilingual Language Models**|Simon Kurz et.al.|[2408.14398](http://arxiv.org/abs/2408.14398)|null|近期在大型语言模型（LLM）的剪枝领域取得的进展，在无需重新训练的情况下实现了卓越的压缩效果，并保持了高预测性能。然而，这类研究主要关注于使用英语文本进行剪枝校准，而忽略了现代LLM的多语言性质及其在非英语语言中的广泛应用。本文旨在探索用于剪枝多语言模型的有效策略。  我们进行了首个全面的实证研究，对比了不同校准语言在多语言任务、模型和最先进的剪枝技术下对剪枝的影响。我们的结果提供了实用的建议，例如，在目标语言上进行校准可以有效地降低困惑度，但不一定能促进下游任务的性能提升。进一步的分析实验揭示，目标语言上的校准主要贡献在于保留与流畅性和连贯性相关的语言特定特性，但可能无法捕捉到与理解能力和推理能力等语言通用特性的关联。  最后，我们为未来的实践者提供了实际的建议。|
|**2024-08-26**|**Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning**|Sakhinana Sagar Srinivas et.al.|[2408.14387](http://arxiv.org/abs/2408.14387)|null|空间时间预测在交通系统、物流和供应链管理等多个领域发挥着关键作用。然而，现有方法受限于处理大规模复杂数据的能力。为了克服这一限制，我们提出了一种结合开源大型和小型语言模型（LLMs 和 LMs）与传统预测方法的混合策略。通过引入动态提示和分组查询、多头注意力机制，该策略能够更有效地捕捉演变非线性时间序列数据中的内部系列和跨系列依赖关系。此外，我们利用低秩适配与激活记忆减少技术（LoRA-AMR），在消费级硬件上对开源小型 LM 进行定制化微调，以分析时间序列趋势，同时保留推理延迟并降低计算开销和激活存储内存需求。我们将语言模型处理与传统时间序列表示学习方法相结合，实现跨模态集成，从而获得稳健且准确的预测结果。通过在多个实际世界数据集上的广泛实验，该框架的效能得到了充分验证，其预测准确性显著优于现有方法。|
|**2024-08-26**|**Probing Causality Manipulation of Large Language Models**|Chenyang Zhang et.al.|[2408.14380](http://arxiv.org/abs/2408.14380)|**[link](https://github.com/tongjinlp/llm-causality-probing)**|**大型语言模型（LLM）在自然语言处理任务上展现了多种能力，包括因果关系问题。预训练的模型通常基于统计关联工作，而非专注于句子中的因果与影响。因此，探索LLM内部对因果性的操纵是必要的。本文提出了一种新颖的方法，通过提供不同的捷径并观察模型行为来探查因果性操纵的层级。我们利用检索增强生成（RAG）和上下文学习（ICL）技术，针对设计的因果分类任务，对主流LLM进行实验，包括GPT-4以及一些较小的和特定领域的模型。  我们的实验结果表明，LLM能够识别与因果性相关的实体，并认识到直接的因果关系。然而，LLM缺乏专门的因果认知能力，只是将因果性视为句子整体语义的一部分。**|
|**2024-08-26**|**SWE-bench-java: A GitHub Issue Resolving Benchmark for Java**|Daoguang Zan et.al.|[2408.14354](http://arxiv.org/abs/2408.14354)|**[link](https://github.com/multi-swe-bench/multi-swe-bench-env)**|**GitHub问题解决是软件工程中的关键任务，近期在行业和学术界都受到了广泛关注。在这个领域内，SWE-bench已经发布，旨在评估大型语言模型（LLMs）的问题解决能力，但目前仅关注Python版本。然而，支持更多编程语言同样至关重要，因为工业界对此有强烈需求。作为迈向多语言支持的第一步，我们开发了Java版的SWE-bench，称为SWE-bench-java。我们已公开发布了数据集，并提供了基于Docker的评估环境和排行榜，这些都将持续维护和更新。为了验证SWE-bench-java的可靠性，我们实现了经典方法SWE-agent，并在其中测试了几种强大的LLMs。众所周知，构建高质量的多语言基准既耗时又费力，因此我们欢迎通过拉取请求或合作来加速其迭代和改进，为完全自动化的编程铺平道路。**|
|**2024-08-23**|**MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?**|Yi-Fan Zhang et.al.|[2408.13257](http://arxiv.org/abs/2408.13257)|null|近期，全面评估多模态大型语言模型（MLLMs）在研究社区中引发了广泛关注。然而，我们注意到现有基准测试存在一些普遍的障碍，使得衡量模型面临的实际世界挑战变得困难，包括：1）数据规模较小导致性能波动大；2）依赖模型生成注释造成数据质量受限；3）任务难度不足，尤其是由于图像分辨率有限。为了克服这些问题，我们引入了MME-RealWorld。具体而言，我们从公共数据集和互联网收集了超过30万张图片，并筛选出13,366张高质量图片进行标注。这一过程中，我们动用了25名专业注释员和7名MLLM领域的专家，共贡献了29,429个问题-答案对，涵盖了5种真实世界场景下的43个子任务，这些任务甚至对人类来说也极具挑战性。据我们所知，MME-RealWorld是迄今为止最大的人工标注基准，其特征为最高分辨率以及专注于真实世界应用的目标导向。  我们进一步对28个领先的MLLM进行了详尽的评估，如GPT-4o、Gemini 1.5 Pro和Claude 3.5 Sonnet。我们的结果显示，即使是最先进的模型也无法应对我们的基准测试，其中没有一个模型达到60%的准确率。感知高分辨率图像和理解复杂的真实世界场景仍然是亟待解决的关键问题。相关的数据和评估代码已发布在https://mme-realworld.github.io/ 。|
|**2024-08-23**|**Domain-specific long text classification from sparse relevant information**|Célia D'Cruz et.al.|[2408.13253](http://arxiv.org/abs/2408.13253)|null|大型语言模型无疑在自然语言处理领域实现了重大革新，当前的趋势是推动单一模型解决所有任务（如情感分析、翻译等）。然而，在处理稀疏信息或弱信号时，这些模型的统计机制难以有效利用关键信息。例如，在长篇特定领域文档的分类中，相关性往往依赖于一个或几个关键术语。医疗领域中，确定某个报告是否包含了关于患者状况的关键信息至关重要。这些关键信息通常基于一两个特定的孤立术语。  本文提出了一种层次化模型，该模型利用一个潜在目标术语列表来检索候选句子，并将这些句子表示为包含它们的目标术语的上下文嵌入。对目标术语（或术语）的嵌入进行聚合导致文档表示被用于分类。我们分别在英语和法语的公开医疗文档基准数据集以及私有医疗数据集上评估了我们的模型。结果显示，我们的窄层级模型在特定领域背景下检索相关长文档方面优于大型语言模型。|
|**2024-08-23**|**Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time**|Yingyu Liang et.al.|[2408.13233](http://arxiv.org/abs/2408.13233)|null|本文提出了一种新型的快速计算方法，用于多层变换器模型中的梯度计算。该方法在几乎线性时间内 $n^{1+o(1)}$计算整个多层变换器模型的梯度，其中$n$ 是输入序列长度。这一突破极大地降低了传统二次时间复杂度相关的计算瓶颈。我们的理论适用于任何损失函数，并在全模型上保持可控制的近似误差。此外，我们的分析还考虑了多层变换器模型包含许多实用子模块的情况，如残差连接、因果掩码和多头注意力。通过提高大型语言模型中梯度计算的效率，我们期望通过基于我们的理论结果改进长上下文语言模型的训练和部署，使这些模型更加有效。|
|**2024-08-23**|**EUR-USD Exchange Rate Forecasting Based on Information Fusion with Large Language Models and Deep Learning Methods**|Hongcheng Ding et.al.|[2408.13214](http://arxiv.org/abs/2408.13214)|null|准确预测EUR/USD汇率对投资者、企业和政策制定者至关重要。本文提出了一种创新框架IUS，该框架结合了新闻和分析的非结构化文本数据与汇率和金融指标的结构化数据，以增强汇率预测能力。IUS框架利用大型语言模型进行文本情感极性评分和汇率变动分类。这些文本特征与定量特征相结合，并输入到因果驱动特征生成器中。然后使用Optuna优化的Bi-LSTM模型预测EUR/USD汇率。实验结果表明，所提出的模型在减少平均绝对误差（MAE）10.69%和根均方误差（RMSE）9.56%方面优于基准模型。结果显示，通过融合非结构化和结构化数据，准确性比仅使用结构化数据更高。此外，使用顶级12个重要定量特征和文本特征相结合进行特征选择证明是最有效的。提出的IUS框架和Optuna-Bi-LSTM模型提供了一种强大的新方法，用于多源数据集成的汇率预测。|
|**2024-08-23**|**DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation**|Qiming Zhu et.al.|[2408.13204](http://arxiv.org/abs/2408.13204)|null|代码基准，如HumanEval，广泛用于评估大型语言模型（LLMs）的能力，提供了它们优势与不足的洞察。然而，当前的基准主要集中在通用编码任务上（例如：冒泡排序、最大公约数），对领域特定编码任务（如计算、系统、加密）的探索则较少。为了填补这一空白，我们提出了一种多领域代码基准DOMAINEVAL，旨在全面评估LLMs的编码能力。我们的流程以全自动方式工作，允许从代码仓库中构建格式化的研究主题进行底部推动式构建。通过使用12个代表性LLM在DOMAINEVAL上的评估，我们观察到了一些有趣的结果。  我们注意到，LLMs在计算任务上表现良好，但在加密和系统编码任务上却有所欠缺。某些LLM在这些领域的性能差距可能高达68.94%（80.94%-12.0%）。我们也发现生成更多样本可以提高LLMs的整体性能，但领域偏见甚至可能增加。本研究的贡献包括一个代码生成基准数据集DOMAINEVAL，涵盖六个流行领域，以及一个完全自动化的管道用于构建代码基准，并基于在DOMAINEVAL上的性能识别了LLMs在代码生成任务上的局限性，提供了未来研究改进的方向。领导者板可在https://domaineval.github.io/查看。|
|**2024-08-23**|**Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning**|Hourui Deng et.al.|[2408.13184](http://arxiv.org/abs/2408.13184)|null|在大型语言模型（LLM）领域，空间推理是实现感知智能的基础。然而，在简单的迷宫环境中，LLM在长期路径规划方面仍面临挑战，主要受到其空间幻觉和长期推理导致的上下文不一致幻觉的影响。为了应对这一挑战，本研究提出了一种创新模型——空间到关系转换与递进Q学习（S2RCQL）。为解决LLM的空间幻觉问题，我们提出了“空间到关系”的方法，将空间提示转化为实体关系和表示实体关系链的路径，充分挖掘了LLM在序列思考方面的潜力。在此基础上，我们设计了一种基于Q学习的路径规划算法，以缓解上下文不一致幻觉，增强LLM的推理能力。通过将状态动作的Q值作为提示的辅助信息，我们纠正了LLM的幻觉，引导LLM学习最优路径。最后，我们提出了一种基于LLM的反向课程学习技术，进一步缓解了上下文不一致幻觉。该技术通过降低任务难度并利用成功经验，帮助LLM快速积累，并以此来应对更复杂任务。我们在百度自主研发的LLM：ERNIE-Bot 4.0上进行了全面实验。结果显示，我们的S2RCQL在成功率和最优性方面分别提高了23%至40%，相较于先进的提示工程方法取得了显著进步。|
|**2024-08-23**|**IntelliCare: Improving Healthcare Analysis with Variance-Controlled Patient-Level Knowledge from Large Language Models**|Zhihao Yu et.al.|[2408.13073](http://arxiv.org/abs/2408.13073)|**[link](https://github.com/yzhHoward/IntelliCare)**|在电子健康记录（EHR）数据的深度学习方法取得巨大进步的同时，它们在处理有限数据中的多样化的医学代码时往往难以全面捕捉其语义。引入大型语言模型（LLM）的知识整合为提升医疗保健预测提供了有前景的途径。然而，LLM分析可能会因歧义问题和不一致性导致显著的波动，这阻碍了其有效利用。为解决这些挑战，我们提出了一种名为IntelliCare的新型框架，旨在通过利用LLM提供高质量的患者级外部知识并增强现有的EHR模型来改善医疗保健预测。具体来说，IntelliCare通过识别患者群体，并利用与任务相关的统计信息来增强LLM的理解和生成能力，有效地解决了歧义问题。此外，它通过结合EHR模型和困惑度量来细化从LLM获取的知识，采用混合方法生成多个分析结果并进行校准。在三个临床预测任务上对两个大规模EHR数据集的实验评估表明，IntelliCare能够显著提高现有方法的表现，凸显了其在推进个性化医疗保健预测和决策支持系统方面的潜力。|
|**2024-08-23**|**Guiding IoT-Based Healthcare Alert Systems with Large Language Models**|Yulan Gao et.al.|[2408.13071](http://arxiv.org/abs/2408.13071)|null|在医疗健康警报系统（HAS）领域，随着人工智能（AI）、物联网（IoT）技术的快速发展以及公众健康意识的提高，HAS正经历着快速的变革。尽管取得了显著的进步，但存在一个核心挑战：如何在资源有限的环境中，在个性化健康警报的准确性与严格隐私保护之间找到平衡点。  为了解决这一问题，我们提出了一种统一框架——LLM-HAS（大型语言模型医疗健康警报系统）。该框架将大型语言模型（LLM）融入到HAS中，以显著提升警报的准确性、确保用户隐私，并增强个性化医疗服务，同时改善用户体验的质量（QoE）。我们的创新框架采用混合专家（MoE）方法，结合LLM，通过分析用户的个性化偏好和潜在健康风险来处理额外的文本工作描述。这种分析指导了专门的深度强化学习（DDPG）专家的选择，他们负责提供精确的健康警报。此外，LLM-HAS能够处理对话式用户反馈，不仅允许对DDPG进行微调，还能加深用户参与度，从而提高健康管理策略的准确性和个性化程度。  模拟结果验证了LLM-HAS框架的有效性，表明其作为利用生成型人工智能（GAI）提供高度准确可靠警报的突破性方法的潜力。|
|**2024-08-23**|**VFM-Det: Towards High-Performance Vehicle Detection via Large Foundation Models**|Wentao Wu et.al.|[2408.13031](http://arxiv.org/abs/2408.13031)|**[link](https://github.com/event-ahu/vfm-det)**|**现有车辆检测器通常通过在基于预训练主干（如ResNet、ViT）的预训练典型检测器（例如YOLO、RCNN、DETR系列）上进行车辆图像训练获得。一些研究者还利用并增强大型基础模型来提升检测性能。然而，我们认为这些检测器可能仅获得次优结果，因为它们使用的大型模型并非专门为车辆设计。此外，他们的结果高度依赖于视觉特征，并且很少考虑车辆语义信息与视觉表示之间的对齐。  在此工作中，我们提出了一种基于预训练的车辆模型（VehicleMAE）和大型语言模型（T5）的新车辆检测范式，称为VFM-Det。它遵循区域建议框检测框架，每个提议的特征可以通过VehicleMAE增强。更重要的是，我们提出了一种新的VAtt2Vec模块，用于预测这些提议的车辆语义属性并将它们转换为特征向量，通过对比学习增强视觉特征。对三个车辆检测基准数据集的广泛实验充分证明了我们的车辆检测器的有效性。具体而言，我们的模型分别在Cityscapes数据集上的 $AP_{0.5}$、$AP_{0.75}$指标上，相较于基线方法提高了$+5.1\%$、$+6.2\%$ 。此工作的源代码将在https://github.com/Event-AHU/VFM-Det发布。**|
|**2024-08-23**|**In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting**|Haowei Du et.al.|[2408.13028](http://arxiv.org/abs/2408.13028)|null|在当前的学术界，对基于指令增强的少量实例的大规模语言模型（Large Language Models, LLM）进行上下文学习（In-context Learning, ICL）引起了越来越多的关注。现有的用于ICL的示例选择方法利用稀疏或密集检索器，并且能够产生有效性能。然而，这些方法并未充分利用LLM对反馈信息的利用来训练检索器，所选的示例可能无法显著提升LLM的类比能力。  为了克服这一问题，我们提出了基于强化学习的策略框架（Policy-based Reinforcement Learning Framework, RLS）用于示例选择。该框架由语言模型（Language Model, LM）选择器和LLM生成器组成。语言模型选择器将候选示例编码为密集表示，并从中选择top-k个示例作为LLM的示范。通过采用LLM的输出来计算奖励和策略梯度，优化语言模型选择器。  我们在不同数据集上进行了实验，显著优于现有的示例选择方法。此外，我们的方法在少量样本设置下相较于监督微调（Supervised Fine-tuning, SFT）模型显示出优势。进一步的实验结果表明，示例的数量丰富性和与测试案例的相似性对于ICL中的LLM性能至关重要。|
|**2024-08-22**|**Controllable Text Generation for Large Language Models: A Survey**|Xun Liang et.al.|[2408.12599](http://arxiv.org/abs/2408.12599)|**[link](https://github.com/iaar-shanghai/ctgsurvey)**|**在自然语言处理（NLP）领域，大型语言模型（LLMs）展现了卓越的文本生成质量。然而，在实际应用中，LLMs需要满足日益复杂的需求。除了避免误导性或不适当的内容，LLMs还被期望根据特定用户需求进行调整，如模仿特定的写作风格或生成富有诗意的文本。这些多样的需求推动了可控文本生成（CTG）技术的发展，旨在确保输出内容符合预设的控制条件，如安全性、情感倾向、主题一致性以及语言风格，同时保持高质量的有用性、流畅性和多样性。  本文系统地回顾了CTG在LLMs领域的最新进展，详细定义了其核心概念，并明确了控制条件和文本质量的要求。我们将CTG任务分为两大类：内容控制和属性控制，并对每种类型的方法进行了讨论，包括模型重训练、微调、强化学习、提示工程、潜在空间操纵和解码时干预。我们分析了每种方法的特点、优势和局限性，提供了实现生成控制的深入见解。此外，我们回顾了CTG评估方法、总结了其跨领域的应用，并指出了当前研究的关键挑战，如流畅度和实用性的降低。我们还提出了若干呼吁，强调未来研究应更注重实际应用。本文旨在为该领域的研究人员和开发者提供有价值的指导。我们的参考文献列表和中文版本已开源在https://github.com/IAAR-Shanghai/CTGSurvey。**|
|**2024-08-22**|**RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment**|Xiaohan Wang et.al.|[2408.12579](http://arxiv.org/abs/2408.12579)|null|大型语言模型（LLM）如GPT-4、MedPaLM-2和Med-Gemini在各类医疗评估指标上表现出与医学专家竞争的性能。然而，它们在与医生相媲美的专业诊断方面仍面临挑战，特别是在高效收集患者信息以及推理最终诊断的过程中。为此，我们提出了一种名为RuleAlign的框架，旨在使LLM与特定诊断规则保持一致。我们构建了一个包含基于规则的医患对话数据集，并设计了一种通过偏好学习进行对齐的学习方法。实验结果证明了所提出方法的有效性。我们期望我们的工作能够启发探索LLM作为AI医师的潜力。|
|**2024-08-22**|**Jamba-1.5: Hybrid Transformer-Mamba Models at Scale**|Jamba Team et.al.|[2408.12570](http://arxiv.org/abs/2408.12570)|null|我们推出了Jamba-1.5，基于我们Jamba架构的新型指令优化大型语言模型。Jamba是一种混合Transformer-Mamba专家混合架构，它在上下文长度范围内提供了高吞吐量和低内存使用，同时保持与Transformer模型相同或更好的质量。我们发布了两种模型大小：Jamba-1.5-Large，具有94B个活跃参数；以及Jamba-1.5-Mini，具有12B个活跃参数。这两种模型均针对多种对话和指令遵循能力进行了微调，并且具有256K令牌的最大有效上下文长度，在开放权重模型中最大。为了支持成本效益的推理，我们引入了ExpertsInt8，这是一种新颖的量化技术，允许在处理256K令牌上下文时将Jamba-1.5-Large模型放入具有8个80GB GPU的机器上而不会损失质量。当在一系列学术和聊天机器人基准上进行评估时，Jamba-1.5模型取得了出色的结果，同时提供了高吞吐量并优于其他开放权重模型在长上下文基准上的性能。两种大小的模型的权重都根据Jamba开放模型许可公开提供，并且我们发布了ExpertsInt8作为开源软件。|
|**2024-08-22**|**ssProp: Energy-Efficient Training for Convolutional Neural Networks with Scheduled Sparse Back Propagation**|Lujia Zhong et.al.|[2408.12561](http://arxiv.org/abs/2408.12561)|**[link](https://github.com/lujiazho/ssprop)**|**近期，深度学习取得了显著进展，尤其是在生成模型领域，如大型语言模型和概率性扩散模型。然而，训练这些模型往往需要大量的计算资源，消耗数十亿的浮点运算（petaFLOPs），导致巨大的能源消耗和碳足迹，引发了对环境的重大担忧。在训练深度学习模型的过程中，反向传播（Back-propagation, BP）是主要的计算负担来源。  为了推动能源效率的提高，并允许在任何机器和设备上实现稀疏学习，我们提出了一种通用、能源高效的卷积模块，它能够无缝集成到任何深度学习架构中。具体来说，我们引入了通道级稀疏性，并基于假设BP通常密集且低效，这可能导致过拟合和高计算消耗，提出了额外的梯度选择调度器，在反向传播阶段进行选择。实验结果表明，我们的方法可以减少40%的计算量，同时有可能提升模型性能，在图像分类和生成任务上得到验证。这种减少可以带来显著的能源节省和较低的碳足迹，尤其是在大型AI系统的研究与开发阶段。此外，我们的方法以不同于Dropout的方式缓解了过拟合问题，允许它与Dropout结合使用，进一步提高模型性能并降低计算资源消耗。广泛实验表明，我们的方法适用于各种数据集和任务，并与多种深度学习架构和模块兼容。相关代码已公开发布在https://github.com/lujiazho/ssProp。**|
|**2024-08-22**|**Towards Evaluating and Building Versatile Large Language Models for Medicine**|Chaoyi Wu et.al.|[2408.12547](http://arxiv.org/abs/2408.12547)|**[link](https://github.com/magic-ai4med/meds-ins)**|**在这项研究中，我们提出了一种全面的基准测试——MedS-Bench，旨在评估大型语言模型（LLMs）在临床场景中的性能。与现有侧重于多项选择问题回答的基准不同，MedS-Bench覆盖了11个高级别临床任务，包括临床报告摘要、治疗建议、诊断、实体识别和医学概念解释等。我们使用少量提示对六款领先的LLM进行了评估，如MEDITRON、Mistral、InternLM 2、Llama 3、GPT-4和Claude-3.5，发现即使是最高级的模型在这些复杂任务上也存在挑战。为了应对这些局限性，我们开发了MedS-Ins，一个面向医学领域的大型指令调优数据集。MedS-Ins包含了58个医学相关的语言语料库，总计1350万样本，涵盖了122个任务。通过展示该数据集的用途，我们在一个轻量级、开源的医疗语言模型上进行了指令调优实验，结果得到了名为MMedIns-Llama 3的新模型，它在几乎所有临床任务上的表现都超过了现有模型。为了促进对LLMs应用于临床挑战的进一步发展，我们已将MedS-Ins数据集完全公开，并邀请研究社区参与其扩展。此外，我们启动了一个动态排行榜，计划定期更新测试集，以跟踪进展并增强通用LLM在医学领域中的适应能力。排行榜：https://henrychur.github.io/MedS-Bench/。Github：https://github.com/MAGIC-AI4Med/MedS-Ins。**|
|**2024-08-22**|**MEDCO: Medical Education Copilots Based on A Multi-Agent Framework**|Hao Wei et.al.|[2408.12496](http://arxiv.org/abs/2408.12496)|null|大型语言模型（LLMs）在医学和健康领域等多个研究领域产生了重大影响，然而LLMs作为医疗教育中的助手潜力尚未得到充分探索。当前的AI辅助教育工具受限于单一学习方法以及无法模拟实际医疗培训的多学科性和互动性。为了克服这些局限性，我们提出了一种名为MEDCO（Medical EDucation COpilots）的新型多代理助手系统，专门用于模拟真实世界医疗培训环境。MEDCO整合了三个核心代理：一个自主患者、一位专家医生和一位放射科医师，从而构建了一个多模态和互动的学习环境。我们的框架着重于教授高效提问技巧、跨学科协作以及学生之间的同伴讨论。  实验结果显示，经过MEDCO训练的虚拟学生不仅实现了与高级模型相媲美的显著性能提升，还展现出类似人类的学习行为和进步，并且学习样本数量增加。这项工作对医疗教育领域做出了贡献，通过引入一种互动和协作的学习方法。此外，它还提供了关于集成AI的训练模式有效性的宝贵见解。|
|**2024-08-22**|**GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models**|Kunsheng Tang et.al.|[2408.12494](http://arxiv.org/abs/2408.12494)|**[link](https://github.com/kstanghere/gendercare-ccs24)**|**大型语言模型（LLM）在自然语言生成方面展现了惊人的能力，但也被观察到放大了社会偏见，尤其是与性别相关的偏见。针对这一问题，已经提出了若干基准测试来评估LLM中的性别偏见。然而，这些基准测试往往缺乏实际的灵活性或无意中引入了偏见。为了应对这些问题，我们引入了GenderCARE框架，这是一个全面的框架，包括创新的准则、评估、减少技术以及评价指标，旨在量化和减轻LLM中的性别偏见。  首先，我们确立了开创性的性别平等基准准则，覆盖了包容性、多样性、可解释性、客观性、稳健性和现实性等多个维度。根据这些准则，我们构建了GenderPair，一个新颖的配对基准，旨在全面评估LLM中的性别偏见。我们的基准提供了标准化且现实的评估，包括以前被忽视的性别群体，如跨性别者和非二元个体。此外，我们开发了有效的去偏技术，包括反事实数据增强和专门的微调策略，以在不损害LLM整体性能的前提下减少性别偏见。  广泛的实验表明，在17个不同的LLM上，各种性别偏见基准的显著减少，最高可达超过90%，平均值超过35%。重要的是，这些减少带来的主流语言任务方面的变异性保持在2%以下。通过提供真实性的评估和针对性别偏见的定制减少，我们希望GenderCARE能够代表在LLM中实现公平和公正的一个重要步骤。更多细节请参阅https://github.com/kstanghere/GenderCARE-ccs24。**|
|**2024-08-23**|**Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese**|Khang T. Doan et.al.|[2408.12480](http://arxiv.org/abs/2408.12480)|null|在这份报告中，我们引入了Vintern-1B，这是一个针对越南语任务的可靠的一百亿参数多模态大型语言模型（MLLM）。通过整合Qwen2-0.5B-Instruct语言模型与InternViT-300M-448px视觉模型，Vintern-1B优化了在光学字符识别（OCR）、文档提取和越南语上下文中的通用问题回答等应用。该模型在超过三百万张图像-问题-答案对的数据集上进行了微调，实现了在多个越南语基准测试如OpenViVQA和ViTextVQA上的稳健性能和可靠结果。Vintern-1B足够小，可以轻松地集成到各种离线应用中。此外，我们还开源了几组用于文本和图表的越南语视觉问答（VQA）数据集，使用的是Gemini 1.5 Flash创建的。我们的模型可以在以下链接获取：https://huggingface.co/5CD-AI/Vintern-1B-v2。|
|**2024-08-22**|**Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action Recognition**|Bozheng Li et.al.|[2408.12475](http://arxiv.org/abs/2408.12475)|null|本文提出了一种新颖的时序序列感知模型（TSAM）以进行少量样本动作识别（FSAR），该模型在预训练框架中引入了序列感知器适配器，旨在整合空间信息和序列时间动态到特征嵌入中。与现有通过探索所有帧之间关系来捕获时间信息的细调方法不同，我们的基于感知器的适配器能够沿时间线递归地捕捉序列动态，并感知顺序变化。为了获取每个类别的判别性表示，我们扩展了从大型语言模型（LLMs）导出的文本库，对视觉原型进行了丰富，通过整合上下文语义信息。此外，我们引入了一种不平衡最优传输策略来进行特征匹配，以减轻与类别无关特征的影响，从而促进更有效的决策。在五个FSAR数据集上的实验结果表明，我们的方法创下了新的基准，与第二好的竞争对手相比取得了显著的优势。|
|**2024-08-22**|**DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender Systems**|Jiaju Chen et.al.|[2408.12470](http://arxiv.org/abs/2408.12470)|**[link](https://github.com/jiaju-chen/dlcrec)**|大型语言模型（LLM）在推荐系统中的集成显著提升了性能，但往往伴随着推荐多样性下降的问题，这可能损害用户体验。为了克服这一挑战，可控推荐系统应运而生，它允许用户指定偏好并获得满足其多样化需求的推荐。尽管具有潜力，现有的可控推荐系统通常依赖于简单机制，如单一提示，来调节多样性，这种做法未能充分捕捉用户偏好的复杂性。针对这些局限性，我们提出了一种名为DLCRec的新框架，旨在实现基于LLM的推荐系统的精细粒度多样性控制。与传统方法不同，DLCRec采用精细任务分解策略，将推荐过程拆分为三个依次进行的子任务：体裁预测、体裁填充和项目预测。这些子任务独立训练并在用户定义的控制数指导下依次推理，确保了对多样性的更精确控制。此外，稀缺且分布不均的多样性相关用户行为数据的缺乏构成了对微调的严峻挑战。为解决这些问题，我们引入了两种数据增强技术，以增强模型对噪声和离群数据的鲁棒性。这些技术使模型接触到更广泛的模式，从而提高其生成不同多样性的推荐的适应性。我们的全面实验结果表明，DLCRec不仅提供了对多样性的精确控制，而且在多个推荐场景中都优于最先进的基线方法。|
|**2024-08-21**|**SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs**|Yuanyang Yin et.al.|[2408.11813](http://arxiv.org/abs/2408.11813)|null|近期，多模态大型语言模型（MLLMs）在感知和推理能力方面展现出了惊人的表现，它们通常由视觉编码器、适配器和大型语言模型（LLM）组成。适配器作为视觉与语言组件之间的关键桥梁。然而，通过图像级监督训练适配器往往会导致显著的对齐偏差，这会削弱LLM的能力并限制多模态LLM的潜力。为了解决这一问题，我们引入了监督嵌入对齐（SEA），这是一种基于视觉语言预训练模型（如CLIP）的分词级对齐方法，通过对比学习来调整视觉分词与LLM嵌入空间的一致性。这种方法确保了视觉和语言表示之间更协调的整合，从而增强多模态LLM的性能和可解释性，同时保留其固有特性。广泛实验表明，SEA有效地提高了MLLMs，特别是对于较小的模型，无需额外的数据或推理计算。此外，SEA也为开发更通用和适应性强的解决方案以增强多模态系统奠定了基础。|
|**2024-08-21**|**Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models**|Yuzhou Huang et.al.|[2408.11801](http://arxiv.org/abs/2408.11801)|null|传统视觉叙事复杂，需要专业知识和大量资源，但往往受限于人类的创造力与创作精度。尽管大型语言模型（LLMs）增强了视觉叙事能力，现有方法往往局限于二维视觉效果或通过动作合成和行为模拟简化故事，未能生成全面、多维的叙事。为此，我们提出Story3D-Agent，一种创新的方法，利用LLM的能力将提供的叙事转化为三维渲染可视化。通过集成程序建模，我们的方法能够精确控制多角色的动作和动态，以及各种装饰元素，确保长期和动态的三维表现。此外，我们的方法支持通过逻辑推理进行叙事扩展，确保生成的内容与现有条件保持一致。我们对Story3D-Agent进行了详尽的评估，以验证其有效性，并提供了基本框架来推动三维故事表示的发展。|
|**2024-08-21**|**PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting and Permitting domain**|Rounak Meyur et.al.|[2408.11800](http://arxiv.org/abs/2408.11800)|null|在自然语言处理（NLP）和文本生成领域快速发展的背景下，检索增强生成（RAG）的兴起为通过利用用户指定数据库中的信息来提高生成文本的质量和可靠性提供了有前景的途径。基准测试对于评估和比较不同RAG配置在检索器和生成器方面的性能至关重要，提供了这些配置的有效性、可扩展性和特定领域和应用的适用性的洞察。本文提出了一种全面框架，用于生成与特定领域相关的RAG基准。该框架基于自动问题答案生成与人类（领域专家）-人工智能大型语言模型（LLM）协作的自动化过程。以案例研究的形式，我们通过引入PermitQA作为风场选址和许可领域的首个基准进行了框架展示，该基准包含了与风能项目环境影响相关的多篇科学文档/报告。  我们的框架系统地使用多种指标和不同复杂度级别的问题类型来评估RAG性能。我们还展示了不同模型在我们的基准上的表现。|
|**2024-08-21**|**EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model**|Feipeng Ma et.al.|[2408.11795](http://arxiv.org/abs/2408.11795)|null|在多模态研究领域，众多研究利用大量的图像-文本对进行模态对齐学习，将大型语言模型（Large Language Models, LLMs）转化为多模态LLMs，并在各种视觉语言任务上表现出色。目前主要的实现方法分为两类：自注意力基和交叉注意力基方法。自注意力基方法因其简单的多层感知机（MLP）架构而具有较高的数据效率，但在计算效率方面却相对较低，原因在于其需要将视觉和文本令牌作为输入进行连接。而交叉注意力基方法虽然在额外的学习参数方面不如自注意力基方法高效，但由于避免了为LLM提供过长序列输入，因此在计算效率方面表现更高。为了平衡这些权衡，我们提出了数据高效且计算高效的多模态大型语言模型（EE-MLLM）。EE-MLLM在不引入额外模块或可学习参数的情况下，实现了数据和计算效率的提升。具体来说，我们对多模态LLM中的原始自注意力机制进行了改进，引入了一种复合注意力机制。该机制有两个关键特性：1）消除视觉令牌内部的自注意力计算，以实现计算效率；2）重用LLM每一层的权重，以促进视觉与语言之间的有效模态对齐，从而实现数据效率。实验结果表明，EE-MLLM在包括MMBench、SeedBench等通用性数据集以及TextVQA、DocVQA等精细粒度任务在内的多种基准测试中都展现出显著的有效性。|
|**2024-08-21**|**Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design**|Nathaniel H. Park et.al.|[2408.11793](http://arxiv.org/abs/2408.11793)|null|分子属性预测和通过深度学习模型进行生成设计是研究的热点领域，这主要归因于它在加速新材料开发方面的潜力。随着大型语言模型（LLMs）和由LLM驱动的代理系统的出现，这些工作流程得到了显著增强，这些系统利用预训练模型在更复杂的研究任务背景下进行预测。尽管有效，但在材料设计任务中的信息检索方面，代理系统仍有改进空间。此外，对预测深度学习模型的替代应用，如利用它们的潜在表示来促进跨模态检索增强生成，在由LLM驱动的代理系统中实现任务特定的材料设计，这一领域尚未得到探索。  在此，我们证明了大规模、预训练的化学基础模型可以作为使化学信息检索语义化的基础，适用于小分子、复杂聚合物材料和反应。此外，我们展示了化学基础模型与图像模型（如OpenCLIP）相结合，能够实现跨多个表征数据域的前所未有的查询和信息检索。最后，我们展示了这些系统在多代理系统中的集成，以支持结构和拓扑为基础的自然语言查询和信息检索，从而促进复杂研究任务的执行。|
|**2024-08-21**|**Critique-out-Loud Reward Models**|Zachary Ankner et.al.|[2408.11791](http://arxiv.org/abs/2408.11791)|**[link](https://github.com/zankner/cloud)**|**传统的奖励模型在从人类反馈进行强化学习（RLHF）时，仅用于直接预测偏好分数，而不利用底层大型语言模型（LLM）的生成能力。这限制了奖励模型的能力，因为它们必须通过单一前向传递来隐式地推理响应的质量，即，必须在偏好建模过程中完成推理。为了使奖励模型能够显式地推理响应的质量，我们引入了“口头批评”（CLoud）奖励模型。CLoud奖励模型首先生成对助手响应的自然语言批评，然后使用这些批评来预测响应质量的标量奖励。  我们证明了对于Llama-3-8B和70B基础模型，CLoud奖励模型的成功：与经典奖励模型相比，CLoud奖励模型分别在RewardBench上提高了8B和70B基础模型的二元偏好分类准确率4.65和5.84个百分点。此外，当作为Best-of-N评分模型使用时，CLoud奖励模型在ArenaHard上的胜率也实现了帕累托改进。最后，我们探索了如何利用CLoud奖励模型的动态推理计算能力，通过自我一致性解码来进行奖励预测。  以上是关于“口头批评”（CLoud）奖励模型的摘要翻译，它展示了这种新型奖励模型在提升强化学习系统性能方面的潜力。**|
|**2024-08-21**|**DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework**|Zhifei Xie et.al.|[2408.11788](http://arxiv.org/abs/2408.11788)|null|我们提出了一种名为“DreamFactory”的LLM基框架，它能解决当前视频生成模型在创建长视频时遇到的挑战。DreamFactory通过多智能体协作原则和关键帧迭代设计方法，确保了长视频的一致性和风格统一。它利用链式思维（Chain of Thought，COT）来处理大型语言模型固有的不确定性。DreamFactory能够生成长、风格一致且复杂的视频。  对于这些长形式视频的评估提出了挑战。为此，我们提出了新的评估指标，如跨场景面部距离分数和跨场景风格一致性分数。为了促进这一领域的进一步研究，我们贡献了一个包含超过150个由人类评分的多场景视频的多场景视频数据集。|
|**2024-08-21**|**Personality Alignment of Large Language Models**|Minjun Zhu et.al.|[2408.11779](http://arxiv.org/abs/2408.11779)|**[link](https://github.com/zhu-minjun/palign)**|**为了弥补现有大语言模型（LLM）对齐方法在反映人类普遍价值观和行为时的不足，忽视了个体用户独特特征和偏好的问题，我们提出了个性对齐的概念。该方法旨在根据个体用户或紧密关联群体的具体偏好调整LLM的响应与决策。受心理测量学的启发，我们构建了Personality Alignment with Personality Inventories (PAPI) 数据集，包含了30万真实主体的数据，每个主体基于五大人格因素提供行为偏好信息。这一数据集使我们能够定量评估LLM在多大程度上能够与每个主体的行为模式相匹配。鉴于个性对齐面临的挑战：如个人数据有限、偏好多样以及可扩展性需求，我们开发了一种激活干预优化方法。这种方法利用最少的数据和计算资源提高了LLM高效对齐个体行为偏好的能力。我们的方法PAS不仅在性能上超越了DPO，而且优化时间仅为后者的五分之一，具有实际价值，推动了个性化的AI系统决策与推理的发展，增强了与每位用户的交互相关性和意义，促进了以人为本的人工智能的进步。相关代码已发布在<https://github.com/zhu-minjun/PAlign>。**|
|**2024-08-21**|**Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards**|Omar Erak et.al.|[2408.11775](http://arxiv.org/abs/2408.11775)|**[link](https://github.com/Nouf-Alabbasi/oKUmura_AI_Telecom_challenge)**|**近期的研究揭示了大型语言模型（LLMs）在电信标准方面的技术规范挑战。本文提出了一种基于Phi-2小型语言模型（SLM）的微调检索增强生成（RAG）系统，旨在作为通信网络的权威答案来源。我们开发的系统利用前瞻性的语义分块来动态确定解析断点，依据嵌入相似度进行调整，从而有效处理多种文档格式。针对技术标准中可能出现的多个相似上下文问题，我们采用了重新排名算法以优先考虑最相关的提取片段。考虑到Phi-2的小语境窗口限制，我们引入了一种名为SelfExtend的最新技术，在推理过程中扩展语境窗口，不仅提升了性能，还能适应客户到专业技术人员的各种查询和设计需求。为了微调，我们使用了低秩适配（LoRA）技术，在训练时提高计算效率，并在小数据集上实现有效的微调。我们的全面实验表明，在电信领域对现有问答方法的显著改进，性能超过GPT-4（大约是其规模的880倍）。这项工作展示了利用SLM在通信网络中的新方法，提供了高效性和性能之间的平衡，可作为构建智能语言模型的基础。**|
|**2024-08-21**|**Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks**|Yiyi Chen et.al.|[2408.11749](http://arxiv.org/abs/2408.11749)|**[link](https://github.com/siebeniris/vec2text_exp)**|大型语言模型（LLM）面临着来自网络攻击者的恶意影响，如对抗性、后门和嵌入反转攻击。对此，新兴的LLM安全领域致力于研究并防御此类威胁。迄今为止，该领域的大多数工作都集中在英语单一语言模型上，然而，最新研究表明，多语言LLM可能比其单一语言同僚更易受到各种攻击。尽管先前的研究已经探讨了在部分欧洲语言上的嵌入反转，但要将这些发现推及到不同语系和不同书写系统的语言，却极具挑战性。因此，本研究旨在探索多语言LLM在嵌入反转攻击下的安全性，并在20种语言中进行跨语言和跨书写的反转测试，覆盖8个语系和12种书写系统。我们的研究结果表明，阿拉伯字母和西里尔字母书写的语言以及印度-雅利安语系的语言特别容易受到嵌入反转的影响。我们进一步观察到反转模型倾向于出现语言混淆，有时大幅度降低了攻击的有效性。因此，我们系统地探索了这一瓶颈，揭示了一些可预测模式，这可能被攻击者利用。最终，本研究旨在深化对多语言LLM面临的主要安全漏洞的理解，并提高对最易受这些攻击影响的语言的意识。|
|**2024-08-20**|**Revisiting VerilogEval: Newer LLMs, In-Context Learning, and Specification-to-RTL Tasks**|Nathaniel Pinckney et.al.|[2408.11053](http://arxiv.org/abs/2408.11053)|**[link](https://github.com/nvlabs/verilog-eval)**|大型语言模型（LLM）在数字硬件代码生成领域的应用是一个新兴领域。大多数LLM主要是在自然语言和软件代码上进行训练的。硬件代码，如Verilog，只占训练数据的一小部分，而且很少有硬件基准存在。为了填补这一缺口，2023年发布了一个名为VerilogEval的开源基准，它提供了一个一致的评估框架，用于LLM在代码完成任务上的性能。该基准在当时的领先模型，包括GPT-4，进行了测试。然而，VerilogEval和其他Verilog生成基准缺乏失败分析，当前形式下也不利于探索提示技术。此外，在VerilogEval发布后，商业和开源模型都经历了持续的发展。  在这个工作中，我们评估了新发布的商业和开源模型的不同规模，针对改进后的VerilogEval基准套件。我们增强了VerilogEval的基础架构和数据集，通过自动分类失败，引入了支持上下文学习（ICL）示例的新提示，并扩展了支持的任务到规格到RTL转换。我们发现商业领域的最新模型有了可测量的改进，其中GPT-4 Turbo在规格到RTL任务上达到了59%的成功率。我们也研究了新出现的开源和领域特定模型的性能，并展示了模型从上下文学习中获得显著益处的可能性。我们发现最近发布的Llama 3.1 405B模型在性能上与GPT-4 Turbo相当，实现了58%的成功率，而较小的领域特定的RTL-Coder 6.7B模型则取得了令人印象深刻的37%的成功率。然而，提示工程对于实现良好的成功率至关重要，并且随着模型和任务的变化而变化。一个允许进行提示工程和失败分析的基准基础设施对于持续的模型开发和部署至关重要。|
|**2024-08-20**|**FLAME: Learning to Navigate with Multimodal LLM in Urban Environments**|Yunzhe Xu et.al.|[2408.11051](http://arxiv.org/abs/2408.11051)|**[link](https://github.com/xyz9911/FLAME)**|**大型语言模型（LLM）在视觉与语言导航（VLN）任务中展现出了潜在能力，但当前的应用仍面临挑战。虽然LLM在通用对话场景中表现出色，但在专门的导航任务上却表现不佳，相较于专为VLN设计的模型，其性能往往较低下。我们引入了FLAME（FLAMingo架构化实体代理），这是一种基于多模态LLM的新型代理和架构，旨在解决城市VLN任务，并能高效处理多个观察结果。  我们的方法采用了三阶段调优技术以实现对导航任务的有效适应：单感知调整用于街道视图描述、多感知调整用于轨迹总结以及端到端训练在VLN数据集上的综合能力。生成的数据集通过自动化过程合成而成。实验结果表明，FLAME在Touchdown数据集上的任务完成率较现有方法提高了7.3%，超越了最先进的方法。这项工作展示了多模态LLM在复杂导航任务中的潜力，代表了向实际应用多模态LLM于实体人工智能领域迈出的重要一步。项目页面：https://flame-sjtu.github.io**|
|**2024-08-20**|**MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding**|Jian Chen et.al.|[2408.11049](http://arxiv.org/abs/2408.11049)|**[link](https://github.com/infini-ai-lab/magicdec)**|大型语言模型（LLM）在诸如交互式聊天机器人、文档分析和代理工作流程等长期上下文应用中变得越来越普遍，但提供长上下文请求时，要实现低延迟和高吞吐量是一个挑战。推测性解码（SD）是一种广泛使用的降低延迟的技术，传统观点认为其效能仅限于较小的批次大小。然而，在MagicDec中，我们揭示了令人惊讶的事实：即使在高吞吐量推理环境中，对于中等到较长序列，SD仍能实现加速。更有趣的是，基于我们的严谨分析，一种智能起草策略可以在批次大小增加时获得更好的加速效果。  MagicDec首先识别出随着批次大小和序列长度增加的瓶颈转移，并利用这些洞察来更有效地部署推测性解码以支持高吞吐量推理。然后，它通过利用稀疏KV缓存的草案模型来解决随着序列长度和批次大小增加而扩展的KV瓶颈问题。|
|**2024-08-20**|**Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research**|Sreyoshi Bhaduri et.al.|[2408.11043](http://arxiv.org/abs/2408.11043)|null|本文提出了一种新颖的方法，利用基于检索增强生成（RAG）的大型语言模型（LLM）来分析访谈记录，以解决手动分析定性数据需要大量时间和努力的问题。研究旨在将研究问题设定为由LLM作为初级研究助手进行辅助的模式。本研究探讨了将LLM视为人才管理领域研究人员的初级质性研究助手的思维模型。通过扩展基于RAG的LLM方法，本文展示了这些模型在对半结构化访谈数据进行主题建模方面的灵活性，超越了它们在信息检索和搜索中的传统应用。  研究结果表明，基于LLM的RAG方法能够成功提取感兴趣的议题，与从同一数据集手动生成的主题相比，覆盖范围显著更高。这证明了使用LLM作为初级质性研究助手的可行性。此外，研究建议，使用此类模型的研究者应严格遵循传统质性研究中使用的质量标准，以确保其方法的严谨性和可靠性。  最后，论文提出了针对希望将LLM与现有质性研究范式相融合的行业实践者的关键建议，提供了一条有效整合这些强大但初级的人工智能工具在定性数据分析中的路径，特别是在人才领域。|
|**2024-08-20**|**Scaling Law with Learning Rate Annealing**|Howe Tissue et.al.|[2408.11029](http://arxiv.org/abs/2408.11029)|null|我们发现神经语言模型在训练过程中，交叉熵损失曲线遵循了一个与学习率（LR）衰减相关的缩放定律： $L(s) = L_0 + A\cdot S_1^{-\alpha} - C\cdot S_2$。其中，$S_1$代表前向区域，$S_2$ 代表学习率衰减区域。这一公式考虑了两个因素：（1）传统的缩放律定义的前向缩放；以及（2）学习率衰减带来的额外损失下降。因此，该公式能够描述每个步骤的完整损失曲线，而非仅限于训练结束时的单一损失点。通过应用包含学习率衰减的缩放律，并仅通过一到两次训练曲线拟合，我们能够准确预测语言模型训练在任何给定步骤和任何学习率调度（LRS）下的损失。  此外，这一方程准确地描述了训练过程中的动态，并为先前研究中关注的学习率调度和学习率衰减的相关实验发现提供了理论验证和解释。由此产生的洞察，也为研究人员在开发大型语言模型时提前选择关键的学习率调度策略提供了指导。最重要的是，由于整个训练曲线上的所有点都遵循该方程，我们可以在任何给定步骤和任何学习率调度下实现准确的损失预测，而所需计算成本仅为使用小松鼠缩放法则拟合语言模型损失所需的1%以下。这一方法极大地促进了缩放律拟合和预测在开发大型语言模型过程中的普及性。|
|**2024-08-20**|**Athena: Safe Autonomous Agents with Verbal Contrastive Learning**|Tanmana Sadhu et.al.|[2408.11021](http://arxiv.org/abs/2408.11021)|null|由于新兴能力，大型语言模型（LLMs）被用作基于语言的代理，执行各种任务并以不断增长的程度自主做出决策。这些自主代理能够理解高级指令、与环境互动，并使用可用给它们的工具集执行复杂任务。随着代理能力的扩展，确保它们的安全性和可信度变得越来越重要。在这项研究中，我们引入了Athena框架，它利用了口头对比学习的概念，通过将过去安全和不安全的轨迹作为上下文（对比）示例来指导代理向安全性发展，同时完成给定的任务。该框架还整合了一个批判性机制，在每个步骤上引导代理避免风险行为。此外，由于缺乏对基于LLM的代理安全推理能力的现有基准，我们收集了涵盖8个类别共计80个工具包和180个场景的一组数据集，提供了一种安全评估基准。我们的实验评估表明，口头对比学习和交互级批判性思考显著提高了安全性率。|
|**2024-08-20**|**While GitHub Copilot Excels at Coding, Does It Ensure Responsible Output?**|Wen Cheng et.al.|[2408.11006](http://arxiv.org/abs/2408.11006)|**[link](https://github.com/sensente/security-attacks-on-lccts)**|**快速发展的大型语言模型（LLMs）在代码补全能力方面取得了显著进步，催生了新一代基于LLM的代码补全工具（LCCT）。与通用LLM不同，这些工具具有独特的操作流程，整合多种信息源作为输入，并优先考虑代码建议而非自然语言交互，这引入了特定的安全挑战。此外，LCCT通常依赖于专有代码数据集进行训练，引发了关于敏感数据泄露的担忧。本文利用LCCT的独特特性，开发了针对两种关键安全风险的针对性攻击方法：越狱攻击和训练数据提取攻击。  实验结果揭示了LCCT中存在的重大漏洞，包括在GitHub Copilot上的99.4%成功越狱攻击率，在Amazon Q上的46.3%成功率。我们还成功从GitHub Copilot中提取了敏感用户数据，包括54个真实电子邮件地址和314个与GitHub用户名关联的物理地址。研究还表明，这些基于代码的攻击方法对通用LLM（如GPT系列）同样有效，突显了现代LLM处理代码时存在的更广泛安全问题。这些发现强调了LCCT面临的关键安全挑战，并提出了加强其安全框架的重要方向。  为了验证我们的研究成果，我们提供了相关代码示例和攻击样本，它们可从https://github.com/Sensente/Security-Attacks-on-LCCTs获取。**|
|**2024-08-20**|**CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language Models**|Michael Reinisch et.al.|[2408.10995](http://arxiv.org/abs/2408.10995)|null|新医疗治疗方法的开发需要多个临床试验阶段。尽管将药物推向市场的成本高昂且具有挑战性，但只有不到20%的药物能从第一阶段过渡到最后的批准。近期的研究文献表明，试验方案的设计对试验表现有着显著影响。我们研究了临床试验结果预测（CTOP），旨在通过利用试验设计文件自动预测不同阶段的转换。我们提出了首个基于大型语言模型（LLM）的CTOP模型——CTP-LLM。我们还引入了一个名为PhaseTransition（PT）的数据集，该数据集根据试验在监管过程中的进展进行标记，并作为CTOP评估的标准基准。  我们的精细调参GPT-3.5为基础的模型（CTP-LLM）能够通过分析原始协议文本来预测临床试验阶段的转换，无需依赖人类选择的特征。CTP-LLM在所有阶段的预测中达到了67%的准确率，在预测从第三阶段到最终批准的转换时，准确率更达到了75%。我们的实验结果强调了LLM驱动应用在预测临床试验结果和评估试验设计方面的潜力。|
|**2024-08-20**|**Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models**|Yuyan Chen et.al.|[2408.10947](http://arxiv.org/abs/2408.10947)|null|教师在传授知识和引导学习者方面发挥着重要作用，大型语言模型（LLMs）作为潜在教育者的角色正在成为一个重要研究领域。认识到LLMs生成教育内容的能力可以推动自动化和个性化学习的进展。虽然LLMs在理解力和解决问题能力方面的测试已经进行，但它们在教学方面的潜力仍鲜为人知。在教学中，提问是一项关键技能，能够指导学生分析、评估并综合核心概念和原理。因此，我们的研究引入了一个基准来评估教育中LLMs的提问能力，通过评估它们生成的教育问题，利用安德森和克拉夫霍夫的分类法覆盖一般、单学科和跨学科领域。我们从将LLMs视为学习者转向将其视为教育者，通过评估它们生成问题的能力来评估它们的教学能力。我们应用了四个指标，包括相关性、覆盖率、代表性以及一致性，来评估LLMs输出的教育质量。我们的结果表明，GPT-4在教授一般、人文学科和科学课程方面显示出显著潜力；Claude2似乎更适合担任跨学科教师。此外，自动评分与人类观点一致。|
|**2024-08-20**|**Large Language Model Driven Recommendation**|Anton Korikov et.al.|[2408.10946](http://arxiv.org/abs/2408.10946)|null|### 摘要  本文探讨了利用语言模型（LLM）构建个性化推荐系统的新机遇。在之前的章节中，我们关注的是基于标准化、非言语用户反馈的推荐系统，如购买、观看和点击等行为。然而，随着LLM能力的增强，它们能够进行通用自然语言推理，这为使用自然语言交互来构建高度个性化的推荐系统开辟了新途径。  本章首先通过分类的方式介绍关键的数据源，涵盖商品描述、用户与系统的交互以及用户档案。接着，详细讨论了基于LLM的推荐技术，包括调优和未调优情况下的编码器仅使用和自回归推荐方法。然后，转向多模块推荐架构，其中LLM与其他组件如检索器和推荐系统在多阶段管道中协作。最后，介绍了对话式推荐系统（CRS），在这些系统中，LLM促进多轮对话，每一轮不仅提供推荐，还提供了与用户的互动，用于偏好提取、批评和问答。  ### 翻译  本文探讨了语言模型（LLM）在构建个性化推荐系统方面的新型应用。此前章节主要关注基于标准、非言语用户反馈的推荐系统，例如购买、浏览和点击等行为。然而，随着LLM能力的提升，它们具备了通用自然语言推理的能力，从而打开了使用自然语言交互构建高度定制化推荐系统的可能性。  本章首先通过分类方式概述了关键数据源，包括商品描述、用户与系统交互以及用户档案。随后，深入探讨了基于LLM的推荐技术，涵盖了编码器仅使用和自回归推荐方法，无论是在调优还是未调优状态下。接着，讨论了多模块推荐架构，其中LLM与其他组件如检索器和推荐系统在多阶段流程中协同工作。最后，介绍了对话式推荐系统（CRS），在这些系统中，LLM支持多轮对话，每一轮不仅用于生成推荐，还能与用户进行互动，进行偏好收集、评价和问答。|
|**2024-08-19**|**Demystifying the Communication Characteristics for Distributed Transformer Models**|Quentin Anthony et.al.|[2408.10197](http://arxiv.org/abs/2408.10197)|null|深度学习（DL）模型基于变换器架构在大型语言模型（LLMs）、视觉变换器、音频生成和时间序列预测等众多DL应用领域实现了革命性进展。这一系列进步很大程度上得益于分布式训练，然而分布式通信仍然是影响训练进度的一个重大瓶颈。本文旨在探讨变换器模型的通信行为，即在使用多节点/多GPU DL训练时，不同并行方案如何在变换器背景下进行数据通信。我们以GPT为基础的语言模型作为变换器架构案例研究的主要对象，由于其广泛的应用而被选中。通过我们的通信日志验证了所获得的实验结果，并使用分析模型对这些结果进行了确认。  总体而言，我们的分析揭示了进一步优化小消息点到点通信的必要性、序列长度、每GPU吞吐量、模型大小以及所用优化之间的相关性，以及在框架和高性能计算中间件设计与优化方面可能需要引导的进一步优化方向。|
|**2024-08-19**|**SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models**|Anke Tang et.al.|[2408.10174](http://arxiv.org/abs/2408.10174)|**[link](https://github.com/tanganke/fusion_bench)**|**深度模型在大规模数据集上的训练日益变得成本高昂，这促使人们广泛采用深度模型融合技术，以利用现有模型的知识。从简单的权重平均到更复杂的AdaMerging等方法，模型融合能够有效提升模型性能，并加速新模型的开发。然而，个体模型参数间的相互干扰以及融合过程的可解释性不足仍然是挑战。现有方法往往试图通过评估参数属性（如大小或符号）或进行参数修剪来解决参数干扰问题。本研究首先从线性层微调的角度出发，通过子空间分析明确地定义了参数干扰作为优化问题，以揭示这一主题。随后，我们引入了一种名为零样本稀疏混合低秩专家（SMILE）构造的创新模型融合方法，该方法允许在无需额外数据或进一步训练的情况下，将源模型升级为混合专家模型（MoE）。我们的方法基于以下观察：微调主要保留了预训练的重要部分，但使用较少重要或未使用的区域来适应新任务。此外，在原始参数空间中固有的参数干扰问题，可以通过扩展维度来管理。我们在多种场景下进行了广泛的实验，包括图像分类和文本泛化任务，使用全量微调和LoRA微调，并将我们的方法应用于大型语言模型（CLIP模型、Flan-T5模型和Mistral-7B模型），突出了SMILE的适应性和可扩展性。代码已开源于https://github.com/tanganke/fusion_bench**|
|**2024-08-19**|**Customizing Language Models with Instance-wise LoRA for Sequential Recommendation**|Xiaoyu Kong et.al.|[2408.10159](http://arxiv.org/abs/2408.10159)|**[link](https://github.com/akalikong/ilora)**|基于大型语言模型（LLM）在知识理解和推理方面的优势，近期的研究通过语言生成范式将LLM应用于序列推荐系统中。这些方法将用户行为序列转换为LLM微调的提示，利用LoRA模块来细化推荐。然而，在不同用户行为之间进行统一应用时，LoRA有时无法捕捉到个体差异性，导致性能不佳以及在不同行为序列间的负迁移。为了应对这些挑战，我们提出了一种基于实例的LoRA（iLoRA），它结合了LoRA与混合专家（MoE）框架。iLoRA创建了一个多样化的专家集合，每个专家都能够捕获特定的用户偏好方面，并引入了一个由历史交互序列引导的门控函数。该门控函数处理历史交互序列以生成增强表示，从而指导门控网络输出定制的专家参与权重。这种定制化的方法可以减少负迁移并动态适应多样的行为模式。在三个基准数据集上的广泛实验显示了iLoRA的有效性，证明了其在捕捉用户特定偏好和提高推荐准确度方面的优越性能。|
|**2024-08-19**|**Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models**|Amey Hengle et.al.|[2408.10151](http://arxiv.org/abs/2408.10151)|**[link](https://github.com/AmeyHengle/multilingual-needle-in-a-haystack)**|在近期的大型语言模型（LLM）展示了在多种语言中响应查询的能力之后，它们处理长多语言上下文的能力尚未得到探索。因此，在多语言背景下评估LLM的长期上下文能力至关重要，特别是在信息检索的背景下。为此，我们引入了多语言针在草堆中的测试（MultiLingual Needle-in-a-Haystack，简称MLNeedle），旨在评估模型从多语言干扰文本集合（草堆）中检索相关信息（针）的能力。这一测试扩展了多语言问答任务，涵盖了单语言和跨语言检索。我们对当前的四大先进LLM进行了MLNeedle测试。我们的发现显示，模型性能在不同语言和针的位置上存在显著差异。具体而言，我们观察到当针位于英语语系之外的语言中以及输入上下文的中间位置时，模型的性能最低。此外，尽管某些模型声称具有高达8k个令牌的上下文大小，但在上下文长度增加时，它们都没有表现出满意的跨语言检索性能。我们的分析提供了关于LLM在多语言背景下处理长上下文的关键见解，以指导未来的评估方法。据我们所知，这是首次研究LLM在多语言背景下的长上下文行为。|
|**2024-08-19**|**In-Context Learning with Representations: Contextual Generalization of Trained Transformers**|Tong Yang et.al.|[2408.10147](http://arxiv.org/abs/2408.10147)|null|本文通过非线性回归任务的视角来探讨Transformer在梯度下降过程中的训练动态。在此类任务中，我们可以通过学习每个任务的模板函数实现上下文泛化，所有模板函数都位于包含 $m$ 个基函数的线性空间内。我们对单层多头Transformer进行了分析，以在部分标记提示下预测未标记输入的上下文内预测能力，其中标签包含高斯噪声，每个提示中的示例数量不足以确定模板。  在温和假设下，我们证明了单层多头Transformer的训练损失会线性收敛至全局最小值。此外，Transformer有效地学习了在基函数上进行岭回归的方法。据我们所知，这是首次通过理论证明展示了当提示仅包含少量查询-答案对时，Transformer能够学习上下文信息（即模板）以对未见过的示例和任务进行泛化。|
|**2024-08-19**|**Instruction Finetuning for Leaderboard Generation from Empirical AI Research**|Salomon Kabongo et.al.|[2408.10141](http://arxiv.org/abs/2408.10141)|null|本文展示了预训练大型语言模型（LLMs）指令微调在自动化生成AI研究排行榜中的应用，从文章中提取（任务，数据集，指标，分数）四元组。该研究旨在通过从传统的、基于社区的手动整理转变为利用自动化、生成式LLM方法来简化AI研究进展的传播，从而超越依赖于特定分类的自然语言推理（NLI）模型的传统方式。通过利用FLAN-T5模型，本研究增强了LLMs在信息抽取方面的适应性和可靠性，并提供了一种新颖的方法来构建结构化知识表示。|
|**2024-08-19**|**Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**|Tianyu Zhang et.al.|[2408.10124](http://arxiv.org/abs/2408.10124)|**[link](https://github.com/zhangtia16/molgraph-lardo)**|**分子属性预测是药物发现的基础。近年来，预训练深度学习模型在这一领域得到了广泛应用，并取得了显著成果。一些将生物化学领域的先验知识融入预训练框架的方法表现出了令人印象深刻的性能。然而，这些方法高度依赖于生物化学专家，获取和总结大量的领域知识文献既耗时又昂贵。大型语言模型（LLMs）在理解并高效提供通用知识方面表现出卓越的能力。然而，它们偶尔会出现幻觉，并缺乏生成特定领域知识的精确性。与此相反，领域特定小型模型（DSMs）拥有丰富的领域知识，能够准确计算与分子领域相关的指标。然而，由于它们的模型大小有限且功能单一，它们缺乏全面的表示学习所需的广泛知识。为了在分子属性预测中充分利用两种方法的优势，我们提出了一种名为MolGraph-LarDo的新型分子图表示学习框架，该框架融合了大型语言模型和领域特定小型模型。技术上，我们设计了一个两阶段提示策略，其中引入DSMs来校准LLMs提供的知识，从而增强领域特定信息的准确性，使LLMs能够为分子样本生成更精确的文字描述。随后，我们采用多模态对齐方法协调包括分子图及其对应描述文本在内的各种模态，以指导分子表示的预训练。广泛的实验结果证明了所提出方法的有效性。**|
|**2024-08-20**|**PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities**|Yuanjian Xu et.al.|[2408.10111](http://arxiv.org/abs/2408.10111)|null|金融时间序列建模对于理解与预测市场行为至关重要，但面临着非线性、非平稳性和高噪声等挑战。传统的模型在捕捉复杂模式时受到这些因素的影响，同时受到计算资源和模型容量的限制。受自然语言处理领域大型语言模型成功启发，我们提出了一种名为 $\textbf{PLUTUS}$的模型，其全称为$\textbf{P}$re-trained $\textbf{L}$arge $\textbf{U}$nified $\textbf{T}$ransformer-based模型，用于揭示金融时间序列中的规律。$\textbf{PLUTUS}$通过结合可逆嵌入模块、对比学习和自动编码技术，创建了原始数据与块嵌入之间的近似一一映射。  TimeFormer，一个基于注意力的架构，构成了$\textbf{PLUTUS}$的核心，有效地处理了高噪声时间序列数据。我们引入了一种新颖的注意力机制，以跨变量和时间维度捕获特征。$\textbf{PLUTUS}$在规模空前的1000亿个观察值的数据集上进行预训练，旨在适应嘈杂的金融市场环境。据我们所知，$\textbf{PLUTUS}$ 是首个开源的、大规模的预训练金融时间序列模型，参数超过十亿个。它在各种任务上实现了最先进的性能，展示了强大的迁移性，并为金融领域建立了一个坚实的基础模型。我们的研究提供了预训练金融时间序列数据的技术指导，确立了该领域的全新标准。|
|**2024-08-19**|**ARMADA: Attribute-Based Multimodal Data Augmentation**|Xiaomeng Jin et.al.|[2408.10086](http://arxiv.org/abs/2408.10086)|null|在多模态语言模型（MLMs）中，手动标注高质量的图像-文本配对数据以进行微调和对齐的成本非常高。尽管现有的多模态数据增强框架提出了增强图像-文本配对的方法，但它们要么在文本和图像之间存在语义不一致，要么生成不切实际的图像，导致与现实世界示例的知识差距。为了应对这些问题，我们提出了一种名为Attribute-based Multimodal Data Augmentation (ARMADA)的新型多模态数据增强方法，通过知识引导的提及实体视觉属性的修改来增强数据。具体来说，我们从原始文本数据中提取实体及其视觉属性，然后在知识库（KBs）和大型语言模型（LLMs）的指导下搜索视觉属性的替代值。接着，我们利用图像编辑模型根据提取的属性编辑图像。ARMADA是一个新颖的多模态数据生成框架：(i) 从符号知识库中提取知识关联的属性，实现语义一致且具有区别的图像-文本对生成；(ii) 利用知识库层次结构中的同类别实体生成视觉上相似但不同类别的图像；(iii) 使用LLMs的常识知识调节辅助视觉属性，如背景，以更全面地表示原始实体。我们的实验证明，在四个下游任务上，我们的框架能够产生高质量的数据并提高模型性能。这也强调了利用外部知识代理以增强可解释性和现实世界相关性的必要性。|
|**2024-08-19**|**FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant**|Zhengchao Huang et.al.|[2408.10072](http://arxiv.org/abs/2408.10072)|**[link](https://github.com/thu-huangzc/FFAA)**|快速发展的深度伪造技术引发了公众的广泛关注，尤其是在对公共信息安全构成严重威胁的面部伪造方面。然而，未知和多样的伪造技术、多变的面部特征以及复杂的环境因素给面部伪造分析带来了巨大挑战。现有数据集在描述这些方面时存在不足，使得仅通过视觉信息难以在各种干扰因素中区分真实与伪造的面部。此外，现有的方法未能提供用户友好且可解释的结果，复杂化了模型决策过程的理解。  为解决这些挑战，我们提出了一项新颖的“开放世界面部伪造分析问答”（OW-FFA-VQA）任务及其相应的基准。为了应对这一任务，我们首先建立了一个包含真实和伪造面部图像的多样集合，并配有关键描述和可靠伪造推理的数据集。基于此数据集，我们引入了“面部伪造分析助手”（FFAA），它由一个微调的多模态大型语言模型（MLLM）和一个多答案智能决策系统（MIDS）组成。通过结合假设性提示与MIDS，有效消除了模糊分类边界的影响力，增强了模型的鲁棒性。广泛的实验结果表明，我们的方法不仅提供了用户友好的可解释结果，而且在准确性与鲁棒性方面显著超越了以往的方法。|
|**2024-08-16**|**PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars**|Sumanth Prabhu et.al.|[2408.08869](http://arxiv.org/abs/2408.08869)|null|自一致性等依赖于准确答案提取过程的自我集丛技术已经在大型语言模型（LLM）的准确性上取得了显著的提升。然而，这些技术在聚合多个输出时需要较高的推理成本，相较于贪心解码而言，生成相对较多的输出令牌。研究显示，自一致性方法产生的自由文本输出可以通过LLM可靠地聚合以产生最终输出。此外，最近的LLM推理进展表明，在提示中使用多样化的示例能够诱导LLM输出的多样性。这些已经证明的技术可以很容易地扩展到自我集丛方法中，以实现文本生成的整体性能改进。  本文介绍了一种名为PEDAL（基于示例多样性的LLM聚合）的混合自我集丛方法。该方法结合了基于多样示例提示和LLM聚合的优势，以实现性能的提升。在公开的SVAMP和ARC数据集上进行的实验揭示，与基于贪心解码的策略相比，PEDAL能够在较低的推理成本下获得更好的准确性，与基于自一致性的方法相比具有优势。|
|**2024-08-16**|**Visual Agents as Fast and Slow Thinkers**|Guangyan Sun et.al.|[2408.08862](http://arxiv.org/abs/2408.08862)|**[link](https://github.com/guangyans/sys2-llava)**|实现与人类相当的智能需要对认知上的第一系统和第二系统思维进行细化。当前的人工智能，尤其是基于大型语言模型的AI，虽然表现出类似人类的特点，但并未达到真正的认知水平。在从结构化基准向真实世界场景过渡的过程中，视觉代理面临挑战，往往导致回答既不准确又过于自信。为了解决这一问题，我们引入了FaST（快速与缓慢思考），它将快速与缓慢思考机制融入到视觉代理中。FaST采用切换适配器动态选择系统1/2模式，根据任务的复杂性调整解决问题的方法。面对不确定和未见过的对象时，通过调整模型的信心并整合新的上下文数据，它能够灵活应对。  我们提倡一个灵活的系统、层次化的推理能力和透明的决策流程，这些都使得FaST能够模仿人类在视觉智能中的认知过程。实验结果表明，FaST在视觉问答(VQA^{v2})任务上达到了80.8%的准确率，在推理分割(ReasonSeg)任务上获得了48.7%的GIoU分数，这充分展示了FaST的优越性能。广泛的测试验证了FaST核心组件的有效性和稳健性，显示了其在推动人工智能系统中认知视觉代理的发展方面的潜力。|
|**2024-08-16**|**ECG-Chat: A Large ECG-Language Model for Cardiac Disease Diagnosis**|Yubao Zhao et.al.|[2408.08849](http://arxiv.org/abs/2408.08849)|**[link](https://github.com/YubaoZhao/ECG-Chat)**|在医疗辅助领域，多模态大型语言模型（MLLMs）的成功展现出巨大潜力，使得患者能够利用生理信号数据进行对话。然而，通用的MLLMs在心脏病诊断方面表现不佳，尤其是在ECG数据解析与长文本医学报告生成的整合上，主要原因是ECG数据解析的复杂性以及文本与ECG信号模态之间的差距。此外，模型在长文本生成时往往存在严重的稳定性问题，这主要是由于缺乏与用户查询紧密相关的精确知识。  为了解决这些问题，我们提出了一种名为ECG-Chat的多任务MLLM，专注于ECG医学报告生成，并提供基于心脏病学知识的跨模态对话能力。我们采用了对比学习方法，将ECG波形数据与文本报告结合，以精细的方式对齐ECG特征与报告内容。这种方法还产生了一个在零样本报告检索任务中表现出色的ECG编码器。此外，我们通过扩展现有数据集，构建了包含19K个ECG诊断数据集和25K个多轮对话数据集用于训练和微调ECG-Chat，从而提供专业的诊断和对话能力。此外，ECG-Chat可以通过自动化LaTeX生成管道来生成全面的ECG分析报告。我们为ECG报告生成任务建立了基准，并在多个基线上测试了我们的模型。ECG-Chat在分类、检索、多模态对话和医学报告生成任务中均取得了最佳性能。我们的报告模板设计也得到了医疗专业人员的一致认可。|
|**2024-08-16**|**PsychoLex: Unveiling the Psychological Mind of Large Language Models**|Mohammad Amin Abbasi et.al.|[2408.08848](http://arxiv.org/abs/2408.08848)|null|这篇论文探讨了心理学与人工智能的交汇点，通过开发和评估专用于心理任务的大型语言模型（LLMs）。我们引入了PsychoLex套件，旨在增强LLMs在波斯语和英语中的心理任务处理能力。主要贡献包括PsychoLexQA数据集，用于教学内容的创建，以及PsychoLexEval数据集，用于对LLMs在复杂心理情景下的严格评估。此外，我们还介绍了PsychoLexLLaMA模型，该模型特别优化以适用于心理应用，其性能明显优于通用模型。研究结果强调了定制LLMs在推进心理研究和应用方面的潜力，同时也指出了进一步改进的领域。这项研究为将LLMs融入特定的心理学领域奠定了基础，对未来AI驱动的心理实践的发展具有重要意义。|
|**2024-08-16**|**FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats**|Xuanliang Zhang et.al.|[2408.08841](http://arxiv.org/abs/2408.08841)|**[link](https://github.com/zhxlia/FLEXTAF)**|**## 上文背景 表格推理任务旨在根据给定的表格回答问题。目前，使用大型语言模型（LLMs）是表格推理的主要方法。现有的大多数方法都采用固定的表格格式来表示表格，这可能限制了性能。鉴于每个实例需要不同的能力，而模型具有不同的能力，我们断言不同实例和模型适用于不同的表格格式。通过实验结果的定量分析，我们证明了这一点：使用不同的表格格式，不同实例和模型可以获得不同的性能。在此基础上，我们提出了一种增强表格推理性能的方法FLEXTAF-Single和FLEXTAF-Vote，通过使用灵活的表格格式。具体来说，(i) FLEXTAF-Single训练一个分类器，基于实例和LLM预测最适合的表格格式。(ii) FLEXTAF-Vote在不同格式之间集成结果。我们在WikiTableQuestions和TabFact上的实验结果显示了显著的改进，与使用固定表格格式并结合贪婪解码和自我一致性解码达到的最佳性能相比，平均提高了2.3%和4.8%，从而验证了我们方法的有效性。**|
|**2024-08-16**|**Artificial Intelligence and Strategic Decision-Making: Evidence from Entrepreneurs and Investors**|Felipe A. Csaszar et.al.|[2408.08811](http://arxiv.org/abs/2408.08811)|null|本文探讨了人工智能（AI）如何影响企业战略决策过程。我们通过实例展示了AI如何增强现有战略决策工具，并提供了来自领先加速器计划和创业竞赛的实证证据，证明当前的大规模语言模型（LLMs）在生成和评估策略方面的能力与企业家和投资者相当。接着，我们分析了战略决策背后的关键认知过程——搜索、表示和聚合，并提出AI有可能提升战略分析的速度、质量和规模，同时还能启用如虚拟战略模拟等新方法。然而，AI对企业发展的影响最终取决于竞争动态以及AI能力的发展。我们提出了一个框架，将AI在战略决策中的应用与企业结果联系起来，并讨论了AI如何重塑竞争优势的来源。最后，我们考虑了AI如何既支持又挑战基于理论的战略观的核心原则。整体而言，我们的工作描绘了一个AI与战略领域正在形成的研究前沿。|
|**2024-08-16**|**Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge**|Ravi Raju et.al.|[2408.08808](http://arxiv.org/abs/2408.08808)|null|大型语言模型（LLM）在机器学习领域带来了革命性变化，然而现有的基准测试往往难以全面捕捉这些模型在实际应用中的多样行为。一个基准测试的价值在于它能否清晰区分不同能力级别的模型（可分性）以及与人类偏好的紧密匹配度。当前的框架如Alpaca-Eval 2.0 LC \cite{dubois2024lengthcontrolledalpacaevalsimpleway} 和Arena-Hard v0.1 \cite{li2024crowdsourced}主要关注通用查询，并且缺乏跨法律、医学等领域的多样性。本文通过引入一种新颖的数据管道，来定制一系列多元化的、针对LLM-as-a-Judge框架的领域特定评估集，以解决这些问题。我们的方法结合了人工筛选、半监督学习生成聚类以及分层抽样，确保在广泛领域和语言中都有均衡的代表性。产生的评估集包括1573个样本，分布在14个类别中，显示出高可分性（84%）和对前十大模型的性能差异，同时与Chatbot Arena的共识度（84%）和Spearman相关系数（0.915）也表现出良好的一致性。与AlpacaEval 2.0 LC的共识度相比，这一值高出9%，与Arena Hard相比则高出20%，而与Spearman系数相比则是下一个最佳基准的0.7倍，这表明我们在基准测试的有效性方面取得了显著进步。我们还提供了一个开源的评估工具，允许用户自定义类别进行精细分析，从而为实践者提供有价值的洞察。这项工作对增强LLM评估方法的透明度、多样性和有效性做出了贡献。|
|**2024-08-16**|**EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics**|Chenwei Wan et.al.|[2408.08782](http://arxiv.org/abs/2408.08782)|**[link](https://github.com/cw-wan/EmoDynamiX-v2)**|**设计能够提供慰藉和建议的具有情感智能的对话系统，以帮助那些经历压力的人们，是一个极具吸引力的研究领域。过去的研究工作着重于构建模块化对话系统，并将其社会情感策略预测视为辅助任务，通过定制解码器生成条件化的响应。最近，在大型语言模型（LLM）方面的发展使得无需明确的社会情感策略预测步骤的端到端对话代理变得流行起来。尽管它们在语言生成方面表现出色，但最近的研究表明，LLM固有的偏好偏见，倾向于某些社会情感策略，阻碍了提供高质量情感支持的能力。  为了应对这一挑战，我们提出了一种新的方法：将策略预测与语言生成分离，并引入了一个名为EmoDynamiX的新型对话策略预测器。该预测器利用异构图来建模用户情绪与系统策略之间的对话动态。此外，我们利用了对话中情感识别（ERC）任务，并设计了一个灵活的混合情绪模块，以捕捉用户的细微情感状态。在两个ESC数据集上的实验结果表明，EmoDynamiX显著超越了先前最先进的方法。  请注意，上述翻译已经移除了","字符。**|
|**2024-08-16**|**Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions**|Chenming Tang et.al.|[2408.08780](http://arxiv.org/abs/2408.08780)|null|通过利用上下文学习（ICL），大型语言模型在各种任务上取得了令人印象深刻的性能。然而，在ICL过程中描述性指令的作用仍然有待探索。本研究提出了一种集成提示框架，用于描述多个上下文示例的选择标准，并在六个翻译方向的机器翻译（MT）任务上的初步实验表明，这种框架能够提升ICL性能。出乎意料的是，LLM可能并不关心描述的具体内容，性能提升主要源于集成格式，即使使用随机描述名词，该框架也能带来改进。我们进一步在常识、数学、逻辑推理和幻觉任务上应用了这种新的集成提示，并使用三种LLM取得了有希望的结果，这再次表明设计适当的提示格式比专注于特定描述更为有效和高效。在论文发表后，我们的代码将公开提供。|
|**2024-08-16**|**DAC: Decomposed Automation Correction for Text-to-SQL**|Dingzirui Wang et.al.|[2408.08779](http://arxiv.org/abs/2408.08779)|**[link](https://github.com/zirui-HIT/DAC)**|**文本到SQL是一个重要的任务，它通过自动生成SQL查询帮助人们从数据库中获取信息。考虑到出色的性能，基于大型语言模型（LLM）的方法成为了文本到SQL的主流方式。在这类方法中，自动修正成为一种有效手段，能够通过纠正生成结果中的错误来进一步提升性能。现有修正方法要求LLM直接对生成的SQL进行修正，而先前的研究表明，LLM并不知道如何检测错误，导致了较差的性能。因此，在这篇论文中，我们提出采用分解式修正来增强文本到SQL的性能。首先，我们证明了分解式修正优于直接修正，因为与SQL相比，通过结果分解子任务来检测和修复错误更为容易。基于这一分析，我们引入了分解自动化修正（DAC），该方法通过将文本到SQL分解为实体链接和骨架解析两个子任务来修正SQL。DAC首先生成与问题对应的实体和骨架，然后比较初始SQL与生成的实体和骨架之间的差异作为修正反馈。实验结果显示，与基线方法相比，我们的方法在Spider、Bird和KaggleDBQA上的平均性能提高了3.7%，证明了DAC的有效性。**|
|**2024-08-15**|**Can Large Language Models Understand Symbolic Graphics Programs?**|Zeju Qiu et.al.|[2408.08313](http://arxiv.org/abs/2408.08313)|null|Assessing the capabilities of large language models (LLMs) is often challenging, in part, because it is hard to find tasks to which they have not been exposed during training. We take one step to address this challenge by turning to a new task: focusing on symbolic graphics programs, which are a popular representation for graphics content that procedurally generates visual data. LLMs have shown exciting promise towards program synthesis, but do they understand symbolic graphics programs? Unlike conventional programs, symbolic graphics programs can be translated to graphics content. Here, we characterize an LLM's understanding of symbolic programs in terms of their ability to answer questions related to the graphics content. This task is challenging as the questions are difficult to answer from the symbolic programs alone -- yet, they would be easy to answer from the corresponding graphics content as we verify through a human experiment. To understand symbolic programs, LLMs may need to possess the ability to imagine how the corresponding graphics content would look without directly accessing the rendered visual content. We use this task to evaluate LLMs by creating a large benchmark for the semantic understanding of symbolic graphics programs. This benchmark is built via program-graphics correspondence, hence requiring minimal human efforts. We evaluate current LLMs on our benchmark to elucidate a preliminary assessment of their ability to reason about visual scenes from programs. We find that this task distinguishes existing LLMs and models considered good at reasoning perform better. Lastly, we introduce Symbolic Instruction Tuning (SIT) to improve this ability. Specifically, we query GPT4-o with questions and images generated by symbolic programs. Such data are then used to finetune an LLM. We also find that SIT data can improve the general instruction following ability of LLMs.|
|**2024-08-15**|**ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws**|Ruihang Li et.al.|[2408.08310](http://arxiv.org/abs/2408.08310)|null|High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity.|
|**2024-08-15**|**Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors**|Usman Syed et.al.|[2408.08302](http://arxiv.org/abs/2408.08302)|null|In this paper, we explore the capabilities of state-of-the-art large language models (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level transportation engineering problems. We introduce TransportBench, a benchmark dataset that includes a sample of transportation engineering problems on a wide range of subjects in the context of planning, design, management, and control of transportation systems. This dataset is used by human experts to evaluate the capabilities of various commercial and open-sourced LLMs, especially their accuracy, consistency, and reasoning behaviors, in solving transportation engineering problems. Our comprehensive analysis uncovers the unique strengths and limitations of each LLM, e.g. our analysis shows the impressive accuracy and some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving TransportBench problems. Our study marks a thrilling first step toward harnessing artificial general intelligence for complex transportation challenges.|
|**2024-08-15**|**HELP: Hierarchical Embeddings-based Log Parsing**|Andy Xu et.al.|[2408.08300](http://arxiv.org/abs/2408.08300)|null|Logs are a first-hand source of information for software maintenance and failure diagnosis. Log parsing, which converts semi-structured log messages into structured templates, is a prerequisite for automated log analysis tasks such as anomaly detection, troubleshooting, and root cause analysis. However, existing log parsers fail in real-world systems for three main reasons. First, traditional heuristics-based parsers require handcrafted features and domain knowledge, which are difficult to generalize at scale. Second, existing large language model-based parsers rely on periodic offline processing, limiting their effectiveness in real-time use cases. Third, existing online parsing algorithms are susceptible to log drift, where slight log changes create false positives that drown out real anomalies. To address these challenges, we propose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first online semantic-based parser to leverage LLMs for performant and cost-effective log parsing. We achieve this through a novel hierarchical embeddings module, which fine-tunes a text embedding model to cluster logs before parsing, reducing querying costs by multiple orders of magnitude. To combat log drift, we also develop an iterative rebalancing module, which periodically updates existing log groupings. We evaluate HELP extensively on 14 public large-scale datasets, showing that HELP achieves significantly higher F1-weighted grouping and parsing accuracy than current state-of-the-art online log parsers. We also implement HELP into Iudex's production observability platform, confirming HELP's practicality in a production environment. Our results show that HELP is effective and efficient for high-throughput real-world log parsing.|
|**2024-08-15**|**The ShareLM Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community**|Shachar Don-Yehiya et.al.|[2408.08291](http://arxiv.org/abs/2408.08291)|null|Human-model conversations provide a window into users' real-world scenarios, behavior, and needs, and thus are a valuable resource for model development and research. While for-profit companies collect user data through the APIs of their models, using it internally to improve their own models, the open source and research community lags behind.   We introduce the ShareLM collection, a unified set of human conversations with large language models, and its accompanying plugin, a Web extension for voluntarily contributing user-model conversations. Where few platforms share their chats, the ShareLM plugin adds this functionality, thus, allowing users to share conversations from most platforms. The plugin allows the user to rate their conversations, both at the conversation and the response levels, and delete conversations they prefer to keep private before they ever leave the user's local storage. We release the plugin conversations as part of the ShareLM collection, and call for more community effort in the field of open human-model data.   The code, plugin, and data are available.|
|**2024-08-15**|**Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model**|Jin Wang et.al.|[2408.08282](http://arxiv.org/abs/2408.08282)|null|Enabling humanoid robots to perform autonomously loco-manipulation in unstructured environments is crucial and highly challenging for achieving embodied intelligence. This involves robots being able to plan their actions and behaviors in long-horizon tasks while using multi-modality to perceive deviations between task execution and high-level planning. Recently, large language models (LLMs) have demonstrated powerful planning and reasoning capabilities for comprehension and processing of semantic information through robot control tasks, as well as the usability of analytical judgment and decision-making for multi-modal inputs. To leverage the power of LLMs towards humanoid loco-manipulation, we propose a novel language-model based framework that enables robots to autonomously plan behaviors and low-level execution under given textual instructions, while observing and correcting failures that may occur during task execution. To systematically evaluate this framework in grounding LLMs, we created the robot 'action' and 'sensing' behavior library for task planning, and conducted mobile manipulation tasks and experiments in both simulated and real environments using the CENTAURO robot, and verified the effectiveness and application of this approach in robotic tasks with autonomous behavioral planning.|
|**2024-08-15**|**BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts**|Qizhen Zhang et.al.|[2408.08274](http://arxiv.org/abs/2408.08274)|null|The Mixture of Experts (MoE) framework has become a popular architecture for large language models due to its superior performance over dense models. However, training MoEs from scratch in a large-scale regime is prohibitively expensive. Existing methods mitigate this by pre-training multiple dense expert models independently and using them to initialize an MoE. This is done by using experts' feed-forward network (FFN) to initialize the MoE's experts while merging other parameters. However, this method limits the reuse of dense model parameters to only the FFN layers, thereby constraining the advantages when "upcycling" these models into MoEs. We propose BAM (Branch-Attend-Mix), a simple yet effective method that addresses this shortcoming. BAM makes full use of specialized dense models by not only using their FFN to initialize the MoE layers but also leveraging experts' attention parameters fully by initializing them into a soft-variant of Mixture of Attention (MoA) layers. We explore two methods for upcycling attention parameters: 1) initializing separate attention experts from dense models including all attention parameters for the best model performance; and 2) sharing key and value parameters across all experts to facilitate for better inference efficiency. To further improve efficiency, we adopt a parallel attention transformer architecture to MoEs, which allows the attention experts and FFN experts to be computed concurrently. Our experiments on seed models ranging from 590 million to 2 billion parameters demonstrate that BAM surpasses baselines in both perplexity and downstream task performance, within the same computational and data constraints.|
|**2024-08-15**|**DaRec: A Disentangled Alignment Framework for Large Language Model and Recommender System**|Xihong Yang et.al.|[2408.08231](http://arxiv.org/abs/2408.08231)|null|Benefiting from the strong reasoning capabilities, Large language models (LLMs) have demonstrated remarkable performance in recommender systems. Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment. In this work, we prove that directly aligning the representations of LLMs and collaborative models is sub-optimal for enhancing downstream recommendation tasks performance, based on the information theorem. Consequently, the challenge of effectively aligning semantic representations between collaborative models and LLMs remains unresolved. Inspired by this viewpoint, we propose a novel plug-and-play alignment framework for LLMs and collaborative models. Specifically, we first disentangle the latent representations of both LLMs and collaborative models into specific and shared components via projection layers and representation regularization. Subsequently, we perform both global and local structure alignment on the shared representations to facilitate knowledge transfer. Additionally, we theoretically prove that the specific and shared representations contain more pertinent and less irrelevant information, which can enhance the effectiveness of downstream recommendation tasks. Extensive experimental results on benchmark datasets demonstrate that our method is superior to existing state-of-the-art algorithms.|
|**2024-08-15**|**RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science**|David Farr et.al.|[2408.08217](http://arxiv.org/abs/2408.08217)|null|Large language models (LLMs) have enhanced our ability to rapidly analyze and classify unstructured natural language data. However, concerns regarding cost, network limitations, and security constraints have posed challenges for their integration into work processes. In this study, we adopt a systems design approach to employing LLMs as imperfect data annotators for downstream supervised learning tasks, introducing novel system intervention measures aimed at improving classification performance. Our methodology outperforms LLM-generated labels in seven of eight tests, demonstrating an effective strategy for incorporating LLMs into the design and deployment of specialized, supervised learning models present in many industry use cases.|
|**2024-08-15**|**Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models**|Javier González et.al.|[2408.08210](http://arxiv.org/abs/2408.08210)|null|Recent advances in AI have been significantly driven by the capabilities of large language models (LLMs) to solve complex problems in ways that resemble human thinking. However, there is an ongoing debate about the extent to which LLMs are capable of actual reasoning. Central to this debate are two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS). This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures. By viewing LLMs as abstract machines that process information through a natural language interface, we examine the conditions under which it is possible to compute suitable approximations of PN and PS. Our research marks an important step towards gaining a deeper understanding of when LLMs are capable of reasoning, as illustrated by a series of math examples.|
|**2024-08-14**|**The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models**|Karime Maamari et.al.|[2408.07702](http://arxiv.org/abs/2408.07702)|null|Schema linking is a crucial step in Text-to-SQL pipelines, which translate natural language queries into SQL. The goal of schema linking is to retrieve relevant tables and columns (signal) while disregarding irrelevant ones (noise). However, imperfect schema linking can often exclude essential columns needed for accurate query generation. In this work, we revisit the need for schema linking when using the latest generation of large language models (LLMs). We find empirically that newer models are adept at identifying relevant schema elements during generation, without the need for explicit schema linking. This allows Text-to-SQL pipelines to bypass schema linking entirely and instead pass the full database schema to the LLM, eliminating the risk of excluding necessary information. Furthermore, as alternatives to schema linking, we propose techniques that improve Text-to-SQL accuracy without compromising on essential schema information. Our approach achieves 71.83\% execution accuracy on the BIRD benchmark, ranking first at the time of submission.|
|**2024-08-15**|**Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities**|Enneng Yang et.al.|[2408.07666](http://arxiv.org/abs/2408.07666)|**[link](https://github.com/ennengyang/awesome-model-merging-methods-theories-applications)**|**Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at \url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.**|
|**2024-08-14**|**Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models**|Yi-Cheng Lin et.al.|[2408.07665](http://arxiv.org/abs/2408.07665)|**[link](https://github.com/dlion168/spoken_stereoset)**|Warning: This paper may contain texts with uncomfortable content.   Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.|
|**2024-08-14**|**Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining of Probability Distributions**|Quan Liu et.al.|[2408.07663](http://arxiv.org/abs/2408.07663)|**[link](https://github.com/gigabaozi/aed)**|**Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines AED and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach. Code is available at https://github.com/GIGABaozi/AED.git.**|
|**2024-08-14**|**WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs**|Weijian Xie et.al.|[2408.07611](http://arxiv.org/abs/2408.07611)|null|Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However, LLMs are prone to produce factually incorrect information and often produce "phantom" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a "Retrieval-Augmented Generation (RAG)" system. First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.|
|**2024-08-14**|**Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey**|Hamza Kheddar et.al.|[2408.07583](http://arxiv.org/abs/2408.07583)|null|With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.|
|**2024-08-15**|**MathScape: Evaluating MLLMs in multimodal Math Scenarios through a Hierarchical Benchmark**|Minxuan Zhou et.al.|[2408.07543](http://arxiv.org/abs/2408.07543)|**[link](https://github.com/PKU-Baichuan-MLSystemLab/MathScape)**|With the development of Multimodal Large Language Models (MLLMs), the evaluation of multimodal models in the context of mathematical problems has become a valuable research field. Multimodal visual-textual mathematical reasoning serves as a critical indicator for evaluating the comprehension and complex multi-step quantitative reasoning abilities of MLLMs. However, previous multimodal math benchmarks have not sufficiently integrated visual and textual information. To address this gap, we proposed MathScape, a new benchmark that emphasizes the understanding and application of combined visual and textual information. MathScape is designed to evaluate photo-based math problem scenarios, assessing the theoretical understanding and application ability of MLLMs through a categorical hierarchical approach. We conduct a multi-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark is challenging even for the most sophisticated models. By analyzing the evaluation results, we identify the limitations of MLLMs, offering valuable insights for enhancing model performance.|
|**2024-08-15**|**Usefulness of data flow diagrams and large language models for security threat validation: a registered report**|Winnie Bahati Mbaka et.al.|[2408.07537](http://arxiv.org/abs/2408.07537)|null|The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well. Threat analysis and risk assessment are used to identify security threats for new or refactored systems. Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis. Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats. We propose a controlled experiment with practitioners to investigate whether some analysis material (like LLM-generated advice) is better than none and whether more material (the system's data flow diagram and LLM-generated advice) is better than some material. In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design. Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions).|
|**2024-08-14**|**Development of a Multi-Agent Clinical Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in Emergency Departments**|Seungjun Han et.al.|[2408.07531](http://arxiv.org/abs/2408.07531)|null|Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.   We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.   The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.   Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.|
|**2024-08-14**|**Large Language Models Know What Makes Exemplary Contexts**|Quanyu Long et.al.|[2408.07505](http://arxiv.org/abs/2408.07505)|null|In-context learning (ICL) has proven to be a significant capability with the advancement of Large Language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without needing to update millions of parameters. This paper presents a unified framework for LLMs that allows them to self-select influential in-context examples to compose their contexts; self-rank candidates with different demonstration compositions; self-optimize the demonstration selection and ordering through reinforcement learning. Specifically, our method designs a parameter-efficient retrieval head that generates the optimized demonstration after training with rewards from LLM's own preference. Experimental results validate the proposed method's effectiveness in enhancing ICL performance. Additionally, our approach effectively identifies and selects the most representative examples for the current task, and includes more diversity in retrieval.|
|**2024-08-13**|**Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents**|Kexun Zhang et.al.|[2408.07060](http://arxiv.org/abs/2408.07060)|null|大型语言模型（LLM）代理在解决实际世界软件工程（SWE）问题方面展现出巨大的潜力。最先进开源的SWE代理能够在SWE-Bench Lite中解决超过27%的实际GitHub问题。然而，这些复杂的代理框架在表现上存在差异，有的在特定任务中表现出色，在其他任务中则表现不佳。为了充分利用这些代理的多样性，我们提出了一种名为DEI（多元化智能）的框架，该框架利用了它们的独特专长。DEI作为一个位于现有SWE代理框架之上的元模块，管理代理集体以实现增强的问题解决能力。  实验结果显示，由DEI指导的代理委员会能够显著超越单个代理的最佳性能。例如，一组开源的SWE代理，其个体解决率最高为27.3%在SWE-Bench Lite中，通过采用DEI，可以达到34.3%的解决率，实现了25%的改进，并击败了许多闭源解决方案。我们的最佳性能组表现出色，达到了55%的解决率，在SWE-Bench Lite中获得了最高排名。我们的研究结果对合作型人工智能系统的研究领域做出了贡献，展示了它们在解决复杂软件工程挑战方面的潜力。|
|**2024-08-13**|**LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs**|Yushi Bai et.al.|[2408.07055](http://arxiv.org/abs/2408.07055)|**[link](https://github.com/thudm/longwriter)**|**当前的大型语言模型（LLMs）能够处理最多10万字的输入，然而在生成超过2千字的输出时却力不从心。通过控制实验，我们发现模型的有效生成长度本质上受到其在监督微调（SFT）期间所见样本的限制。换句话说，它们的输出限制源于现有SFT数据集中长输出示例的稀缺性。为了解决这个问题，我们引入了AgentWrite，这是一种基于代理的管道，将超长生成任务分解为子任务，从而使现有的LLMs能够生成超过2万字的连贯输出。  借助AgentWrite，我们构建了LongWriter-6k数据集，其中包含了6000个SFT数据，输出长度范围从2千到32千字。通过将此数据集纳入模型训练，我们成功地将现有模型的输出长度扩展至超过1万字，同时保持了输出质量。我们也开发了LongBench-Write，这是一个全面的基准，用于评估超长生成能力。我们的9亿参数模型，在经过DPO进一步改进后，在这一基准上实现了最先进的性能，甚至超过了更大规模的专有模型。  总的来说，我们的工作表明，现有的长上下文LLMs实际上已经具备了更大的输出窗口的能力——你只需要在模型对齐过程中使用带有延长输出的数据即可解锁这一能力。我们的代码和模型可以在：https://github.com/THUDM/LongWriter找到。**|
|**2024-08-13**|**Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models**|Chun Jie Chong et.al.|[2408.07004](http://arxiv.org/abs/2408.07004)|null|基于网络的大型语言模型（LLM）服务已被广泛采用，并已成为我们互联网体验不可或缺的一部分。第三方插件通过提供对现实世界数据和服务的访问，增强了LLM的功能性。然而，与这些服务及其第三方插件相关的隐私后果并未得到充分理解。敏感提示数据在云基LLM提供商和第三方插件中被存储、处理和共享。本文提出了一种名为Casper的提示净化技术，旨在通过检测并从用户输入中删除敏感信息来保护用户隐私，从而在发送给LLM服务之前保护用户隐私。Casper完全作为浏览器扩展运行在用户的设备上，无需对在线LLM服务进行任何更改。Casper的核心是一个三层净化机制，包括规则基于过滤器、机器学习（ML）命名实体识别器和浏览器本地LLM主题标识器。我们使用4000个合成提示集对Casper进行了评估，结果显示，它能够以高准确率（98.5%）有效地过滤出个人可识别信息（PII）和隐私敏感话题（89.9%）。|
|**2024-08-13**|**LLMs can Schedule**|Henrik Abgaryan et.al.|[2408.06993](http://arxiv.org/abs/2408.06993)|**[link](https://github.com/starjob42/datasetjsp)**|**工作车间调度问题(JSSP)在优化生产流程方面仍是一个重大挑战。该问题涉及有效分配任务到有限数量的机器上，以最小化总处理时间或作业延迟等因素。尽管近期人工智能领域的进步已经产生了有前景的解决方案，例如强化学习和图神经网络，但本文探讨了大型语言模型(LLM)在JSSP中的潜力。我们首次引入了一个专门为训练LLM设计的120k数据集，专门针对JSSP。令人惊讶的是，我们的发现表明，基于LLM的调度可以实现与其它神经方法相当的性能。此外，我们提出了一种采样方法，以提高LLM在解决JSSP时的有效性。**|
|**2024-08-13**|**OpenResearcher: Unleashing AI for Accelerated Scientific Research**|Yuxiang Zheng et.al.|[2408.06941](http://arxiv.org/abs/2408.06941)|**[link](https://github.com/gair-nlp/openresearcher)**|**快速发展的科学文献对研究人员在各自领域保持最新进展和探索新领域带来了重大挑战。我们提出了一种创新平台——OpenResearcher，它利用人工智能技术加速研究过程，通过回答研究人员的多种问题来帮助他们。OpenResearcher基于检索增强生成（RAG）构建，结合了大型语言模型（LLMs）与特定领域的最新知识。此外，我们开发了各种工具，使OpenResearcher能够理解研究人员的问题、从科学文献中搜索、筛选检索到的信息、提供准确全面的答案，并自我优化这些答案。OpenResearcher灵活地使用这些工具，在效率与有效性之间找到平衡。结果，OpenResearcher帮助研究人员节省时间，提高他们发现新见解和推动科学研究突破的潜力。演示、视频和代码可在以下链接获取：https://github.com/GAIR-NLP/OpenResearcher。**|
|**2024-08-13**|**Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas**|Louis Kwok et.al.|[2408.06929](http://arxiv.org/abs/2408.06929)|**[link](https://github.com/louiskwoklf/llms-cultural-adaptability)**|大型语言模型（LLM）在多文化环境中的成功取决于它们理解用户不同文化背景的能力。我们通过让LLM模拟代表各种国籍的人类角色进行问卷式心理学实验来衡量这一能力。具体而言，我们使用GPT-3.5对来自15个国家的7,286名参与者阅读并回应具有说服力的新闻文章的反应进行模拟；并将结果与拥有相同人口统计特征的真实参与者数据集进行比较。我们的分析显示，明确指定一个人的居住国可以提高GPT-3.5与他们的反应的一致性。相比之下，使用母语提示引入的变化显著降低了整体一致性，并且某些语言特别影响了性能。这些发现表明，尽管直接提供国籍信息可以增强模型的文化适应性，但使用母语提示并不一定能可靠地提高模拟准确性，反而可能损害模型的有效性。|
|**2024-08-13**|**Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives**|Zhihu Wang et.al.|[2408.06904](http://arxiv.org/abs/2408.06904)|null|随着大规模语言模型（LLM）的持续扩展，它们在性能上的增强往往不足以解决特定领域的任务。系统性地分析这些失败并有效提升其性能仍然是一个重大挑战。本文提出了Re-TASK框架，这是一种新颖的理论模型，从能力、技能、知识的角度重新审视LLM任务，遵循布卢姆分类法和知识空间理论的原则。Re-TASK框架提供了一种系统的方法来深化我们对LLM的理解、评估和提升，特别针对特定领域任务。它探索了LLM的能力、处理的知识以及应用的技能之间的相互作用，阐明了这些元素如何相互关联并影响任务表现。  通过应用Re-TASK框架，我们揭示了许多特定领域任务失败的原因主要归咎于知识不足或技能适应度不够。基于这一洞察，我们提出了一系列结构化的策略来增强LLM，通过有针对性的知识注入和技能适应。具体而言，我们识别与任务相关的关键能力项，并采用精心设计的提示策略来提升任务性能，从而减少大量微调的需求。或者，我们使用能力特定指令对LLM进行微调，进一步验证了框架的有效性。实验结果证实了框架的有效性，展示了显著提高LLM在性能和适用性方面的效果。|
|**2024-08-13**|**Leveraging Language Models for Emotion and Behavior Analysis in Education**|Kaito Tanaka et.al.|[2408.06874](http://arxiv.org/abs/2408.06874)|null|分析学生的情绪和行为对于提升学习效果与个性化教育体验至关重要。传统方法往往依赖于对侵入性的视觉和生理数据收集，这引发了隐私问题并限制了规模性应用。本文提出了一种新颖的方法，利用大型语言模型（LLMs）和提示工程来分析学生的文本数据。我们的策略通过定制的提示引导LLMs检测情感和参与状态，提供一种非侵入性、可扩展的解决方案。我们使用Qwen、ChatGPT、Claude2和GPT-4进行了实验，将我们的方法与基础模型和链式思考（CoT）提示进行了比较。结果表明，我们的方法在准确性和上下文理解方面均显著优于基线模型。这项研究强调了大型语言模型结合提示工程在提供实用有效工具以进行教育情绪和行为分析方面的潜力。|
|**2024-08-13**|**LoRA $^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models**|Jia-Chen Zhang et.al.|[2408.06854](http://arxiv.org/abs/2408.06854)|null|细调大规模语言模型（LLMs）以实现高参数效率并应用于下游任务已成为新的研究方向。低秩适应（LoRA）显著降低了细调时的可训练参数数量。尽管它在性能上表现出色，但在复杂下游任务中，仅在单一尺度上调参可能并非最优策略。  本文提出了一种扩展LoRA的方法，称为LoRA$^2$。首先，通过结合正交投影理论，我们训练了两组在相互正交平面上的LoRA集合。然后，我们改进了重要性评分算法，该算法大约减少了98.5%的参数敏感度计算。通过去除具有较低重要性分数的奇异值，从而提高了对各种下游任务的适应能力。  我们在两个广泛使用的预训练模型上进行了大量实验，以验证LoRA$^2$ 的有效性。结果显示，与全量细调相比，它仅将可训练参数数量减少至0.72%，同时仍能展现出令人印象深刻的性能。即使进一步将参数减少至0.17M，其结果也与基线模型（参数量多出8倍）相当。  我们的代码已在此处提供：<https://anonymous.4open.science/r/LoRA-2-5B4C>|
|**2024-08-13**|**Causal Agent based on Large Language Model**|Kairong Han et.al.|[2408.06849](http://arxiv.org/abs/2408.06849)|**[link](https://github.com/kairong-han/causal_agent)**|**大型语言模型（LLM）在各个领域取得了显著成功。然而，因果问题的内在复杂性和因果理论使得用自然语言准确描述它们变得困难，这阻碍了LLM有效地理解和使用它们的能力。用自然语言传达因果方法并不容易，这限制了LLM应用它们的准确性。此外，因果数据集通常以表格形式存在，而LLM在处理自然语言数据方面表现出色，这种结构上的不匹配妨碍了对表格数据的有效推理。缺乏因果推理能力限制了LLM的发展。  为了解决这些问题，我们为LLM配备了因果工具，并将其置于一个代理框架中，称为“因果代理”。该代理包括工具、记忆和推理模块。在工具模块中，因果代理通过将表格数据与自然语言对齐来应用因果方法。在推理模块中，因果代理采用ReAct框架多次迭代使用这些工具进行推理。在记忆模块中，因果代理维护了一个字典实例，其中键是唯一的名称，值是因果图。  为了验证因果代理的因果能力，我们建立了一个基准，包括四个层次的因果问题：变量级别、边级别、因果图级别和因果效应级别。我们使用ChatGPT-3.5生成了1300个针对这四个层次问题的测试数据集，并测试了因果代理。我们的方法在四个层次的因果问题上表现出极高的有效性，准确率均超过80%。  为了进一步洞察和实现细节，我们的代码可通过GitHub仓库https://github.com/Kairong-Han/Causal_Agent获取。**|
|**2024-08-12**|**Animate, or Inanimate, That is the Question for Large Language Models**|Leonardo Ranaldi et.al.|[2408.06332](http://arxiv.org/abs/2408.06332)|null|人类的认知核心与“有生命性”这一概念紧密相连，它在塑造记忆、视觉以及多层次语言理解方面发挥着关键作用。虽然“有生命性”在语言中通过动词和形容词的细微约束体现出来，但其学习和精炼过程也依赖于非语言信息。同样地，我们假设大模型在处理“有生命性”时能力有限的原因是它们仅以文本数据进行训练。因此，这篇论文旨在探讨的问题是：大模型是否能够以类似于人类的方式处理“有生命性”？我们通过提示方法进行了系统分析。具体来说，我们通过提示大模型在不同的有生命、无生命、常见和异常情境下进行操作。结果显示，尽管大模型主要基于文本数据进行训练，但在面对典型的有生命体和无生命体时，它们展现出与先前研究一致的人类行为模式。因此，大模型能够适应理解非典型情况，通过识别异常情况为有生命体，而无需依赖人类依赖的未言明的认知触发机制来分解动画。|
|**2024-08-12**|**Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example**|Yanan Chen et.al.|[2408.06318](http://arxiv.org/abs/2408.06318)|null|本文旨在填补大型语言模型（LLM）在自主代理与人工通用智能（AGI）接近过程中研究的空白。尽管LLM展现出出色的泛化能力和涌现能力，但目前缺乏对LLM驱动的代理行为、潜在失败原因以及如何提升其性能的研究，尤其是在具有挑战性的现实世界规划任务中的表现。为了填补这一缺口，我们利用了一个名为TravelPlanner的真实基准，其中的代理必须满足多个约束以生成准确的计划。通过TravelPlanner基准，我们针对四个关键研究问题进行了全面的实验：（1）LLM代理在处理长篇和嘈杂上下文时，对于推理和规划的鲁棒性是否足够？（2）少量提示能否对具有长上下文的场景产生负面影响？（3）我们能否依赖细化来改善计划？（4）是否可以使用正负反馈相结合的方法对LLM进行微调，从而进一步提高性能？  实验结果表明：首先，尽管LLM能够处理大量的参考信息和少量示例，但在处理长篇上下文时，它们往往无法关注关键部分；其次，它们仍然难以分析长期规划，并不能提供准确的反馈供细化使用；第三，我们提出了一种称为反馈感知微调（FAFT）的方法，该方法利用了正负反馈，相较于监督式微调（SFT），它能带来显著的性能提升。我们的发现为社区提供了有关现实世界规划应用方面的深入见解。|
|**2024-08-12**|**The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery**|Chris Lu et.al.|[2408.06292](http://arxiv.org/abs/2408.06292)|**[link](https://github.com/sakanaai/ai-scientist)**|**本文提出了一种全面框架，旨在实现完全自动的科学发现，使前沿的大规模语言模型能够独立进行研究，并传达其研究成果。我们引入了“AI科学家”这一概念，它能生成新颖的研究思路，编写代码，执行实验，可视化结果，撰写完整的科学论文，并进行模拟的同行评审过程以进行评估。理论上，这一过程可以迭代进行，以开放性方式发展想法，就像人类的科学社区一样。  通过将其应用于机器学习的三个不同子领域：扩散建模、基于转换器的语言建模和学习动态，展示了其灵活性。每一篇论文的开发成本低于15美元。为了评估生成的论文，我们设计并验证了一个自动审稿人，结果显示它在评价论文分数方面接近人类水平表现。AI科学家能够产生超过顶级机器学习会议接受阈值的论文，这是由我们的自动审稿人判断的。这一方法标志着机器学习领域科学研究新纪元的开始：将AI代理的变革性优势带入整个研究过程，使我们更接近一个能够释放解决世界最艰巨问题的无限可负担创新与创造力的世界。所有代码已开源在https://github.com/SakanaAI/AI-Scientist。**|
|**2024-08-12**|**MovieSum: An Abstractive Summarization Dataset for Movie Screenplays**|Rohit Saxena et.al.|[2408.06281](http://arxiv.org/abs/2408.06281)|**[link](https://github.com/saxenarohit/moviesum)**|**电影剧本的概述是一个挑战，因为它要求理解长输入上下文和电影特有的各种元素。大型语言模型在文档概述方面已经取得了显著进展，但它们往往在处理长输入上下文时遇到困难。此外，虽然最近的研究关注电视脚本，但电影剧本概述仍然缺乏探索。为了激发这一领域的研究，我们提出一个名为MovieSum的新数据集，用于电影剧本的抽象概述。这个数据集包含了2200个电影剧本及其对应的维基百科剧情概述。我们人工格式化了电影剧本以表示其结构元素。与现有的数据集相比，MovieSum具有几个独特特点：（1）它包括电影剧本，这些剧本比电视剧脚本更长。（2）它的规模是之前电影剧本数据集的两倍。（3）它提供了IMDb ID等元数据，方便获取额外的外部知识。我们还展示了最近发布的大型语言模型在我们的数据集上进行概述的结果，以提供详细的基准。**|
|**2024-08-13**|**Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation**|Jieyong Kim et.al.|[2408.06276](http://arxiv.org/abs/2408.06276)|**[link](https://github.com/jieyong99/exp3rt)**|近期，大型语言模型（LLM）在各类任务中的卓越表现引起了广泛关注，并激发了它们在推荐系统领域的应用潜力。然而，现有方法并未充分利用LLM的潜力，往往受限于输入信息的有限性，未能全面发挥其高级推理能力。为解决这些问题，我们提出了一种名为EXP3RT的新颖LLM推荐系统，旨在利用用户和物品评论中蕴含的丰富偏好信息。  EXP3RT通过从教师LLM中进行知识蒸馏进行微调，以执行关键的三项任务：首先，它从原始评论中提取并封装核心的主观偏好；其次，根据特定标准聚合和总结这些偏好，形成用户和物品的档案；最后，考虑用户/物品档案以及物品描述中的主客观信息，生成详细的推理步骤和预测评级，即基于推理的评级预测。这种由EXP3RT提供的个性化偏好推理能够提高评级预测的准确性，并为推荐系统提供忠实且合理的解释。  广泛实验表明，EXP3RT在评级预测和候选项目重排序（用于top-k推荐）方面均超越了现有方法，同时显著提升了推荐系统的可解释性。|
|**2024-08-12**|**FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data**|Haoran Sun et.al.|[2408.06273](http://arxiv.org/abs/2408.06273)|**[link](https://github.com/tjunlp-lab/fuxitranyu)**|大型语言模型（LLM）在各种任务中展现出了强大的能力。然而，许多LLM在高资源和低资源语言之间的性能存在显著差异。为解决这一挑战，我们提出了一种开源多语言LLM——FuxiTranyu，旨在满足研究社区对平衡且高性能多语言能力的需求。FuxiTranyu-8B，具有80亿参数的基模，从头开始训练在一个精心平衡的多语言数据仓库上，该仓库包含覆盖43种自然语言和16种编程语言的6000亿个令牌。此外，我们还开发了两个指令调优模型：FuxiTranyu-8B-SFT，它基于多元指令数据集进行微调；以及FuxiTranyu-8B-DPO，在偏好数据集上进一步精炼以增强对齐能力的DPO。广泛实验在多种多语言基准上的结果显示，FuxiTranyu在与现有多语言LLM（如BLOOM-7B、PolyLM-13B、Llama-2-Chat-7B和Mistral-7B-Instruct）的比较中表现出竞争性性能。神经元级和表示级可解释性分析表明，FuxiTranyu能够在不同语言之间学习一致的多语言表示。为了促进对多语言LLM及其工作机制的研究，我们发布了基模和指令调优的FuxiTranyu模型，以及58个预训练检查点，通过HuggingFace和Github公开分享。|
|**2024-08-12**|**A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution**|Sampath Rajapaksha et.al.|[2408.06272](http://arxiv.org/abs/2408.06272)|null|在不断演进的网络安全领域，分析师需要密切关注最新的攻击趋势和相关信息，以协助调查与归因网络攻击。本文提出了一种基于检索增强生成（RAG）技术的问答模型及其应用，旨在为网络安全专家提供有关网络攻击调查与归因的信息。我们的问答模型结合了大型语言模型（LLM）和知识库（KB），能够根据知识库或用户提供的外部资源回答用户的查询。  我们通过各种类型的提问，包括基于知识库、元数据、知识库中的特定文档以及外部资源的提问，对我们的问答模型进行了测试与评估。我们将知识库为基础的问题的答案与OpenAI的GPT-3.5及最新GPT-4的LLM答案进行了比较。结果显示，我们的问答模型在提供答案的同时给出了来源信息，并且克服了GPT模型可能产生的幻觉问题，这对于网络攻击的调查与归因至关重要。此外，我们的分析表明，当RAG问答模型在查询之外提供少量示例时，其生成的答案质量通常优于仅提供查询而没有示例的情况。|
|**2024-08-12**|**Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment**|Karel D'Oosterlinck et.al.|[2408.06266](http://arxiv.org/abs/2408.06266)|**[link](https://github.com/contextualai/clair_and_apo)**|大型语言模型（LLM）通常使用对比性对齐目标和偏好对数据集进行对齐。这一过程涉及到模型、配对数据以及目标之间的交互，使得对齐变得复杂，并有时导致不理想的成果。我们对此进行了研究，发现（i）当底层响应具有对比性时，偏好数据提供了更好的学习信号；（ii）对齐目标在训练期间为模型提供了更多的控制，从而导致了更好的性能。基于这些洞察，我们引入了对比学习从AI修订（CLAIR），一种数据创建方法，可以生成更具有对比性的偏好对，以及锚定偏好优化（APO），一个更具可控性和稳定性的对齐目标。我们使用各种可比较的数据集和对齐目标来对Llama-3-8B-Instruct进行对齐，并测量了与人类判断高度相关的MixEval-Hard分数。CLAIR偏好导致所有数据集中的最佳性能，而APO始终优于较少可控的目标。通过在32K CLAIR偏好上使用APO进行训练，我们的最佳模型提高了Llama-3-8B-Instruct的性能达7.65%，将与GPT4-turbo的差距缩小了45%。我们的代码已发布于https://github.com/ContextualAI/CLAIR_and_APO。|
|**2024-08-12**|**On Effects of Steering Latent Representation for Large Language Model Unlearning**|Dang Huu-Tien et.al.|[2408.06223](http://arxiv.org/abs/2408.06223)|**[link](https://github.com/RebelsNLU-jaist/llm-unlearning)**|本文首先通过理论分析证明了引导模型中间层遗忘表示向随机方向偏移，能降低文本生成的置信度，导致大型语言模型（LLM）产生错误或无意义的回答。其次，我们探讨了系数如何影响遗忘样本表示与随机方向的一致性，并暗示了不同网络层下有效的最优系数值，以实现高效的学习撤销。接着，我们展示了利用代表错乱法（RMU）进行学习撤销后的模型能够抵御对抗性逃脱攻击。  最后，我们的实证分析表明，当应用于大型语言模型的中间和后期层时，RMU的有效性较低。为了解决这一问题，我们提出了一种简单而有效的方法——自适应RMU，该方法使大多数层都能够实现高效的学习撤销，且不增加额外的计算成本。广泛的实验结果表明，与先前的研究相比，自适应RMU显著提高了学习撤销的性能。|
|**2024-08-12**|**Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting**|Halley Young et.al.|[2408.06186](http://arxiv.org/abs/2408.06186)|null|生成多样化的文本是大型语言模型（LLM）面临的关键挑战。到目前为止，多样性的研究主要通过 $n$ -gram多样性或BERT嵌入的多样性等指标进行，但这些方法在考虑多样性的维度上缺乏用户控制权。例如，在诗歌领域，用户可能希望在押韵和节奏方面实现多样性，而在代码领域，用户可能更关注解决问题时所使用的表达方式的多样性。  为此，我们提出了一种名为结构多样性（Structural Diversity）的新指标。该指标允许用户提供一个映射，将生成的文本转换为捕获用户关心的多样性的特征。这样，用户可以更具体地控制他们想要探索的多样性维度，如在诗歌领域关注押韵和节奏，在代码领域关注特定的表达方式等。  此外，我们还提出了一个名为链式规范（Chain-of-Specification，CoS）的新型策略，用于通过首先让LLM生成描述特定结构特征实例的规范，然后引导LLM生成满足这些特征的文本来提高多样性；值得注意的是，我们的策略适用于黑盒LLM。在我们的实验中，我们展示了在诗歌和代码领域实现结构多样性时，CoS策略相比多个基线显著提高了多样性。|
|**2024-08-10**|**Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions**|Michele Miranda et.al.|[2408.05212](http://arxiv.org/abs/2408.05212)|**[link](https://github.com/michele17284/awesome-privacy-preserving-llms)**|大型语言模型（LLM）在人工智能领域取得了重大进步，并在多个领域找到了应用。然而，它们依赖于庞大的互联网来源数据集进行训练，这带来了显著的隐私问题，尤其是在关键领域（如医疗保健）的情况下会加剧这些问题。此外，在特定应用场景下，可能需要对这些模型进行针对私有数据的微调。本文对LLM的隐私威胁进行了批判性评估，强调了这些模型可能记住并无意间泄露敏感信息的风险。  我们通过回顾针对LLM的隐私攻击来探讨当前的威胁，并提出全面的解决方案，以在整个学习管道中整合隐私机制。这些解决方案涵盖了从匿名化训练数据到在训练或推理过程中实施差分隐私，以及在训练后执行机器遗忘的范围。我们的文献综述深入研究了现有研究中的持续挑战、可用工具和未来方向，以保护LLM中的隐私。这项工作旨在通过提供对隐私保存方法及其在减轻风险方面的有效性的全面理解，指导开发更安全、更可信的AI系统。|
|**2024-08-09**|**VITA: Towards Open-Source Interactive Omni Multimodal LLM**|Chaoyou Fu et.al.|[2408.05211](http://arxiv.org/abs/2408.05211)|**[link](https://github.com/VITA-MLLM/VITA)**|在这篇论文中，我们引入了VITA，这是首个开源多模态大型语言模型，能够同时处理和分析视频、图像、文本和音频等多元模态信息，并且具备高级的多模态交互体验。从Mixtral 8x7B作为语言基础出发，我们扩展了其在中文方面的词汇，并通过双语指令微调进一步提升了模型能力。我们还通过两阶段多任务学习的方式，为语言模型赋予了视觉和音频处理的能力。  VITA展现了强大的多语言、视觉和音频理解的基础能力，并在一系列单模态与多模态基准测试中表现出色。除了基础能力外，我们在提升自然多模态人机交互体验方面也取得了显著进展。据我们所知，这是首次在多模态大型语言模型中利用非唤醒交互和音频中断功能。  VITA是开源社区探索无缝融合多模态理解和交互的第一步。尽管VITA与专有模型还有较大差距，但我们相信它作为先锋角色可以成为后续研究的重要基石。项目页面：https://vita-home.github.io|
|**2024-08-09**|**Evaluating the capability of large language models to personalize science texts for diverse middle-school-age learners**|Michael Vaccaro Jr et.al.|[2408.05204](http://arxiv.org/abs/2408.05204)|null|近期，大型语言模型（LLMs），尤其是OpenAI的GPT系列，在多个领域取得了显著进步。这些模型因其在不同学科领域的专业知识以及对用户提示的快速适应性而受到关注，并且展现出作为个性化学习（PL）工具的独特潜力。然而，它们在K-12教育中的应用仍处于探索阶段。  本文介绍了一项首次采用随机对照试验方法（样本量为23）来评估GPT-4在中学科学文本个性化方面的有效性的研究。在该研究中，GPT-4用于根据学生在训练阶段做出的选择来分析和预测他们的学习偏好。对于实验组的学生，GPT-4被用来修改科学文本以与学生的预测偏好相匹配；而对于控制组的学生，文本则被修改为与其学习偏好相反。通过曼-惠特尼U检验，研究发现，当文本与学生偏好匹配时，学生明显更倾向于接受（在0.10水平上具有统计学意义，p=0.059）。这些结果表明，GPT-4能够有效地理解和定制教育内容以满足不同学习者的偏好，标志着个性化学习技术领域的一个重要进展。  此外，文章还讨论了这项研究的局限性和在教育中使用人工智能的伦理考虑。|
|**2024-08-09**|**TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning**|Yujie Feng et.al.|[2408.05200](http://arxiv.org/abs/2408.05200)|**[link](https://github.com/WoodScene/TaSL)**|语言模型连续学习（CL）最近引起了广泛关注，因为它有可能在无需重新训练的情况下，适应大型语言模型（LLMs）的动态现实环境。一个关键挑战是灾难性遗忘，即模型在学习新任务时会失去先前获得的知识。现有方法通常使用多个参数效率微调（PEFT）块来为每个任务获取特定于任务的知识，但这些方法缺乏效率，并且忽视了通过任务交互进行知识传递的可能性。  在这篇论文中，我们提出了一种名为任务技能定位与整合（TaSL）的新CL框架，它通过不依赖于记忆重播来增强知识传递。TaSL首先根据参数依赖性将模型分为“技能单元”，这使得对技能单元的控制更加精细。然后，它采用了一种新颖的组级技能定位技术，以识别新任务中技能单元的重要性分布。通过比较这个重要性分布与其他先前任务中的分布，我们实施了一个精细的技能整合策略，保留了特定于任务的知识，从而防止遗忘，并更新了共享任务知识，这促进了双向知识传递。因此，TaSL实现了保持先前知识和在新任务上取得优异表现之间的最佳平衡。  TaSL也展示了强大的泛化能力，适用于通用模型，并可以根据LoRA等PEFT方法进行定制。此外，它还表现出显著的扩展性，允许与记忆重播集成以进一步提高性能。在两个CL基准测试中，使用不同规模的模型（从2.2亿到70亿参数），广泛的实验证明了TaSL及其变体在不同设置下的有效性。|
|**2024-08-09**|**AttackER: Towards Enhancing Cyber-Attack Attribution with a Named Entity Recognition Dataset**|Pritam Deka et.al.|[2408.05149](http://arxiv.org/abs/2408.05149)|null|在网络安全领域，攻击归因是至关重要的过程，它允许专家制定针对攻击者的防御措施和法律行动。目前，分析人员主要通过手动操作来进行归因，这主要是由于任务的复杂性。人工智能，尤其是自然语言处理（NLP）技术可以被用来辅助网络安全分析师在归因过程中进行工作。尽管这些技术非常强大，但在缺乏攻击归因领域的数据集的情况下，它们需要应对挑战。在本文中，我们将填补这一空白，并提供到目前为止我们所知的第一个攻击归因数据集。我们的数据集设计的主要目标是从网络安全文本中提取攻击归因信息，利用NLP领域的命名实体识别（NER）方法。与其它网络安全NER数据集不同，我们的数据集提供了丰富且包含上下文细节的注释，包括一些跨短语和句子的注释。我们进行了大量实验，并应用了NLP技术来展示数据集在攻击归因方面的有效性。这些实验突显了大型语言模型（LLM）能力在改进网络安全数据集中的NER任务以提升攻击归因能力的潜力。|
|**2024-08-09**|**A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning**|Ye Yuan et.al.|[2408.05141](http://arxiv.org/abs/2408.05141)|null|在这篇论文中，我们提出了一种综合优化的增强检索辅助生成（RAG）系统，旨在通过集成外部知识库显著提高大型语言模型（LLM）的准确性和降低幻觉现象。我们的系统进行了多项改进，包括对网页中的文本段落和表格进行细化处理、引入属性预测器以减少幻觉、构建LLM知识抽取器和知识图谱抽取器，并最终建立了一个整合所有参考信息的推理策略。我们通过Meta CRAG KDD杯2024竞赛中的CRAG数据集对系统进行了评估。本地与在线评估均表明，我们的系统在复杂推理能力上实现了显著提升。在本地评估中，相较于基线模型，我们的系统在准确性方面有显著提升，错误率也有所下降，取得了较高的分数。同时，在线评估结果同样表现优异，证明了所提出系统的性能和泛化能力。该系统的源代码已发布于\url{https://gitlab.aicrowd.com/shizueyy/crag-new}。|
|**2024-08-09**|**Is ChatGPT a Good Software Librarian? An Exploratory Study on the Use of ChatGPT for Software Library Recommendations**|Jasmine Latendresse et.al.|[2408.05128](http://arxiv.org/abs/2408.05128)|null|在软件系统功能、效率与可维护性方面，软件库扮演着至关重要的角色。随着开发者越来越多地依赖大型语言模型（LLMs）以简化编码流程，这些模型推荐合适库的有效性仍处于探索阶段。本文评估了ChatGPT作为软件图书馆员的有效性，并识别了改进空间。我们通过使用GPT-3.5 Turbo生成针对10000个Stack Overflow问题的Python代码，进行了一项实证研究。我们的发现表明，ChatGPT比人类开发者更频繁地使用第三方库，倾向于广泛采用且历史悠久的选择。然而，14.2%推荐的库具有限制性的Copyleft许可，这并未由ChatGPT明确传达。此外，有6.5%的库无法直接使用，可能导致开发者困惑和浪费时间。尽管ChatGPT可以作为有效的软件图书馆员，但应提供关于维护指标和许可的更多明确信息。我们建议开发者实施严格的依赖管理实践，并在将LLM生成的代码集成到项目中之前，仔细检查库的许可证。|
|**2024-08-09**|**Large Language Models and Thematic Analysis: Human-AI Synergy in Researching Hate Speech on Social Media**|Petre Breazu et.al.|[2408.05126](http://arxiv.org/abs/2408.05126)|null|在人工智能的快速演进领域，大型语言模型（LLMs）在文本分析中的发展与应用引起了学术界的广泛关注。尽管各种LLMs在进行定性分析时展现出的潜力被寄予厚望，但它们在人文学科和社会科学中的应用并未得到充分探讨。本文通过一项以GPT-4为核心的研究实验，为LLMs在定性分析领域的应用提供了新的视角。研究基于一个来自欧盟资助项目的YouTube数据集，该数据集聚焦于2016年瑞典罗马尼亚移民群体的代表形象，这一时期正值2015年难民危机之后，紧邻2017年的瑞典全国选举。我们的研究旨在探索将人类智慧与AI的规模和效率相结合的可能性，通过分析LLMs在人文学科和社会科学领域的应用优劣，并讨论未来可能的发展方向。|
|**2024-08-09**|**Sportify: Question Answering with Embedded Visualizations and Personified Narratives for Sports Video**|Chunggi Lee et.al.|[2408.05123](http://arxiv.org/abs/2408.05123)|null|随着篮球运动的普及，粉丝们常常因比赛节奏快和复杂度高而感到困惑。篮球战术涉及一系列复杂的动作，需要大量的知识才能完全理解。这种复杂性导致了对额外信息和解释的需求，这可能会分散粉丝们对比赛的关注。为解决这一挑战，我们提出了一种名为Sportify的视觉问答系统，它融合了叙事和嵌入式可视化，旨在为球迷提供篮球战术疑问的清晰解答，帮助他们理解比赛的各种方面。我们提出了三种新型的动作可视化（传球、切入和掩护），以展示关键动作序列。为了解释球员行动背后的原因和逻辑，我们利用大型语言模型（LLM）生成叙事文本。我们采用故事讲述的方法来描述复杂场景，从第一人称和第三人称的角度进行叙述，并融入动作可视化。我们通过与篮球粉丝的评估，探讨了Sportify在深化战术洞察力和增强观赛体验方面的效果。此外，第三人称叙述有助于人们获得深入的比赛解释，而第一人称叙述则增强了粉丝们对比赛的参与感。|
|**2024-08-09**|**A Survey of NL2SQL with Large Language Models: Where are we, and where are we going?**|Xinyu Liu et.al.|[2408.05109](http://arxiv.org/abs/2408.05109)|**[link](https://github.com/hkustdial/nl2sql_handbook)**|翻译如下：  自然语言查询到SQL查询（即NL2SQL）的翻译可以显著降低访问关系数据库的障碍，并支持各种商业应用。随着大型语言模型（LLMs）的出现，NL2SQL的性能得到了大幅提升。本文提供了一个全面的NL2SQL技术综述，基于LLMs驱动，覆盖了从四个方面对整个生命周期的全面审查：（1）模型：处理自然语言的模糊性和不充分性，并正确映射自然语言与数据库模式和实例；（2）数据：从收集训练数据、应对训练数据稀缺的数据合成，到NL2SQL基准；（3）评估：从多个角度使用不同指标对NL2SQL方法进行评估；（4）错误分析：分析NL2SQL错误以找到根本原因，并指导NL2SQL模型发展。此外，我们提供了开发NL2SQL解决方案的一条经验法则。最后，讨论了在LLMs时代NL2SQL的研究挑战和开放问题。  请注意，摘要中已去除所有不必要的字符，包括","符号。|
|**2024-08-08**|**Better Alignment with Instruction Back-and-Forth Translation**|Thao Nguyen et.al.|[2408.04614](http://arxiv.org/abs/2408.04614)|null|我们提出了一种新的方法——指令双向翻译，用于构建基于世界知识的高质量合成数据，以对大型语言模型（LLMs）进行对齐。给定网络语料库中的文档，我们使用了Li等人(2023a)提出的回译方法生成并整理合成指令，并通过根据初始文档进一步改进响应的质量来重写这些指令。通过使用产生的（回译指令，重写响应）对进行微调，我们在AlpacaEval上的获胜率高于使用其他常见指令数据集（如Humpback、ShareGPT、Open Orca、Alpaca-GPT4和Self-instruct）。我们也展示了用LLM重写响应优于直接的蒸馏方法，并且生成的文本分布在这两个方面之间存在显著差异。进一步的分析表明，我们的回译指令的质量比其他合成指令来源更高，而我们的响应在多样性与复杂性上比从蒸馏获得的结果更为出色。总体而言，我们发现指令双向翻译结合了网络上信息多样性和数量的优势，同时确保了响应的质量，这是有效对齐所必需的。|
|**2024-08-09**|**Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models**|Qirui Jiao et.al.|[2408.04594](http://arxiv.org/abs/2408.04594)|**[link](https://github.com/modelscope/data-juicer)**|**本文提出了一种名为Img-Diff的新数据集，旨在通过对比学习和图像差异描述的方法来增强大型语言模型在细微图像识别任务中的性能。该方法通过分析相似图像间的对象差异，要求模型识别相同与不同之处。利用Stable-Diffusion-XL模型及高级图像编辑技术生成突出对象替换的相似图像对。数据生成流程包括差异区域生成器识别对象差异，随后差异描述生成器提供详细的差异说明。结果是创建了一个小而高质量的“对象替换”样本集合。使用此数据集对当前最佳的多模态大语言模型（如MGM-7B）进行微调，显著提高了这些模型在图像差异和视觉问答任务上的表现分数，超越了基于大规模数据集训练的当前最佳模型（如GPT-4V和Gemini）在MMVP基准测试中的表现。此外，本文还探讨了通过“对象移除”方法生成图像差异数据的替代方法，并进行了全面评估以验证数据集的多样性和质量，提供了关于此类对比性数据集合成的深入见解。为了促进进一步的研究并推动多模态数据合成和增强大型语言模型基础能力的发展，我们已将代码和数据集发布在https://github.com/modelscope/data-juicer/tree/ImgDiff上供公众使用。**|
|**2024-08-08**|**Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness**|Xiaojing Fan et.al.|[2408.04585](http://arxiv.org/abs/2408.04585)|null|随着大型语言模型（LLM）实用应用需求的增加，许多关注效率的模型被开发出来以平衡性能和计算成本。然而，这些模型的对抗鲁棒性仍然缺乏深入研究。本研究设计了一个框架，通过比较三个具有不同复杂度和效率水平的主要模型——Transformer++、门控线性注意力（GLA）变换器以及MatMul-Free LM，来探索效率、性能与对抗鲁棒性的权衡关系。利用GLUE和AdvGLUE数据集进行比较。AdvGLUE数据集通过添加旨在挑战模型鲁棒性的对抗样本扩展了GLUE数据集。  我们的结果表明，在GLUE任务上的准确性稍低的情况下，GLA变换器和MatMul-Free LM在AdvGLUE任务上显示出更高的效率，并且在不同攻击级别下，它们的鲁棒性要么优于，要么与Transformer++相匹敌。这些发现强调了简化架构在实现高效能、高性能与对抗鲁棒性之间取得良好平衡的可能性，为资源受限环境和对对抗攻击有高抵抗力需求的应用提供了有价值的见解。|
|**2024-08-08**|**SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals**|Haoran Zheng et.al.|[2408.04575](http://arxiv.org/abs/2408.04575)|null|解释性人工智能（XAI）对于增强人工智能模型的透明度和责任性至关重要，尤其是在自然语言处理（NLP）任务中。本文提出了一种名为SCENE（软反事实评估用于自然语言可解释性）的新方法，该方法利用大型语言模型（LLMs）在零次射击的情况下生成软反事实解释。通过关注基于词元的替换，SCENE创建了上下文相关且语义上具有意义的软反事实，而无需进行大量微调。SCENE采用有效性软和C软指标来评估各种模型无关的XAI方法在文本分类任务中的效果。应用于CNN、RNN和BERT架构，SCENE提供了对各种XAI技术强项和局限性的有价值见解。|
|**2024-08-08**|**Learning Fine-Grained Grounded Citations for Attributed Large Language Models**|Lei Huang et.al.|[2408.04568](http://arxiv.org/abs/2408.04568)|**[link](https://github.com/luckyyysta/fine-grained-attribution)**|**尽管大型语言模型（LLM）在信息查询任务上表现出色，但它们仍然在幻觉问题上存在挑战。基于属性的LLM，通过在生成文本中添加内联引用，显示出减少幻觉并提高可验证性的潜力。然而，当前的方法在生成高质量引用方面效果不佳，这主要是由于它们依赖于上下文学习。此外，只引用粗粒度文档标识的做法使得用户难以进行精细验证。为此，我们提出了FRONT框架，旨在教导LLM生成细粒度相关引用。这些引用通过连接到生成响应的细粒度支持引用来提供指导，不仅提高了引用质量，还便于进行精细验证。在ALCE基准测试上的实验结果表明，FRONT在生成优秀相关响应和高度支持性引用方面非常有效。使用LLaMA-2-7B时，该框架显著优于所有基线，平均提高了14.21%的引用质量，并且超越了ChatGPT。**|
|**2024-08-08**|**Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models**|Yupeng Chang et.al.|[2408.04556](http://arxiv.org/abs/2408.04556)|**[link](https://github.com/cyp-jlu-ai/ba-lora)**|**大型语言模型（LLM）在各种自然语言处理（NLP）任务上表现出令人瞩目的能力。然而，在将这些模型应用于下游应用时，通常需要进行计算密集型和内存消耗大的微调过程。为了解决这个问题，参数高效微调（PEFT）技术已经作为一种有前景的方法出现，旨在以最小的计算成本来定制LLM。尽管PEFT方法提供了显著的优势，但它们并未完全解决从预训练数据继承偏见的问题。为此，我们提出了一种新的PEFT方法——Bias-Aware Low-Rank Adaptation (BA-LoRA)，旨在对抗偏见继承。  BA-LoRA整合了三个不同的正则化项：一致性正则化器、多样性正则化器以及奇异值分解正则化器。这三个正则化器共同旨在提高生成模型在微调过程中的一致性、多样性和泛化能力。通过在多种自然语言理解（NLU）和自然语言生成（NLG）任务上的广泛实验，并使用如LLaMA、Mistral和Gemma等主流LLM，我们展示了BA-LoRA在性能上超越了LoRA及其最先进的变体。此外，我们的方法有效地减轻了预训练偏见的负面影响，导致更可靠且稳健的模型输出。相关代码已开源在https://github.com/cyp-jlu-ai/BA-LoRA。**|
|**2024-08-08**|**Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models**|Fabio Pernisi et.al.|[2408.04522](http://arxiv.org/abs/2408.04522)|null|随着不同语言的多元语言社区和用户采用大型语言模型（LLM），评估这些模型在不同语言环境下的安全性变得至关重要。尽管已经进行了持续的努力以确保LLM的安全性，但它们仍然可以通过“越狱”技术来表现得不安全，这是一种促使模型在其操作准则之外行动的技术。对于LLM安全性以及“越狱”的研究目前主要集中在英语上，这限制了我们对其他语言中LLM安全性的理解。为了填补这一空白，我们通过在意大利语中研究多轮“越狱”的有效性，即使用不安全示例来诱导不安全行为，来贡献于这一领域。为了支持我们的分析，我们创建了一个新的意大利语问题-答案不安全数据集。利用这个数据集，我们在四个开放权重LLM家族中识别出了明显的安全漏洞。我们发现，即使在使用少量不安全示例的情况下，模型也会表现出不安全的行为，并且更令人担忧的是，随着更多示例的出现，这种趋势迅速加剧。|
|**2024-08-08**|**What You Need is What You Get: Theory of Mind for an LLM-Based Code Understanding Assistant**|Jonan Richards et.al.|[2408.04477](http://arxiv.org/abs/2408.04477)|null|在大型语言模型（LLM）用于辅助开发者理解代码的工具数量不断增加的同时，开发者在使用这些工具时仍面临一些障碍，包括用自然语言描述其意图的挑战、解读工具结果的困难，以及调整有效提示以获得有用信息的过程。为此，我们设计了一个基于LLM的对话助手，该助手根据推断出的用户心理状态（如背景知识和经验）提供个性化互动。通过针对十四位新手进行的内部主题研究，我们捕捉了他们的感知和偏好。研究结果为希望创建或改进面向新手的LLM为基础的对话助手以支持代码理解的研究人员和工具开发者提供了见解。|
|**2024-08-08**|**Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate**|Yiqun Zhang et.al.|[2408.04472](http://arxiv.org/abs/2408.04472)|**[link](https://github.com/zhangyiqun018/agent-for-debate)**|**在竞争性辩论这一全面且复杂的计算论辩任务中，大型语言模型（LLMs）面临着幻觉和竞争力不足的问题。为了应对这些挑战，我们提出了一种名为“辩论者”（Agent4Debate）的动态、多代理框架，该框架基于LLMs设计，旨在增强其在竞争性辩论中的能力。该框架受到人类在辩论准备与执行过程中行为的启发，采用协作架构，由四个专门的代理（搜索者、分析者、撰写者和审阅者）动态交互并合作。这四个代理在整个辩论过程中覆盖了从初始研究到论点形成、反驳和总结的多个阶段。  为了全面评估框架的性能，我们构建了一个名为“中国辩论竞技场”的数据库，包含了66个精心挑选的中文辩论议题。我们招募了十位经验丰富的专业辩论者，并收集了涉及Agent4Debate、基线模型和人类的200场辩论记录。评价体系采用了自动评分系统Debatrix以及基于Debatrix-Elo和Human-Elo排名的专业评审团。实验结果显示，最先进的Agent4Debate在能力上与人类相当。进一步的消融研究表明，代理结构中的每个组件的有效性。**|
|**2024-08-08**|**RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents**|Zihao Zhu et.al.|[2408.04449](http://arxiv.org/abs/2408.04449)|**[link](https://github.com/zihao-ai/earbench)**|摘要提出了一种名为RiskAwareBench的自动化框架，旨在评估基于大型语言模型（LLM）的实体化代理在物理风险意识方面的能力。该框架由四个模块组成：安全提示生成、危险场景生成、计划生成和评估，它允许进行全面的风险评估，且所需的人工干预最少。通过使用这个框架，构建了一个名为PhysicalRisk的数据集，涵盖了各种涉及相关安全提示、观察和指令的场景。  实验结果表明，大多数LLM在物理风险意识方面表现不足，并且基础的风险缓解策略带来的提升有限。这强调了在未来改进基于LLM的实体化代理的物理风险意识的紧迫性和重要性。|
|**2024-08-07**|**How Well Can Vision Language Models See Image Details?**|Chenhui Gou et.al.|[2408.03940](http://arxiv.org/abs/2408.03940)|null|大型语言模型驱动的视觉语言模型（LLM-驱动的VLM）在各种视觉语言理解任务上表现出色。然而，这些VLM是否能超越语义层面，深入观察图像细节仍然不明朗。为此，我们引入了一个像素值预测任务（PVP），以探索“视觉语言模型能够看到多细的图像细节？”并协助VLM提升对细节的感知能力。通常，这些模型由冻结的CLIP视觉编码器、大型语言模型和连接模块组成。在对PVP任务进行微调后，我们发现：1）现有的VLM仅通过微调连接模块和LLM，在预测精确像素值方面表现不佳；2）当视觉编码器也得到适应时，预测精度显著提高。此外，我们的研究揭示，将像素值预测作为VLM预训练任务之一，并对视觉编码器进行适应，显著提升了VLM在需要详细图像感知的下游图像语言理解任务上的性能，如引用图像分割（平均cIoU改进+10.19百分点）和视频游戏决策（在两个游戏中分别平均得分改善了+80.34和+70.54）。|
|**2024-08-07**|**SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature**|Vinícius Di Oliveira et.al.|[2408.03936](http://arxiv.org/abs/2408.03936)|null|自然语言处理（NLP）随着大型语言模型（LLMs）的兴起取得了显著进步。然而，对于英语之外的语言，尤其是在特定领域如Mercosur通用商品名称（NCM），巴西协调系统（HS）的应用方面，仍有很大的改进空间。为解决这一缺口，本研究利用TeenyTineLLaMA，一种基础葡萄牙语LLM，作为LLM源，实施NCM应用处理。此外，提出了一种针对任务特定微调的简化检索增强微调（SLIM-RAFT）技术。该方法采用简化的链式思维（CoT）策略进行提示开发，使用简短而集中的文档进行训练，以更紧凑和高效的方式进行。提出的模型在相同任务上显著优于TeenyTineLLaMA和ChatGPT-4，展示了较小LLM微调的高效和成本效益替代方案。尽管研究重点是NCM应用，但所提出的方法可以轻松地适应全球范围内的HS应用。|
|**2024-08-07**|**CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases**|Xiangyan Liu et.al.|[2408.03910](http://arxiv.org/abs/2408.03910)|**[link](https://github.com/modelscope/modelscope-agent)**|**大型语言模型（LLM）在诸如HumanEval和MBPP的独立代码任务中表现出色，但它们在处理整个代码仓库时存在挑战。这促使研究界探索如何在仓库级别上增强LLM与代码库的交互。目前的解决方案依赖于基于相似性的检索或手动工具和API，每种方法都有其显著的缺点。基于相似性的检索在复杂任务中召回率较低，而手动工具和API通常具有特定的任务性，并且需要专家知识，这降低了它们在不同代码任务和实际应用中的通用性。为了减轻这些限制，我们引入了\framework，这是一个系统，它将LLM代理与从代码仓库提取的图数据库接口集成在一起。通过利用图数据库的结构特性以及图查询语言的灵活性，\framework使LLM代理能够构建并执行查询，从而实现精确、代码结构意识的上下文检索和代码导航。我们使用三个基准测试评估\framework：CrossCodeEval、SWE-bench和EvoCodeBench。此外，我们还开发了五个真实世界的编码应用。凭借统一的图数据库模式，\framework在学术和实际环境中都展示了竞争力和潜力，体现了其在软件工程领域的多功能性和有效性。我们的应用演示：https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent。**|
|**2024-08-07**|**Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models**|Shachi H Kumar et.al.|[2408.03907](http://arxiv.org/abs/2408.03907)|null|大型语言模型（LLM）在理解语言和生成与人类水平相当的文本方面表现出色。然而，即使经过监督训练和人类对齐，这些LLM仍容易受到恶意用户的攻击，后者可以通过提示模型生成不希望看到的文本。此外，LLM内嵌有潜在偏见，这可能导致互动中的各种有害影响。当前的偏见评估指标缺乏标准和共识，现有方法往往依赖于人工生成的模板和注释，这既昂贵又费时。  我们的工作旨在通过训练模型自动创建对抗性提示来激发目标LLM生成带有偏见的响应。我们提出了一种基于LLM的偏见评估指标，并分析了多种现有的自动评估方法和指标。我们深入探讨了模型响应的各种细微差别，识别了不同模型家族的优势和劣势，并评估了评估方法的不足之处。我们将这些指标与人工评估进行比较，并验证了“LLM作为法官”的指标与生成偏见判断的人类评价一致。|
|**2024-08-07**|**From Data to Story: Towards Automatic Animated Data Video Creation with LLM-based Multi-Agent Systems**|Leixian Shen et.al.|[2408.03876](http://arxiv.org/abs/2408.03876)|null|创建从原始数据生成数据故事的过程极具挑战性，这主要源于人类有限的注意力和对特定技能的需求。近来，大型语言模型（LLM）的发展为构建利用独立代理实现工作流程自动化以简化数据故事创作流程的系统提供了巨大机遇。尽管多代理系统能够充分挖掘LLM潜力并分解任务供个体代理执行具有诸多优势，但在设计这些系统时，也面临着任务分解、子任务性能优化以及工作流程设计等方面的挑战。为了更深入地理解这些问题，我们开发了Data Director——一个基于LLM的多代理系统，旨在自动化生成动画数据视频，这一类数据故事的典型形式。Data Director通过解析原始数据、拆分任务、设计代理角色以进行自动决策，并无缝整合数据视频中的各种组件来实现这一目标。一个案例研究展示了Data Director在生成数据视频方面的有效性。在整个开发过程中，我们从解决面临的挑战中提炼出了经验教训，这些经验对于指导未来在数据故事叙述领域自主代理的发展具有重要意义。此外，我们也揭示了全球优化、人机交互设计以及高级多模态LLM应用的未来发展方向。|
|**2024-08-07**|**PackMamba: Efficient Processing of Variable-Length Sequences in Mamba training**|Haoran Xu et.al.|[2408.03865](http://arxiv.org/abs/2408.03865)|null|随着大型语言模型的发展，传统的Transformer模型在处理长序列时变得计算密集型，因为其计算量随序列长度的平方增长。Mamba作为生成AI领域的一项突破性架构，展现出在减少计算和内存复杂性的前提下，高效处理长序列的能力。然而，现有的Mamba训练框架在处理变长序列输入时存在效率问题。单序列训练会导致GPU利用率低下，而对变长序列进行批量处理到最大长度则会带来显著的内存和计算开销。  为了解决这一问题，我们分析了Mamba架构中瓶颈操作器在不同张量形状下的性能，并提出了一种名为PackMamba的高吞吐量Mamba，它能够有效地处理变长序列。深入研究状态空间模型（SSMs），我们修改了并行操作器，以避免在各个序列之间传递信息，同时保持高性能。在NVIDIA A100 GPU上的实验结果表明，PackMamba在处理1.4B模型时比基线单序列处理方案提高了3.06倍的速度，在处理2.8B模型时提高了2.62倍的速度。|
|**2024-08-07**|**GAIA -- A Large Language Model for Advanced Power Dispatch**|Yuheng Cheng et.al.|[2408.03847](http://arxiv.org/abs/2408.03847)|null|电力调度对于提供稳定、经济且环保的电力至关重要。然而，随着电力系统规模和复杂性的增长，传统的调度方法在多任务处理、快速问题解决以及人机协作方面遇到挑战。本文介绍了一种专为电力调度任务设计的大型语言模型（LLM）——GAIA。我们开发了一种新颖的数据集构建技术，利用多种数据源对GAIA进行微调，以优化其在该领域的性能。这种方法简化了LLM的训练过程，使得在电力系统管理中能够无缝整合多维数据。此外，我们还设计了专门的提示策略来提高GAIA在调度场景下的输入输出效率。在ElecBench基准测试中，GAIA在多个指标上超越了基础模型LLaMA2。实际应用表明，GAIA能够增强决策过程、提高运营效率，并促进电力调度操作中的人机交互。本文扩展了LLM在电力调度领域的应用，并验证了其实用性，为这一领域未来的创新开辟了道路。|
|**2024-08-07**|**MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models**|Yuchen Dong et.al.|[2408.03841](http://arxiv.org/abs/2408.03841)|null|本文探讨了大型语言模型在自动化软件操作和工具生成（SOTG）领域的应用，以此来提升软件生产力。这一过程类似于人类文明早期通过创造并使用工具加速发展的阶段。这些复杂任务要求AI能够持续总结并改进。当前研究往往忽视了将实时任务经验转化为系统记忆以及区分现有知识未来价值的重要性。本文通过引入“Memory-Loop网络”来解决这些问题，以实现及时的记忆存储与经验引用。  此外，我们还对基于知识精确分段的RAG机制进行了增强，以便根据价值差异利用记忆。针对SOTG设计了MaxMind模型。为了验证我们的方法，我们开发了MaxMind4Sheet，一个遵循MaxMind理念的电子表格处理系统。与SheetCopilot的比较实验显示，任务记忆的积累和循环能够稳步提高任务成功率，在此示例实施中，每轮的成功率提升约为3%-6%。随着记忆的持续增长，这种累积改进可能会非常显著。  引入记忆循环还可以通过高达25%的效率提升增加系统的任务执行效率，并通过记忆转移解决LLM在处理专业任务时面临的再训练问题。这表明MaxMind有潜力显著增强大型语言模型在SOTG领域的功能和生产力。|
|**2024-08-07**|**WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models**|Prannaya Gupta et.al.|[2408.03837](http://arxiv.org/abs/2408.03837)|**[link](https://github.com/walledai/walledeval)**|WalledEval是一个全面的AI安全性测试工具包，旨在评估大型语言模型（LLMs）。它能够兼容各种模型，包括开源和API两种类型，并包含了超过35个覆盖多语言安全、夸张安全以及提示注入等领域的安全基准。该框架支持对LLM和裁判进行基准测试，并且集成自定义突变器，用于测试在不同文本风格变异如将来时态和重述下的安全性。此外，WalledEval引入了WalledGuard，这是一种新的小型高效内容审核工具，以及SGXSTest，用于评估文化背景下的夸大安全问题。我们已将WalledEval公开发布在https://github.com/walledai/walledevalA。|
|**2024-08-07**|**Target Prompting for Information Extraction with Vision Language Model**|Dipankar Medhi et.al.|[2408.03834](http://arxiv.org/abs/2408.03834)|null|近期，大型视觉与语言模型（VLM）领域的发展在构建信息提取系统方面带来了新的变革。这些模型在理解文档和构建跨行业的问题回答系统方面达到了顶尖水平，显著提升了从文档图像生成文本以及提供精确答案的能力。然而，利用这些模型构建精准对话系统时仍存在一些挑战。传统的通用提示技术在大型语言模型上的应用往往不适合这些专门设计的视觉语言模型。使用这类通用输入提示所生成的输出通常较为普通，与文档实际内容相比可能存在信息缺口。为了获得更准确、更具体的答案，视觉语言模型需要针对特定部分的文档图像进行提示，并仅从这些特定区域生成相关答案。本文讨论了一种称为“目标提示”的技术，该技术专注于明确指向文档图像的部分并仅从这些特定区域生成相关的答案。此外，文章还通过使用不同用户查询和输入提示对每种提示技术的响应进行了评估。|
|**2024-08-06**|**TextIM: Part-aware Interactive Motion Synthesis from Text**|Siyuan Fan et.al.|[2408.03302](http://arxiv.org/abs/2408.03302)|null|本文提出了一种名为TextIM的新型框架，旨在合成基于文本驱动的人类交互动作，并特别关注于部分级语义的精确对齐。现有方法往往忽视了交互身体部位的关键作用，并未能充分捕捉和对齐部分级语义，导致了不准确甚至错误的动作结果。为了解决这些问题，TextIM采用了一个解耦条件扩散框架，以增强交互动作与对应文本描述中的语义意图之间详细的对齐。我们的方法利用大型语言模型，作为人类大脑的角色，来识别交互的身体部位并理解交互语义，从而生成复杂的微妙交互动作。在精细动作引导下，TextIM进一步将这些部分动作扩展为整个身体的连贯动作。我们设计了一个空间一致性模块，通过部分图卷积网络在整个身体动作中补充和维持各部分之间的连贯性和和谐性。对于训练和评估，我们精心选择了并重新标记了HUMANML3D中的交互动作数据集，创建了一个专门的数据集。实验结果显示，TextIM能够产生语义上准确的人类交互动作，显著提高了在各种场景下合成交互动作的真实感和应用性，包括与可变形和动态变化物体的交互。|
|**2024-08-06**|**KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models**|Ruizhe Zhang et.al.|[2408.03297](http://arxiv.org/abs/2408.03297)|null|通过整合外部知识，检索增强生成（RAG）策略已成为缓解大型语言模型在处理知识密集型任务时遇到的幻觉问题的有效方法。然而，在整合非参数化的外部支持证据与内部参数化知识的过程中，不可避免的知识冲突可能会产生，导致模型响应中的混淆。为了在不同情境下提升语言模型的知识选择能力，一些研究已经关注于通过指令调整来细化其行为模式。然而，由于缺乏明确的负向信号和比较目标，通过这种方式进行微调的语言模型在复杂的、现实的检索场景中仍然可能表现出不理想的特性。  针对这一挑战，我们提出了一种知识意识偏好优化（KaPO），旨在实现对真实检索场景中知识选择的可控性。具体而言，我们探索并模拟了不同上下文组合下的错误类型，并通过偏好优化方法学习如何避免这些负向信号。同时，通过调整响应长度与表示不同行为模式的偏好数据比例之间的平衡，我们增强了语言模型的适应能力和噪声鲁棒性。  实验结果表明，与先前的方法相比，KaPO在处理知识冲突方面取得了超过37%的性能提升，并且在各种离群数据集上表现出了稳健的泛化能力。|
|**2024-08-07**|**StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation**|Boxi Cao et.al.|[2408.03281](http://arxiv.org/abs/2408.03281)|**[link](https://github.com/c-box/structeval)**|评价是大型语言模型开发的关键工具。当前的评估方式通常采用单一指标评估模式，对每个基本测试目标进行评估，这在区分模型是否真正具备所需能力还是仅仅记忆或猜测特定问题的答案方面存在困难。为了应对这一挑战，我们提出了一种名为StructEval的新评估框架。从基本测试目标出发，StructEval通过在多个认知层次和关键概念上进行结构化的评估来深化和拓宽评估范围，从而为大型语言模型提供全面、稳健且一致的评估。在三个广泛使用的基准上进行的实验表明，StructEval是一个可靠的工具，能够抵抗数据污染的风险并减少潜在偏见的干扰，从而提供关于模型能力更可靠和一致的结论。我们的框架还为未来原理性和可信的大型语言模型评估协议的设计提供了启示。|
|**2024-08-06**|**Synthesizing Text-to-SQL Data from Weak and Strong LLMs**|Jiaxi Yang et.al.|[2408.03256](http://arxiv.org/abs/2408.03256)|null|本文探讨了开源与封闭式大型语言模型（LLM）在文本到SQL任务中的能力差距问题。为此，我们提出了一种合成数据方法，该方法结合了更大、更强大的模型生成的数据（强模型）与较小、不完全对齐模型生成的错误信息数据（弱模型）。这种方法不仅提高了文本到SQL模型的领域泛化能力，还探索了错误数据监督通过偏好学习的潜力。此外，我们利用合成数据方法对开源LLM进行指令调整，由此产生了专门针对文本到SQL任务的模型SENSE。通过在SPIDER和BIRD基准上的表现，证明了SENSE的有效性，成功缩小了开源模型与基于封闭源模型的方法之间的性能差距。|
|**2024-08-06**|**Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons**|Yifei Wang et.al.|[2408.03247](http://arxiv.org/abs/2408.03247)|**[link](https://github.com/wangyifei0047/tfrkn)**|在这篇论文中，我们深入研究了大型语言模型（LLM）在面对推理任务时是否积极地回忆或检索其内部事实知识库。通过分析LLM在每个推理步骤中的内部事实召回情况，即所谓的知识神经元，我们揭示了在某些情况下，LLM未能有效利用关键的事实关联。相反，它们倾向于采取替代的、快捷的路径来回答推理问题。通过手动调整LLM中参数知识的召回过程，我们证明直接增强这一过程可以显著提高推理性能，而抑制它则会导致明显的性能下降。此外，我们评估了链式思考（CoT）提示的影响，这是一种处理复杂推理任务的强大技术。我们的发现表明，CoT可以通过鼓励LLM进行有条理和可靠的推理来增强对事实知识的回忆。进一步地，我们探讨了上下文冲突如何影响推理过程中事实的检索，以获得对LLM事实回忆行为的全面理解。相关代码和数据将在不久后提供。|
|**2024-08-06**|**Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi**|Pranita Deshmukh et.al.|[2408.03172](http://arxiv.org/abs/2408.03172)|null|随着低资源语言数字内容的激增，针对这些语言的高级自然语言处理（NLP）技术需求正在增加。BERT（双向编码表示的Transformer）作为众多NLP架构和语言模型的基础框架，正越来越多地用于开发低资源NLP模型。参数高效微调（PEFT）是一种方法，用于对大型语言模型（LLMs）进行微调，并在一定程度上减少训练参数，以降低训练模型所需的计算成本，并达到与完全微调模型相当的结果。本研究旨在分析PEFT方法在马拉地语低资源语言中的应用。我们对各种单语和多语种马拉地语BERT模型进行了全面分析，这些方法在MahaSent、MahaHate和MahaNews等重要文本分类数据集上进行了评估。PEFT技术的引入显著加快了模型的训练速度，解决了模型开发和部署的关键方面。在本研究中，我们探索了大型语言模型的低秩适应（LoRA）和适配器方法在低资源文本分类中的应用。结果显示，这些方法在准确率上与全量微调相当，且无需损失，可用于马拉地语和其他印度语族语言的NLP能力持续发展。|
|**2024-08-06**|**Conditioning LLMs with Emotion in Neural Machine Translation**|Charles Brazier et.al.|[2408.03150](http://arxiv.org/abs/2408.03150)|null|大型语言模型（LLM）在自然语言处理任务中展现了卓越的性能，特别是在机器翻译领域。本文提出了一种新颖的机器翻译管道，该管道通过将情感信息整合到语言模型中来增强翻译质量，这些情感信息是从语音情感识别（SER）模型中提取的。首先，我们对五个现有的LLM进行Libri-trans数据集的微调，并选择表现最佳的模型。随后，我们以不同维度的情感增强LLM提示，并在这些不同的配置下训练选定的LLM。我们的实验结果表明，将情感信息，尤其是唤醒度，整合到LLM提示中，能够显著提高翻译质量。|
|**2024-08-06**|**Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations**|Leo Donisch et.al.|[2408.03130](http://arxiv.org/abs/2408.03130)|null|大型语言模型在自然语言处理领域无处不在，因为它们能够在无需重新训练的情况下适应新任务。然而，这些模型的规模和复杂性带来了独特的挑战与机遇，促使研究者与实践者探索新型的模型训练、优化和部署方法。本文综述的重点在于各种降低资源需求和压缩大型语言模型的技术，包括量化、剪枝、知识蒸馏以及架构优化。主要目标是深入探讨每种方法，并突出其独特挑战及其实际应用。讨论的方法按照分类学进行组织，提供了一个优化景观的概览，有助于更好地理解研究轨迹。  ## 任务 请将上述论文摘要翻译成中文，不要输出任何无关内容，确保翻译内容中不包含","字符。|
|**2024-08-06**|**Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation**|Artur Guimarães et.al.|[2408.03127](http://arxiv.org/abs/2408.03127)|**[link](https://github.com/araag2/SemEval2024-Task2)**|这篇论文阐述了我们对SemEval-2024安全生物医学自然语言推断在临床试验（NLI4CT）任务的处理策略。该任务涉及对临床试验报告（CTRs）中的陈述进行分类。我们探索了Mistral-7B这种通用的开源大型语言模型（LLM）的能力。我们为NLI4CT任务设计了一个提示，并使用增强后的训练数据集对量化版本的模型进行了微调。实验结果显示，这种方法在宏F1分数方面可以产生显著的结果，但在忠实性和一致性方面存在局限性。所有开发的代码都在GitHub仓库中公开提供。|
|**2024-08-06**|**Evaluating the Translation Performance of Large Language Models Based on Euas-20**|Yan Huang et.al.|[2408.03119](http://arxiv.org/abs/2408.03119)|null|近年来，在深度学习技术的快速发展的推动下，大型语言模型（LLMs）如BERT和GPT在自然语言处理任务上取得了突破性成果。机器翻译作为自然语言处理的核心任务之一，也从大型语言模型的发展中受益匪浅，实现了质的飞跃。尽管大型语言模型在翻译性能上取得了显著进展，但机器翻译仍面临诸多挑战。因此，本文构建了Euas-20数据集，用于评估大型语言模型在翻译任务上的性能、不同语言的翻译能力以及预训练数据对LLMs翻译能力的影响，旨在为研究人员和开发者提供参考。|
|**2024-08-05**|**Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?**|Mohammad Bahrami Karkevandi et.al.|[2408.02651](http://arxiv.org/abs/2408.02651)|null|大型语言模型（LLM）在自然语言任务上表现出令人印象深刻的能力，但它们的安全性和道德性仍然存在争议，因为它们的训练基于互联网文本语料库。为了应对这些担忧，已经开发了对齐技术来提高大型语言模型的公共可用性和安全性。然而，通过这些模型生成有害内容的可能性似乎仍然存在。本文探讨了“反向对齐”LLM的概念——利用对抗触发器逆转其对齐过程。先前的方法，如软嵌入提示、手动构建的提示和基于梯度的自动提示，在黑盒模型上由于需要访问模型和产生有限的手动构建提示的需求而取得了有限的成功，这使得它们容易被阻断。本文提出了一种新的方法，使用强化学习优化对抗触发器，仅需对目标模型进行推理API访问以及一个小型代理模型即可。我们的方法利用BERTScore为基础的奖励函数，增强了对抗触发器在新黑盒模型上的可移植性和有效性。我们展示了这种方法如何在未测试的语言模型上提高了对抗触发器的表现。|
|**2024-08-05**|**SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models**|Muxi Diao et.al.|[2408.02632](http://arxiv.org/abs/2408.02632)|null|随着大型语言模型（LLM）能力与影响力的持续增强，确保其安全性和预防有害输出变得至关重要。为解决这些关切，一种有前景的方法是训练模型自动生成对抗性提示进行红队测试。然而，LLM中漏洞的不断演变使得当前的对抗方法在具体针对和探索这些模型弱点方面显得力不从心。为了应对这些挑战，我们引入了“自我演化安全优化”（SEAS）框架，该框架通过利用模型自身生成的数据来增强安全性。SEAS运作于三个迭代阶段：初始化、攻击和对抗优化，旨在同时提升红队和目标模型的稳健性和安全性。  该框架减少了对人工测试的依赖，并显著增强了LLM的安全性能力。我们的贡献包括一个新颖的对抗性框架、一个全面的安全数据集以及经过三次迭代后，目标模型的安全水平达到了与GPT-4相当的水平，而红队模型在对抗高级模型时的成功率（ASR）有了显著提高。|
|**2024-08-05**|**Progressively Selective Label Enhancement for Language Model Alignment**|Biao Liu et.al.|[2408.02599](http://arxiv.org/abs/2408.02599)|null|大型语言模型在各种语言任务上展现出令人印象深刻的能力，但可能会生成与人类预期不符的内容，从而引发伦理和法律问题。因此，探索这些模型的局限性并实施限制以确保安全性和合规性变得至关重要，其中强化学习从人类反馈（RLHF）是主要方法。然而，由于RLHF阶段在稳定性和可扩展性方面面临的挑战，研究人员正在探索其他方法来实现与RLHF类似的效果。这些方法往往依赖于大量高质量的数据集，并且低效地利用生成的数据。为解决这一问题，我们提出了一种名为PSLE（Progressively Selective Label Enhancement for Language Model Alignment）的框架，它充分利用所有生成数据，通过指导模型遵循原则来使输出与人类期望保持一致。通过动态更新阈值，我们的方法确保了高效的数据利用，通过整合所有生成响应并根据其相应的奖励分数对它们进行加权。在多个数据集上的实验结果表明，PSLE在现有语言模型对齐方法中表现出有效性。|
|**2024-08-05**|**Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization**|Ankan Mullick et.al.|[2408.02584](http://arxiv.org/abs/2408.02584)|null|随着数字信息量的持续增长，用户需要有效方法从长篇文档中提取关键见解。面向方面的总结提供了一种有针对性的方法，生成专注于文档内特定方面的小结。尽管在面向方面的总结研究领域取得了进展，但提高模型性能的持续追求是必要的。鉴于大型语言模型（LLMs）在自然语言处理任务中的潜力，特别是在总结问题上，本文探讨了对LLMs进行微调以执行面向方面的总结任务的可能性。我们评估了开源基础LLMs，包括Llama2、Mistral、Gemma和Aya，对于公开可用的特定领域面向方面的总结数据集的影响。我们的假设是，这种方法能够让这些模型有效地识别并提取与方面相关的信息，从而产生与最先进的方法相比更高质量的面向方面的总结。我们建立了一个全面的评估框架，将微调后的LLMs的性能与竞争性的面向方面的总结方法以及微调前LLMs的原始版本进行比较。我们的工作通过证明对LLMs进行微调可以生成高质量的面向方面的总结，为面向方面的总结领域做出了贡献。此外，它为在不同NLP领域进一步探索使用LLMs进行目标信息抽取任务打开了大门。|
|**2024-08-05**|**Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information**|Yauwai Yim et.al.|[2408.02559](http://arxiv.org/abs/2408.02559)|null|本文探讨了开源与API驱动大型语言模型在复杂、不完全信息环境下的文本游戏协作能力，特别是在非英语环境中的应用潜力。研究对比了这些模型与其他类型代理的性能，并使用理论思维（Theory of Mind, ToM）规划技术来评估它们在需要多智能体协作的不完全信息游戏中表现的能力。通过引入外部工具来解决此卡牌游戏中动态且庞大的行动空间问题，我们的结果揭示了当前大型语言模型在面对高级别任务时与强化学习模型之间的性能差距。尽管存在这一差距，但大型语言模型展现了在游戏场景下的理论思维能力，能够理解盟友和对手的行为，并与盟友建立协作关系，从而持续提升其性能。为了促进进一步的研究与理解，我们已公开了代码库。|
|**2024-08-05**|**Generative AI as a Service in 6G Edge-Cloud: Generation Task Offloading by In-context Learning**|Hao Zhou et.al.|[2408.02549](http://arxiv.org/abs/2408.02549)|null|本文探讨了在6G网络中部署基础模型的创新边缘-云架构。具体目标是通过无线电资源分配和任务卸载来最小化基础模型的服务延迟。主要分为三部分：首先，介绍通信系统模型，即分配无线电资源并计算支持生成内容传输的链路容量；其次，展示基础模型推理模型，用于计算内容生成的延迟；最后，提出了一种新颖的上下文学习方法来优化任务卸载决策。该方法利用基础模型的推理能力，避免了传统机器学习算法中需要专门模型训练或微调的困难。仿真结果表明，提出的边缘-云部署与上下文学习任务卸载方法可以在无需专门模型训练或微调的情况下，实现满意的生成服务质量。|
|**2024-08-05**|**RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation**|Daniel Fleischer et.al.|[2408.02545](http://arxiv.org/abs/2408.02545)|**[link](https://github.com/intellabs/ragfoundry)**|实施检索增强生成（RAG）系统固有地复杂，需要深入了解数据、应用场景以及细致的设计决策。此外，评估这些系统带来了重大挑战，需要通过多维度的方法评估检索准确性和生成质量。我们引入了RAG Foundry，这是一个开源框架，用于在RAG场景中增强大型语言模型的数据。RAG Foundry将数据创建、训练、推理和评估整合到一个工作流程中，从而为在RAG设置中训练和评估大型语言模型创建数据增强集提供了便利。这种整合使得快速原型设计和RAG技术的实验变得容易，允许用户轻松生成数据集并使用内部或专门的知识源训练RAG模型。我们通过使用多种RAG配置对Llama-3和Phi-3模型进行增强和微调，在三个知识密集型数据集上展示了持续改进的有效性。代码作为开源发布在https://github.com/IntelLabs/RAGFoundry。|
|**2024-08-05**|**Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions**|Xinbei Ma et.al.|[2408.02544](http://arxiv.org/abs/2408.02544)|**[link](https://github.com/xbmxb/EnvDistraction)**|本文探讨了多模态大型语言模型（MLLM）代理在图形用户界面（GUI）环境中的忠诚度问题，旨在解决以下研究问题：多模态GUI代理是否可能被环境背景分散注意力。我们提出了一种通用设置，其中用户和代理均为善意角色，而环境虽非恶意，但包含与任务无关的内容。通过我们的模拟数据集，对多种MLLM作为GUI代理进行评估，按照三种不同的工作模式，即具有不同程度感知能力的模式进行。实验结果表明，即便是最强大的模型，无论是通用型代理还是专门用于GUI的代理，都容易受到干扰。虽然近期的研究主要关注多模态代理的动作准确性（即帮助性），但我们的发现揭示了这些代理在面对环境干扰时表现出不忠行为的可能性。此外，我们从对抗性视角出发，实施环境注入策略，展示出利用这种不忠行为可能导致的意外风险。|
|**2024-08-05**|**Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph**|Zhao Kaichen et.al.|[2408.02535](http://arxiv.org/abs/2408.02535)|null|视觉语言导航（VLN）是智能体领域的重要研究之一，旨在使智能体理解周围环境并完成导航任务。在VLN任务中，指令可以分为粗粒度和细粒度两种类型。细粒度指令详细描述了整个任务的步骤，而粗粒度指令则提供了一个抽象的任务描述，更适合人类的习惯。现有的大部分工作都集中在对细粒度指令的研究上，忽视了日常生活中存在的抽象指令。为了克服这一挑战，我们尝试通过事件知识增强的方式考虑VLN中的粗粒度指令。具体来说，我们首先提出了一种基于提示的方法来整合多个主流基准数据集，形成一个全面的事件知识图谱（命名为VLN-EventKG）。通过小规模和大规模语言模型的合作，我们实现了能够处理粗粒度指令输入的事件导航（EventNav）方法，用于VLN任务中的导航规划。此外，我们设计了一个新颖的动态历史回溯模块，能够在实时中纠正潜在的错误动作规划。在各种公共基准上的实验结果表明，使用我们提出的VLN-EventKG的知识增强方法，在使用粗粒度指令的VLN任务中具有超过5%的成功率优势。我们的项目可以在<https://sites.google.com/view/vln-eventkg> 上访问。|
|**2024-08-05**|**Practical Attacks against Black-box Code Completion Engines**|Slobodan Jenko et.al.|[2408.02509](http://arxiv.org/abs/2408.02509)|null|本文提出了一种名为INSEC的新型攻击方法，旨在引导基于大型语言模型的代码补全引擎生成存在安全漏洞的代码。这种攻击方式与市面上大多数商业补全引擎（如GitHub Copilot）相似，仅需要黑盒查询访问目标引擎，无需了解其内部机制。攻击策略通过在补全输入中插入恶意攻击字符串作为简短注释来实施。为了设计出有效的攻击字符串，我们构建了一系列专门的初始化方案，并通过优化过程进一步精炼。我们在开源模型、黑盒商业服务（如OpenAI API和GitHub Copilot）以及五种编程语言下的16个关键错误类别上验证了INSEC的有效性。实验结果表明，与现有技术相比，INSEC显著提高了考虑中的补全引擎生成不安全代码的可能性超过50%，同时仍具备生成功能正确代码的能力。此外，我们的攻击方法资源需求较低，开发成本低于十美元，可在普通硬件上运行。|
|**2024-08-02**|**Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting**|Xiangyu Zhao et.al.|[2408.01423](http://arxiv.org/abs/2408.01423)|null|大型语言模型（LLM）在自然语言处理（NLP）领域展现出了惊人的能力，在执行各种任务时表现出色。然而，这些模型的性能受到特定提示设计策略的影响。主要有两种提示设计方法：一种是通过手动为特定数据集创建专门的提示，被称为专家设计提示（EDP），一旦创建，它们就无法更改，其有效性受限于人类设计者的专业知识。当应用于LLM时，这种固定的方法导致对简单问题和复杂问题采用统一的解决策略，导致对于简单问题过度使用令牌。另一种方法是让LLM自动生成提示，称为LLM衍生提示（LDP），能够针对具体问题提供定制解决方案，从而减轻了EDP的局限性。然而，LDP在处理复杂问题时可能会遇到性能下降的问题，这是因为在解决问题规划过程中可能累积错误。  为了解决这些挑战，我们提出了一个新颖的提示递归搜索（PRS）框架，该框架利用LLM生成针对特定问题的解决方案，同时减少令牌的使用。这个框架包含了对问题复杂性的评估以及可调整的结构，以降低出错的可能性。我们通过使用不同参数数量的LLM模型在多个领域内的多种数据集上进行了广泛的实验，验证了PRS框架的有效性。与链式思考（CoT）方法相比，PRS方法在使用Llama3-7B模型时，BBH数据集上的准确率提高了8%，实现了22%的改进。|
|**2024-08-02**|**Mission Impossible: A Statistical Perspective on Jailbreaking LLMs**|Jingtong Su et.al.|[2408.01420](http://arxiv.org/abs/2408.01420)|null|大型语言模型（LLM）在有限的质量控制下训练于海量文本数据中。这导致LLM可能出现意外甚至有害的行为，如泄露信息、假新闻或仇恨言论。应对策略，通常称为偏好对齐，包括通过精心设计的文本示例精细调整预训练的LLM，以体现期望的行为模式。然而，实证研究表明，即使进行了偏好对齐，LLM也仍可能诱骗至有害行为。这种被称为LLM“越狱”的现象通常通过修改输入提示来实现，以误导LLM。本文从统计学的角度提供对偏好对齐和越狱现象的理论洞察。  在我们的框架下，首先证明了如果训练语料库中存在有害行为，预训练的LLM会模仿这种行为。同样基于这个框架，我们引入了一种统计意义上的对齐概念，并给出了越狱概率的下界，表明在合理假设下，这种现象是无法避免的。基于我们的见解，我们提出了一种对当前普遍采用的对齐策略——强化语言引导反馈（RLHF）的改进。具体来说，我们引入了一个名为E-RLHF的简单修改版RLHF目标，旨在提高安全响应的可能性。E-RLHF不会增加额外的训练成本，且与其它方法兼容。实验结果表明，在不牺牲MT-Bench项目衡量的模型性能的情况下，E-RLHF在AdvBench和HarmBench项目提出的所有对齐问题上均优于RLHF。|
|**2024-08-02**|**DebateQA: Evaluating Question Answering on Debatable Knowledge**|Rongwu Xu et.al.|[2408.01419](http://arxiv.org/abs/2408.01419)|**[link](https://github.com/pillowsofwind/debateqa)**|大型语言模型（LLM）的兴起使得我们能够探讨关于LLM聊天机器人上固有争议性问题的答案，这需要一种可靠的方式来评估它们的能力。然而，传统问答基准假设固定的答案对此目的而言是不足的。为了应对这一挑战，我们引入了DebateQA，这是一个包含2,941个争议性问题的数据集，每个问题都附带了多个由人类注释的片段答案，这些片段答案捕捉了各种视角。我们开发了两个度量标准：观点多样性，用于评估视角的全面性；以及争议意识，用于评估LLM是否认识到问题的争议性。实验结果表明，这两个度量标准与人类偏好一致，并且在不同基础模型之间具有稳定性。通过使用DebateQA和这两个度量标准，我们评估了12种流行的LLM和检索增强生成方法。我们的发现揭示了虽然LLM通常擅长识别争议性问题，但它们提供全面答案、涵盖多样视角的能力存在显著差异。|
|**2024-08-02**|**Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs**|Yilun Hua et.al.|[2408.01417](http://arxiv.org/abs/2408.01417)|null|人类在对话过程中会自发地使用越来越高效的语言，通过适应并形成自定义的约定。这一现象已经通过参考游戏进行了广泛的研究，展示了人类语言超越传达意图的特性。目前，我们尚未探索多模态大型语言模型（MLLM）是否在交互中同样提高了沟通效率，并且它们可能采用何种机制实现这一目的。  我们引入了ICCA框架，这是一个自动化的评估方法，用于在MLLM中评估此类对话适应作为上下文行为的能力。我们对几种最先进的MLLM进行了评估，观察到虽然它们可能理解其对话伙伴的语言越来越高效，但它们本身并不自发地在时间上使自己的语言变得更高效。这种能力仅在某些模型（如GPT-4）中可以通过强烈的提示来激发。这表明，即使这是人类语言的常见特征，当前的训练制度并不能产生这一互动属性。  ICCA框架已开源发布于https://github.com/lil-lab/ICCA。|
|**2024-08-02**|**Coalitions of Large Language Models Increase the Robustness of AI Agents**|Prattyush Mangal et.al.|[2408.01380](http://arxiv.org/abs/2408.01380)|null|大型语言模型（LLM）的兴起从根本上改变了我们与数字系统互动的方式，并推动了对借助于大规模语言模型（LLM）的AI代理以辅助日常流程的研究。尽管LLM具有强大的能力并能够表现出一些涌现特性，但它们并非逻辑推理者，往往在AI代理执行工作流程时所涉及的所有子任务上表现不佳。现有研究通过大规模的一般性预训练或针对工具使用进行专门的微调来解决这一问题，而我们评估了一个由专注于特定子任务的预训练模型组成的联盟是否能与单一模型代理的表现相匹敌。联盟模型的方法展示了其在构建鲁棒性和降低这些AI代理运行成本方面的潜力，通过利用特定模型展现的特性。我们的发现表明，通过考虑一组预训练模型，可以减轻微调的需求，并相信这种方法可以应用于其他利用LLM的非代理系统。|
|**2024-08-02**|**Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation**|Jheng-Hong Yang et.al.|[2408.01363](http://arxiv.org/abs/2408.01363)|null|### 摘要  本文对视觉语言模型（VLMs）在进行相关性评估方面的潜力进行了探索。通过设计一个针对多媒体内容创作的大型零样本检索任务，评估了CLIP、LLaVA和GPT-4V等VLM的性能。初步实验结果如下：  1. **性能比较**：在与人类判断的相关性上，LLaVA和GPT-4V（包括开源和专有视觉指令调优的大规模语言模型）取得了显著的Kendall’s τ≈0.4的成绩，超过了CLIPScore指标。  2. **偏好与偏见**：尽管CLIPScore表现突出，但LLMs在偏见方面相对较少倾向于基于CLIP的检索系统。  3. **一致性分析**：GPT-4V的评分分布与人类判断更为一致，其Cohen’s κ值约为0.08，远高于CLIPScore的约-0.096。这一发现表明，基于LLM的VLM在增强相关性评估方面具有潜力。  ### 结论  本研究揭示了视觉语言模型在相关性评估任务中的应用价值，特别是当它们被用于零样本检索任务时。通过比较不同模型的性能，研究强调了LLMs在多媒体内容创建领域内的潜在优势，并指出了它们在提升内容相关性判断方面的可能性。|
|**2024-08-02**|**Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs**|Peng Ding et.al.|[2408.01355](http://arxiv.org/abs/2408.01355)|**[link](https://github.com/njunlp/hallu-pi)**|多模态大语言模型在视觉语言理解与生成任务上展现出卓越性能。然而，这些模型偶尔会产生与给定图像不一致的内容，即所谓的“幻觉”。先前的研究主要集中在使用标准、未扰动基准评估幻觉上，这忽视了现实世界场景中普遍存在的扰动输入（如图像裁剪或模糊），这是对多模态大语言模型幻觉全面评估的关键。  本篇论文旨在填补这一空白，提出了Hallu-PI，首个专门用于评估多模态大语言模型在扰动输入下的幻觉的基准。Hallu-PI包含了7种扰动情景，涉及1,260张来自11种物体类型的扰动图像。每张图像都附有详细的注释，包括精细粒度的幻觉类型，如存在性、属性和关系等。这些注释配备了一个丰富的问答集，使Hallu-PI适用于辨别性和生成性任务。  在对主流多模态大语言模型（如GPT-4V和Gemini-Pro Vision）进行的广泛实验中，我们发现这些模型在Hallu-PI上的表现显示出显著的幻觉，而在未扰动场景中未观察到此类现象。我们的研究还揭示了多模态大语言模型处理不同类型幻觉时存在的严重偏差。  为此，我们设计了两个专门针对扰动情景的基线，分别为Perturbed-Reminder和Perturbed-ICL。我们希望这项研究能引起研究人员对多模态大语言模型在处理扰动输入时局限性的关注，并激发进一步的调查以解决这一问题。我们的代码和数据集已在GitHub（https://github.com/NJUNLP/Hallu-PI）上公开提供。|
|**2024-08-02**|**MCGMark: An Encodable and Robust Online Watermark for LLM-Generated Malicious Code**|Kaiwen Ning et.al.|[2408.01354](http://arxiv.org/abs/2408.01354)|**[link](https://github.com/KevinHeiwa/MCGTM)**|随着大型语言模型（LLM）的兴起，众多软件服务提供商（SSP）致力于开发针对代码生成任务的定制化LLM，如CodeLlama和Copilot。然而，这些LLM有可能被攻击者利用来生成恶意软件，对软件生态系统构成潜在威胁，例如自动化高级网络钓鱼恶意软件的创建。为应对这一挑战，我们首先进行了一项实证研究，并设计了一个包含约400小时工作量、共计406个恶意代码生成任务的提示数据集MCGTest。利用这个数据集，我们提出了MCGMark，这是首个能够实现稳健、结构感知且可编码的水印方法，用于追踪由LLM生成的恶意代码。我们通过控制令牌选择和基于概率异常值确保输出质量来嵌入可编码信息。此外，我们通过考虑恶意代码的结构特征增强了水印的鲁棒性，避免在易于修改的位置（如注释）嵌入水印。我们使用DeepSeek-Coder验证了MCGMark的有效性和鲁棒性，其最大输出限制为400个令牌时，嵌入成功率达到了88.9%。同时，该方法也展示了强大的鲁棒性，并对输出代码的质量影响极小。我们的方法帮助SSP追踪并追究由LLM生成的恶意代码的源头及责任。|
|**2024-08-02**|**Prompt Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social Science Tasks**|Anders Giovanni Møller et.al.|[2408.01346](http://arxiv.org/abs/2408.01346)|null|大型语言模型是促进社会计算领域复杂文本理解任务的有力工具。它们的多功能性虽然有益，但也带来了在该领域建立标准化最佳实践的障碍。为了提供不同策略价值的清晰度，我们概述了现代基于LLM的分类方法在23个社会知识任务基准上的性能。我们的结果指出了三个最佳实践：选择具有更大词汇量和预训练语料库的模型；避免简单的零次尝试，而倾向于增强提示的人工智能方法；在特定任务数据上进行微调，并考虑在多个数据集上使用更复杂的指令调整，仅当训练数据更为丰富时才这样做。  请注意，这段翻译文本中并未包含任何", "字符。|
|**2024-08-02**|**A Backbone for Long-Horizon Robot Task Understanding**|Xiaoshuai Chen et.al.|[2408.01334](http://arxiv.org/abs/2408.01334)|null|为了应对长时程任务中端到端机器人学习的不可预测性与泛化能力差的问题，我们提出了一种基于Therblig的骨架框架（TBBF），旨在增强机器人任务理解与转移能力。此框架利用Therblig（基本动作元素）作为骨架，将高级机器人任务分解为基本机器人配置，然后结合当前的基础模型来提升任务理解。  该方法包含两个阶段：离线训练与在线测试。在离线训练阶段，我们开发了Meta-RGate SynerFusion（MGSF）网络，用于跨任务精确的Therblig分割。在线测试阶段，通过收集新任务的一次演示，MGSF网络提取高阶知识，并通过Action Registration（ActionREG）编码入图像。此外，我们采用Large Language Model（LLM）-Alignment Policy for Visual Correction（LAP-VC）来确保精确的动作执行，从而在新型机器人场景中实现轨迹转移。  实验结果证实了这些方法的有效性，Therblig分割达到了94.37%的召回率，在真实世界中的在线机器人测试中，对于简单和复杂场景的成功率分别达到了94.4%和80%。补充材料可在以下网站获取：https://sites.google.com/view/therbligsbasedbackbone/home|
|**2024-08-01**|**AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation**|Mengkang Hu et.al.|[2408.00764](http://arxiv.org/abs/2408.00764)|**[link](https://github.com/lazychih114/AgentGen-Reproduction)**|大型语言模型（LLM）基于的代理已引起广泛关注并变得越来越流行。此外，规划能力是LLM基于代理的关键组成部分，涉及与环境的交互和执行动作以完成规划任务，通常包括从初始状态达到预期目标的过程。本文研究了通过指令调整增强LLM规划能力的方法，称为代理训练。近期的研究表明，利用专家级轨迹对指令调整LLM能有效提升其规划能力。然而，现有工作主要集中在从手动设计的任务和环境中合成轨迹。创建这些环境和任务的劳动密集型过程限制了生成足够多样性和广泛性的轨迹的能力。为解决这一局限性，本文探索了自动合成多样化环境以及规划任务的渐进范围，从简单到复杂。我们引入了一个框架AgentGen，利用LLM首先生成环境，随后根据这些环境生成规划任务。具体来说，为了提高环境多样性，我们提出了使用包含各种领域特定文本段落的灵感语料库作为合成环境的上下文。此外，为了增加生成规划任务难度多样性的程度，我们提出了一种双向演化方法Bi-Evol，该方法从容易和困难的两个方向进化规划任务，以合成具有平滑难度曲线的任务集。来自AgentBoard的评估结果显示，AgentGen显著提高了LLM的规划能力，例如，使用AgentGen指令调整的Llama-3 8B在总体性能上超越了GPT-3.5。此外，在某些任务中，它甚至超越了GPT-4。|
|**2024-08-01**|**Tamper-Resistant Safeguards for Open-Weight LLMs**|Rishub Tamirisa et.al.|[2408.00761](http://arxiv.org/abs/2408.00761)|**[link](https://github.com/rishub-tamirisa/tamper-resistance)**|快速发展的大型语言模型（LLM）能力引发了对潜在恶意用途的广泛担忧。针对开放权重的LLM，现有保护措施在抵抗篡改攻击方面缺乏足够的稳定性，这些攻击可以通过微调步骤轻易地移除拒绝和遗忘保护措施。这类漏洞要求采取新的方法来确保安全释放开放权重的LLM。  我们开发了一种名为TAR的方法，旨在将不可篡改的安全防护融入到开放权重的LLM中，使得即使经过数千步的微调，攻击者也无法移除这些防护措施。在全面的评估和红队测试分析中，我们的方法显著提高了防护的不可篡改性，同时保持了良性功能。我们的结果表明，不可篡改性是一个可行的问题，为改进开放权重LLM的安全性和安全性开辟了有前景的新途径。|
|**2024-08-01**|**DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency**|Jovan Stojkovic et.al.|[2408.00741](http://arxiv.org/abs/2408.00741)|null|快速发展的大型语言模型（LLMs）的生成能力使其在各种应用中成为关键的工作负载。如今，LLM推理集群处理大量查询，并对服务质量指标（SLOs）有严格要求。为了达到预期性能，这些模型在能耗高的GPU上执行，导致推理集群消耗大量能源，并产生过量的碳排放。幸运的是，我们发现可以通过利用推理计算特性的异质性以及工作负载的波动，显著提高能效。然而，这种多样性和动态环境创造了一个巨大的搜索空间，不同的系统配置（如实例数量、模型并行性和GPU频率）导致不同的能源和性能折衷。  为了解决这些挑战，我们提出了DynamoLLM，这是首个针对LLM推理环境的能效管理框架。DynamoLLM自动且动态地重新配置推理集群，以优化能源和成本，同时满足服务的性能SLOs。研究表明，在服务层面，DynamoLLM能够节省53%的能源和38%的操作碳排放，并为客户减少61%的成本，同时仍能满足延迟SLOs。|
|**2024-08-01**|**Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions**|Guangzhi Xiong et.al.|[2408.00727](http://arxiv.org/abs/2408.00727)|**[link](https://github.com/teddy-xionggz/medrag)**|大型语言模型（LLM）展现出了解决医疗问题的巨大潜力，它们能够掌握大量医学知识，但仍然可能出现幻觉，并且在知识更新方面具有局限性。为了增强LLM在医学问答方面的能力，提出了基于检索的生成（RAG）方法，通过外部知识库来提升性能。然而，在需要多次信息查询的复杂情况下，RAG可能仍然会失败。为解决这一问题，我们提出了一种迭代RAG方法（i-MedRAG），允许LLM在每次尝试后迭代地提出后续问题。在每次i-MedRAG迭代中，后续问题由基本的RAG系统回答，并用于指导下一个迭代中的查询生成。  实验结果显示，与仅使用RAG的传统方法相比，i-MedRAG显著提高了各种LLM在复杂问题上的性能，这些问题是美国医学生执照考试（USMLE）临床案例和大规模多任务语言理解（MMLU）数据集中的知识测试所涵盖的。特别值得注意的是，我们的零样本i-MedRAG在GPT-3.5上取得了69.68%的准确性，超越了所有现有的提示工程和微调方法在MedQA数据集上的表现。此外，我们还研究了i-MedRAG在不同迭代次数和每迭代查询数量下的扩展特性。  我们的案例研究显示，i-MedRAG能够灵活地提出后续问题形成推理链，深入分析医疗问题。据我们所知，这是首次将后续问题融入医学RAG的研究。|
|**2024-08-01**|**An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models**|Yangzhen Wu et.al.|[2408.00724](http://arxiv.org/abs/2408.00724)|null|在大规模语言模型（LLM）的最优训练配置研究中，特别是在模型规模和计算预算方面的配置，已经进行了大量的探讨。然而，对于推理阶段如何最优化配置LLM以平衡额外的推理计算时间和性能提升的研究还不够深入。本文旨在探索计算优化的推理方法，即设计能够通过调整推理时间的计算量来优化性能的模型和推理策略。  为了理解并设计计算优化的推理方法的第一步，我们对多种推理策略，如贪心搜索、多数投票、最佳N种组合、加权投票及其变体，在两种不同的树搜索算法中进行了评估，涉及不同模型规模和计算预算。我们的研究发现，较小的语言模型配合更先进的解码算法通常能实现帕累托最优的权衡，即在额外的计算成本与性能提升之间找到最佳平衡点。这些结果表明，在预算有限的场景下，如终端设备上部署小型模型，可能具有显著的优势，以提高问题解决的准确率。  例如，我们展示了Llemma-7B模型在使用约两倍于Llemma-34B模型的浮点运算（FLOPs）的情况下，仍能实现与后者相当的MATH500任务准确性。我们的发现可能适用于任何有明确成功度量标准的生成任务。|
|**2024-08-01**|**Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities**|Sunder Ali Khowaja et.al.|[2408.00722](http://arxiv.org/abs/2408.00722)|null|近期，大型语言模型（LLMs）因其在新兴应用中的适应性和可扩展性而备受关注，这些应用包括通信网络。预计6G移动边缘计算网络将能够作为服务支持LLMs，因为它们提供超可靠的低延迟通信和闭环大规模连接。然而，LLMs在数据和模型隐私方面存在漏洞，这影响了在用户服务中部署LLMs的信任度。本文探讨了在6G网络中对LLMs进行微调时的安全漏洞，特别是成员归属攻击。我们定义了攻击网络的特征，该网络可以在访问下游任务细调模型时执行成员归属攻击，前提是攻击者可以访问该模型。我们表明，对于任何下游任务，成员归属攻击都是有效的，这可能导致在使用LLMs作为服务时发生个人数据泄露。实验结果显示，在命名实体识别任务上，攻击成功率可达92%。基于实验分析，我们讨论了可能的防御机制，并提出了可能的研究方向，以使在6G网络背景下LLMs更加可靠。|
|**2024-08-02**|**Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning**|Trapoom Ukarapol et.al.|[2408.00690](http://arxiv.org/abs/2408.00690)|**[link](https://github.com/trapoom555/language-model-sts-cft)**|在自然语言理解任务上展现出卓越性能的大型语言模型因资源密集型的特点而降低了其可获取性。相比之下，小型语言模型如MiniCPM提供了更可持续的扩展性，但往往在没有专门优化的情况下表现不佳。本文旨在通过提升小型语言模型的文本嵌入质量来增强它们的表现。我们选择了三个语言模型：MiniCPM、Phi-2和Gemma，在NLI数据集上进行对比式微调。研究结果表明，这种方法能显著提升所有三种模型在各种基准测试中的文本嵌入质量，其中MiniCPM表现出最显著的平均56.33%性能提升。对比式微调的代码已公开在https://github.com/trapoom555/Language-Model-STS-CFT。|
|**2024-08-01**|**Can Developers Prompt? A Controlled Experiment for Code Documentation Generation**|Hans-Alexander Kruse et.al.|[2408.00686](http://arxiv.org/abs/2408.00686)|null|我们对20名专业人士和30名计算机科学学生进行了一个受控实验，要求他们使用ChatGPT风格的Visual Studio Code扩展来为两个Python函数编写代码文档。实验组自由输入自定义提示，而对照组则执行预设的少量提示。我们的结果表明，无论是专业人士还是学生，都对或无法应用提示工程技巧感到不知所措。尤其是学生，他们认为从自定义提示生成的文档比从准备好的提示生成的文档在可读性、简洁性和有用性方面显著较差。一些专业人士仅通过在自定义提示中加入“Docstring”关键词就能生成更高质量的文档。学生希望获得更多的指导来制定提示，而专业人士则更欣赏自定义提示的灵活性。参与者普遍认为输出并非完美，而是将其视为逐步完善文档的工具。需要进一步的研究来理解开发人员具有的提示技巧和偏好，以及他们完成特定任务所需的支援。|
|**2024-08-01**|**AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models**|Daqin Luo et.al.|[2408.00665](http://arxiv.org/abs/2408.00665)|**[link](https://github.com/tim120526/AutoM3L)**|### 摘要  本文提出了一种创新的多模态机器学习自动化框架——AutoM3L，该框架利用大型语言模型（LLMs）作为控制器，自动构建多模态训练管道。AutoM3L能够理解数据模态并根据用户需求选择合适的模型，提供自动化和互动性。通过消除手动特征工程和超参数优化的需求，我们的框架简化了用户参与过程，并通过指令提供了定制化选项，从而解决了以往基于规则的自动机器学习方法的局限性。  我们对AutoM3L在六个不同类型的多模态数据集上进行了评估，涵盖了分类、回归和检索任务，以及一系列广泛的单模态数据集。实验结果表明，AutoM3L在性能上与传统的基于规则的自动机器学习方法相比具有竞争力或超越性。此外，用户研究进一步验证了AutoM3L在用户友好性和易用性方面的优势，相较于基于规则的自动机器学习方法。|
|**2024-08-01**|**Disentangling Dense Embeddings with Sparse Autoencoders**|Charles O'Neill et.al.|[2408.00657](http://arxiv.org/abs/2408.00657)|null|我们提出了一种应用稀疏自动编码器（SAEs）到大型语言模型生成的密集文本嵌入的首次尝试，展示其在解缠语义概念方面的潜力。通过在超过42万篇计算机科学和天文学领域科学论文摘要的嵌入上训练SAEs，我们展示了所得到的稀疏表示保持了语义一致性的同时提供了可解释性。我们分析这些学习特征，探索不同模型容量下它们的行为，并引入了一种新的方法来识别“特征家族”，这些特征代表了不同抽象级别的相关概念。为了展示我们的方法的实际应用价值，我们展示了如何使用这些可解释特征精确控制语义搜索，从而实现对查询语义的精细控制。这项工作填补了密集嵌入的语义丰富性和稀疏表示的可解释性之间的差距。我们开源了训练后的嵌入、稀疏自动编码器以及可解释特征，同时提供了一个用于探索它们的网页应用程序。|
|**2024-07-31**|**Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs**|Shi Liu et.al.|[2407.21771](http://arxiv.org/abs/2407.21771)|null|现有大视觉语言模型（LVLM）主要通过将视觉编码器的图像特征与大型语言模型（LLM）对齐，利用其强大的文本生成能力。然而，视觉编码器与语言模型之间的规模差异可能导致LLM在多模态理解中占据主导地位。这种LVLM中的不平衡可能引发幻觉现象。具体来说，LVLM可能生成一致的描述，无论是否有视觉输入，这表明某些输出仅受上下文文本的影响。我们将这种现象称为“文本惯性”。为了对抗这一问题，我们提出了一种无需训练的算法来寻找图像理解和语言推断之间的平衡点。具体地，我们动态调整并放大分配给图像令牌的注意力权重，从而赋予视觉元素更大的重要性。同时，我们从多模态输入的logits中减去纯文本输入的logits，有助于LVLM避免过分依赖LLM。通过增强图像令牌并减少LLM的顽固输出，我们可以让LVLM更多地关注图像，从而缓解文本惯性和减少LVLM中的幻觉。我们的广泛实验显示，在不同指标下，这种方法显著减少了各种LVLM中的幻觉输出频率。项目页面可访问：https://lalbj.github.io/projects/PAI/。|
|**2024-07-31**|**ReplanVLM: Replanning Robotic Tasks with Visual Language Models**|Aoran Mei et.al.|[2407.21762](http://arxiv.org/abs/2407.21762)|null|大型语言模型（LLM）在机器人任务规划领域获得了越来越多的关注，这主要得益于它们在文本分析与生成、以及对世界广泛知识方面的出色能力。然而，它们在解析视觉线索方面的能力有限，无法直接感知世界状态，这导致了在描述当前世界状态上的不足。相比之下，视觉语言模型（VLM）通过集成视觉感知模块，填补了这一空白，增强了机器人的自主性。尽管如此，VLM仍面临挑战，例如，在提供准确指令的情况下，任务执行错误的风险依然存在。  为了应对这些问题，本文提出了一种用于机器人任务规划的ReplanVLM框架。该研究重点在于错误修正干预措施。提出了内部错误修正机制和外部错误修正机制，在相应的阶段进行错误纠正。发展了一种重规划策略，当任务执行失败时，用于重新规划任务或修正错误代码。在真实机器人和仿真环境中进行的实验结果表明，所提出的框架具有更高的成功率和更强的开放世界任务中的错误修正能力。有关实验的视频可以在https://youtu.be/NPk2pWKazJc找到。|
|**2024-07-31**|**Adaptive Retrieval-Augmented Generation for Conversational Systems**|Xi Wang et.al.|[2407.21712](http://arxiv.org/abs/2407.21712)|null|尽管在对话系统开发中融入大型语言模型取得了成功，但许多研究显示了检索增强生成（RAG）对于提供信息性响应的有效性。因此，现有研究通常假设对话系统中的每次回复都需要检索增强，而无需明确控制。这引发了一个关于这种必要性的研究问题。本研究旨在探索系统回应是否需要使用外部知识进行增强的必要性。通过利用人类对是否需要适应性增强的二元选择进行判断，我们开发了RAGate——一个闸门模型，该模型通过分析对话上下文和相关输入来预测对话系统是否需要RAG以获得改进的回复。我们在构建和应用RAGate到对话模型以及对不同对话场景进行详尽分析方面进行了广泛的实验。我们的实验结果和分析表明，RAGate在识别需要RAG以生成高质量回复并具有高生成置信度的系统响应方面有有效应用。这项研究还发现了生成置信度水平与增强知识的相关性。|
|**2024-07-31**|**CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**|Stefan Langer et.al.|[2407.21708](http://arxiv.org/abs/2407.21708)|null|本研究提出了一种方法，旨在通过利用已标注文本语料库和从Chebi获取的知识，增强现有知识，并对大型语言模型（LLM）进行微调，以识别科学文献中的化学实体及其作用。实验结果证明了这种方法的有效性。通过结合本体论知识与LLM的语言理解能力，我们实现了在科学文献中识别化学实体及其作用的高精确度和召回率。进一步地，我们从8000篇ChemRxiv文章中提取这些实体和角色，然后使用第二个LLM构建了一个化学实体和角色的知识图谱（CEAR），该图谱不仅为ChEBI提供了补充信息，还能帮助扩展其内容。|
|**2024-07-31**|**TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities**|Ming Zhang et.al.|[2407.21693](http://arxiv.org/abs/2407.21693)|**[link](https://github.com/konglonggefdu/transfertod)**|任务导向对话（TOD）系统旨在有效处理任务导向的对话，包括信息收集。如何准确、高效且有效地利用TOD进行信息收集一直以来都是一个关键且具有挑战性的任务。近期的研究表明，大型语言模型（LLMs）在对话、指令生成和推理方面表现出色，并能够通过微调显著提高TOD性能。然而，当前的数据集主要针对用户驱动的系统，并局限于预定义的特定场景和槽位，因此需要在TOD的主动性、多样性和能力方面进行改进。本研究介绍了一个多领域任务导向对话数据构建过程以及基于此过程生成的中文对话数据集——\textbf{TransferTOD}，该数据集真实模拟了在30个流行生活服务场景中的人机对话。利用这个数据集，我们训练了一个使用全参数微调的\textbf{TransferTOD-7B}模型，展示了在各种下游场景中的显著的填槽能力和提问能力。我们的工作证明了其在不同数据应用场景下的强大泛化能力，显著提高了数据使用效率和系统性能。数据已发布于https://github.com/KongLongGeFDU/TransferTOD。|
|**2024-07-31**|**Synth-Empathy: Towards High-Quality Synthetic Empathy Data**|Hao Liang et.al.|[2407.21669](http://arxiv.org/abs/2407.21669)|**[link](https://github.com/aurora-slz/synth-empathy)**|近年来，随着大型语言模型（LLM）的迅速发展，实现出色同理心响应能力已成为一个至关重要的前提。因此，管理和理解同理心数据集的重要性日益凸显。然而，同理心数据通常由人类标注，导致数据量不足和大量的人力浪费。为此，我们提出了一种名为Synth-Empathy的LLM基于的数据生成与质量、多样性选择管道，该管道能够自动生成高质量的同理心数据并筛选掉低质量数据。通过利用低同理心模型生成的数据，我们进一步提高了同理心响应性能，并在多个基准上达到了最先进的（SoTA）结果。此外，我们的模型在各种人类评估基准上均表现出色，证明了其在实际应用中的有效性和鲁棒性。进一步地，我们展示了数据量与质量之间的权衡，提供了同理心数据生成与选择方面的见解。|
|**2024-07-31**|**LLM-for-X: Application-agnostic Integration of Large Language Models to Support Personal Writing Workflows**|Lukas Teufelberger et.al.|[2407.21593](http://arxiv.org/abs/2407.21593)|null|为了提高生产力并优化工作流程，将大型语言模型（LLM）功能嵌入应用程序的趋势正在增长，从基于浏览器的网络应用到在个人计算机上运行的原生应用。我们介绍了一种系统级快捷方式层——LLM-for-X，它通过轻量级弹出式对话框无缝地向任何应用程序添加LLM服务。我们的原生层通过统一的聊天前端作为编程接口或自定义API调用，将前端应用程序与流行的LLM后端（如ChatGPT和Gemini）无缝连接。我们展示了LLM-for-X在Microsoft Office、VSCode、Adobe Acrobat以及Overleaf等流行网络应用中的优势。在评估中，我们将LLM-for-X与ChatGPT的网页界面进行了任务比较，证明了我们的方法能够提供快速、高效且易于使用的LLM辅助，无需切换上下文支持写作和阅读任务，同时对特定应用无特定依赖。|
|**2024-07-31**|**A Performance Study of LLM-Generated Code on Leetcode**|Tristan Coignion et.al.|[2407.21579](http://arxiv.org/abs/2407.21579)|null|本文研究了大型语言模型（LLM）在代码生成方面的效率，并使用来自LeetCode的数据集评估了它们与人类编写的解决方案的性能。我们对比了18个LLM，考虑了模型温度和成功率等因素对代码性能的影响。研究引入了一种新颖的方法来度量和比较LLM生成代码的速度，结果表明，采用不同LLM时，生成的代码性能相当。我们还发现，LLM生成的代码平均而言比人类编写的代码更高效。论文进一步讨论了使用LeetCode作为基准数据集、潜在数据污染带来的限制以及平台测量可靠性的问题。我们认为，我们的发现有助于更好地理解LLM在代码生成领域的能力，并为该领域未来的优化奠定了基础。|
|**2024-07-31**|**PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning**|Min Jae Jung et.al.|[2407.21571](http://arxiv.org/abs/2407.21571)|null|大型语言模型（LLM）在持续学习过程中遇到重大挑战，主要在于灾难性遗忘现象，即新信息会覆盖之前获得的知识。这一局限性导致了大量环境和经济资源的浪费。本研究引入了一种名为PMoE（Progressive Mixture of Experts with Asymmetric Transformer）的解决方案，旨在通过采用具有浅层用于一般知识和深层用于新知识的不对称设计来最小化遗忘。PMoE在深层引入了逐步增加的专家，并配备了一个路由器，该路由器能够高效地将新知识分配给合适的专家。  路由器位于深层附近，利用深度特征聚合已整合的信息。这使得路由器能够有效地执行任务，将新知识分配给逐步增加的深层专家。通过在TRACE数据集和通用语言理解数据集上的广泛实验，证明了所提出的PMoE方法优于先前的最先进的方法。|
|**2024-07-31**|**CXSimulator: A User Behavior Simulation using LLM Embeddings for Web-Marketing Campaign Assessment**|Akira Kasuga et.al.|[2407.21553](http://arxiv.org/abs/2407.21553)|null|本文提出了一种名为客户体验（CX）模拟器的新型框架，旨在通过用户行为模拟来评估未测试的网络营销活动的影响。该提出的框架利用大型语言模型（LLM）将用户行为历史中的各种事件，如查看商品、使用优惠券或购买商品等，表示为语义嵌入向量。我们训练了一个模型，用于从其LLM嵌入中预测事件之间的过渡，甚至可以从多样化的训练数据中学习，从而对未知事件进行泛化。在web营销应用中，我们利用这个过渡预测模型来模拟当新的营销活动或产品展示给用户时，用户可能如何反应不同。这使得我们能够消除在线测试的高昂成本，并增强营销人员揭示洞察力的能力。我们的数值评估和使用Google商品商店的大规模公共数据集进行的用户研究证明了我们框架的有效性。|
|**2024-07-30**|**ThinK: Thinner Key Cache by Query-Driven Pruning**|Yuhui Xu et.al.|[2407.21018](http://arxiv.org/abs/2407.21018)|null|大型语言模型（LLM）在自然语言处理领域引发了一场革命，通过利用更大的模型规模和序列长度，实现了前所未有的性能。然而，随之而来的计算和内存成本的增加带来了挑战，尤其是在处理长序列时，由于注意力机制的二次复杂性，对缓存内存管理提出了严峻考验。本文专注于长上下文场景，针对推理过程中KV缓存内存消耗的效率问题进行深入探讨。与现有方法侧重于基于序列长度优化内存不同，我们揭示了KV缓存通道在权重分布不均和低秩结构特征下存在显著冗余。基于这些观察结果，我们提出了一种名为ThinK的新型查询依赖型KV缓存剪枝方法，旨在最小化注意力权重损失的同时，有选择地剪枝掉最不重要的通道。我们的方法不仅能够保持或提升模型准确率，而且相比传统的KV缓存淘汰方法，能实现超过20%的内存成本减少。通过在LLaMA3和Mistral模型上对多个长序列数据集进行的广泛评估，证明了ThinK的有效性，确立了在不牺牲性能的前提下高效部署LLM的新标准。我们还展望了将我们的方法扩展到值缓存剪枝的可能性，展示了ThinK在降低内存和计算开销方面的广泛适用性和潜力。|
|**2024-07-30**|**CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning**|Yuexi Du et.al.|[2407.21011](http://arxiv.org/abs/2407.21011)|**[link](https://github.com/xypb/cleft)**|**近期，对比语言-图像预训练（CLIP）的进展在多任务自监督表示学习领域取得了显著成果。然而，现有CLIP类方法往往需要大量的GPU资源和长时间的训练周期，这主要是由于模型和数据集的规模巨大，对于医学应用而言，大规模数据集并不总是常见。同时，语言模型提示主要基于与图像关联的标签进行手动提取，可能忽视了训练样本内的丰富信息。  我们提出了一种名为“高效大语言模型与提示微调”（CLEFT）的语言-图像对比学习方法，它充分利用了广泛预训练的语义和视觉模型的优势。此外，我们还提出了一种有效策略来学习基于上下文的提示，以缩小临床诊断数据与简单类别标签之间的差距。我们的方法在多个胸部X光和乳腺X光数据集上的表现均优于各种基线，达到了最先进的性能水平。  所提出的参数高效的框架可以将总可训练模型大小减少39%，并将可训练语言模型减少到仅4%，与当前的BERT编码器相比。**|
|**2024-07-30**|**MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning**|Yupeng Chen et.al.|[2407.20999](http://arxiv.org/abs/2407.20999)|null|近期，大型语言模型（LLMs）在各种任务中展现了非凡的能力。通常，LLM通过大量语料库进行预训练，并随后针对特定任务的数据集进行微调。然而，在微调过程中，LLM可能会忘记在预训练阶段学到的知识，导致一般能力下降。为了应对这一问题，我们提出了一种新的微调算法——动量过滤优化器（MoFO）。MoFO的核心思想是迭代地选择并更新具有最大动量幅度的模型参数。与全参数训练相比，MoFO在保持参数接近预训练模型的同时实现了相似的微调性能，从而减轻了知识遗忘的问题。与现有的大多数遗忘缓解方法不同，MoFO具备以下两个优势。首先，MoFO不需要访问预训练数据。这使得MoFO特别适用于预训练数据不可用的微调场景，如使用开源LLM的检查点进行微调。其次，MoFO不会改变原始损失函数。这可以避免损害模型在微调任务上的性能。我们通过严谨的收敛性分析和广泛的实验验证了MoFO的优越性，证明了它在缓解遗忘和增强微调性能方面的优势。|
|**2024-07-30**|**From Feature Importance to Natural Language Explanations Using LLMs with RAG**|Sule Tekkesinoglu et.al.|[2407.20990](http://arxiv.org/abs/2407.20990)|**[link](https://github.com/suletekkesinoglu/xai_llm_rag)**|随着机器学习在涉及人类交互的自主决策过程中的作用日益重要，理解模型输出变得越来越关键。最近，基础模型正被探索用作事后解释器，提供了一种揭示预测模型决策机制的途径。本文介绍了一种可追踪问答方法，通过利用外部知识库来指导大型语言模型（LLM）对场景理解任务中的用户查询进行响应。该知识库包含了关于模型输出的上下文细节，包括高级特征、特征重要性以及替代概率。  我们采用减法反事实推理计算特征重要性，这是一种分析在分解语义特征后输出变化的方法。为了保持对话流畅，我们从社会科学研究中提炼出四个关键特性——社交性、因果性、选择性和对比性，并将其整合到一个即时提示中，以此指导响应生成过程。我们的评估表明，生成的解释包含了这些元素，这表明它有可能在复杂模型输出与自然语言表达之间架起桥梁。|
|**2024-07-30**|**Large Language Models (LLMs) for Semantic Communication in Edge-based IoT Networks**|Alakesh Kalita et.al.|[2407.20970](http://arxiv.org/abs/2407.20970)|null|随着第五代（5G）和第六代（6G）通信技术以及物联网（IoT）的兴起，语义通信正受到研究者的关注，因为当前的通信技术正接近香农极限。另一方面，大型语言模型（LLMs）能够理解并生成类似于人类的文本，基于对数十亿参数的广泛数据集进行训练。考虑到最近的就近计算技术如边缘计算，本文概述了一个框架及其模块，其中LLMs可以在物联网网络的网络边缘下，作为语义通信的一部分，以提高高效通信效率。最后，我们讨论了一些应用，并分析了发展此类系统的挑战和机遇。|
|**2024-07-30**|**Automated Review Generation Method Based on Large Language Models**|Shican Wu et.al.|[2407.20906](http://arxiv.org/abs/2407.20906)|**[link](https://github.com/tju-ecat-ai/automaticreviewgeneration)**|**文献研究对于科学进步至关重要，但面对海量信息的挑战，我们提出了一种基于大型语言模型（LLM）的自动化综述生成方法，旨在简化文献处理流程并减轻认知负担。以丙烷脱氢（PDH）催化剂为例，该方法从343篇文章中迅速生成了全面的综述，平均每篇文章每LLM账户耗时仅数秒。对1041篇文章的进一步分析揭示了催化剂组成、结构和性能的深入见解。  认识到LLM可能出现幻觉的问题，我们实施了多层次的质量控制策略，确保了方法的可靠性和有效缓解幻觉的能力。专家验证证实，通过这种方法生成的综述不仅准确且引文完整，LLM幻觉的风险已降至低于0.5%，置信度超过95%。发布的Windows应用程序支持一键生成综述，帮助研究人员跟踪最新进展并推荐相关文献。这一方法展示了LLM在提升科学研究生产力方面的潜力，并为进一步探索奠定了基础。**|
|**2024-07-30**|**ThinkRepair: Self-Directed Automated Program Repair**|Xin Yin et.al.|[2407.20898](http://arxiv.org/abs/2407.20898)|**[link](https://github.com/vinci-grape/ThinkRepair)**|**尽管已经提出了许多自动程序修复（APR）方法，并且在修复一些特定类型的错误时取得了显著的性能，但这些方法在处理需要对错误程序的逻辑进行分析和推理的复杂错误时仍存在局限性。最近，通过提示工程训练的大规模语言模型（LLMs）因其在解决包括错误修复在内的多种任务的强大能力而引起了广泛关注。然而，提示的质量会极大地影响LLMs的能力，而手动构建高质量的提示是一个耗时的过程。  为了解决这一限制，我们提出了一种自我导向的LLM基于自动程序修复方法ThinkRepair，它分为两个主要阶段：收集阶段和修复阶段。在收集阶段，通过使用链式思考（CoT）提示指导LLMs，自动收集构成预修复知识的各种思考链。在修复阶段，目标是通过首先选择用于少量学习的示例并其次与LLMs自动交互来修复错误，根据测试信息提供反馈（如果需要的话）。  在对两个广泛研究的数据集（Defects4J和QuixBugs）的评估中，与12个最先进的APR方法进行比较，表明ThinkRepair在修复错误方面的优先级。值得注意的是，在Defects4J V1.2上，ThinkRepair成功修复了98个错误，相较于基线提升了27%-344.4%。在Defects4J V2.0上，ThinkRepair比最先进的APR方法多修复了12-65个错误。此外，在Java和Python上，ThinkRepair在QuixBugs上的表现也有了显著提升（最多分别达到31和21）。**|
|**2024-07-30**|**Effective Black Box Testing of Sentiment Analysis Classification Networks**|Parsa Karbasizadeh et.al.|[2407.20884](http://arxiv.org/abs/2407.20884)|null|基于转换器的神经网络在自然语言处理任务如情感分析中展现了卓越性能。然而，确保这些复杂架构通过全面测试保持可靠性的挑战依然存在。本文提出了一组专门设计用于评估为基于转换器的情感分析网络构建的测试套件的覆盖标准。我们的方法采用输入空间划分的黑盒策略，考虑了与情感相关的关键语言特征，包括动词、形容词、副词和名词。为了有效地生成涵盖广泛情感元素的测试用例，我们采用了k投影覆盖度量。该度量通过一次检查k个特征的子集来减少问题的复杂性，从而降低维度。通过大型语言模型生成展示特定情感特征组合的句子。从情感分析数据集实验中获得的结果表明，我们的标准和生成的测试平均提高了16%的测试覆盖率。同时，模型准确度平均下降了6.5%，显示了识别脆弱性的能力。我们的工作为通过全面测试评估改进基于转换器的情感分析系统提供了基础。|
|**2024-07-30**|**Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification**|Boyang Zhang et.al.|[2407.20859](http://arxiv.org/abs/2407.20859)|null|近期，基于大型语言模型（LLM）的自主代理在理论研究和实际应用上均取得了显著进展。这些代理能够通过外部组件扩展基础LLM的能力，在多种方式下增强性能。例如，利用GPT-3.5-Turbo核心构建的代理可能在某些任务上超越更先进的GPT-4模型。更重要的是，工具的应用使系统能够与现实世界互动，使其从仅仅生成文本转变为执行实际操作。鉴于代理的实际应用范围以及其对环境进行操作的能力，评估潜在漏洞变得至关重要。如果被黑客入侵，这些自主系统造成的损害可能会超过单一语言模型。尽管已有研究探讨了LLM代理的有害行为，但我们的研究从不同角度审视这一问题。我们引入了一种新型攻击方法，旨在误导代理执行重复或无关的操作，从而引发故障。我们使用各种攻击手段、场景和属性进行全面评估，以确定其易感性。实验结果显示，在多个场景中，这些攻击可导致超过80%的失败率。通过在多代理环境中针对实现并部署的代理进行攻击，我们强调了此类漏洞所伴随的现实风险。为了减轻此类攻击，我们提出了自我检查检测方法。然而，我们的发现显示，仅使用LLM很难有效检测到这些攻击，这凸显了这种漏洞所带来的重大风险。|
|**2024-07-30**|**Learn by Selling: Equipping Large Language Models with Product Knowledge for Context-Driven Recommendations**|Sarthak Anand et.al.|[2407.20856](http://arxiv.org/abs/2407.20856)|null|快速发展的大型语言模型(Large Language Models, LLMs)为基于上下文的产品推荐应用提供了新的可能性。然而，这些模型在这一领域的有效性高度依赖于它们对产品库存的全面理解。本文提出了一种新颖的方法来增强LLMs的产品知识能力，通过训练它们响应包含产品ID的合成搜索查询，以进行上下文相关回复。我们深入分析了这种方法，评估其效果，概述其优点，并指出了限制因素。文章还讨论了此方法的改进潜力和未来方向，提供了对LLMs在产品推荐中角色的全面理解。  请注意，上述翻译已删除所有','字符。|
|**2024-07-29**|**Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing**|Ekaterina Iakovleva et.al.|[2407.20232](http://arxiv.org/abs/2407.20232)|null|文本编辑的扩散模型在用户输入指令存在歧义时表现出有限的性能。为了解决这一问题，我们提出了Specify ANd Edit（SANE），一个用于基于扩散的编辑系统的零样本推理管道。我们利用大型语言模型（LLM）将输入指令分解为具体的指令，即应用到输入图像以满足用户请求的具体干预措施。通过一种专门为任务设计的新颖去噪指导策略，我们可以从LLM生成的指令以及原始指令中受益。我们的实验在三个基线和两个数据集上展示了SANE在所有设置中的优势。此外，我们的管道提高了编辑模型的可解释性，并增强了输出多样性。我们还证明了我们的方法可以应用于任何编辑，无论是否存在歧义。我们的代码已公开在https://github.com/fabvio/SANE。|
|**2024-07-29**|**Can Editing LLMs Inject Harm?**|Canyu Chen et.al.|[2407.20224](http://arxiv.org/abs/2407.20224)|null|知识编辑技术正逐渐被采用以高效地纠正大型语言模型（LLMs）中的错误或过时知识，这主要是因为从头开始重新训练的高成本。同时，一个亟待探索但未充分研究的问题是：知识编辑是否可以用于向LLMs注入危害？在本文中，我们提出将知识编辑重新定义为LLMs面临的一种新类型安全性威胁，即编辑攻击，并通过构建一个新的数据集EditAttack进行了系统性的调查。  具体而言，我们聚焦于编辑攻击的两个典型安全性风险：误导性信息注入和偏见注入。对于误导性信息注入的风险，我们首先将其细分为常识误导性信息注入和长尾误导性信息注入。然后，我们发现编辑攻击能够有效地向LLMs注入这两种类型的误导性信息，尤其是对常识误导性信息注入的有效性特别高。  对于偏见注入的风险，我们揭示了一个关键点，即不仅可以通过高有效性向LLMs注入有偏见的句子，而且单个有偏见的句子注入就足以导致LLMs的总体输出出现显著偏见增加，即使这些输出与注入的句子高度无关，这表明了编辑攻击对LLMs整体公平性的灾难性影响。  进一步地，我们展示了编辑攻击的高隐蔽性，通过其对LLMs一般知识和推理能力的影响来衡量，以及在实证证据的基础上说明了防御编辑攻击的困难性。我们的发现揭示了知识编辑技术在损害LLMs安全对齐方面正在出现的滥用风险。|
|**2024-07-29**|**QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval**|Hongming Tan et.al.|[2407.20207](http://arxiv.org/abs/2407.20207)|null|在密集检索领域，将长文本转化为稠密向量时可能会导致信息丢失，从而影响查询与文本的匹配准确性。此外，质量较低、噪声过多或关键信息稀疏的文本往往难以与相关查询良好匹配。当前研究主要集中在提升句嵌入模型或检索流程上。本工作提出了一种新颖的文本增强框架用于密集检索。该框架通过将原始文档转化为信息密集型文本格式，以补充原文本，有效解决上述问题，同时无需修改嵌入或检索方法。通过大型语言模型（LLM）零样本提示生成两种文本表示：问题-答案对和事件驱动元素。我们将此方法命名为QAEA-DR：统一问题生成与事件提取的文本增强框架，用于密集检索。为了进一步提升生成文本的质量，引入了一种基于评分的评估与再生成机制于LLM提示过程中。我们的QAEA-DR模型对密集检索产生了积极影响，这一观点得到了理论分析和实验证据的支持。|
|**2024-07-29**|**MindSearch: Mimicking Human Minds Elicits Deep AI Searcher**|Zehui Chen et.al.|[2407.20183](http://arxiv.org/abs/2407.20183)|**[link](https://github.com/internlm/mindsearch)**|**信息检索与整合是一个复杂认知任务，需要投入大量时间和精力。受到大型语言模型（LLM）近期显著进展的启发，近期工作尝试通过结合搜索引擎与LLM解决这一问题。然而，这些方法仍然因三个挑战而获得不令人满意的性能：（1）复杂的请求往往无法准确且完整地由搜索引擎检索；（2）需要整合的信息分布在多个网页上，并夹杂着大量噪音；（3）大量长文本的网页可能迅速超过LLM的最大上下文长度。  受人类在解决这些问题时思维过程的启发，我们引入了MindSearch，旨在模仿人类在互联网信息检索与整合过程中的思维模式，可通过一个简单而有效的基于LLM的多代理框架实现。WebPlanner以动态图构建过程来模拟人类多步骤信息检索的思维：它将用户查询分解为原子子问题作为图中的节点，并根据从WebSearcher获取的搜索结果逐步扩展图。WebSearcher承担每个子问题，执行分层信息检索并从搜索引擎收集有价值的信息供WebPlanner使用。MindSearch的多代理设计使其整体框架能够并行从超过300个网页中检索和整合信息，仅需3分钟，相当于节省了3小时的人类努力。  MindSearch在深度和广度上显著提高了响应质量，适用于封闭集和开放集的问答问题。此外，基于InternLM2.5-7B的MindSearch生成的响应被人类认为优于ChatGPT-Web和Perplexity.ai应用，这表明MindSearch已经能够提供与专有AI搜索引擎相竞争的解决方案。**|
|**2024-07-29**|**Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning**|Xingchen Zeng et.al.|[2407.20174](http://arxiv.org/abs/2407.20174)|**[link](https://github.com/zengxingchen/chartqa-mllm)**|**新兴的多模态大型语言模型（MLLMs）在图表问题回答（CQA）领域展现出巨大的潜力。近期的努力主要集中在通过数据收集和合成扩大训练数据集（包括图表、数据表格和问答对）。然而，我们对现有MLLMs和CQA数据集的实证研究揭示了显著的差距。  首先，当前的数据收集和合成工作侧重于数据量，而忽略了精细的视觉编码和问答任务的考虑，导致数据分布与实际CQA场景大相径庭，不平衡性明显。其次，现有的工作遵循了最初设计用于自然图像的基础MLLMs的训练配方，对于图表的独特特性，如丰富的文本元素的适应性探索不足。  为了填补这一空白，我们提出了一个可视化参考指令调整方法，以指导训练数据集的增强和模型开发。具体来说，我们提出了一种新颖的数据引擎，能够从现有数据集中有效地筛选出多样性和高质量的数据，并随后利用基于LLM的生成技术对数据进行细化和扩充，使其更好地与实际问答任务和视觉编码相匹配。  然后，为了促进对图表特性的适应性，我们利用丰富化数据来训练一个MLLM，通过解冻视觉编码器并引入混合分辨率适应策略，以增强细微粒度识别能力。实验结果验证了该方法的有效性。即使使用较少的训练示例，我们的模型也始终优于现有的CQA模型，在已建立的基准上表现出色。我们还贡献了一个数据集分割作为未来研究的基准。该论文的源代码和数据集可访问于https://github.com/zengxingchen/ChartQA-MLLM。**|
|**2024-07-29**|**Diffusion Feedback Helps CLIP See Better**|Wenxuan Wang et.al.|[2407.20171](http://arxiv.org/abs/2407.20171)|**[link](https://github.com/baaivision/diva)**|对比语言-图像预训练（CLIP）在跨领域和模态抽象开放世界表示方面表现出色，已成为各种视觉和多模态任务的基础。然而，近期的研究揭示了CLIP在视觉方面的严重局限性，如难以区分方向、数量、颜色、结构等。这些视觉局限性也限制了基于CLIP构建的大型多模态语言模型（MLLMs）的感知能力。主要原因是用于训练CLIP的图像-文本对固有偏见，由于文本的不明确性和图片多样性不足。  为此，我们提出了一种针对CLIP模型的简单后处理方法，通过自我监督的扩散过程极大地克服了其视觉局限性。我们引入了DIVA，即作为CLIP视觉辅助的扩散模型。具体而言，DIVA利用文本到图像扩散模型的生成反馈来优化CLIP表示，仅使用图像（不包括对应文本）。我们证明DIVA在MMVP-VLM基准上显著提高了CLIP的性能，该基准广泛评估了细微的视觉能力（例如，3-7%）。此外，我们的框架增强了MLLMs和视觉模型在多模态理解和分割任务上的表现。在29个图像分类和检索基准上的全面评估证实了我们的框架保留了CLIP强大的零样本能力。代码将在https://github.com/baaivision/DIVA公开。|
|**2024-07-29**|**Language-Conditioned Offline RL for Multi-Robot Navigation**|Steven Morad et.al.|[2407.20164](http://arxiv.org/abs/2407.20164)|null|我们提出了一种方法，用于为多机器人团队开发能够理解并遵循自然语言指令的导航策略。我们利用预训练大型语言模型（LLM）的嵌入来条件化这些策略，并通过使用仅20分钟随机收集的数据进行离线强化学习来训练它们。在五台真实机器人的实验中，这些策略对未见过的命令具有良好的泛化能力，表明它们理解了LLM的潜在空间。我们的方法不需要模拟器或环境模型，并产生低延迟的控制策略，可以直接部署到真实机器人上而无需进一步调优。更多信息和实验视频请参阅https://sites.google.com/view/llm-marl。|
|**2024-07-29**|**rLLM: Relational Table Learning with LLMs**|Weichen Li et.al.|[2407.20157](http://arxiv.org/abs/2407.20157)|**[link](https://github.com/rllm-project/rllm)**|**我们提出了一种名为rLLM（关系LLM）的PyTorch库，旨在通过大型语言模型（LLMs）实现关系表学习（RTL）。核心理念是将最先进的图神经网络、LLMs和表神经网络分解为标准化模块，以实现快速构建新型RTL型模型的简单“组合、对齐和联合训练”方式。为了说明rLLM的使用，我们引入了一个简单的RTL方法名为BRIDGE。此外，我们通过增强经典数据集，提出了三个新的关系表格数据集（TML1M、TLF2K和TACM12K）。我们希望rLLM能够作为用于RTL相关任务的有用且易于使用的开发框架。我们的代码可在以下位置获取：https://github.com/rllm-project/rllm。**|
|**2024-07-29**|**ByteCheckpoint: A Unified Checkpointing System for LLM Development**|Borui Wan et.al.|[2407.20143](http://arxiv.org/abs/2407.20143)|null|在构建实际世界大型语言模型（LLMs）时，需要在持久存储中检查训练状态以防止潜在的软件和硬件故障，并支持训练管道内的检查点转移以及跨任务使用。由于LLMs的规模庞大，保存和加载检查点往往会导致令人难以接受的分钟级延迟，极大地降低了训练效率。此外，在跨任务转移检查点时，通常需要执行检查点重新分片，即根据特定任务的特性和资源配额将检查点加载到不同的并行配置中。先前的检查点系统假设并行配置一致，未能解决在重新分片期间转换检查点的复杂性。而且，在工业平台中，开发者从不同的训练框架创建检查点，每个框架都有其独特的存储和I/O逻辑，这增加了统一管理和优化检查点的复杂性。为了解决这些挑战，我们引入了ByteCheckpoint，一个支持自动在线检查点重新分片的PyTorch原生多框架LLM检查点系统。ByteCheckpoint采用数据/元数据分离的存储架构，解耦了检查点存储与所采用的并行策略和训练框架。我们设计了一种高效的异步张量合并技术来解决不规则张量分片问题，并提出了多项I/O性能优化措施，显著提高了检查点保存和加载的效率。实验结果表明，ByteCheckpoint在减少检查点保存（最高可达529.22倍）和加载（最高可达3.51倍）成本方面具有明显优势，与基线方法相比。|
|**2024-07-29**|**Orca: Ocean Significant Wave Height Estimation with Spatio-temporally Aware Large Language Models**|Zhe Li et.al.|[2407.20053](http://arxiv.org/abs/2407.20053)|null|显著波高（SWH）在海洋科学中是一个关键指标，精确的SWH估计对于各种应用至关重要，例如海洋能开发、渔业、潜在风险的早期预警系统等。基于数值模型和物理理论的传统SWH估算方法受到计算效率低下的限制。近年来，机器学习作为一种有吸引力的替代方案，已用于提高准确度并减少计算时间。然而，由于观测技术有限和成本高昂，实际数据的稀缺性限制了机器学习模型的潜力。为了克服这些局限性，我们提出了一种海洋SWH估算框架，名为Orca。具体而言，Orca通过引入一个新颖的空间时间感知编码模块，增强了经典语言模型在空间时间和数据量有限情况下的推理能力。通过将有限的浮标观测数据进行时间分割、编码浮标的地理位置、设计提示模板，Orca利用大语言模型的强大泛化能力，有效地使用有限的数据对显著波高进行估算。在墨西哥湾的实验结果表明，Orca在SWH估算方面达到了最先进的性能水平。|
|**2024-07-26**|**Small Molecule Optimization with Large Language Models**|Philipp Guevorguian et.al.|[2407.18897](http://arxiv.org/abs/2407.18897)|**[link](https://github.com/yerevann/chemlactica)**|**近期大型语言模型的发展为生成分子药物设计带来了新的可能性。我们介绍了一种名为“Chemlactica”和“Chemma”的语言模型，它们均基于一个含有1.1亿个分子及计算得出属性的全新数据集，共计400亿个令牌进行微调。这些模型在生成具有指定属性的分子以及从有限样本预测新分子特性方面表现出色。  我们提出了一种新型优化算法，该算法利用我们的语言模型对任意属性进行优化，同时仅通过黑盒式接口访问有限信息。我们的方法结合了遗传算法、拒绝采样和提示优化的概念。该算法在多个分子优化基准测试中均取得了最先进的性能，包括在与先前方法相比提高了8%的“Practical Molecular Optimization”任务上表现出色。  我们公开发布了训练数据集、语言模型和优化算法的代码。**|
|**2024-07-26**|**Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models**|Mutahar Safdar et.al.|[2407.18827](http://arxiv.org/abs/2407.18827)|null|数据驱动的增材制造(AM)研究在近年来取得了显著的成功，这导致了大量的科学文献涌现。这些文献中的知识涉及AM和人工智能(AI)的上下文，但尚未以集成的方式进行挖掘和形式化。从这些作品中提取科学信息需要大量的努力和时间。在AM领域的专家已经贡献了超过二十多篇综述论文来总结这些工作。然而，与AM和AI相关的特定信息仍然需要手动努力来提取。最近，基础模型如BERT（双向编码表示变换器）或GPT（预训练生成型变换器）在文本数据上的成功，为加速科学信息提取提供了可能性。我们提出了一种框架，旨在促进AM和AI专家之间的合作，以连续从数据驱动的AM文献中提取科学信息。基于提出的框架实现了一个演示工具，并开展了一个案例研究，以提取与数据集、建模、传感和AM系统类别相关的信息。我们展示了大型语言模型(LLMs)加快从数据驱动的AM文献中提取相关信息的能力。在未来，该框架可以用于从工程学科的设计和制造文献中提取信息。|
|**2024-07-26**|**Automatic Detection of Moral Values in Music Lyrics**|Vjosa Preniqi et.al.|[2407.18787](http://arxiv.org/abs/2407.18787)|**[link](https://github.com/vjosapreniqi/ismir-mft-values)**|道德价值观在评估信息、做出决策和对重要社会问题形成判断方面发挥着基础性作用。从歌词中快速提取道德价值的可能性使我们对音乐聆听行为有更深的理解。基于道德基础理论（MFT），我们对一组经过大型语言模型（GPT-4）生成的2,721个合成歌词微调的变压器基语言模型（BERT）进行了任务，以检测200首由两位专家注释的真实音乐歌词中的道德价值观。我们通过一系列基准测试（包括离域（BERT在MFT注释的社交媒体文本上微调）和零射击（GPT-4）分类）来评估它们的预测能力。所提出的方法在所有实验中均表现出最佳准确性，平均F1加权得分为0.8。与基准模型相比，该性能平均高出5%。在二元分类的精确度上，所提出的方法平均高出基准模型12%。我们的方法贡献了无注释的歌词道德学习以及对大型语言模型在音乐中道德表达的知识提炼，并提供了这些技术对创意产业和音乐文化潜在影响的有用见解。|
|**2024-07-26**|**The power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs**|Aleix Sant et.al.|[2407.18786](http://arxiv.org/abs/2407.18786)|null|本文通过大型语言模型（LLM）的视角探讨了机器翻译中的性别偏见问题。研究使用了四个广泛使用的测试集，对英语到加泰罗尼亚语（En $\rightarrow$Ca）和英语到西班牙语（En$\rightarrow$ Es）的翻译方向进行基准测试，与最先进的神经机器翻译（NMT）模型进行对比，评估各种基础LLM的翻译质量和性别偏见情况。研究发现，所有模型普遍存在性别偏见现象，其中基础LLM的偏见程度比NMT模型更高。为了对抗这种偏见，研究探索了对指令调优LLM应用的提示工程技巧。研究识别出一种提示结构，能够显著降低性别偏见，相比更直接的提示，在WinoMT评估数据集上减少了高达12%的性别偏见。这些结果显著缩小了LLM与传统NMT系统在性别偏见准确性方面的差距。|
|**2024-07-26**|**TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on OGD portals**|Kevin Kliimask et.al.|[2407.18764](http://arxiv.org/abs/2407.18764)|null|自2000年代中期以来，推动开放政府数据（OGD）的努力在各级政府中获得了显著的势头。随着越来越多的数据集被发布到OGD门户上，查找特定数据变得越来越困难，导致信息过载。完整且准确的数据集文档，包括与数据集关联的适当标签，对于提高数据集可发现性和可访问性至关重要。对爱沙尼亚开放数据门户的分析揭示，11%的数据集没有关联标签，而26%的数据集仅有一个标签被分配，这表明了门户内数据可发现性和可访问性面临的挑战。根据最近的开放数据成熟度报告，该门户被认为是领先者。本研究的目标是提出一种自动化解决方案，以改善OGD门户上的数据集标签，从而提高数据集的可发现性。本文介绍了Tagify——一个利用大型语言模型（LLM），如GPT-3.5-turbo和GPT-4自动为数据集生成标签的原型，以英语和爱沙尼亚语为数据集生成标签，从而增强数据发布者准备的元数据，并通过改善数据用户在OGD门户上的数据发现性来提高数据的可访问性。开发的解决方案经过用户评估，并收集了他们的反馈，以定义未来原型改进的议程。|
|**2024-07-26**|**Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery**|Yuni Susanti et.al.|[2407.18752](http://arxiv.org/abs/2407.18752)|**[link](https://github.com/littleflow3r/kg-structure-as-prompt)**|**本文探讨了基于元数据而非实际数据值的大型语言模型（LLMs）在因果发现问题上的新视角，即知识导向的因果发现。我们关注小型语言模型（SLMs，参数少于10亿）如何通过提示式学习进行知识导向的因果发现。具体而言，我们提出了一种名为“基于知识图谱的结构提示”（KG Structure as Prompt）的新方法，用于将知识图谱中的结构信息，如共邻节点和元路径，整合到提示式学习中，以增强SLMs的能力。  在三种类型的生命科学和开放域数据集下的少量样本设置下，我们的实验结果表明，这种方法的有效性超越了许多基线，并且甚至超过了在完整数据集上进行常规微调的传统方法。我们的研究进一步揭示了小型语言模型的强大能力：结合知识图谱和提示式学习，小型语言模型显示出超越参数更多LLMs的潜力。  我们已经在GitHub上提供了代码和数据集。**|
|**2024-07-26**|**Towards Effective and Efficient Continual Pre-training of Large Language Models**|Jie Chen et.al.|[2407.18743](http://arxiv.org/abs/2407.18743)|null|这篇技术报告介绍了持续预训练（CPT）方法的最新进展，特别关注了增强大型语言模型在特定领域或任务上的能力。报告以Llama-3（8B）为例，这是一个显著提升了其在中文理解和科学推理能力的基线模型。为了在增强新能力的同时保持原有能力，我们设计了数据混合和课程策略，利用现有数据集并合成高质量数据集。具体地，我们基于相关网页生成多学科的科学问题与答案（QA）对，并将这些合成数据融入模型训练，以提升Llama-3的科学推理能力。经过这一系列改进后的模型被称为Llama-3-SynE（合成数据增强的Llama-3）。报告还通过较小规模的TinyLlama模型进行调参实验，并利用从这些实验中得到的发现来训练基线模型。  多个评估基准的广泛实验结果表明，我们的方法能显著提高基线模型的性能，包括通用能力（C-Eval上+8.81分，CMMLU上+6.31分）和科学推理能力（MATH上+12.00分，SciEval上+4.13分），而不会损害原有的能力。该模型、数据和代码已开源发布于https://github.com/RUC-GSAI/Llama-3-SynE。|
|**2024-07-26**|**Towards Generalized Offensive Language Identification**|Alphaeus Dmonte et.al.|[2407.18738](http://arxiv.org/abs/2407.18738)|null|互联网上具有攻击性的内容，包括仇恨言论和网络欺凌，是一个全球性问题。因此，机器学习（ML）和自然语言处理（NLP）社区对此给予了广泛关注。为了应对这一挑战，已经开发出了多种自动识别可能有害内容并减轻其影响的系统。这些系统主要采用两种策略：（1）使用公开可用的模型和应用端点，包括激发大型语言模型（LLMs）；（2）注释数据集，并在这些数据集上训练机器学习模型。然而，这两种方法的通用性尚不清楚，而且它们在实际环境和非领域内的应用也常受到质疑。本文通过一个新颖的通用基准对攻击性语言检测模型和数据集的通用性进行了实证评估。我们针对通用性提出了三个研究问题，并得出了结论。这些发现将有助于构建更强大的现实世界攻击性语言检测系统。|
|**2024-07-26**|**LLASP: Fine-tuning Large Language Models for Answer Set Programming**|Erica Coppolillo et.al.|[2407.18723](http://arxiv.org/abs/2407.18723)|null|近期，大型语言模型（LLMs）在自然语言处理任务中展现出了巨大的潜力，尤其是在代码生成方面。尽管在适应LLMs以生成多种指令性编程语言和任务的代码方面取得了显著进展，但它们在处理声明式形式化语言，如答案集编程（ASP）时的能力仍有待探索。本文旨在探讨LLMs在ASP代码生成方面的应用可能性。首先，我们对当前最先进的LLMs进行了系统评估。尽管这些模型在参数数量、训练数据和计算资源等方面表现出色，但实证结果表明，它们在生成正确ASP程序方面的表现并不理想。因此，我们提出了一种名为LLASP的轻量级模型，专门用于编码ASP程序的基本模式。为了实现这一目标，我们创建了一个包含广泛基本问题规范的自定义数据集，这些规范可以被编码为ASP。我们的实验结果显示，LLASP生成的ASP程序的质量令人印象深刻。与未经过微调的版本相比，以及与大多数渴望型LLM候选者，尤其是从语义角度来看，其表现均优于多数。所有用于执行实验的代码和数据都已公开发布于https://anonymous.4open.science/r/LLASP-D86C/。|
|**2024-07-26**|**Neurosymbolic AI for Enhancing Instructability in Generative AI**|Amit Sheth et.al.|[2407.18722](http://arxiv.org/abs/2407.18722)|null|生成式人工智能，特别是通过大型语言模型（LLMs），在文本、图像和音乐等内容创作领域实现了革命性变革，展示了遵循指令的提示能力，很大程度上得益于指令调优。指令调优是一种监督式微调方法，通过训练数据集来实现特定任务及其对应指令格式化，这种方法系统性地增强了模型执行提供指示的能力。尽管如此，LLMs 在一致理解和执行复杂、多步骤指令以及将这些指令推广到新任务方面仍面临挑战，这对于更广泛地应用于实际场景至关重要。本文探讨了为何神经符号AI能提供增强LLMs指令可理解性的更好途径。我们探索使用符号任务规划器分解高级指令为结构化任务，使用神经语义解析器将这些任务落地为可执行操作，以及使用神经符号执行器实施这些操作的同时动态维护明确的状态表示。我们也寻求展示，神经符号方法能够增强任务执行的可靠性和上下文意识，使LLMs能够以更高的精度和灵活性动态解释和响应更广泛的指令上下文。|
|**2024-07-26**|**Recursive Introspection: Teaching Language Model Agents How to Self-Improve**|Yuxiao Qu et.al.|[2407.18219](http://arxiv.org/abs/2407.18219)|null|在使基础模型具备自我反省能力以促进智能代理行为的关键方面在于使其能够对其行为、推理以及在可用计算或交互增加时纠正错误的能力进行自我反思。即使是最强的专有大型语言模型（LLMs）也未能展现出在明确告知其犯错的情况下，能够连续改进其响应序列的能力。本文提出了一种名为RISE（递归内省）的方法，用于微调LLMs以引入这一能力，尽管之前的研究曾假设这种能力可能无法实现。我们的方法规定了一个迭代微调过程，该过程尝试教授模型如何在其解决困难测试时问题的不成功尝试后修改其响应，并可选地获得额外的环境反馈。RISE将单轮提示的微调视为解决多轮马尔科夫决策过程（MDP），其中初始状态为提示。受在线模仿学习和强化学习原理的启发，我们提出了多轮数据收集和训练策略，旨在赋予LLM递归检测并修正其先前错误并在后续迭代中进行纠正的能力。  我们的实验表明，RISE使Llama2、Llama3和Mistral模型在数学推理任务上通过更多轮次改善自己，与给定等量推理时间计算相比，超过了几种单轮策略。我们还发现，RISE具有良好的可扩展性，通常随着更强大的模型而获得更大的收益。我们的分析显示，RISE对困难提示的响应进行了有意义的改进，以达到正确的解决方案，同时没有因为表达更复杂的分布而导致单轮能力受到影响。|
|**2024-07-26**|**Exploring Scaling Trends in LLM Robustness**|Nikolaus Howe et.al.|[2407.18213](http://arxiv.org/abs/2407.18213)|**[link](https://github.com/AlignmentResearch/scaling-llm-robustness-paper)**|语言模型的能力可预测地通过增加模型的大小和训练数据而得到改善。受此启发，已训练了一系列越来越大的语言模型，这些模型展现出了令人印象深刻的能力。然而，这些模型对对抗性提示（如“越狱”攻击）非常脆弱，这类攻击会操控模型执行不希望的行为，从而构成了重大的误用风险。先前的研究表明，随着模型和数据规模的增加，计算机视觉模型的鲁棒性也会提高，因此提出了这样一个问题：语言模型的鲁棒性是否也会随规模的扩大而提升？我们通过实证研究回答了这个问题，发现更大的模型在对抗性训练下有显著更好的表现，但在没有明确防御措施的情况下，模型规模的增加并没有带来任何益处。|
|**2024-07-25**|**Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models**|Sanae Lotfi et.al.|[2407.18158](http://arxiv.org/abs/2407.18158)|null|大型语言模型（LLM）在预测序列中的下一个令牌方面表现出色。近期的研究通过压缩技术计算了LLM的非空泛化边界，但对于十亿参数级别的大型模型，这些边界显得无意义。此外，这些边界是在非常有限的压缩技术下获得的，限制了生成质量较低文本的压缩模型。更关键的是，现有边界依赖于训练集中独立同分布（IID）文档的数量，而忽略了训练集内数量庞大的非IID构成令牌，这使得进一步提高边界紧致性潜力未被充分利用。  本研究采用鞅的性质来推导泛化边界，这些边界能够从大型语言模型训练集中包含的大量令牌中获益。与训练集相比，数据集包含的令牌数量远多于文档，因此我们的泛化边界不仅容忍了更为宽松的压缩方案，实际上还能从这些方案中获益。我们通过Monarch矩阵、Kronecker因子分解和后训练量化等方法，为LLM（如LLaMA2-70B）实现了非空泛化边界。与以往的方法不同，我们的工作首次为在实践中部署并生成高质量文本的模型实现了非空泛化边界。|
|**2024-07-26**|**Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic**|Fakhraddin Alwajih et.al.|[2407.18129](http://arxiv.org/abs/2407.18129)|null|近期的进展显著提高了多模态大型语言模型（MLLM）在生成和理解图像到文本内容方面的功能。尽管取得了这些成功，但进步主要局限于英语，由于其他语言如阿拉伯语高质量多模态资源的稀缺性，这限制了阿拉伯语等语言中竞争性模型的发展。为了缓解这一状况，我们引入了一个高效的阿拉伯语多模态助手——Dallah，它基于LLaMA-2先进语言模型来促进多模态交互。Dallah在阿拉伯语MLLM中表现出色，实现了最先进的性能。通过细调六个阿拉伯方言，Dallah展示了其处理包含文本和视觉元素的复杂方言互动的能力。该模型在两个基准测试中表现出色：一个评估其现代标准阿拉伯语（MSA）性能，另一个专门用于评估方言响应。  除了在多模态交互任务中的稳健性能外，Dallah有望引领进一步开发方言意识的阿拉伯语MLLM的发展。|
|**2024-07-25**|**Fine-Tuning Large Language Models for Stock Return Prediction Using Newsflow**|Tian Guo et.al.|[2407.18103](http://arxiv.org/abs/2407.18103)|null|大型语言模型（LLM）及其微调技术在各种语言理解和生成任务中表现出优越的性能。本文探讨了将LLM用于基于金融新闻流的股票回报预测的微调方法。在量化投资领域，回报预测是后续任务如股票挑选和组合优化等的基础。我们构建了一个包括文本表示和预测模块的模型。提出了比较仅编码器和仅解码器LLM的两种方法，因为它们以不同的方式生成文本表示。这些不同表示对预测性能的影响仍是一个开放的问题。同时，我们比较了将LLM的token级表示集成到预测模块中的两种简单方法。在真实新闻和投资范围内进行的实验揭示以下结果：（1）从LLM的token级嵌入聚合的表示通常能产生增强长期和长期短期投资组合性能的回报预测；（2）在相对较大的投资范围内，基于解码器的LLM预测模型导致更强的投资组合，而在较小的范围内，没有一致的赢家；（3）从LLM文本表示中导出的回报预测对于投资组合构造是一个强大的信号，优于传统的情绪得分。|
|**2024-07-25**|**PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization**|Christopher Clarke et.al.|[2407.18078](http://arxiv.org/abs/2407.18078)|**[link](https://github.com/ChrisIsKing/Parameter-Efficient-Personalization)**|**近期，大型语言模型（Large Language Models，LLMs）的兴起为人类与AI的交互开辟了新的篇章。这些先进模型，以Chat-GPT为代表，展现了在语言理解方面的惊人能力。然而，随着LLM规模的指数级增长，一个关键维度——模型个性化——的研究却相对匮乏。大型基础模型如GPT-3等侧重于构建通用模型，适用于广泛的任务和用户群体。这种策略强调了模型的泛化能力，将用户视为整体而非个体。虽然在许多常见应用中实用，但这种一刀切的方法往往无法满足人类多样性和个性化需求的丰富性。为了探讨这一问题，我们引入了PEFT-U基准：一个用于构建和评估面向用户的自然语言处理（NLP）模型的新数据集。PEFT-U包含了多元且个性化的表达任务，其中同一输入对于不同用户可能有不同的偏好。通过PEFT-U，我们探索了如何高效地个性化LLM以适应用户特定偏好，特别是在多样化的用户中心任务背景下。**|
|**2024-07-25**|**C2P: Featuring Large Language Models with Causal Reasoning**|Abdolmahdi Bagheri et.al.|[2407.18069](http://arxiv.org/abs/2407.18069)|null|因果推理是大型语言模型（LLM）达到人类级智能的主要障碍。为此，我们引入了因果链提示（C2P），这是第一个为当前LLM提供因果推理能力的推理框架。C2P自主运行，在因果学习和推理阶段均无需依赖外部工具或模块，并且可以无缝集成到LLM的训练或微调过程中。在各种基准数据集上的实验结果表明，C2P显著提高了LLM的因果学习和后续推理准确性。  我们展示了如何通过C2P增强LLM在现实世界场景中的因果推理能力，解决医疗、医学、经济学、教育、社会科学、环境科学和市场营销等领域中的复杂问题。利用少示例学习，GPT-4 Turbo 使用C2P，仅使用六个示例就实现了显著的性能提升，推理准确性比在类似情况下近乎随机运行的最先进LLM高出33%以上。这证明了将C2P集成到LLM训练或微调过程中的潜力，从而赋予这些模型高级因果推理能力，具有变革性意义。|
|**2024-07-25**|**ComPeer: A Generative Conversational Agent for Proactive Peer Support**|Tianjian Liu et.al.|[2407.18064](http://arxiv.org/abs/2407.18064)|**[link](https://github.com/liutj9/compeer)**|本文探讨了交互式代理（CA）作为同伴支持者在心理健康领域的广泛应用及益处。然而，当前的同伴支持型CA要么由用户主动触发，要么遵循预设规则以启动对话，这可能阻碍用户与CA建立长期关系，从而影响长期益处。为了克服这一挑战，我们开发了ComPeer——一种生成式CA，它能够主动提供适应性的同伴支持。  ComPeer利用大型语言模型检测并反映对话中的关键事件，以此来策略性地规划主动关怀的时间和内容。此外，ComPeer还整合了同伴支持策略、对话历史以及其个性化的元素到生成的消息中。通过一项为期一周的跨组实验（参与人数：24），我们展示了ComPeer在长时间内提供同伴支持的能力，并且与基于用户的主动触发的CA相比，显著提升了用户的参与度。  这项研究强调了生成式CA在同伴支持领域的潜力，特别是它们如何通过主动关怀策略促进更深入、更持续的人际互动，从而为用户提供长期的心理健康益处。|
|**2024-07-25**|**Audio Entailment: Assessing Deductive Reasoning for Audio Understanding**|Soham Deshmukh et.al.|[2407.18062](http://arxiv.org/abs/2407.18062)|**[link](https://github.com/microsoft/audioentailment)**|**近期文献在构建音频基础模型时使用了语言。这些音频-语言模型（ALMs）通过大量音频文本对进行训练，并在文本到音频检索、字幕和问答等任务上表现出卓越的性能。然而，它们在执行更复杂的开放性任务，如交互式问答时的能力，需要逻辑推理技能，而这一领域尚未得到充分评估。  我们引入了一个名为音频蕴含的新任务，用于评估ALM的演绎推理能力。这个任务评估音频内容的文本描述（假设）是否可以从音频记录（前提）中推断出来，结论可能是蕴含、中立或矛盾，取决于证据的充分性。我们创建了两个数据集来完成这项任务，音频记录来自两个音频字幕数据集——AudioCaps和Clotho，而假设则由大型语言模型（LLMs）生成。  我们对最先进的ALMs进行了基准测试，并发现它们在零次学习和线性探针评估中的逻辑推理能力存在不足。最后，我们提出了“先字幕后推理”这一中间步骤，通过这种方式可以分别提高ALMs在零次学习和线性探针评估中的表现绝对值6%和3%。**|
|**2024-07-25**|**Difficulty Estimation and Simplification of French Text Using LLMs**|Henri Jamet et.al.|[2407.18061](http://arxiv.org/abs/2407.18061)|null|我们利用生成式大型语言模型来开发外语学习应用，专注于评估外语文本的难度并将其简化至较低难度级别。我们将这两个任务都视为预测问题，并通过使用有标签示例、迁移学习和大型语言模型构建了一个难度分类模型，相较于以往方法，该模型在准确性上表现出色。对于简化过程，我们评估了简化质量与意义保留之间的权衡，比较了零初始化和微调大语言模型的表现。结果显示，通过有限的微调，可以获得具有意义的文本简化结果。我们的实验在法语文本上进行，但我们的方法具有语言无关性，并直接适用于其他外语。|
|**2024-07-24**|**I Could've Asked That: Reformulating Unanswerable Questions**|Wenting Zhao et.al.|[2407.17469](http://arxiv.org/abs/2407.17469)|**[link](https://github.com/wenting-zhao/couldask)**|**在从不熟悉文档中获取信息时，用户经常提出无法由文档回答的问题。现有的大型语言模型（LLMs）能够识别这些无法回答的问题，但它们并未帮助用户重新构建问题，从而降低了它们的整体实用性。我们精心编排了CouldAsk，一个用于文档支持的问答任务的评估基准，旨在研究重新构建无法回答问题的能力。这个基准包括了现有的和新的数据集。我们对最先进的开源和专有LLMs在CouldAsk上的表现进行了评估。结果表明，这些模型在重新构建问题方面能力有限。具体而言，GPT-4和Llama2-7B仅成功地重新构建了问题的26%和12%。错误分析显示，失败的重新构建中有62%的原因是模型只是重述了问题，甚至生成了完全相同的问题。我们公开发布了这个基准以及重现实验所需的代码。**|
|**2024-07-24**|**WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries**|Wenting Zhao et.al.|[2407.17468](http://arxiv.org/abs/2407.17468)|null|在大型语言模型（LLM）的幻觉问题普遍存在的情况下，现有的事实性评估基准未能覆盖现实世界用户寻求信息的多样化知识领域。为了填补这一缺口，我们引入了WildHallucinations基准，旨在评估事实性。该基准通过促使LLM生成来自野外用户-聊天机器人对话中的实体的信息来实现这一目标。这些生成内容随后自动与从网络搜索系统收集的有组织的知识库进行事实检查。值得注意的是，一半以上的实际世界实体并没有相关的维基百科页面。我们在15个LLM上对7919个实体进行了118785次生成的评估。我们发现，LLM在没有维基百科页面的实体上产生更多的幻觉，并且不同领域的幻觉率存在差异。最后，在使用相同的底层模型时，仅增加检索组件可以略微减少幻觉，但无法完全消除幻觉。|
|**2024-07-24**|**CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models**|Jiawei Gu et.al.|[2407.17467](http://arxiv.org/abs/2407.17467)|null|大型语言模型（LLM）在各种任务上表现出色，但往往在特定领域内表现不佳，因为缺乏特定领域的或专有语料库。连续预训练（CPT）通过回放通用语料并注入新领域的特定知识来增强LLM的能力，以此防止灾难性遗忘。然而，在通用语料和领域特定语料的混合比例上，人们通常采取的是启发式方法，这导致了实际训练效率的低下。在此背景下，我们尝试从CPT的核心出发重新审视LLM的缩放行为，并发现损失、混合比率与训练令牌规模之间的幂律关系。我们正式定义了通用能力和领域特定能力之间的权衡，从而确定了通用数据和领域数据的临界混合比率（CMR）。通过找到平衡点，CMR保持了模型的通用能力，并实现了期望的领域迁移，确保了可用资源的最大化利用。因此，如果重视效率与效果之间的平衡，CMR可以被认为是最佳混合比率。  通过大量实验，我们证实了CMR的可预测性，并提出了CMR缩放定律，并对其一般性进行了验证。这些发现提供了优化LLM在特定领域内的训练的实用指南，确保在有效管理训练资源的同时，既保持通用性能又实现领域特定性能。|
|**2024-07-24**|**$VILA^2$ : VILA Augmented VILA**|Yunhao Fang et.al.|[2407.17453](http://arxiv.org/abs/2407.17453)|null|视觉语言模型(VLMs)的发展迅速，得益于大型语言模型(LLLs)的成功。尽管模型架构和训练基础设施在快速进步，但数据收集与整理的工作仍被忽视。当数据的数量与质量成为瓶颈时，现有方法要么直接从互联网上爬取更多原始数据，这些数据的质量无法保证，要么从黑盒商业模型（例如GPT-4V/金牛座）中提取数据，导致性能受到该模型的限制。本文提出了一种新颖的方法，包括自我增强步骤和专家增强步骤，以迭代地提高数据质量和模型性能。  在自我增强步骤中，VLM重新生成其自身的预训练数据，以提升数据质量，并从这个精炼的数据集重新训练，以改善模型性能。这一过程可以重复进行多次。一旦自我增强达到饱和，我们将采用几个专门领域VLM，这些VLM是从自我增强的VLM中微调而来的，具有特定领域的专业知识。通过任务导向的重新生成和重新训练，进一步将专家知识注入通用模型中。  通过结合自我增强和专家增强的训练，我们引入了VILA²（VILA增强-VILA）模型家族，该家族在广泛的任务上持续提高了准确性，超越了以往的成果，并在开放源代码模型中MMMU排行榜上达到了新的最先进结果。|
|**2024-07-24**|**Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?**|Michael-Andrei Panaitescu-Liess et.al.|[2407.17417](http://arxiv.org/abs/2407.17417)|null|本文首先探讨了在大型语言模型（LLM）中嵌入水印作为防止生成版权侵权文本的有效手段。通过理论分析和实证评估，我们证明了在LLM中融入水印能够显著降低生成版权内容的可能性，从而解决LLM部署过程中的一项关键问题。此外，我们还研究了水印对成员归属推断攻击（Membership Inference Attacks，MIAs）的影响，MIAs旨在识别样本是否属于预训练数据集，这可能用于检测版权违规行为。令人惊讶的是，我们发现水印降低了MIAs的成功率，使检测预训练数据集中版权文本变得复杂。最后，我们提出了一种适应性技术来提高在水印环境下最近MIAs的成功率。我们的发现强调了开发适应性方法以研究具有潜在法律影响的LLM关键问题的重要性。|
|**2024-07-24**|**(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent HyperNetwork**|Tianjin Huang et.al.|[2407.17412](http://arxiv.org/abs/2407.17412)|null|大型神经网络在不同领域如视觉和语言处理方面展现了卓越的性能，尽管这伴随着巨大的计算资源成本。压缩文献中提出的结构模型剪枝算法是促进模型效率的关键方法，得益于其加速友好的稀疏性模式。结构剪枝的核心问题是如何估计通道的重要性。与此并行，数据为中心的人工智能工作表明，基于提示的技术能够使大型语言模型在各种下游任务中表现出惊人的泛化能力。本文探讨了一个迷人的可能性——利用视觉提示来捕捉通道重要性，并推导出高质量的结构稀疏性。为此，我们提出了一种新颖的算法框架，即\texttt{PASS}。它是一种定制的超网络，接受视觉提示和网络权重统计作为输入，并以递归方式输出逐层通道稀疏性。这种设计考虑了层之间通道的内在依赖性。跨多个网络架构和六个数据集的全面实验显示了\texttt{PASS}在定位良好结构稀疏性的优势。例如，在相同的FLOPs水平下，\texttt{PASS}子网络在Food101数据集上实现了1%-3%更高的准确性；或者在获得与基线相同的80%准确度时，\texttt{PASS}子网络能够实现0.35倍更多的速度提升。|
|**2024-07-24**|**Grammar-based Game Description Generation using Large Language Models**|Tsunehiko Tanaka et.al.|[2407.17404](http://arxiv.org/abs/2407.17404)|**[link](https://github.com/tsunehiko/ggdg)**|为了降低游戏设计开发的门槛，自动化游戏设计领域通过计算过程生成游戏设计，已经进行了探索。在自动化游戏设计中，基于机器学习的技术，如进化算法已取得成功。得益于深度学习领域在计算机视觉和自然语言处理应用方面的显著进展，游戏生成方面也有了进步。然而，由于游戏设计领域的数据量有限，深度学习在任务如游戏描述生成上应用不足。为了开拓处理有限数据在自动化游戏设计中的新途径，我们聚焦于大型语言模型（LLMs）的上下文内学习。LLMs可以从少量示范示例中捕获任务特征，并利用预训练期间获得的能力进行应用。我们引入了游戏描述的语法，有效地对游戏设计空间进行了结构化，使LLMs能够捕捉游戏描述生成这一复杂任务的特性。此外，我们提出了一种解码方法，通过利用语法迭代改进生成的输出。我们的实验结果表明，这种方法在生成游戏描述方面表现良好。|
|**2024-07-24**|**3D Question Answering for City Scene Understanding**|Penglei Sun et.al.|[2407.17398](http://arxiv.org/abs/2407.17398)|null|在三维多模态问答（MQA）领域，通过使智能体理解其所在环境中的三维空间，对于场景理解具有至关重要的作用。当前的研究主要集中在室内家庭任务和室外道路自动驾驶任务上，而对于城市级别的场景理解任务探索有限。现有研究在理解城市场景时面临挑战，主要是由于缺乏城市层面的空间语义信息以及人类与环境的互动信息。  为了应对这些挑战，我们从数据集和方法两个角度对三维MQA进行了深入研究。从数据集角度来看，我们引入了一个名为City-3DQA的新颖三维MQA数据集，它是首个融合城市场景语义和人与环境交互任务的数据集。从方法角度来看，我们提出了一个基于场景图的城市级别理解方法（Sg-CityU），利用场景图引入空间语义信息。在City-3DQA的不同设置下，我们的Sg-CityU方法取得了63.94%和63.76%的准确率，相比室内三维MQA方法和使用先进大型语言模型（LLMs）的零样本方法，在鲁棒性和泛化能力方面均达到了最先进的性能水平。|
|**2024-07-24**|**ViPer: Visual Personalization of Generative Models via Individual Preference Learning**|Sogand Salehi et.al.|[2407.17365](http://arxiv.org/abs/2407.17365)|null|不同的用户对于同一提示生成的不同图像有不同的偏好。这催生了个性化图像生成的概念，即创建与个人视觉偏好相匹配的图像。目前的生成模型是无个性化的，它们被调整为吸引广泛受众。用户使用这些模型生成符合个人偏好的图像依赖于通过多次迭代手动调整提示的过程，这一过程既低效又不理想。  我们提出了一种方法来个性化图像生成过程：首先通过邀请用户对一小部分图像进行评论，解释他们喜欢或不喜欢的原因，从而捕捉用户的通用偏好。基于这些评论，我们利用大型语言模型推断出用户的结构化喜好的和不喜好的视觉属性，即他们的视觉偏好。这些属性用于指导文本到图像模型生成更贴近个人用户视觉偏好的图像。  通过一系列用户研究和大型语言模型引导的评估，我们证明了所提出的方法能够产生与个人用户视觉偏好高度一致的生成结果。|
|**2024-07-24**|**Scalify: scale propagation for efficient low-precision LLM training**|Paul Balança et.al.|[2407.17353](http://arxiv.org/abs/2407.17353)|**[link](https://github.com/graphcore-research/jax-scalify)**|**低精度格式，如float8，已被引入机器学习加速硬件中，以提高大型语言模型训练和推理的计算效率。然而，由于需要复杂的、有时是脆弱的技术来匹配更高精度的训练准确度，ML社区对低精度格式的采纳速度较慢。本文提出了一种名为Scalify的端到端的缩放传播范式，用于计算图，它泛化并形式化了现有的张量缩放方法。实验结果表明，Scalify支持直接使用float8进行矩阵乘法和梯度表示，以及float16优化器状态存储。我们对Scalify的JAX实现已经开源在https://github.com/graphcore-research/jax-scalify。**|
|**2024-07-23**|**Can Large Language Models Automatically Jailbreak GPT-4V?**|Yuanwei Wu et.al.|[2407.16686](http://arxiv.org/abs/2407.16686)|null|GPT-4V因其在整合和处理多模态信息方面的卓越能力而引起广泛关注。同时，其面部识别功能也引发了隐私泄露的安全担忧。尽管研究者通过强化学习与人类反馈（RLHF）或预处理过滤器等手段努力实现安全对齐，但仍然可能存在被利用的漏洞。在我们的研究中，我们引入了AutoJailbreak，这是一种创新的自动越狱技术，灵感来源于提示优化。我们利用大型语言模型（LLMs）进行红队训练，以精炼越狱提示，并采用弱到强的上下文内学习提示来提高效率。此外，我们提出了一种有效的方法，结合早期停止策略，以最小化优化时间和令牌消耗。实验结果表明，AutoJailbreak显著超越传统方法，实现了超过95.3%的成功攻击率（ASR）。这项研究揭示了加强GPT-4V安全性的潜力，突显了LLMs可能被用于破坏GPT-4V完整性的风险。|
|**2024-07-23**|**RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent**|Huiyu Xu et.al.|[2407.16667](http://arxiv.org/abs/2407.16667)|null|近期，大型语言模型（LLMs）如GPT-4已被集成至诸多实际应用，例如代码助手Copilot。这些集成显著扩展了LLM的攻击面，使其面临多种威胁。其中，通过“越狱”攻击诱导出毒性响应的“越狱”提示引起了安全领域的广泛关注。为了识别这些威胁，越来越多的红队策略通过构建“越狱”提示来模拟潜在的对抗场景，以此测试目标LLM。然而，现有红队策略并未考虑LLM在不同情境下的独特脆弱性，使得构建针对特定情境的“越狱”提示变得困难。同时，这些方法仅依赖于少数变异操作对“越狱”模板进行细化，缺乏适应不同情境的自动化和规模化能力。  为了实现情境感知和高效红队策略，我们抽象并建模现有攻击行为为一个统一概念——“越狱策略”，并提出了一种多智能体LLM系统RedAgent。该系统利用这些策略生成情境感知的“越狱”提示，并通过额外的记忆缓冲区自我反思情境反馈，持续学习如何利用这些策略在特定情境下实现有效“越狱”。广泛的实验表明，我们的系统可以在五个查询内成功“越狱”大多数黑盒LLM，相较于现有红队方法效率提升两倍。此外，RedAgent能够更高效地针对定制化的LLM应用进行“越狱”。  通过生成针对特定应用的“越狱”提示，我们发现了60个严重漏洞存在于实际应用中的GPTs上，仅需每漏洞两次查询。我们已报告所有发现的问题，并与OpenAI和Meta进行了沟通以修复漏洞。|
|**2024-07-23**|**Course-Correction: Safety Alignment Using Synthetic Preferences**|Rongwu Xu et.al.|[2407.16637](http://arxiv.org/abs/2407.16637)|**[link](https://github.com/pillowsofwind/course-correction)**|### 摘要  本文对大型语言模型（LLM）在执行“课程纠正”任务的能力进行了一项系统性研究，即模型能够自主地避免生成有害内容。首先，我们引入了\textsc{C $^2$-Eval}基准用于定量评估，并分析了10个流行LLM的性能，揭示了当前安全调优的LLM在课程纠正方面存在显著差异。为了改进这一问题，我们提出了使用偏好学习对LLM进行微调的方法，强调及时课程纠正的重要性。通过自动化流程，我们创建了\textsc{C$^2$ -Syn}合成数据集，包含75万对偏好，以此通过数据驱动的偏好学习教授模型及时课程纠正的概念。在\textsc{Llama2-Chat 7B}和\textsc{Qwen2 7B}两个LLM上的实验表明，我们的方法有效提高了课程纠正能力，同时不影响总体性能，并且特别有效地提升了LLM的安全性，尤其是抵抗逃脱攻击的能力。|
|**2024-07-23**|**Lawma: The Power of Specialization for Legal Tasks**|Ricardo Dominguez-Olmedo et.al.|[2407.16615](http://arxiv.org/abs/2407.16615)|null|法律文本的注释与分类是实证法学研究的核心部分。传统上，这些任务往往由受过训练的研究助理承担。在语言模型取得进展的背景下，实证法律学者越来越多地转向使用商业模型，希望以此减轻人工标注的巨大成本。尽管这类方法的应用日益广泛，但关于如何最有效地利用大型语言模型进行法律任务的相关研究仍然有限。  我们进行了一项全面的研究，涵盖了几乎全部针对机器学习社区的新法律文本分类任务。从GPT-4作为基准开始，我们发现它在零样本准确度上的表现具有非同寻常但高度多变性，经常表现出可能不足以满足法律工作需求的性能。接着，我们展示了轻度微调后的Llama 3模型在几乎所有任务上的表现均远超GPT-4，通常提高了两位数百分点的准确性。我们发现，更大的模型在微调时响应效果更好。几十到几百个示例足以实现高分类准确性。值得注意的是，我们可以在所有260个任务上同时微调一个模型，相对于为每个任务单独创建模型，仅在准确性方面略有损失。  我们的工作指出了替代现有做法的一种可行选择。对于具备一定标注数据的特定法律任务，研究人员更应考虑使用开源模型进行微调。|
|**2024-07-23**|**Shared Imagination: LLMs Hallucinate Alike**|Yilun Zhou et.al.|[2407.16604](http://arxiv.org/abs/2407.16604)|null|尽管大型语言模型（LLM）的最近发展呈现了显著的增长，但它们的训练方法——包括模型架构、预训练数据和优化算法——往往极为相似。这自然引发了一个问题：这些模型之间的相似性如何？本文提出了一种新颖的设置，即想象问题回答（IQA），以更深入地理解模型之间的相似性。在IQA中，我们让一个模型生成完全虚构的问题（例如，关于物理中完全不存在的概念），然后让另一个模型进行回答。令人惊讶的是，尽管这些问题完全虚构，但所有模型都能成功回答对方的问题，这表明在这样的幻觉过程中，这些模型共享着一个“共同的想象空间”。  我们对这一现象进行了系列调查，并讨论了它对模型同质性、幻觉以及计算创造力的启示。|
|**2024-07-23**|**Exploring Automatic Cryptographic API Misuse Detection in the Era of LLMs**|Yifan Xia et.al.|[2407.16576](http://arxiv.org/abs/2407.16576)|null|本文探讨了大型语言模型（LLM）在检测加密API误用方面所面临的挑战与机遇。在当前自动化检测技术进步的基础上，对于复杂目标的精确度下降主要归因于手动定义模式的依赖。LLM以其上下文理解能力，在此关键安全领域展现出巨大的潜力。然而，将LLM应用于这一领域存在挑战，尤其是由于它们固有的随机性和众所周知的幻觉问题导致的不稳定性。  为了系统地评估LLM在检测加密误用方面的可靠性，并探索潜在解决方案，本文提出了一种全面的评估框架，利用涵盖人工构建样本和实际项目的大规模数据集进行分析。通过深入分析11,940份LLM生成的报告，我们揭示了LLM固有不稳定性的普遍存在，导致超过一半的报告被误报为误用。然而，我们证明了通过限制问题范围并与LLM的自我修正能力相结合，可以显著提高检测的可靠性。优化的方法实现了接近90%的检测率，超越传统方法，并在现有基准中发现了未被发现的误用。此外，我们识别了持续阻碍LLM可靠性的失败模式，包括加密知识不足和代码语义误解。  基于这些洞察，我们开发了一种以LLM为基础的工作流程来检查开源仓库，最终发现了63个真实的加密误用案例。其中46个已被开发社区认可，23个正在处理中，6个已得到解决。考虑到开发者反馈，我们提供了未来研究和LLM安全工具发展的建议。|
|**2024-07-23**|**Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models**|Ioana Buhnila et.al.|[2407.16565](http://arxiv.org/abs/2407.16565)|**[link](https://github.com/ATILF-UMR7118/pRAGe)**|近期，大型语言模型（LLMs）的广泛应用对公众而言变得愈发便捷。这可能导致人们在医疗建议方面使用此类模型的情况难以追踪。大型语言生成模型存在两个关键问题：首先，它们容易出现错误推理，因此用于医疗目的时需要具备科学性和事实性；其次，由于模型规模巨大，对计算资源构成重大挑战。  本研究引入了一种名为pRAGe的管道，旨在通过小型语言模型（SLM）进行检索增强生成与评估，以实现法语医学短语生成。我们探讨了小型语言模型的有效性以及外部知识库在医学短语生成中的影响。|
|**2024-07-23**|**Patched RTC: evaluating LLMs for diverse software development tasks**|Asankhaya Sharma et.al.|[2407.16557](http://arxiv.org/abs/2407.16557)|**[link](https://github.com/codelion/optillm/blob/main/rto.py)**|这篇论文提出了一种名为“补丁往返正确性（Patched RTC）”的新型评估方法，应用于大型语言模型（LLMs）在多种软件开发任务中的应用，特别是“外循环”活动，如错误修复、代码审查和文档更新。Patched RTC是对原往返正确性方法的扩展，适用于任何LLM和下游任务，提供了一个自我评估框架，无需人工干预即可测量模型响应的一致性和稳健性。研究显示了Patched RTC分数与特定任务准确性指标之间的相关性，将其作为替代LLM-as-Judge范式的方法，用于开放域任务评估。我们通过一个名为patchwork的开源框架实现Patched RTC，在各种补丁流中实现了对不同软件开发任务的透明评估。  比较GPT-3.5和GPT-4模型在不同软件开发任务上的实验结果揭示了Patched RTC能够有效地区分模型性能和任务难度。论文还探讨了一致性提示对提高模型准确性的影响，表明Patched RTC可以指导提示优化和模型选择，以适应复杂的软件开发流程。|
|**2024-07-24**|**MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues**|Liyun Zhang et.al.|[2407.16552](http://arxiv.org/abs/2407.16552)|null|在视觉、听觉和语言等多模态线索的视频中，多模态大型语言模型（MLLMs）展示了卓越的多模态情绪识别能力，能够综合这些线索来识别人类的情绪状态。然而，现有的方法忽视了捕捉面部微表情的时间动态局部特征以及视频中话语意识片段的上下文依赖性，从而在一定程度上限制了它们的有效性。为此，我们提出了一种时间敏感的多模态大型语言模型MicroEmo，旨在将注意力集中于面部微表情的时间动态细节和视频中的话语意识片段的上下文依赖性。  我们的模型包含了两个关键的架构贡献：  1. 全局-局部注意力视觉编码器，它结合了全局帧级时间绑定图像特征与面部微表情的时间动态局部特征，实现了对整体和局部信息的有效融合； 2. 一个话语意识的视频Q-Former，它通过为每个话语段落和整个视频生成视觉令牌序列来捕获多层次和上下文依赖性，然后将它们组合在一起，以捕捉多尺度的上下文依赖关系。  初步的定性实验表明，在一个利用多模态和多方面线索以开放词汇（OV）方式预测情绪的新解释性多模态情绪识别（EMER）任务中，MicroEmo相较于最新的方法显示出了其有效性。|
|**2024-07-23**|**AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game**|Yizhou Chi et.al.|[2407.16521](http://arxiv.org/abs/2407.16521)|**[link](https://github.com/cyzus/among-agents)**|战略性的社交推断游戏是评估语言模型理解和推理能力的宝贵实验平台，对于社会科学研究、人工智能领域以及策略性游戏都有重要价值。本文集中于在模拟环境中构建人类行为的代理，使用《Among Us》作为研究模拟人类行为的工具。通过创建一个基于文本的游戏环境，称为AmongAgent，该环境复制了《Among Us》的游戏动态。玩家扮演太空船上的船员，任务是识别破坏太空船的冒名顶替者并消除船员。在这个环境中，模拟语言代理的行为被分析。实验涉及不同船员和冒名顶替者人格原型配置的多样化的游戏序列。我们的工作表明，最先进的大型语言模型能够有效地掌握游戏规则，并根据当前上下文做出决策。这项工作旨在促进对在信息不完整和复杂动作空间中的目标导向游戏中的语言模型性能进行进一步探索，这些设置提供了评估语言模型在社会驱动场景中表现的宝贵机会。|
|**2024-07-22**|**AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description**|Junyu Xie et.al.|[2407.15850](http://arxiv.org/abs/2407.15850)|**[link](https://github.com/Jyxarthur/AutoAD-Zero)**|**我们的目标是无需训练地生成电影和电视连续剧的音频描述（AD）。我们利用现成的视觉语言模型（VLM）和大型语言模型（LLM），并开发了视觉和文本提示策略来完成这项任务。我们的贡献有三点：(i) 我们证明，如果通过视觉指示直接提示VLM提供角色信息，VLM可以成功命名和引用角色，无需任何微调；(ii) 我们开发了一种两阶段过程来生成AD，第一阶段让VLM全面描述视频，第二阶段使用LLM将密集的文本信息总结为一个简洁的AD句子；(iii) 我们制定了一个新的电视音频描述数据集。我们的方法AutoAD-Zero在AD生成方面表现出色（甚至与一些在真实AD上微调的模型相匹敌），实现了电影和电视连续剧的最高CRITIC评分。**|
|**2024-07-22**|**LLMmap: Fingerprinting For Large Language Models**|Dario Pasquini et.al.|[2407.15847](http://arxiv.org/abs/2407.15847)|**[link](https://github.com/pasquini-dario/LLMmap)**|我们提出了一种针对LLM集成应用的首代指纹识别攻击工具——LLMmap。该工具采用积极的指纹识别策略，通过向应用发送精心设计的查询并分析响应信息，以识别所使用的具体LLM模型。仅需8次交互，LLMmap即可在95%以上的准确率下精确识别出LLM模型。更重要的是，LLMmap被设计得具有跨不同应用层的鲁棒性，使其能够识别在各种系统提示、随机抽样超参数以及复杂的生成框架如RAG或Chain-of-Thought等环境下的LLM模型。|
|**2024-07-22**|**SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models**|Mingze Xu et.al.|[2407.15841](http://arxiv.org/abs/2407.15841)|**[link](https://github.com/apple/ml-slowfast-llava)**|我们提出了一种名为“慢速-LLaVA”（或简称为SF-LLaVA）的无需训练的视频大型语言模型（LLM），它能够同时捕捉详细的空间语义和长时序上下文，而不会超出通常使用的LLM的令牌预算。这一目标通过使用视频LLM输入的双流设计实现，有效地聚合了从采样视频帧中提取的特征。具体而言，慢速路径以较低的帧率提取尽可能多的空间细节的特征（例如，以24x24的令牌），而快速路径则以较高的帧率操作，但使用较大的空间池化步长（例如，下采样6x）来关注运动线索。因此，这种设计允许我们适当地捕获对于理解视频中的详细信息有益的时空特性。实验结果表明，SF-LLaVA在各种视频任务上都超越了现有的无需训练的方法。在某些基准测试中，它甚至与在视频数据集上进行微调的最先进的视频LLM实现了相当或更好的性能。|
|**2024-07-22**|**MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with Extensive Diversity**|Yangzhou Liu et.al.|[2407.15838](http://arxiv.org/abs/2407.15838)|**[link](https://github.com/yuecao0119/mminstruct)**|尽管视觉语言预训练模型在视觉任务上的微调表现出显著的性能提升，但现有的视觉指令调优数据集存在以下局限性：  1. 指令注释质量：虽然现有的视觉语言预训练模型在性能上表现出色，但它们生成的指令可能仍会包含不准确性，如幻觉现象。  2. 指令和图像多样性：指令类型范围有限以及图像数据缺乏多样性可能会影响模型生成多样性和接近真实世界场景输出的能力。  为解决这些挑战，我们构建了一个高质量、多样性的视觉指令调优数据集MMInstruct，包含来自24个领域共计973K条指令。该数据集包括四种指令类型：判断、多项选择、长视觉问题回答和短视觉问题回答。  为了构建MMInstruct，我们提出了一种指令生成数据引擎，利用GPT-4V、GPT-3.5和人工校正。我们的指令生成引擎允许半自动、低成本、多领域的指令生成，成本仅为手动构建的六分之一。  通过广泛的实验验证和消融实验，我们证明了MMInstruct能够显著提高视觉语言预训练模型的性能，例如，基于MMInstruct的模型微调在12个基准中的10个上达到了新的状态最优表现。代码和数据将在https://github.com/yuecao0119/MMInstruct提供。|
|**2024-07-22**|**dMel: Speech Tokenization made Simple**|He Bai et.al.|[2407.15835](http://arxiv.org/abs/2407.15835)|null|大型语言模型通过利用大规模文本数据的自我监督预训练，在自然语言处理领域实现了革命性的进步。受此成功启发，研究人员探索了复杂语音分词方法，以将连续的语音信号离散化，从而使语言建模技术可以应用于语音数据。然而，现有方法要么建模语义令牌，可能会丢失声学信息，要么建模声学令牌，又可能面临丢失语义信息的风险。具有多种令牌类型也使架构变得复杂，并需要额外的预训练。我们展示了将梅尔滤波器通道离散化为离散强度单元（dMel）产生了一个简单表示，其性能优于其他现有语音分词方法。使用仅解码器的变换器架构进行语音-文本建模，我们全面评估了不同的语音分词方法在语音识别（ASR）和语音合成（TTS）任务上的性能。我们的结果表明，dMel在联合建模语音和文本的统一框架中实现高性能的有效性，为高效且有效的语音与文本联合建模铺平了道路。|
|**2024-07-22**|**Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight**|Ziyuan Huang et.al.|[2407.15819](http://arxiv.org/abs/2407.15819)|null|这篇论文提出了一种名为“链视图”的视觉-语言桥梁模块，旨在加速多模态大型语言模型（MLLMs）的预训练过程。我们的方法采用了序列化的视觉重采样器，能够有效地捕捉不同空间尺度的视觉细节。这种架构不仅能够有效利用全局和局部视觉上下文，还通过复合令牌缩放策略灵活扩展视觉令牌的数量，最多可以增加16倍的令牌数量，而无需在预训练后进行微调。因此，“链视图”在预训练阶段所需的视觉令牌数量远少于微调阶段，这有意地减少了视觉令牌的数量，显著加速了预训练过程，节省了大约73%的实际训练时间。  在一系列视觉-语言基准测试上的实验结果表明，通过“链视图”加速预训练过程并不会牺牲性能，其表现与在整个训练过程中使用所有视觉令牌的标准流程相当或更好。进一步增加预训练阶段的视觉令牌数量会导致更强的表现，在一系列基准测试中与现有方法竞争。  请注意，上述摘要已经转换成了中文表述，并且遵循了不包含特殊符号的指示。|
|**2024-07-22**|**Extracting Structured Insights from Financial News: An Augmented LLM Driven Approach**|Rian Dolphin et.al.|[2407.15788](http://arxiv.org/abs/2407.15788)|null|金融新闻在金融市场决策过程中扮演着关键角色，但将其转化为结构化数据的过程一直充满挑战。本文提出了一种新颖的金融新闻处理方法，利用大型语言模型（LLMs）克服了以往提取结构化信息时遇到的限制。我们引入了一套系统，该系统能够从原始新闻文章内容中提取相关公司代码，并在不依赖于预结构化数据流的情况下进行公司层面的情绪分析和生成摘要。我们的方法结合了LLMs的生成能力、以及最新的提示技术，配以一个定制的字符串相似度验证框架。  通过使用包含5530篇金融新闻文章的数据集进行评估，证明了我们的方法的有效性。相比现有数据提供商，我们有90%的文章不会遗漏任何公司代码，而有22%的文章会额外提供相关的公司代码。此外，我们还实现了这一方法的大规模部署，并通过实时API端点提供了经过处理的数据，更新了最新新闻信息。据我们所知，这是我们首次作为数据提供商提供从新闻文章中对每个公司的细致情绪分析服务，增强了市场参与者可获取的信息深度。  为了促进进一步的研究利用金融新闻，我们还发布了包含5530篇处理后文章的评估数据集。|
|**2024-07-22**|**MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation**|Marco Simoni et.al.|[2407.15748](http://arxiv.org/abs/2407.15748)|null|在这篇论文中，我们引入了MoRSE（混合RAG安全专家），这是首个专门的AI聊天机器人用于网络安全。MoRSE旨在提供全面且完整的网络安全知识。MoRSE使用了两个RAG（检索增强生成）系统，设计用于从多维度的网络安全上下文中检索和组织信息。与传统的RAG不同，MoRSE采用了并行检索器协同工作，以在不同格式和结构中检索语义相关的信息。  不同于依赖参数知识库的传统大型语言模型（LLMs），MoRSE响应用户查询时从非参数知识库中检索相关文档。随后，MoRSE利用这些信息生成准确的答案。此外，MoRSE受益于其知识库的实时更新，这使得系统能够在不重新训练的情况下持续的知识丰富。  我们对MoRSE的有效性进行了评估，针对600个特定的网络安全问题进行了实验性评估。实验结果表明，与GPT-4、Mixtral 7x8等已知解决方案相比，在答案的相关性和正确性的改进上超过了10%。|
|**2024-07-22**|**OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a German Migration Context**|Steffen Kleinle et.al.|[2407.15736](http://arxiv.org/abs/2407.15736)|null|当移民到一个新的国家时，人们很容易因需要获取有关财政支持、住房、教育、语言课程以及其他问题的信息而感到不知所措。如果搬迁过程匆忙或甚至被迫进行，对这些问题的高质量解答变得尤为迫切。官方移民顾问通常过于繁忙，而在线系统可以引导新移民找到所需信息或合适的咨询服务。因此，我们提出了OMoS-QA数据集，它包含德语和英语问题与相关可信文档以及手动标注的答案，专门针对这一场景。问题是由开源大型语言模型（LLM）自动生成的，答案句子由具有高度一致性的众包工作者选择。通过我们的数据，我们在德语和英语上对5个预训练的LLM进行了提取式问答任务的比较。在所有模型和两种语言中，选择答案句子的精确度高，召回率低至中等，这是一个有利的权衡，以避免误导用户。这种性能即使在问题语言与文档语言不匹配时也能保持不变。在根据上下文识别不可回答的问题方面，两种语言之间存在更大的差异。|
|**2024-07-22**|**TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON**|John Chong Min Tan et.al.|[2407.15734](http://arxiv.org/abs/2407.15734)|**[link](https://github.com/simbianai/taskgen)**|TaskGen是一个开源的代理框架，通过使用代理来解决任意任务并将其分解为子任务来实现。每个子任务被映射到一个装备函数或另一个代理执行。为了减少冗余（从而减少令牌使用），TaskGen使用了StrictJSON，确保大型语言模型（LLM）输出的JSON格式，并具备类型检查和迭代错误修正等额外功能。TaskGen的核心理念在于基于需求管理信息/记忆。  我们对TaskGen在各种环境中进行了实证评估，包括40x40动态迷宫导航，其中障碍物位置会变化（100%的成功率），文本世界逃脱房间解谜，具有密集奖励和详细目标（96%的成功率），网络浏览（69%的动作成功），解决MATH数据集（在100个Level-5问题上，成功率71%），以及自然问题检索增强生成（F1分数为47.03%）。|
|**2024-07-19**|**Internal Consistency and Self-Feedback in Large Language Models: A Survey**|Xun Liang et.al.|[2407.14507](http://arxiv.org/abs/2407.14507)|**[link](https://github.com/iaar-shanghai/icsfsurvey)**|**本文总结了一个理论框架，称为内部一致性（Internal Consistency），它为大型语言模型（LLM）在推理不足和生成幻觉内容等问题上的表现提供了一致的解释。内部一致性评估了LLM的潜在层、解码层和响应层之间的内在一致性，基于采样方法。  在此基础上，我们引入了Self-Feedback框架，这是一个简洁而有效的理论框架，用于挖掘内部一致性的信息。Self-Feedback框架包括两个模块：自我评估（Self-Evaluation）和自我更新（Self-Update）。  我们系统地按任务和研究方向对这些研究进行了分类；总结了相关的评估方法和基准；深入探讨了“Self-Feedback真的有效吗？”这一问题。我们提出了几个关键观点，包括“内部一致性的发展钟楼”、“一致性几乎是正确性”的假设以及“潜意识与显式推理悖论”。此外，我们还概述了未来研究的有前景的方向。  我们已经开源了实验代码、参考列表和统计数据，供公众访问，链接为：[](https://github.com/IAAR-Shanghai/ICSFSurvey)**|
|**2024-07-19**|**On Pre-training of Multimodal Language Models Customized for Chart Understanding**|Wan-Cyuan Fan et.al.|[2407.14506](http://arxiv.org/abs/2407.14506)|null|近期的研究在针对特定领域任务定制多模态大型语言模型（MLLMs）方面取得了令人鼓舞的成果，特别是在科学图表理解领域。这些研究通常通过使用专门的数据集进行视觉指令调优来增强问答（QA）准确性。然而，它们往往忽视了自然图像-描述预训练数据与数字图表图像-QA数据之间的基本差异，特别是对于模型从图表中提取潜在数值的能力。本文旨在解决这一疏漏，探索改进MLLMs对图表理解所需的关键训练过程。我们提出了三个关键发现：（1）在对齐预训练过程中融入原始数据值显著提高了对图表数据的理解能力。（2）在端到端微调过程中随机替换图像为文本表示，能够将语言推理能力转移到图表解释技能上。（3）要求模型首先提取底层图表数据，然后在微调过程中回答问题，可以进一步提高准确性。  因此，我们引入了CHOPINLLM，一种专为深入图表理解定制的MLLM。CHOPINLLM有效地解析各种类型的图表，包括未标注的图表，同时保持了强大的推理能力。此外，我们建立了一个新的基准，用于评估MLLMs在不同图表类型和理解水平上的理解能力。实验结果表明，CHOPINLLM在理解各种类型、带有标注和未标注的图表方面表现出强大的性能。|
|**2024-07-19**|**Evaluating the Reliability of Self-Explanations in Large Language Models**|Korbinian Randl et.al.|[2407.14487](http://arxiv.org/abs/2407.14487)|**[link](https://github.com/k-randl/self-explaining_llms)**|**本文探讨了大型语言模型在被提示解释其先前输出时生成的解释可靠性。我们利用三种先进的大语言模型（参数从2B到8B）在两种不同的分类任务（客观和主观）上评估了两种类型的自我解释——抽取式和反事实式。我们的发现表明，尽管这些自我解释与人类判断相关联，但它们并不完全且准确地遵循模型的决策过程，指出了一种感知与实际模型推理之间的差距。我们显示，通过提示大语言模型进行反事实解释，可以产生忠实、信息丰富且易于验证的结果。这些反事实为传统可解释性方法（例如SHAP、LIME）提供了有前景的替代方案，前提是对特定任务定制提示并检查其有效性。**|
|**2024-07-19**|**Contrastive Learning with Counterfactual Explanations for Radiology Report Generation**|Mingjie Li et.al.|[2407.14474](http://arxiv.org/abs/2407.14474)|null|由于解剖学的常见内容和与之对应的影像学图像之间的高度相似性，这种固有的数据偏见可能导致自动报告生成模型学习纠缠和相关性增强的表示，从而产生误诊报告。为了应对这些问题，我们提出了一种新颖的“Co”unter“F”actual “E”xplanations（CoFE）框架用于放射学报告生成。反事实解释是一种强大的工具，用于理解算法决策如何通过提出“如果”场景而被改变。通过利用这一概念，CoFE可以通过对比正例和负例之间的表示来学习非相关性视觉表示，从而学习非相关性视觉表示。具体来说，通过在正例和负例之间交换补丁直到预测诊断发生变化，我们推导出反事实图像。在这里，正例和负例是最语义上相似的，但具有不同的诊断标签。此外，CoFE采用可学习提示高效地对预训练的大语言模型进行微调，封装了正事实例和反事实实例的内容，提供更通用的提示表示。在两个基准上的广泛实验表明，利用反事实解释使CoFE能够生成语义上连贯且事实完整的报告，并在语言生成和临床有效性指标方面表现出色。|
|**2024-07-19**|**Check-Eval: A Checklist-based Approach for Evaluating Text Quality**|Jayr Pereira et.al.|[2407.14467](http://arxiv.org/abs/2407.14467)|null|评估大型语言模型（LLM）生成文本的质量仍然是一个重大挑战。传统的评估标准往往与人类的判断不匹配，尤其是在需要创造性和细微差别的任务中。本文提出了一种名为Check-Eval的新评估框架，通过利用LLM以检查表为基础的方法来评估生成文本的质量。Check-Eval可以作为无参考和有参考的评估方法使用，提供了一个结构化且可解释的文本质量评估体系。该框架主要由两个阶段组成：检查表生成和检查表评估。我们在两个基准数据集上验证了Check-Eval：葡萄牙语法律语义文本相似性以及SummEval。我们的结果显示，Check-Eval与现有指标（如G-Eval和GPTScore）相比，在与人类判断的相关性方面取得了更高的分数，这表明其作为自然语言生成任务更可靠和有效的评估框架的潜力。我们的实验代码可在https://anonymous.4open.science/r/check-eval-0DB4获取。|
|**2024-07-19**|**Undermining Mental Proof: How AI Can Make Cooperation Harder by Making Thinking Easier**|Zachary Wojtowicz et.al.|[2407.14452](http://arxiv.org/abs/2407.14452)|null|大型语言模型和其他高度先进的AI系统在决定说什么或做什么时提供了便利，但这便利性实际上削弱了在社会情境下采取有效行动的能力。我们通过引入“心理证明”这一整合性理论概念来解释这种看似矛盾的现象。“心理证明”发生在使用可观察的行为来证实不可观察的心理事实的情况中。从招聘到约会，“心理证明”使人们能够在低信任环境中相互传达价值观、意图、知识状态等心理特征，这些环境中的诚实难以得到强制执行。  基于经济学、理论生物学和计算机科学的研究成果，我们描述了使人类能够实施心理证明的核心理论机制。对这些机制的分析揭示了人工智能如何在使思考变得容易的同时，却可能使低信任合作变得更难。  通过理解心理证明的工作原理及其在不同情境下的应用，我们可以设计出既能促进高效沟通又能维护社会协作的AI系统。例如，在招聘过程中，AI可以通过分析候选人的行为模式和历史数据来间接评估其技能、团队合作能力以及对公司文化的适应性，从而帮助雇主做出更可靠的人才选择决策。在约会场景中，AI可以利用社交媒体活动、兴趣爱好等信息来构建用户的心理画像，以此帮助用户找到与自己价值观和生活方式相匹配的伴侣。  总之，通过合理地设计和应用AI技术，我们不仅可以在低信任环境下增强人类的交流和合作能力，而且还能促进更加公正、透明和高效的决策过程。|
|**2024-07-19**|**Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding**|Renshan Zhang et.al.|[2407.14439](http://arxiv.org/abs/2407.14439)|**[link](https://github.com/JiuTian-VL/TokenCorrCompressor)**|**当前主流的多模态大型语言模型（Multimodal Large Language Models, MLLMs）在进行文档理解时，普遍采用对高分辨率文档图像进行裁剪，从而生成多个子图像的方法。大多数现有的文档理解方法会保留所有子图像内的标记，并同等对待它们，这忽视了这些标记的不同信息价值性，导致了大量不必要的图像标记增加。为了实现更加适应性和高效的文档理解，我们提出了一种名为“Token级相关性引导压缩”的无参数且可插拔方法，旨在优化标记处理过程。该方法首先引入了一种创新的评估模式重复性的方法，基于每个片段标记之间的相关性进行。这种方法能够识别冗余标记，从而确定子图像的信息密度。其次，我们提出了一个针对Token级别的采样方法，通过深入分析[CLS]标记与片段标记之间的相关性，高效捕捉最具信息价值的标记。通过结合这两种策略，我们开发了一个可无缝集成到使用裁剪技术的MLLMs中的自适应压缩模块。这一模块不仅在训练和推理过程中显著提升了处理速度，同时保持了与现有压缩方法相当的性能水平。我们使用当前最佳的文档理解模型mPLUG-DocOwl1.5进行了实验，并通过与其他压缩方法的广泛对比，验证了其有效性。**|
|**2024-07-19**|**The Vision of Autonomic Computing: Can LLMs Make It a Reality?**|Zhiyang Zhang et.al.|[2407.14402](http://arxiv.org/abs/2407.14402)|null|《自治计算愿景与大型语言模型在微服务管理中的应用》一文回顾了超过二十年前提出的自治计算（ACV）愿景，旨在构建能够自我管理和适应环境变化的计算系统，这一目标至今仍面临挑战。近年来，大型语言模型（LLMs）的发展为解决这些挑战提供了可能，它们通过利用广泛的知识、语言理解能力以及任务自动化能力来实现这一愿景。  本文探讨了通过基于LLM的多代理框架实现微服务管理自主性的可行性，并提出了一个五级分类体系，用于描述自主服务维护的不同层次。文中还设计了一个基于“Sock Shop”微服务演示项目的在线评估基准，以评估该框架的性能。研究结果表明，通过LLMs可以显著提升微服务体系结构中问题检测和解决的能力，实现了第三级自主性水平的突破，这标志着大型语言模型在微服务管理框架集成方面的应用取得了重要进展，为构建更适应性和自我管理的计算系统铺平了道路。  为了促进这一领域的研究和发展，相关的代码将通过<https://aka.ms/ACV-LLM>公开提供。|
|**2024-07-19**|**Open Artificial Knowledge**|Vadim Borisov et.al.|[2407.14371](http://arxiv.org/abs/2407.14371)|null|《开放人工知识（OAK）数据集：促进大型语言模型发展与解决数据稀缺与隐私问题》  当前，基于对话的AI系统如ChatGPT、Claude和Gemini的成功，主要得益于大规模语言模型（LLMs）对海量数据集的训练。然而，获取高质量、多样性和伦理来源的数据仍然面临重大挑战。我们提出了一种名为“开放人工知识”（OAK）数据集，这是一个包含超过5亿个令牌（撰写时）的大型资源库。OAK通过集合包括GPT4o、LLaMa3-70B、LLaMa3-8B、Mixtral-8x7B、Gemma-7B和Gemma-2-9B在内的最先进的LLMs，利用维基百科的主要类别来引导文本生成，确保广泛的领域覆盖，同时保持连贯性和事实准确性。OAK数据集旨在促进更强大、更对齐的语言模型的发展，并解决大规模语言模型训练中的关键问题，如数据稀缺性和隐私问题。目前，该数据集是免费提供在www.oakdataset.org。|
|**2024-07-19**|**Enhancing Zero-shot Audio Classification using Sound Attribute Knowledge from Large Language Models**|Xuenan Xu et.al.|[2407.14355](http://arxiv.org/abs/2407.14355)|**[link](https://github.com/wsntxxn/attrenhzsac)**|这篇论文提出了一种新的方法来进行零样本音频分类，即识别和分类模型在训练过程中从未见过的音频类别。我们提议列出一系列音频属性，并利用大型语言模型的领域知识为每个类别生成详细的属性描述。与以往主要依赖类别标签或简单描述的方法不同，我们的方法专注于多维度的内在听觉属性，捕捉音频类别的不同特性。此外，我们还采用了对比学习方法来增强基于文本标签的零样本学习。我们在VGGSound和AudioSet上验证了我们方法的有效性（代码可访问：https://www.github.com/wsntxxn/AttrEnhZsAc）。结果表明，在零样本分类准确性方面取得了显著提高。消融实验结果显示，无论模型架构如何，性能增强都非常稳健。|
|**2024-07-18**|**SegPoint: Segment Any Point Cloud via Large Language Model**|Shuting He et.al.|[2407.13761](http://arxiv.org/abs/2407.13761)|null|尽管三维点云分割领域取得了显著进展，但现有的方法主要针对特定任务，依赖于明确的指令来识别目标，缺乏在统一框架中理解和推断用户隐含意图的能力。本文提出了一种名为SegPoint的模型，它利用多模态大型语言模型（LLM）的推理能力，在多种任务上进行点级分割：1）三维指令分割，2）三维指称分割，3）三维语义分割，以及4）三维开放词汇语义分割。为了推动三维指令研究，我们创建了一个新的基准Instruct3D，用于评估从复杂和隐含指令文本进行分割性能，包含2,565个点云-指令对。实验结果显示，SegPoint在ScanRefer指称分割和ScanNet语义分割等既有基准上表现出竞争力，同时在Instruct3D数据集上的表现优异。据我们所知，SegPoint是首个在一个框架内处理这些多样化的分割任务并达到满意性能的模型。|
|**2024-07-18**|**Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models**|Zhuo Chen et.al.|[2407.13757](http://arxiv.org/abs/2407.13757)|null|## 任务  本研究关注于Retrieval-Augmented Generation（RAG）模型在面对黑盒攻击时的脆弱性，尤其是在意见操纵方面的应用。RAG旨在解决大语言模型的幻觉问题和实时约束，但同时也暴露出对抗检索篡改攻击的弱点。当前的研究主要集中在白盒和封闭领域问答任务中的RAG不稳定性。本文的目标是揭示当RAG模型遭遇黑盒攻击时，对用户认知和决策的影响，从而为提高模型的可靠性和安全性提供新见解。  我们通过操控RAG中检索模型的排名结果，利用这些操纵后的数据训练一个代理模型。接着，采用对抗性检索攻击方法针对代理模型实施黑盒迁移攻击，进一步影响RAG的生成过程。在涉及多个主题的意见数据集上进行实验，结果显示，提出的攻击策略能显著改变RAG生成内容的观点极性，这揭示了模型的易受攻击性，并且潜在地指出对用户认知和决策的负面影响，使得误导用户接受错误或有偏见的信息变得更加容易。|
|**2024-07-18**|**CellularLint: A Systematic Approach to Identify Inconsistent Behavior in Cellular Network Specifications**|Mirza Masfiqur Rahman et.al.|[2407.13742](http://arxiv.org/abs/2407.13742)|null|近年来，人们越来越关注蜂窝网络的安全性，常常将安全漏洞归咎于底层协议设计描述的问题。这些通常长达数千页的详细规格文档可能包含错误、不完整描述、隐含假设和内部矛盾。鉴于此，我们提出CellularLint——一个针对4G和5G非接入层（Non-Access Stratum，NAS）和安全规范的半自动框架，利用一套自然语言处理技术。我们的方法基于领域适应的大语言模型进行改良的少量样例学习。该模型预训练在大量的蜂窝网络协议数据上，能够同时检测不同语义层次和实际使用案例中的不一致性，以一种可扩展的方式提升协议规格的自动化分析。通过研究，我们在4G和5G网络中发现了157个不一致点，准确率为82.67%。经过对开源实现和17款商用设备的验证，我们确认这些不一致确实对设计决策有重大影响，可能导致隐私、完整性、可用性和互操作性方面的担忧。|
|**2024-07-18**|**Baba Is AI: Break the Rules to Beat the Benchmark**|Nathan Cloos et.al.|[2407.13729](http://arxiv.org/abs/2407.13729)|null|人类解决问题既依赖于遵循现有规则和程序，也依赖于创新思维来重新定义规则和目标。为了检验这些能力，我们设计了一个新的基准，它基于游戏《Baba Is You》。在这个游戏中，代理需要操控环境中的物体和可移动的文字规则瓷砖，以实现特定目标并赢得比赛。我们测试了三种最先进的多模态大型语言模型（OpenAI GPT-4、Google Gemini-1.5-Pro和Gemini-1.5-Flash），发现当需要对游戏规则进行操纵和组合时，它们的表现大幅下滑。|
|**2024-07-18**|**CoDefeater: Using LLMs To Find Defeaters in Assurance Cases**|Usman Gohar et.al.|[2407.13717](http://arxiv.org/abs/2407.13717)|**[link](https://gitlab.com/anonymousdot/codefeater)**|构建保证案例是一种常用且有时必要的方法，用于证明安全关键系统在其规划环境中将安全运行。为了降低错误和边缘情况遗漏的风险，引入了“反驳”概念，即挑战保证案例中论点或证据的论据。反驳有助于及时发现论点中的弱点，促使进一步调查和及时补救。然而，捕捉反驳依赖于专家判断、经验和创新思维，并且必须随着需求和法规的变化进行迭代。这篇论文提出CoDefeater，一种利用大型语言模型（LLMs）来自动寻找反驳的自动化过程。初步结果表明，LLMs能够有效地找到已知和未知的合理反驳，从而帮助安全分析师增强保证案例的完整性和信心。|
|**2024-07-18**|**Understanding Reference Policies in Direct Preference Optimization**|Yixin Liu et.al.|[2407.13709](http://arxiv.org/abs/2407.13709)|**[link](https://github.com/yale-nlp/refdpo)**|## 背景  直接偏好优化（Direct Preference Optimization，简称 DPO）已成为大型语言模型（Large Language Models，LLMs）指令微调的常用训练方法。本研究关注DPO的一个未充分探讨的方面：其对参考模型或策略的依赖性。这些参考策略通常表现为待进一步微调的模型，它们对于DPO的效果至关重要。因此，本工作针对以下三个相关问题进行了探究：  1. 首先，我们研究了DPO中的KL散度约束强度的最佳选择，该约束惩罚与参考策略的偏离，发现DPO对此敏感。 2. 其次，我们从理论和实证上比较了DPO与其他学习目标，以探讨参考策略在指令微调中的必要性，并显示了DPO的优势。 3. 最后，我们探讨了更强的参考策略是否有利于DPO，结果表明，当参考策略与被微调模型相似时，更强的参考策略可能会提高性能。  我们的发现揭示了参考策略在DPO中的混淆作用，提供了最佳实践的见解，同时也为未来研究提出了开放性问题。|
|**2024-07-18**|**A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice**|Shaina Raza et.al.|[2407.13699](http://arxiv.org/abs/2407.13699)|**[link](https://github.com/vectorinstitute/recommender-systems-survey)**|## 背景  推荐系统（RS）通过提供个性化项目建议，对提升用户体验至关重要。本综述回顾了从2017年至2024年间RS领域的进展，将理论创新与实际应用紧密结合。我们探讨了从传统方法如基于内容和协同过滤的推荐，到高级技术如深度学习、图模型、强化学习以及大型语言模型的发展。此外，我们还关注了专门化的系统，如上下文感知、评论驱动和公平性考量的RS。本调查的目标是连接理论与实践，关注电子商务、医疗保健和金融等领域的挑战，强调对可扩展、实时且值得信赖解决方案的需求。通过此综述，我们鼓励学术研究与行业实践的紧密合作。本研究提供的洞见旨在帮助业界专业人员优化RS部署，并激发未来研究的新方向，特别是在应对新兴技术和社会趋势时。|
|**2024-07-18**|**Prover-Verifier Games improve legibility of LLM outputs**|Jan Hendrik Kirchner et.al.|[2407.13692](http://arxiv.org/abs/2407.13692)|null|为了提高大型语言模型（LLMs）输出结果的可信度，一个方法是支持清晰易验证的推理，我们称之为可读性。本文以解决小学数学问题为背景，研究了可读性，并发现仅优化连贯思维解题的准确性可能会降低其可读性。为缓解这一损失，我们提出了一种受Anil等人（2021）的证明器-验证器游戏启发的训练算法。该算法迭代地训练小型验证器预测解题正确性，"有帮助"的证明器生成验证器接受的正确解答，以及"狡猾"的证明器生成欺骗验证器的错误解答。实验表明，有帮助证明器的准确性和验证器对抗攻击的鲁棒性在训练过程中提高。此外，我们发现，针对小型验证器的可读性训练能够转移给时间有限的人类，他们在验证解决方案正确性时的准确性会随着训练提高，而在验证狡猾证明器的解决方案时会下降。因此，通过小型验证器进行可读性训练可能是一种实际可行的方法，用于提升大型LLMs对人类的可读性，从而有助于超级人类模型的对齐。我们的结果表明，对小型验证器的可读性训练是一个实用的途径，可以增强大型LLMs的可读性，对人类来说更易于理解和信任。|
|**2024-07-18**|**COMCAT: Leveraging Human Judgment to Improve Automatic Documentation and Summarization**|Skyler Grandel et.al.|[2407.13648](http://arxiv.org/abs/2407.13648)|null|这篇论文主要探讨了软件维护中代码理解的重要性，以及如何通过自动化生成注释来提升这一过程。作者提出了一种名为COMCAT的方法，它结合大型语言模型（LLMs）与领域专家指导，旨在为源代码提供有助于理解的注释。COMCAT流程包括自动识别代码中适合添加注释的位置、预测每个位置最适合的注释类型，并根据选定位置和类型生成注释。在人类受试者的研究中，结果显示COMCAT生成的注释显著提高了开发人员在三个典型软件工程任务中的代码理解能力，对于87%的参与者，提升幅度达到12%。此外，研究还表明COMCAT生成的注释在准确性、可读性上至少与人工注释相当，并且在92%的代码片段中，开发者更偏好COMCAT生成的注释，而非标准的ChatGPT生成的注释。论文还介绍了开发并公开了一个包含源代码片段、人工编写注释和标注的类别数据集。总的来说，COMCAT利用LLMs在多种软件工程任务中显著提升了代码理解水平。|
|**2024-07-18**|**Weak-to-Strong Reasoning**|Yuqing Yang et.al.|[2407.13647](http://arxiv.org/abs/2407.13647)|**[link](https://github.com/gair-nlp/weak-to-strong-reasoning)**|当大型语言模型（LLMs）的性能超越人类时，为其提供全面而精确的监督变得困难。在这种情况下，弱到强学习方法，即利用能力较弱的模型激发较强模型的潜在能力，显示出价值。然而，这种策略在处理复杂推理任务时的效果尚未得到充分检验，且当前缺乏有效的方法来避免模型盲目模仿弱导师，包括其错误。本文提出了一种渐进学习框架，使强模型能够自主优化其训练数据，无需依赖高级模型或人工标注的数据。该框架首先对选定的小而高质量数据进行监督微调，然后在强模型自行识别的对比样本上进行偏好优化。我们在GSM8K和MATH数据集上的大量实验表明，我们的方法显著提升了Llama2-70b的推理能力，通过三种不同的弱模型进行验证。此外，我们还在前瞻性的实验设置中验证了这种方法，Llama3-8b-instruct成功指导Llama3-70b在极具挑战性的OlympicArena数据集上。这项工作为提升人工智能的推理能力提供了一种更可扩展和高级的策略。所有相关代码和资源可在<https://github.com/GAIR-NLP/weak-to-strong-reasoning>获取。|
|**2024-07-17**|**EchoSight: Advancing Visual-Language Models with Wiki Knowledge**|Yibin Yan et.al.|[2407.12735](http://arxiv.org/abs/2407.12735)|null|**摘要：**  知识驱动的视觉问答（KVQA）任务要求利用丰富背景知识解答图像相关问题，但生成模型在这方面常面临挑战。为此，我们提出EchoSight，一个新颖的多模态检索增强生成（Retrieval-Augmented Generation，RAG）框架，旨在帮助大型语言模型（LLMs）处理需要详尽百科知识的视觉问答。EchoSight首先仅使用图像信息在维基百科中搜索文章，然后对候选文章根据它们与文本-图像查询的相关性进行二次排序，从而显著提升多模态知识的整合，进而提高检索效果和答案的准确性。我们在Encyclopedic VQA和InfoSeek数据集上的实验结果表明，EchoSight在知识型视觉问答中实现了新的state-of-the-art成绩，Encyclopedic VQA任务上达到41.8%的准确率，InfoSeek任务上达到31.3%。|
|**2024-07-17**|**NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model**|Zhongqun Zhang et.al.|[2407.12727](http://arxiv.org/abs/2407.12727)|null|### 背景  在三维手部-物体重建中，精确的手部与物体之间的物理接触是提升手部姿态估计准确性和生成新的人类抓握动作的标准。然而，现有的方法依赖于难以定义或控制的几何约束。本文提出了一项新的任务：通过自然语言描述进行可控的三维手部-物体接触建模。面临的挑战包括：一、从语言到接触的复杂跨模态建模；二、缺乏描述接触模式的文本数据。为解决这些问题，我们设计了NL2Contact模型，它利用分段扩散模型生成可控制的接触。给定对手和接触的自然语言描述，NL2Contact能够生成逼真且忠实的三维手部-物体接触。  ### 任务  我们开发了NL2Contact模型，旨在通过自然语言描述生成具有控制性的三维手部-物体接触。为训练这个模型，我们创建了首个名为\textit{ContactDescribe}的数据集，其中包含基于精心设计的提示（如抓取动作、抓取类型、接触位置和自由手指状态）生成的丰富多样的手部中心接触描述。我们的模型在优化抓握姿势和基于文本描述生成新的人类抓握动作方面展示了应用潜力。|
|**2024-07-17**|**Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?**|Ben Yao et.al.|[2407.12725](http://arxiv.org/abs/2407.12725)|null|在大型语言模型（LLMs）解决复杂问题的能力方面，通过逐步推理步骤的扩展显著提升其性能，因为这促使模型进行序列思考。然而，人类对讽刺的理解通常被视为一种直觉且整体的认知过程，它整合了语言、上下文和情感线索，形成对说话者真实意图的全面理解，这种理解被认为不局限于一步步的推理过程。为了验证这一观点，我们提出了一种新的提示框架，称为SarcasmCue，它包含了四种提示策略：连锁矛盾（CoC）、线索图（GoC）、线索集合（BoC）和线索张量（ToC）。这些方法旨在引导LLMs通过考虑顺序和非顺序提示来识别人类的讽刺。我们在四个基准数据集上的全面实证比较表明，我们的四种提示方法明显优于标准的输入-输出提示、CoT和ToT，而且非顺序提示通常优于顺序提示。|
|**2024-07-17**|**The Future of Learning: Large Language Models through the Lens of Students**|He Zhang et.al.|[2407.12723](http://arxiv.org/abs/2407.12723)|null|随着大型语言模型（LLMs）的不断发展，它们在性能上的提升和功能扩展对教育领域产生了显著影响。本研究通过访谈14名学生，探讨他们日常与ChatGPT的互动。初步结果显示，学生们在享受ChatGPT提高学习效率和信息获取便利的同时，也面临着信任危机和伦理顾虑。他们认为ChatGPT相较于传统AI更显“人性化”。然而，这种矛盾情绪、行为不一致以及对学生整体上积极的态度，凸显了ChatGPT在教育领域的潜在价值。但值得注意的是，尽管其智能程度高，可能带来负面效应。因此，我们强调在应用时需谨慎，并致力于在未来的开发中减少潜在的危害。|
|**2024-07-17**|**MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models**|Leyang Shen et.al.|[2407.12709](http://arxiv.org/abs/2407.12709)|**[link](https://github.com/jiutian-vl/mome)**|**在多项视觉-语言任务中，多模态大型语言模型（MLLM）展现出卓越的能力。然而，通常情况下，通用的MLLM在大多数VL任务上的性能不如专门化的MLLM，这是因为存在任务干扰。为此，我们在这篇论文中提出了一种混合多模态专家（MoME）架构，旨在减轻任务干扰，从而获得一个全能的MLLM。MoME主要由两个关键组件构成：视觉专家混合体（MoVE）和语言专家混合体（MoLE）。MoVE能够自适应地调整来自不同视觉编码器的特征，并在转换架构上具有良好的兼容性。MoLE通过稀疏门控专家融入到语言模型中，实现了几乎无额外成本的性能提升。为了应对任务干扰，MoME专注于视觉和语言两种模态，以适应任务间的差异。大量的实验结果表明，MoME显著提高了通用MLLM在各种VL任务中的表现。源代码已在https://github.com/JiuTian-VL/MoME上发布。**|
|**2024-07-17**|**Patch-Level Training for Large Language Models**|Chenze Shao et.al.|[2407.12665](http://arxiv.org/abs/2407.12665)|**[link](https://github.com/shaochenze/patchtrain)**|**随着大型语言模型（LLMs）在语言理解和生成方面取得显著进步，其训练效率成为一个关键问题。传统上，LLMs通过预测序列中的下一个令牌进行训练。尽管基于令牌的训练方法取得了成功，但其计算成本高昂，因为需要处理大量令牌。为此，这篇论文提出了一种名为“patch-level training”的方法，它通过将多个令牌压缩成单个patch来缩短序列长度。在patch-level训练中，我们输入更短的patch序列，让模型学习预测下一个patch，从而大幅度减少了大部分训练数据的处理成本。接着，模型会进行剩余训练数据的令牌级训练，以适应推理模式。实验在不同规模的模型（370M-2.7亿参数）上进行，结果表明patch-level训练可以将总体计算成本降低至0.5倍，同时不会影响模型性能。源代码可在此获取：\url{https://github.com/shaochenze/PatchTrain}。**|
|**2024-07-17**|**Zero-shot Text-guided Infinite Image Synthesis with LLM guidance**|Soyeong Kwon et.al.|[2407.12642](http://arxiv.org/abs/2407.12642)|null|**背景：** 文本引导的图像编辑和生成方法在现实世界中有广泛的应用。然而，文本引导的无限图像合成面临着一些挑战。首先，缺乏高分辨率且具有丰富情境多样性的文本-图像配对数据集。其次，根据文本扩展图像需要全局连贯性和丰富的局部上下文理解能力。以往的研究主要集中在有限类别，如自然风景，且需要在高分辨率图像及其配文上进行训练。为解决这些问题，我们提出了一种新颖的方法，利用大型语言模型（LLMs）同时处理全局连贯性和局部上下文理解，无需任何高分辨率的文本-图像配对训练数据。  **方法：** 我们在训练扩散模型时，让它根据LLM生成的全局和局部描述以及视觉特征来扩展图像。在推理阶段，给定一张图片和一个全局描述，我们使用LLM生成下一个局部描述来扩展输入图像。然后，我们结合全局描述、生成的局部描述和视觉特征来扩展图像，以确保全局一致性并考虑空间局部上下文。  **实验结果：** 实验表明，我们的模型在定量和定性上都优于基线。此外，我们的模型展示了在零样本情况下，借助LLM引导进行文本引导任意大小图像生成的能力。  总结： 本文介绍了一种利用大型语言模型进行文本引导的图像扩展方法，无需依赖高分辨率的配对数据，能够实现全局连贯性和局部上下文理解，并在实验中表现出色，支持零样本任意大小图像生成。|
|**2024-07-17**|**Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages: Technologies and Experiences**|Claudio Pinhanez et.al.|[2407.12620](http://arxiv.org/abs/2407.12620)|null|自2022年以来，我们一直在探索人工智能（AI）和现代自然语言处理（NLP），特别是大型语言模型（LLMs）的应用领域，以支持和促进濒临消失的土著语言的使用与文档化。首先，我们关注世界语言多样性的减少，并讨论与处理土著语言相关的独特伦理挑战。为应对这些挑战，我们提出了一种基于社区参与和使用的AI开发新循环。接着，我们报告了使用少量数据微调最先进的翻译器，成功开发出高质量的土著语言机器翻译的鼓舞人心的成果，并讨论了避免开发过程中的一些常见陷阱。我们还展示了2023年和2024年在巴西与土著社区合作项目中的原型，目标是简化写作，以及发展土著语言模型（ILMs）作为创建拼写检查器、下一个词预测器等工具的可复制和可扩展方法。最后，我们展望一个未来，濒危的语言将通过互动的语言模型得以保存。|
|**2024-07-17**|**AudienceView: AI-Assisted Interpretation of Audience Feedback in Journalism**|William Brannon et.al.|[2407.12613](http://arxiv.org/abs/2407.12613)|**[link](https://github.com/mit-ccc/AudienceView-demo)**|****背景：** 记者理解和利用受众反馈至关重要，但如今他们在线面临大量观众评论，这是一项艰巨的任务。我们推出了AudienceView，一个在线工具，旨在通过大型语言模型（LLMs）帮助记者对这些反馈进行分类和解读。AudienceView识别主题和话题，将它们与特定评论关联，展示评论的情感倾向和分布，并协助用户构思后续报道项目。我们将探讨这类工具如何融入记者的工作流程，并强调情境理解及人类判断的重要性。  请记住，以上翻译不包含","字符。**|
|**2024-07-17**|**E5-V: Universal Embeddings with Multimodal Large Language Models**|Ting Jiang et.al.|[2407.12580](http://arxiv.org/abs/2407.12580)|**[link](https://github.com/kongds/e5-v)**|**### 背景  大规模多模态语言模型（MLLMs）在通用视觉和语言理解方面取得了显著进步。然而，如何利用MLLMs处理多模态信息的表示方式尚未充分研究。本文提出了一种新的框架E5-V，旨在使MLLMs适应实现通用多模态嵌入。研究结果表明，与先前方法相比，MLLMs在处理多模态输入方面展现出巨大潜力。通过结合提示，E5-V有效地弥合了不同类型输入之间的模态鸿沟，即使在无需微调的情况下也能表现出强大的多模态嵌入能力。  ### 方法  E5-V采用单一模态训练策略，仅使用文本对进行训练，这相较于传统的基于图像-文本对的多模态训练，显著提高了性能，同时降低了大约95%的训练成本，避免了收集昂贵的多模态训练数据的需求。实验在四种任务上进行了广泛的验证，以展示E5-V的有效性。  ### 结果  作为一款通用多模态模型，E5-V不仅在各任务中实现了顶尖性能，甚至在某些情况下超越了现有技术水平，所有这些都是基于单模态训练完成的。**|
|**2024-07-16**|**UrbanWorld: An Urban World Model for 3D City Generation**|Yu Shang et.al.|[2407.11965](http://arxiv.org/abs/2407.11965)|**[link](https://github.com/urban-world/urbanworld)**|城市作为人类生活的基本环境，包含了建筑、道路和植被等多元物理元素，这些元素之间存在着复杂的相互关联。构建逼真且互动的3D城市环境对于研发能在现实世界环境中感知、决策和行动的AI至关重要。然而，传统的手工制作过程耗时且精细，需要设计师投入大量精力来精确呈现复杂的城市特征。为此，我们提出UrbanWorld，这是一个首个自动生成定制化、真实且互动的3D城市世界的模型，支持灵活的控制条件。UrbanWorld的生成流程包括四个关键步骤：利用公开的OSM数据进行3D布局生成、借助强大的城市多模态大语言模型（Urban MLLM）进行城市场景规划与设计、通过先进的3D扩散技术实现可控资产渲染，以及MLLM辅助的场景细化。UrbanWorld生成的高保真3D城市环境为通用AI和机器感知系统在模拟中的真实反馈和交互提供了可能。我们致力于将UrbanWorld作为开源且多功能的平台，用于评估和提升AI在真实城市环境中的感知、决策和互动能力。|
|**2024-07-16**|**NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?**|Mo Li et.al.|[2407.11963](http://arxiv.org/abs/2407.11963)|**[link](https://github.com/open-compass/opencompass)**|**本文介绍了一个名为NeedleBench的框架，它是一系列评估大语言模型（LLMs）长文本理解能力的逐步升级任务。该框架涉及不同长度区间（4k、8k、32k、128k、200k、1M乃至更长）和深度范围，通过在不同文本深度区域插入关键数据点，系统性地测试模型在各种情境下的检索和推理能力。针对于双语长文本，我们利用这个框架来考察主流开源模型识别与问题相关的关键信息，并运用这些信息进行推理的能力。  此外，我们提出了祖先追踪挑战（Ancestral Trace Challenge，ATC），旨在模拟现实世界中长文本逻辑推理任务的复杂性，提供一个简单的方法来评估LLMs处理复杂长文本上下文的能力。研究结果显示，当前的LLMs在实际的长文本应用中仍有很大的提升空间，因为它们在处理逻辑推理难题时面临挑战。所有代码和资源可在OpenCompass项目（https://github.com/open-compass/opencompass）获取。**|
|**2024-07-16**|**Code Documentation and Analysis to Secure Software Development**|Paul Attie et.al.|[2407.11934](http://arxiv.org/abs/2407.11934)|null|我们介绍了一种名为Code Documentation and Analysis Tool（CoDAT）的工具。CoDAT旨在保持代码文档之间的连贯性，例如，如果代码片段中的某行被修改，相应的注释也会自动更新，确保内部一致性以及与代码的一致性。通过标记过时的注释，CoDAT提醒开发者维护最新的文档。我们利用大型语言模型检查代码片段与其描述的语义一致性，从而也能识别出语义不一致和过时的注释。这有助于程序员编写正确实现代码草图的代码，支持逐步细化方法，从代码草图逐步演变为经过一两次或更多次细化迭代的代码。  CoDAT在IntelliJ IDEA IDE中实现，利用Code Insight守护程序包结合自定义正则表达式算法，标记对应代码块已更改的标记注释。CoDAT的后端结构上是去中心化的，支持分布式账本框架，以实现代码一致性跟踪和架构编译管理。|
|**2024-07-16**|**What's Wrong? Refining Meeting Summaries with LLM Feedback**|Frederic Kirstein et.al.|[2407.11919](http://arxiv.org/abs/2407.11919)|null|随着数字会议的普及，会议摘要提炼成为关键任务。大型语言模型（LLMs）在这一领域展现出巨大潜力，它们在连贯性和理解上下文中超越了传统方法。然而，它们仍需改进以保持相关性并避免错误。我们提出了一种基于多LLM的会议摘要修正方法，通过两阶段过程模拟人类审查：错误识别和摘要精炼。我们发布了QMSum Mistake，这是一个包含200份由人工标注的自动生成会议摘要数据集，针对结构、遗漏和不相关等九种错误类型进行了标记。实验表明，LLMs能够准确识别这些错误。我们将识别出的问题转化为可操作的反馈，以此提升摘要的质量，如相关性、信息量、简洁性和连贯性。这种事后优化策略通过利用多个LLMs来验证输出质量，有效提高了摘要质量。我们的多LLM会议摘要方法对于需要稳健性、行动计划和目标导向的复杂文本生成任务具有潜在应用价值。|
|**2024-07-16**|**Ascend-CC: Confidential Computing on Heterogeneous NPU for Emerging Generative AI Workloads**|Aritra Dhar et.al.|[2407.11888](http://arxiv.org/abs/2407.11888)|null|在云工作负载中，基于大型语言模型（LLMs）的生成AI占据主导地位。专用硬件加速器，如GPU、NPUs和TPUs，因其在AI应用中的卓越性能超越了通用CPU。AI模型和数据通常具有高度敏感性，并来自相互不信任的各方。现有的基于CPU的可信执行环境（TEE），如英特尔SGX或AMD SEV，提供的保护不够充分。像Nvidia-CC这样的设备中心TEE仅针对紧密耦合的CPU-GPU系统，且采用专有方案，需要在主机CPU上部署TEE。另一方面，现有的学术提案大多针对特定的CPU-TEE平台。  为填补这一空白，我们提出了Ascend-CC，一种基于离散NPUs的机密计算架构，无需对主机系统信任。Ascend-CC通过确保数据和模型加密，保护数据、模型参数和运算符二进制，提供强大的安全性。它利用委托式内存语义确保与主机软件栈的隔离，并通过任务鉴权提供模型完整性的强有力保证。我们的Ascend-CC实现和与最新LLMs（如Llama2和Llama3）的评估表明，Ascend-CC引入的开销极小，无需修改AI软件栈。|
|**2024-07-16**|**Schema Matching with Large Language Models: an Experimental Study**|Marcel Parciak et.al.|[2407.11852](http://arxiv.org/abs/2407.11852)|**[link](https://github.com/uhasselt-dsi-data-systems-lab/code-schema-matching-llms-artefacs)**|**该论文探讨了大型语言模型（LLMs）在关系数据库架构（schema）匹配中的应用。目标是仅通过元素名称和描述找出两个关系模式之间的语义对应。研究者构建了一个来自健康领域的基准测试，并提出了不同的任务范围，即使用不同数量上下文信息提示模型进行schema匹配。他们对比了基于LLM的匹配方法与基于字符串相似度的基线，考察了匹配质量、验证工作量、决策确定性和互补性。研究发现，缺乏上下文信息会降低匹配质量，过多的信息也会有负面影响。新版本的LLMs通常能提高决策确定性。有些任务范围下的验证工作相对适度，且能成功识别大量真正意义上的语义匹配。研究结果表明，LLMs有潜力作为schema匹配的初始工具，数据工程师可以利用它们的名称和描述信息快速进行匹配，无需依赖实际数据实例。**|
|**2024-07-16**|**LoFTI: Localization and Factuality Transfer to Indian Locales**|Sona Elza Simon et.al.|[2407.11833](http://arxiv.org/abs/2407.11833)|**[link](https://github.com/csalt-research/lofti)**|**大型语言模型（LLMs）通过训练在互联网上爬取的大型网页数据集，积累了大量的世界知识。然而，这些数据集通常倾向于英语和西欧国家，导致LLMs对来自其他地区，特别是印度的本地化查询产生偏见或虚构的回答。为此，我们提出一个新的基准LoFTI（印度本地化与事实转移），用于评估LLMs的本地化和事实文本转换能力。LoFTI包含关于全球源地点和印度目标地点（包括国家、州和城市的不同层级）实体的事实陈述，涉及各类广泛的主题。我们使用LoFTI来评估Mixtral、GPT-4以及两种适用于本地化事实转移任务的Mixtral衍生方法。实验表明，LoFTI是一个高质量的评估标准，包括GPT-4在内的所有模型在不同层级的本地化上都表现出偏差。**|
|**2024-07-16**|**GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text**|Kyle Hamilton et.al.|[2407.11827](http://arxiv.org/abs/2407.11827)|null|尽管机器学习在检测文本中的宣传手段方面引起了广泛关注，但大多数方法侧重于“黑盒”解决方案，其内部工作原理不透明。可解释的方法提供了解决方案，但它们依赖于精心的特征工程和昂贵的专家标注数据。此外，关于说服性文本的语言特性通常由修辞学家或语言学家关注，但没有适合机器学习的标记有此类特性的数据集。本研究旨在编纂文献中识别出的22个修辞和语言特征，目的是对一个已标注有宣传手段的现有数据集进行注释。为了帮助人类专家在自然语言句子上标注这些特征，我们特别设计了名为RhetAnn的网络应用，以减少原本较大的认知负担。接着，使用一小部分标注数据，我们利用GPT-3.5，一种生成大型语言模型（LLM），对剩余数据进行微调，同时兼顾成本效益和分类精度。这项研究表明，结合少量人工标注示例与GPT，可以有效地以传统仅依赖人类专家的标注成本的十分之一左右实现大规模标注过程的扩展。结果与撰写时表现最好的模型（GPT-4）相当，且成本降低10倍。我们的贡献包括这些特征、它们的属性、定义以及示例的机器可读格式，以及RhetAnn的代码、GPT提示和微调流程，这些都推动了可解释的宣传手段检测领域的最新进展。|
|**2024-07-16**|**PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation**|Branden Butler et.al.|[2407.11798](http://arxiv.org/abs/2407.11798)|null|近年来，大型语言模型（LLMs）在分布式计算机集群上的推理已成为研究热点，许多加速技术借鉴了CPU的推测执行策略。这些技术旨在缓解内存带宽瓶颈，但会增加每次推理运行的端到端延迟，需要高推测接受率来提升性能。然而，由于任务间接受率的变异性，推测性推理可能导致性能下降。此外，管道并行设计需要大量用户请求以保持高利用率。针对这些问题，我们提出了PipeInfer，这是一种旨在减少跨令牌延迟、提高单请求场景下系统利用率的管道化推测加速技术，同时增强了对低推测接受率和低带宽互联的容忍度。  PipeInfer通过连续异步推测和早期推理取消实现了显著的改进。连续异步推测允许同时进行单令牌推理与多个推测运行，从而降低延迟和生成速度。而早期推理取消则能够在推理过程中跳过无效运行的计算，进一步提升速度和延迟。PipeInfer在生成速度上比标准推测性推理最高可提升2.15倍。|
|**2024-07-16**|**Large Language Models as Misleading Assistants in Conversation**|Betty Li Hou et.al.|[2407.11789](http://arxiv.org/abs/2407.11789)|null|大型语言模型（LLMs）在各种信息查询任务上能够提供帮助。然而，模型输出可能会误导用户，无论是无意的还是故意的。我们针对阅读理解任务探讨了LLMs在欺骗性辅助方面的能力，将其作为人类用户的代理。实验对比了三种情况：（1）模型被提示提供真实信息，（2）模型被提示进行微妙误导，以及（3）模型被提示支持错误答案。结果显示，GPT-4能够有效误导GPT-3.5-Turbo和GPT-4自身，欺骗性助手导致任务准确率下降高达23%，相比于使用真实助手。此外，我们发现向用户模型提供更多的上下文信息可以部分抵消欺骗模型的影响。这项研究揭示了LLMs生成误导性信息的能力及其在现实场景中的潜在影响。|
|**2024-07-15**|**VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation**|Bocheng Zou et.al.|[2407.10972](http://arxiv.org/abs/2407.10972)|**[link](https://github.com/vgbench/VGBench)**|**在视觉模型领域，主要的表示方式是使用像素来绘制视觉世界。然而，这并非总是最佳或唯一的表示视觉内容的方法，特别是对于设计师和艺术家，他们常用多边形等几何形状来构建图形。矢量图形（VG）提供了一种文本形式的视觉内容表示，对于卡通或素描等类型的内容可能更为精炼和强大。近期的研究表明，强大的大语言模型（LLMs）在处理矢量图形方面展现出令人鼓舞的结果。但这些工作主要侧重于定性分析、理解或特定类型的矢量图形。我们提出VGBench，这是一个全面的基准，用于评估LLMs在处理矢量图形方面的性能，包括：(a) 对视觉理解和生成的双重关注，(b) 多种矢量图形格式的评估，(c) 不同类型的提问，(d) 广泛的提示技巧，以及(e) 在多种LLMs下的表现。通过对收集的4279个理解样本和5845个生成样本进行评估，我们发现LLMs在这两个方面都表现出强大能力，但在低级格式（如SVG）上表现稍逊。我们的数据和评估流程将在<https://vgbench.github.io>上开源。**|
|**2024-07-15**|**Q-Sparse: All Large Language Models can be Fully Sparsely-Activated**|Hongyu Wang et.al.|[2407.10969](http://arxiv.org/abs/2407.10969)|null|我们提出了一种简单但有效的训练方法，称为Q-Sparse，专为大规模语言模型（LLMs）设计。Q-Sparse使得LLMs的激活全为稀疏，从而在推理阶段带来显著的效率提升。这一方法通过应用顶部K稀疏化技术对激活进行处理，并结合直通估计进行训练。主要成果包括：(1) Q-Sparse在保持与基线LLM结果相当的同时，具有更高的推理时的效率；(2) 我们给出了稀疏激活LLMs的最优推理缩放定律；(3) Q-Sparse在各种场景下表现优秀，包括从头开始训练、预训练模型的继续训练和微调；(4) Q-Sparse适用于全精度和1位精度的LLMs，如BitNet b1.58。特别是，BitNet b1.58与Q-Sparse（可配备MoE）的结合，为未来LLMs的效率提升，包括成本和能耗，提供了基石和清晰路径。|
|**2024-07-15**|**Fast Matrix Multiplications for Lookup Table-Quantized LLMs**|Han Guo et.al.|[2407.10960](http://arxiv.org/abs/2407.10960)|**[link](https://github.com/hanguo97/flute)**|大型语言模型（LLMs）的部署通常受到内存带宽的限制，其中主要瓶颈是将模型参数从GPU全局内存传输到寄存器的成本。通过结合权重只量化，可以减少内存移动，从而加速推理速度。然而，为量化后的LLMs设计高性能内核是一项重大挑战，尤其是当权重被压缩到非均匀分隔的位宽（如3位），并采用非均匀查找表（LUT）量化时。本文介绍了一种灵活的查找表引擎FLUTE，它通过对量化权重矩阵进行离线重构，以最小化解压相关的位操作，并通过向量化和复制查找表来缓解共享内存带宽限制。在小批量（小于32）和量化组大小为128（LLM推理中的典型值）的情况下，FLUTE内核的速度可以比现有GEMM内核快2-4倍。作为FLUTE的应用，我们探讨了查找表基的NormalFloat量化的一种简单扩展，并将其应用于量化LLaMA3，获得了与强大基准相当的量化性能，同时实现了端到端吞吐量的1.5到2倍提升。|
|**2024-07-15**|**MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models**|Chengguang Gan et.al.|[2407.10953](http://arxiv.org/abs/2407.10953)|null|## 任务  **背景：** 互惠增强效应（MRE）在信息抽取和多任务研究中展现出巨大潜力。然而，由于仅有的MRE混合数据集局限于日语，这限制了全球研究界的广泛探索。为了克服这一局限，我们构建了一个多语言MRE混合数据集（MMM），包含英语、日语和汉语的21个子集。本文还提出了一种利用大型语言模型（LLMs）辅助的数据集翻译方法，通过利用LLMs将原始日语文本进行翻译，大大减少了数据集构建时的人工标注时间。  **贡献：** 我们扩展了数据集，加入了开放领域命名实体识别（NER）和句子分类任务。基于这个扩充后的数据集，我们开发了一个统一的输入-输出框架，训练了一个开放域信息抽取大语言模型（OIELLM）。实验表明，OIELLM模型能够有效处理新的MMM数据集，并表现出显著的性能提升。  总之，我们的工作旨在通过提供多语言资源和高效的翻译策略，推动互惠增强效应在多语言信息抽取领域的应用研究。|
|**2024-07-15**|**Can Textual Semantics Mitigate Sounding Object Segmentation Preference?**|Yaoting Wang et.al.|[2407.10947](http://arxiv.org/abs/2407.10947)|**[link](https://github.com/gewu-lab/sounding-object-segmentation-preference)**|**## 任务  音频-视觉分割（Audio-Visual Segmentation，AVS）任务的目标是利用音频线索在视觉空间中分割出发声物体。然而，研究指出，现有的AVS方法过于依赖对可听见对象的分割偏好，而非精确的音频指导。问题在于，相比于视觉，音频在多声源音场中的语义表现较弱，导致其在指导视觉空间时作用有限。鉴于文本模态经过深入探索，包含丰富的抽象语义，我们提出利用视觉场景中的文本提示来增强音频指导的精确性。  我们的方法首先通过现成的图像描述器获取场景描述，然后利用预训练的大语言模型推断潜在的发声物体作为文本线索。接着，我们设计了一个新颖的基于语义的音频建模模块，引入动态掩码，将音频特征与文本线索融合，生成具有代表性的发声物体特征。这些特征不仅包含音频信息，还蕴含了生动的语义，从而为视觉空间提供更为清晰的指引。我们在AVS基准数据集上的实验结果表明，借助文本提示，我们的方法对音频的敏感度得到提升，在所有三个子集上表现出高度竞争力。项目页面：[https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference](https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference)。**|
|**2024-07-15**|**GRUtopia: Dream General Robots in a City at Scale**|Hanqing Wang et.al.|[2407.10943](http://arxiv.org/abs/2407.10943)|**[link](https://github.com/openrobotlab/grutopia)**|**近期的研究正在探索Embodied AI领域的规模法则。鉴于收集现实世界数据的高昂成本，我们认为模拟到现实（Sim2Real）方法对于扩展embodied模型的学习至关重要。本文介绍项目GRUtopia，这是一个专为各种机器人设计的首个互动三维社会。它具有多项创新：(a) 场景数据集GRScenes包含了10万张交互式、精细注释的场景，这些场景可以自由组合成城市规模的环境。与以往主要关注家庭环境的作品不同，GRScenes涵盖了89个多样化的场景类别，弥合了服务导向环境中机器人初始部署的差距。(b) GRResidents是一个由大型语言模型驱动的非玩家角色（NPC）系统，负责社交互动、任务生成和任务分配，从而模拟embodied AI应用中的社会场景。(c) 标准化基准GRBench支持各种机器人，但以腿足机器人为主，提供涉及物体导航、社交导航和移动操作的任务，这些任务具有适度的挑战性。我们期望这项工作能够缓解该领域高质量数据的匮乏，并为Embodied AI研究提供更全面的评估。项目代码可从https://github.com/OpenRobotLab/GRUtopia获取。**|
|**2024-07-15**|**FinDKG: Dynamic Knowledge Graphs with Large Language Models for Detecting Global Trends in Financial Markets**|Xiaohui Victor Li et.al.|[2407.10909](http://arxiv.org/abs/2407.10909)|**[link](https://github.com/xiaohui-victor-li/FinDKG)**|动态知识图谱（DKGs）是一种流行的数据结构，用于表示随时间变化的对象之间的各种连接。它们在处理复杂无结构数据源（如文本和图像）提取的信息时展现出高效性。在金融应用中，DKGs可用于基于财经新闻文章探测投资策略的趋势。本研究探索大型语言模型（LLMs）作为动态知识图谱生成器的特性，为此我们提出了一种开源的Fine-tuned LLM，称为集成上下文知识图谱生成器（ICKG）。利用ICKG，我们从财经新闻文章中创建了一个新的开源动态知识图谱，称为FinDKG。此外，我们设计了注意力机制的图神经网络架构（KGTransformer），用于分析这个图谱。我们在基准数据集和FinDKG上测试了模型性能，结果显示在链接预测任务中，KGTransformer表现优异。最后，我们评估了KGTransformer在FinDKG上的主题投资性能，证明它能超越现有的主题交易所交易基金（ETF）。|
|**2024-07-15**|**Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique**|Mark Russinovich et.al.|[2407.10887](http://arxiv.org/abs/2407.10887)|null|随着对大型语言模型（LLMs）被盗和误用的担忧加剧，模型指纹化的必要性提升。在这种背景下，成功的指纹应具备五个特性：透明性、效率、持久性、鲁棒性和不可伪造性。本文首先定义了这些要求。接着，我们提出了一种新的简单指纹方法——Chain & Hash，它融合了加密理念，实现了所有这些特性。Chain & Hash涉及生成一组问题（指纹）及其可能的答案，然后使用安全哈希技术将它们合并，以确定每个问题的值，从而保证不可伪造性，防止对手声称虚假所有权。我们在多个模型上评估了Chain & Hash技术，并展示了它对良性操作（如在不同数据集上微调）和敌意删除指纹的鲁棒性。实验表明，带指纹的模型在各种基准测试中的性能几乎与非指纹化模型相当，同时保持了高效性及其实用价值。|
|**2024-07-15**|**SLIP: Securing LLMs IP Using Weights Decomposition**|Yehonathan Refael et.al.|[2407.10886](http://arxiv.org/abs/2407.10886)|null|随着大型语言模型（LLMs）在学术界和工业界的广泛应用，这些模型的价值作为知识产权（IP）日益凸显，反映出其背后巨大的投资。然而，由于云部署成本高，边缘设备部署的需求增加，这可能导致模型参数被盗用和未经授权使用。当前的保护方法在实用性、准确性损失或适应性方面存在局限。本文提出了一种新颖的混合推理算法，称为SLIP（Secure Lightweight Inference Protocol），旨在保护部署在边缘的模型免受盗窃。SLIP是首个兼顾实际应用的实用性和严格安全性的混合协议，同时保持零精度下降和低延迟影响。  SLIP通过矩阵分解实现了模型在两个计算资源之间的划分：一个安全但昂贵，另一个成本效益高但易受攻击。关键在于，安全资源保留了模型IP中最敏感的部分，同时执行最少的计算，而脆弱资源则相反。此外，该协议提供了防止攻击者利用分割获取保密信息的安全保障。最后，我们展示了实验结果，证明了SLIP的稳健性和有效性，使其成为保护LLMs的理想解决方案。|
|**2024-07-15**|**Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models**|Rui Zhang et.al.|[2407.10873](http://arxiv.org/abs/2407.10873)|null|自动化启发式设计（AHD）因其在自动开发高效启发式方法方面的潜力而受到广泛关注。随着大型语言模型（LLMs）的兴起，人们开始探索将AHD视为进化程序搜索（EPS）问题的新途径。然而，当前的基准设置不一致，基础比较不足，且缺乏对LLM与搜索策略结合必要性的深入分析，这使得现有基于LLM的EPS方法的实际进展难以得到充分证明。本研究通过一项大规模基准测试，涵盖了四项基于LLM的EPS方法和四项AHD问题，跨越九种LLM，并进行了五次独立运行。我们的广泛实验提供了有价值的见解，实证了在LLM驱动的AHD方法中的进化搜索的重要性，同时也推动了未来EPS算法开发的进步。为了促进可访问性和可重复性，我们已经全面开源了我们的基准和相关结果。|
|**2024-07-12**|**FairyLandAI: Personalized Fairy Tales utilizing ChatGPT and DALLE-3**|Georgios Makridis et.al.|[2407.09467](http://arxiv.org/abs/2407.09467)|null|在这个充满人工智能驱动的叙事多样性世界中，有一个独特的机会是通过定制和个性化的叙述吸引年轻观众。本文介绍FairyLandAI，这是一个专为儿童开发的创新大型语言模型（LLM），基于OpenAI的API构建。其特别之处在于，FairyLandAI不仅能生成引人入胜、适合各年龄段且反映各种传统的故事，还能自动生成适合高级图像生成工具（如GenAI和Dalle-3）的创意提示，从而丰富讲故事的体验。FairyLandAI精准地适应儿童的想象力世界，提供既教育又娱乐的故事，并与不同年龄阶段所蕴含的价值观相一致。它的独特之处在于根据个体孩子的喜好和文化背景定制故事，标志着个性化叙事新时代的到来。此外，它与图像生成技术的结合提供了全面的叙事体验，激发口头和视觉创造力。实证评估显示，FairyLandAI在创作吸引孩子们的故事方面表现出色，这些故事不仅娱乐，还体现了多元传统中的道德教诲。这个模型对于家长和教育工作者来说是一个宝贵的工具，帮助他们通过引人入胜的故事传递深刻的人生道理。FairyLandAI代表了利用LLMs，特别是OpenAI API进行教育和文化提升的开创性一步，使复杂而富有教育意义的道德故事对年轻、富有想象力的心灵变得易于理解和享受。|
|**2024-07-12**|**Human-like Episodic Memory for Infinite Context LLMs**|Zafeirios Fountas et.al.|[2407.09450](http://arxiv.org/abs/2407.09450)|**[link](https://github.com/em-llm/EM-LLM-model)**|大型语言模型（LLMs）展现了惊人的能力，但它们在处理长序列时仍面临保持连贯性和准确性的问题。人类大脑在组织和检索跨长时间尺度的亲身经历方面尤为出色，能够覆盖一生的记忆。本文提出了一种新颖的方法，称为EM-LLM，它将人类的 episodic memory（情景记忆）和事件认知关键要素融入到LLMs中，使其能够有效处理几乎无限长度的上下文，同时保持计算效率。EM-LLM通过结合贝叶斯惊奇度和图论边界细化技术，在线方式组织令牌序列成连贯的事件。当需要时，通过两阶段的记忆过程——结合相似度和时间邻接的检索，实现高效且类似人类的信息访问。在LongBench数据集上的实验显示，EM-LLM的表现优于最先进的InfLLM模型，总体相对提高了4.3%，在各种任务中，包括提升了33%的PassageRetrieval任务。此外，我们的分析揭示了EM-LLM事件分割与人类感知事件之间的强相关性，暗示了这个人工系统与生物对应机制之间的桥梁。这项工作不仅提升了LLMs处理长序列的能力，还为探索人类记忆机制提供了计算框架，开辟了人工智能和认知科学交叉研究的新途径。|
|**2024-07-12**|**ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts**|Amelia F. Hardy et.al.|[2407.09447](http://arxiv.org/abs/2407.09447)|**[link](https://github.com/sisl/astprompter)**|## 背景  通常的自动化大型语言模型（LLMs）红队对抗策略集中在寻找能触发冻结语言模型（即防御者）生成有毒文本的提示。这可能导致对抗模型（即攻击者）产生难以理解、不自然的输出。在此，我们提出了一种强化学习框架来处理LLMs的红队对抗任务，目标是找到既能（1）触发防御者生成有毒文本，又能（2）保持低困惑度（即防御者打分）的提示。我们认为在红队对抗场景中，这些情况最相关，因为它们很可能在防御者模型的常规使用中出现。我们通过一种新颖的在线和弱监督的Identity Preference Optimization（IPO）变体解决了这个问题，应用于GPT-2和GPT-2 XL作为防御者。实验表明，我们的策略能够生成既可能又会触发毒性的提示。最后，我们分析了学习策略、可能性与毒性之间的权衡，并讨论了相关含义。该项目的源代码可在这里获取：https://github.com/sisl/ASTPrompter/。|
|**2024-07-12**|**MUSCLE: A Model Update Strategy for Compatible LLM Evolution**|Jessica Echterhoff et.al.|[2407.09435](http://arxiv.org/abs/2407.09435)|null|## 背景 大型语言模型（LLMs）由于数据或架构的调整而经常更新以提升性能。在升级过程中，开发者通常侧重于提高总体性能指标，对与旧版本兼容性的关注较少。然而，用户往往会对他们使用的机器学习模型的功能和能力形成心理模型，并随着每次更新需要调整这个模型。频繁的模型变更可能导致用户满意度下降。实际上，下游任务微调器依赖预训练的LLM基模型。当基模型更新时，面向用户的这些下游任务模型可能会出现实例退化或负面翻转——先前正确的实例现在被预测错误。即使下游任务的训练流程保持不变，这种情况也会发生。我们的工作旨在为用户提供无缝的模型更新体验，方法有两个方面。首先，我们提出了一套评估指标，用于衡量模型与旧版本的兼容性，特别适用于生成任务，也可应用于分类任务。我们观察到不同模型版本和更新之间存在退化和不一致性，尤其是在多样化的任务上。  ## 任务 我们的研究旨在通过以下两个途径提供对用户友好的模型更新：一是开发一种兼容性评估标准，用于检测生成任务或其他任务中的模型版本间差异；二是提出一种训练策略，通过训练兼容性模型来减少模型更新中的不一致，从而降低从Llama 1到Llama 2等版本更新时的负面翻转率，最多可减少40%。这样，用户可以更轻松地适应新版本，而无需频繁调整他们的预期和使用方式。|
|**2024-07-12**|**Open (Clinical) LLMs are Sensitive to Instruction Phrasings**|Alberto Mario Ceballos Arroyo et.al.|[2407.09429](http://arxiv.org/abs/2407.09429)|**[link](https://github.com/alceballosa/clin-robust)**|## 背景 基于指令的大型语言模型（LLMs）能够根据自然语言指令执行各种任务，但它们对指令表述的敏感性是一个问题。在医疗领域尤其关键，因为临床医生可能不是提示工程方面的专家，且错误输出的潜在后果更为严重。这就提出了一个实际问题：针对临床自然语言处理任务，指令调优的LLMs对于自然（非攻击性的）指令表述变化有多稳健？我们收集了来自不同任务的医生提示，衡量了七种LLM（包括通用和专用的）对指令表述细微差异的敏感度。研究发现，所有模型的表现差异显著，令人意外的是，专门针对临床数据训练的模型相较于通用领域的模型，其稳定性较差。此外，随意的表述变化可能影响公平性，例如，用于预测死亡率的有效但不同的指令不仅会导致整体性能的波动，还会在不同人群间产生差异。|
|**2024-07-12**|**TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models**|Hang Zou et.al.|[2407.09424](http://arxiv.org/abs/2407.09424)|null|该论文首次提出了一种方法，旨在将大型通用语言模型（LLMs）适应到电信领域的专用模型。为此，我们收集并构建了电信特定的预训练数据集、指令数据集和偏好数据集，分别用于持续预训练、指导调优和对齐调优。由于电信领域缺乏广泛接受的评估基准，我们扩展了现有的评估标准，并提出了三个新的基准：电信数学建模、电信开放性问题与答案（TeleQnA）以及电信代码任务。这些新基准全面评估了LLMs在电信领域的数学建模、开放式问题回答、代码生成、填充、总结和分析等能力。我们的优化模型TelecomGPT在电信数学建模基准上显著优于最先进的模型，如GPT-4、Llama-3和Mistral，并在TeleQnA、3GPP技术文档分类、电信代码摘要与生成以及填充任务上表现出相当的性能。|
|**2024-07-12**|**Mitigating Entity-Level Hallucination in Large Language Models**|Weihang Su et.al.|[2407.09417](http://arxiv.org/abs/2407.09417)|**[link](https://github.com/oneal2000/entityhallucination)**|**随着大型语言模型（LLMs）的兴起，用户获取信息的方式发生了转变，从传统的搜索引擎转向直接与LLMs进行问答交互。然而，LLMs的广泛应用暴露出一个挑战，即“幻觉”生成，即模型生成看似连贯但事实性错误的回答，这导致用户对基于LLMs的信息检索系统产生怀疑。为解决这一问题，本文提出了一种新颖的方法：动态检索增强基于幻觉检测（DRAD）。DRAD改进了传统检索增强技术，通过实时幻觉检测来动态调整检索过程。它主要包括两个核心组件：实时幻觉检测（RHD），用于在无需外部模型的情况下识别潜在的幻觉；以及基于外部知识的自我纠正（SEK），利用外部知识修正这些错误。实验结果表明，DRAD在检测和减少LLMs中的幻觉方面表现出色。我们已将所有代码和数据开源，供学术界使用：https://github.com/oneal2000/EntityHallucination。**|
|**2024-07-12**|**SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers**|Shraman Pramanick et.al.|[2407.09413](http://arxiv.org/abs/2407.09413)|**[link](https://github.com/google/spiqa)**|**### 任务  在深入阅读科学论文时，快速查找信息是关键。然而，现有的基于论文的问题 answering（QA）数据集在规模和内容上存在局限，主要关注文本部分。为弥补这一不足，我们推出了SPIQA（科学论文图像问题回答），这是一个专门设计的大型QA数据集，旨在理解计算机科学各领域的复杂图表、表格和结果可视化。借助多模态大语言模型（MLLMs）的强大理解能力，我们通过自动化和人工筛选创建了这个数据集。SPIQA包含了27万条问题，分为训练、验证和三个不同的评估分段。通过与12个基础模型的广泛实验，我们评估了当前多模态系统理解科研文章细微之处的能力。此外，我们提出了一种链式思维（Chain-of-Thought，CoT）评价策略，结合上下文检索，实现了细致的逐步骤评估，有助于提升模型性能。我们还探讨了额外文本信息对性能提升的上限，这表明了其对未来研究的潜力，并预示着该数据集将革新我们与科学文献互动的方式。**|
|**2024-07-12**|**PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents**|Saber Zerhoudi et.al.|[2407.09394](http://arxiv.org/abs/2407.09394)|**[link](https://github.com/padas-lab-de/PersonaRAG)**|大型语言模型（LLMs）由于知识过时和胡编乱造而难以生成可靠的结果。为了解决这个问题，检索增强生成（RAG）模型通过结合外部知识改进了LLMs，但往往无法个性化检索过程。这篇论文提出了一种新颖的框架——PersonaRAG，它引入了以用户为中心的代理，能够根据实时用户数据和交互来调整检索和生成。在多个问答数据集上的评估显示，PersonaRAG相较于基础模型表现出显著优势，能更好地满足用户的个性化需求。实验结果表明，用户适应的信息检索系统具有广阔的发展前景。|
|**2024-07-12**|**GAVEL: Generating Games Via Evolution and Language Models**|Graham Todd et.al.|[2407.09388](http://arxiv.org/abs/2407.09388)|**[link](https://github.com/gdrtodd/gavel)**|自动创建新颖有趣的游戏是一个复杂任务，它涉及如何以计算机可处理的形式表达游戏规则、搜索庞大的潜在游戏空间，以及准确评估未见过游戏的原创性和质量。先前的研究主要关注于有限的规则表示，并依赖于特定领域的启发式方法。在这个工作中，我们专注于在Ludii游戏描述语言中生成新奇的游戏，该语言编码了各种风格和玩法的1000多款棋盘游戏规则。我们借鉴了大型语言模型和进化计算的最新进展，训练了一个能够智能地变异和重组以代码形式表达的游戏机制的模型。我们通过定量和定性分析表明，我们的方法能够创造出新的、有吸引力的游戏，包括那些现有Ludii数据集中未覆盖的游戏区域。生成的一些游戏示例可通过Ludii门户在线体验。|
|**2024-07-11**|**MAVIS: Mathematical Visual Instruction Tuning**|Renrui Zhang et.al.|[2407.08739](http://arxiv.org/abs/2407.08739)|**[link](https://github.com/zrrskywalker/mavis)**|**### 背景  多模态大型语言模型（MLLMs）近年来在学术界和工业界引起了广泛关注。尽管它们在多模态场景中的表现突出，但对数学图解的数学问题求解能力研究尚显不足。为此，我们指出了MLLM在数学视觉领域的三个关键改进领域：数学图解的视觉编码、图解与语言的对齐以及数学推理技能。这促使我们需要大规模、高质量的视觉数学数据和训练流程。本文提出MAVIS（Mathematical VISual instruction tuning for MLLMs），一个针对MLLM的数学视觉指导调参范式，包括一系列数学视觉数据集和专门的MLLM。  ### 方法  MAVIS分为三个阶段进行从头开始的训练。首先，我们创建了MAVIS-Caption，包含558,000个图解-描述对，通过对比学习来微调专为数学设计的视觉编码器（CLIP-Math），以提升图解的视觉理解能力。其次，利用MAVIS-Caption，我们通过投影层将CLIP-Math与大型语言模型（LLM）进行关联，增强数学领域的视觉语言对齐。最后，我们引入MAVIS-Instruct，包含900,000个精心收集和标注的视觉数学问题，用于最终指导调参，以增强MLLM的稳健数学推理能力。在MAVIS-Instruct中，我们提供了每个问题的完整链式思考（Chain-of-Thought, CoT）理由，并减少文本冗余，使模型更专注于视觉元素。  ### 结果  数据和模型已发布在https://github.com/ZrrSkywalker/MAVIS。通过MAVIS，我们旨在填补数学视觉理解的空白，提升MLLM在解决实际数学问题时的表现。**|
|**2024-07-11**|**Real-Time Anomaly Detection and Reactive Planning with Large Language Models**|Rohan Sinha et.al.|[2407.08735](http://arxiv.org/abs/2407.08735)|null|这篇论文探讨了如何利用大规模语言模型（如大型语言模型）在机器人系统中检测和应对异常情况，以提高其鲁棒性和安全性。主要挑战包括减少模型的计算开销以便实现实时应用，以及将模型的判断融入到安全控制框架中。研究者提出了一种两阶段推理框架：首先是一个快速的二元异常分类器，它在语言模型嵌入空间中分析观测数据，如果发现异常，会触发后续的慢速推理阶段，利用生成式语言模型进行深入的逻辑推理。这种设计类似于模型预测控制中的决策分支，考虑到慢速推理器的延迟，可以立即采取备份计划，确保系统的安全性。  通过与最先进的GPT模型的自回归推理方法进行比较，研究发现，即使使用小型语言模型，他们的快速异常分类器也表现出色。这使得他们开发的运行时监控器能够在资源和时间限制下，提升动态机器人系统，如四旋翼无人机或自动驾驶车辆的信任度。论文的视频示例可以在项目页面上查看：https://sites.google.com/view/aesop-llm。|
|**2024-07-11**|**Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist**|Zihao Zhou et.al.|[2407.08733](http://arxiv.org/abs/2407.08733)|null|### 翻译  **摘要：**  强大的数学推理能力是大型语言模型（LLMs）卓越性能的关键体现。如何定义和全面评估LLMs的数学能力，以及在实际应用中反映用户体验，已成为关键问题。目前的基准测试主要侧重于问题解决能力，这可能导致模型过拟合，并无法准确反映真正的数学推理能力。我们认为，如果模型真正理解了问题，它应该能在各种任务中稳健且灵活地应用。在此启发下，我们提出MATHCHECK，一个旨在测试任务泛化和推理鲁棒性的精心设计的清单，以及一个自动生成清单的工具。MATHCHECK包含多个数学推理任务和测试类型，以促进对数学推理能力和行为测试的全面评估。我们利用MATHCHECK创建了MATHCHECK-GSM和MATHCHECK-GEO，分别针对数学文本推理和多模态推理能力进行评估，它们是GSM8k、GeoQA、UniGeo和Geometry3K等基准的升级版。我们使用MATHCHECK-GSM和MATHCHECK-GEO对超过20种LLM和11种多模态LLMs进行了评估，以检验它们的综合数学推理能力。结果显示，尽管前沿模型如GPT-4表现出色，但其他模型家族在清单上的表现显著下降。进一步实验表明，与传统数学基准相比，MATHCHECK更好地反映了真正的数学能力，线性度更高，从而支持我们的设计。通过MATHCHECK，我们可以轻松进行详细的行为分析，深入探究模型。|
|**2024-07-11**|**A Taxonomy for Data Contamination in Large Language Models**|Medha Palavalli et.al.|[2407.08716](http://arxiv.org/abs/2407.08716)|null|大型语言模型在基于广泛网络语料库的预训练后，在众多下游任务上展现出卓越性能。然而，数据污染问题日益引起关注，即评估数据可能存在于预训练数据中，导致模型表现虚高。去污染（decontamination）作为一种可能的解决方案，试图检测并移除这些污染数据。然而，污染数据可能源于测试集的修改版本，这使得检测变得困难。目前尚不清楚不同类型的污染如何影响语言模型在下游任务中的性能。我们提出了一种分类体系，对语言模型在预训练阶段遇到的各种污染类型进行划分，并确定了哪些类型的风险最高。我们通过分析总结和问答两个关键自然语言处理任务，揭示了不同类型污染如何影响模型在实际评估中的表现。|
|**2024-07-11**|**GTA: A Benchmark for General Tool Agents**|Jize Wang et.al.|[2407.08713](http://arxiv.org/abs/2407.08713)|**[link](https://github.com/open-compass/GTA)**|**人们普遍关注大型语言模型（LLMs）与各种工具的整合，以开发通用代理，但这对LLMs的工具使用能力提出了挑战。当前的评估方法存在明显缺陷，如使用AI生成的查询、单步骤任务、模拟工具以及仅限文本的交互，未能充分展示这些模型在实际问题解决中的能力。因此，我们提出GTA（通用工具代理基准），它包含三个关键特性：（1）真实的用户查询：由人类编写，具有简单的现实世界目标，但隐含了工具使用需求，要求LLMs能推理出合适的工具并规划解决方案步骤。（2）真实部署的工具：一个配备有感知、操作、逻辑和创新类工具的评估平台，用于评估模型的实际任务执行性能。（3）真实的多模态输入：包括空间场景图片、网页截图、表格、代码片段和打印/手写材料等，以贴近真实世界的场景。  我们设计了229个现实生活任务和可执行的工具链，来评估主流LLMs。实验结果显示，对于真实的用户查询，现有的LLMs面临严峻挑战，GPT-4完成的任务不足一半，大多数模型的成绩低于25%。这个评估揭示了当前LLMs在实际工具使用能力上的瓶颈，为提升通用工具代理的研究提供了方向。GTA的相关代码和数据集已可在<https://github.com/open-compass/GTA>获取。**|
|**2024-07-11**|**Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models**|Zhening Xing et.al.|[2407.08701](http://arxiv.org/abs/2407.08701)|null|大型语言模型因其单向时间注意力机制，在文本和音频流数据生成方面展现出卓越的效果。然而，尽管对实时视频处理的需求日益增长，但视频流处理的研究却相对较少。现有的视频扩散模型依赖双向时间注意力，这限制了它们处理直播视频的能力。为此，我们提出Live2Diff，这是首个专为实时视频翻译设计的具有单向时间注意力的视频扩散模型。与先前工作不同，我们的方法通过与前一帧及其少数预热帧相关联，保持了时间一致性和平滑性，无需考虑未来帧。同时，我们采用高效的降噪方案，包括KV缓存机制和流水线处理，以支持互动帧率下的视频流翻译。大量的实验结果表明，我们的注意力机制和流水线设计显著优于先前的方法，在保持时间平滑性和/或效率方面表现出色。|
|**2024-07-11**|**Mitigating Catastrophic Forgetting in Language Transfer via Model Merging**|Anton Alexandrov et.al.|[2407.08699](http://arxiv.org/abs/2407.08699)|null|随着开放型大型语言模型（LLMs）在英语任务中的性能不断提升，研究人员正致力于将其扩展到其他语言。然而，这种语言适应往往会导致基础模型能力的灾难性遗忘，限制了改编后模型的实用性。为此，我们提出了一种新的适应方法——Branch-and-Merge（BaM），它基于迭代地合并多个针对部分训练数据进行微调的模型。BaM的核心理念在于，这种方法产生的是幅度较小但质量更高的权重调整，从而减少对源领域的遗忘，同时保持对目标领域的学习。  我们在保加利亚语和德语的广泛实证研究中展示了BaM的优势：它能显著降低遗忘，同时在不同模型架构上与标准持续预训练和指令微调相比，能够匹配甚至提升目标领域的性能。|
|**2024-07-11**|**Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight**|Zhiqiang Xie et.al.|[2407.08694](http://arxiv.org/abs/2407.08694)|null|在现代云系统中，运行时故障和性能下降是常态。对于云服务提供商而言，自动确定问题的根本原因是保证高可靠性和可用性的关键，因为快速的故障定位有助于加快诊断和优先级排序，以实现及时解决。近期的研究中，因果推理利用因果图来捕捉不同云系统性能指标之间的关系是一个有前景的解决方案。然而，系统开发者需要精确定义系统的因果图，这是一项耗时、脆弱且挑战性的工作，尤其对于庞大和动态的系统，且需要深厚的专业知识。数据驱动的方法在云系统中的效果有限，因为故障事件的发生频率相对较低。  本工作中，我们提出了一种新颖的解决方案——Atlas，它能够自动合成云系统的因果图。Atlas利用大规模语言模型（LLMs）结合系统文档、日志和部署反馈生成因果图。Atlas与数据驱动的因果发现技术相辅相成，并通过数据驱动的验证步骤进行增强。我们在一系列故障定位场景中评估了Atlas，结果表明，Atlas能够在可扩展和普适的方式下生成因果图，其性能远超数据驱动算法，并与基准线相当。|
|**2024-07-11**|**SEED-Story: Multimodal Long Story Generation with Large Language Model**|Shuai Yang et.al.|[2407.08683](http://arxiv.org/abs/2407.08683)|**[link](https://github.com/tencentarc/seed-story)**|**随着图像生成和开放形式文本生成的显著进步，交错的图像-文本内容创作领域变得越来越有吸引力。多模态故事生成，即生成叙事文本与生动图像的交错序列，作为一种有价值的实用任务，因其广泛的应用前景而受到关注。然而，这一任务面临着理解文本和图像复杂交互、生成连贯且相关文本和视觉内容的挑战。本工作中，我们提出SEED-Story，这是一种新颖的方法，它利用强大的多模态大型语言模型（MLLM）来生成扩展的多模态故事。我们的模型基于MLLM的强大理解能力，既能预测文本令牌，也能预测视觉令牌，然后通过适应的视觉解令牌化器处理，生成具有一致角色和风格的图像。我们还引入了多模态注意力沉降机制，使得在高度自动递归的方式下，能够生成长达25个序列（仅用10个进行训练）的故事。此外，我们还提供了大规模高分辨率的StoryStream数据集，用于训练我们的模型，并量化评估多模态故事生成任务在多个方面的性能。**|
|**2024-07-11**|**Uncertainty Estimation of Large Language Models in Medical Question Answering**|Jiaxin Wu et.al.|[2407.08662](http://arxiv.org/abs/2407.08662)|null|## 任务  大型语言模型（LLMs）在医疗领域的自然语言生成方面展现出潜力，但存在产生错误事实的风险。为了在医疗问题解答中部署这些模型，需要可靠的不确定性估计（UE）方法来识别幻觉。本研究中，我们在医学问答数据集上对流行UE方法及其不同模型规模进行了评估。结果显示，当前方法在该领域通常表现不佳，凸显了医疗应用中的UE挑战。我们还观察到，更大的模型往往能获得更好的结果，这表明模型规模与UE可靠性可能存在关联。  为应对这些挑战，我们提出了一种名为“两阶段验证”的概率自由不确定性估计方法。首先，LLM生成逐步解释和初始答案，接着制定核查问题以检查解释中的事实陈述。模型会两次回答这些问题：一次独立，一次参考解释。两种答案之间的不一致度衡量原始响应的不确定性。我们在三个生物医学问答数据集上使用Llama 2 Chat模型评估我们的方法，并将其与基准基线方法进行比较。  实验结果显示，我们的两阶段验证方法在各个数据集和模型规模上实现了最佳的整体准确性和稳定性，并且其性能随模型大小的增加而提升。|
|**2024-07-10**|**Training on the Test Task Confounds Evaluation and Emergence**|Ricardo Dominguez-Olmedo et.al.|[2407.07890](http://arxiv.org/abs/2407.07890)|**[link](https://github.com/socialfoundations/training-on-the-test-task)**|**我们研究了一个大型语言模型评估中的核心问题，称为在测试任务上训练。这并非如数据泄露或污染等不当做法，而是一种逐渐增长的包括任务相关数据在预训练阶段的技术。我们发现，在测试任务上训练会混淆模型的相对评估和关于涌现能力的声明。我们提出，不同模型家族之间的看似优势可能由他们在测试任务上的训练程度差异所解释。为此，我们提出了一种有效方法，即在比较前对每个模型进行相同的任务相关数据微调，以校正这种训练。结果显示，一旦调整了在测试任务上的训练，涌现行为的实例大多消失。同样适用于那些无法用评价指标解释的涌现行为报告案例。我们的工作推动了对大型语言模型的新评价视角，对基准测试和涌现能力研究具有广泛影响。**|
|**2024-07-10**|**Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization**|Junkang Wu et.al.|[2407.07880](http://arxiv.org/abs/2407.07880)|**[link](https://github.com/junkangwu/dr_dpo)**|**本研究关注在训练数据中噪声对Direct Preference Optimization (DPO)方法的挑战，该方法用于调整大型语言模型（LLMs）以符合人类偏好。我们区分了两类噪声：点噪声，涉及低质量的数据点；和成对噪声，影响偏好的正确排序。通过分布式鲁棒优化（DRO），我们增强了DPO抵抗这些噪声的能力。理论分析揭示，DPO本质上蕴含了DRO原理，对点噪声具有天然的鲁棒性，其中正则化系数 $\beta$在抗噪声方面起关键作用。在此基础上，我们提出分布式鲁棒增强的DPO（Dr. DPO），它通过优化最坏情况的成对场景来集成成对鲁棒性。Dr. DPO中的新超参数$\beta'$ 允许对数据对可靠性进行精细控制，平衡了在嘈杂训练环境中的探索与利用。实证评估显示，Dr. DPO显著提高了生成文本的质量和响应准确性，无论在有噪声还是无噪声的设置下都表现出色。代码已在https://github.com/junkangwu/Dr_DPO上提供。**|
|**2024-07-10**|**FACTS About Building Retrieval Augmented Generation-based Chatbots**|Rama Akkiraju et.al.|[2407.07858](http://arxiv.org/abs/2407.07858)|null|随着生成式人工智能驱动的企业聊天机器人日益成为提升员工生产力的关键工具，基于检索增强生成（RAG）的、大型语言模型（LLMs）以及如Langchain和Llamaindex之类的orchestration框架在构建这些聊天机器人中扮演了重要角色。然而，创建有效的企业聊天机器人是一项挑战，需要精心设计的RAG管道工程。这包括微调嵌入和LLMs、从向量数据库提取文档、重述查询、重新排名结果、设计提示、遵守文档访问控制、提供简洁的回答、包含引用、保护个人信息以及构建orchestration代理。我们基于三个NVIDIA聊天机器人（分别用于IT/HR福利、财务收益和通用内容）的经验，提出了一种构建RAG聊天机器人的框架——FACTS（Freshness、Architectures、Cost、Testing、Security）。我们的贡献有三方面：首先介绍FACTS框架，其次列出十五个RAG管道控制点，最后提供了关于大模型和小模型在准确性和延迟之间权衡的实证结果。据我们所知，这是首篇全面探讨构建安全企业级聊天机器人的方法和解决方案的论文。|
|**2024-07-10**|**OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training**|Sami Jaghouar et.al.|[2407.07852](http://arxiv.org/abs/2407.07852)|**[link](https://github.com/PrimeIntellect-ai/OpenDiLoCo)**|**OpenDiLoCo是一个开源的分布式低通信（DiLoCo）训练方法的实现和复制，针对大型语言模型。我们提供了可复现的DiLoCo实验，通过Hivemind库构建了一个可扩展的去中心化训练框架。我们在两个大洲和三个国家之间训练模型，同时保持90-95%的计算资源利用率。此外，我们进行了关于算法计算效率、工作器数量可扩展性的研究，并表明其梯度可以使用FP16进行全归一化而不会影响性能。最后，我们将OpenDiLoCo扩展到原始工作的三倍规模，证明了它在百亿参数模型上的有效性。**|
|**2024-07-10**|**Natural Language Mechanisms via Self-Resolution with Foundation Models**|Nicolas Della Penna et.al.|[2407.07845](http://arxiv.org/abs/2407.07845)|null|在实际操作中，代理人通常受限于诸如交易或订单之类的有限报告格式，这可能限制了他们表达信息的能力。我们提出了一种新型机制，它促使代理人以自然语言提交报告，并利用大型语言模型（LLM）的强大功能来选择结果和分配报酬。我们确定了这些机制在LLM作为良好的世界模型以及强烈的跨代理信息过度确定条件下的激励兼容性和效率的必要条件。实验表明，当传统预测市场在信号结构上存在问题时，这些基于LLM的机制能够成功地整合信息。|
|**2024-07-10**|**Transformer Alignment in Large Language Models**|Murdock Aubry et.al.|[2407.07810](http://arxiv.org/abs/2407.07810)|null|大型语言模型（LLMs）在自然语言处理方面取得了显著进步，深入理解其内部机制至关重要。我们视LLMs为高维空间中的离散、耦合的非线性动力系统，通过研究tokens在Transformer块中的轨迹，并沿着这些轨迹线性化系统，利用雅可比矩阵进行分析。在对38个公开可用的LLMs进行研究后，我们观察到残差雅可比矩阵的上左和右奇异向量之间的对齐，以及线性性和层内指数增长的出现。值得注意的是，我们发现对齐度的提高与模型性能呈正相关。训练后的评估显示，相比于随机初始化权重时的指标，有显著改善，这强调了训练在Transformer架构中的重要影响。这些发现揭示了一种以前未被充分认识的规律性，强化了动力学解释，并为进一步理解和优化LLM架构铺平了道路。|
|**2024-07-10**|**Attribute or Abstain: Large Language Models as Long Document Assistants**|Jan Buchmann et.al.|[2407.07799](http://arxiv.org/abs/2407.07799)|**[link](https://github.com/ukplab/arxiv2024-attribute-or-abstain)**|**## 背景 大语言模型（LLMs）能够辅助处理长篇文档，但它们也存在胡言乱语的问题。增加可信度的方法是通过提供证据支持响应，提高可验证性。当前的归因方法仅在基于检索的生成（RAG）环境中评估过，这与无需检索的长文档场景不同，可能仍有应用价值。因此，缺乏针对长文档的归因专门评估。为此，我们提出LAB，一个包含6个多样化的长文档任务的基准，并在四种不同大小的LLM（即提示和微调）上试验了不同的归因方法。研究结果显示，一步生成引用（citation，即同时进行响应生成和证据提取）的表现最佳。我们还探究了“迷失在中间”现象是否适用于归因，但未发现这种情况。此外，我们发现证据质量在简单响应的场景下可以预测响应质量，但对于复杂响应则不然，因为模型在为复杂主张提供证据时面临挑战。我们公开了代码和数据，以供进一步研究。**|
|**2024-07-11**|**Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard**|Oguzhan Topsakal et.al.|[2407.07796](http://arxiv.org/abs/2407.07796)|**[link](https://github.com/research-outcome/llm-game-benchmark)**|**我们提出了一种新颖且可扩展的大型语言模型（LLM）基准测试，通过网格型游戏如井字棋、连接四和围棋进行。开源的游戏模拟代码在GitHub上提供，允许LLMs竞技，并生成JSON、CSV、TXT和PNG格式的详细数据文件，用于排行榜排名和进一步分析。我们展示了包括Anthropic的Claude 3.5 Sonnet和Claude 3 Sonnet，Google的Gemini 1.5 Pro和Gemini 1.5 Flash，OpenAI的GPT-4 Turbo和GPT-4o，以及Meta的Llama3-70B在内的领先LLM之间的比赛结果。我们鼓励其他LLM提交结果。总共进行了2,310场模拟比赛（每对模型进行5轮，共7个模型间的对局，以及与随机玩家的比赛），涵盖三种类型的游戏，使用了列表、插图和图像三种提示方式。结果显示，LLM在不同游戏和提示类型下的性能存在显著差异，分析内容包括胜率、错失机会和无效动作。排行榜和结果矩阵的详细数据作为开放访问数据在GitHub上提供。这项研究加深了我们对LLM在未专门训练的游戏中的能力的理解，有助于评估它们的规则理解能力和战略思维。在通向人工智能通用性的道路上，这项研究为未来探索它们在复杂决策场景中的实用性奠定了基础，揭示了它们的战略思考能力，并为深入探究LLM在基于游戏框架内的局限性提供了方向。**|
|**2024-07-10**|**Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities**|Tianjie Ju et.al.|[2407.07791](http://arxiv.org/abs/2407.07791)|**[link](https://github.com/Jometeorie/KnowledgeSpread)**|**随着大型语言模型（LLMs）在多代理系统中的迅速应用，它们在协作问题解决和自主谈判等领域的出色性能引起了关注。然而，这些基于LLM的多代理系统的安全问题尚未得到充分研究，尤其是在知识操纵传播方面。本文通过构建详细的威胁模型和模拟环境，模拟现实世界中的多代理部署在可信平台上，探讨这一关键问题。我们提出了一种新颖的两阶段攻击方法，包括说服性注入和操纵知识注入，来系统地探究在无明确提示操纵的情况下，如何潜在地传播操纵知识（如虚构和有害知识）。我们的方法利用了LLMs处理世界知识固有的漏洞，攻击者可以借此无意识地传播编造的信息。实验结果表明，我们的攻击方法能够成功诱导基于LLM的代理在交流中传播这两种操纵的知识，同时不会显著降低它们的基础功能。此外，我们发现这些操纵会持续存在于流行的检索增强生成框架中，即使交互结束，若干良性代理也可能继续受到操纵聊天记录的影响。我们的发现揭示了LLM多代理系统中的重大安全风险，强调了对操纵知识传播进行强大防御的迫切需求，比如引入“守护”代理和先进的事实核查工具。**|
|**2024-07-10**|**WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment**|Jiefu Ou et.al.|[2407.07778](http://arxiv.org/abs/2407.07778)|null|本文探讨了在物理环境中部署人工智能（AI）代理时所需的基本操作（API）数量和设计问题。研究者设想，如果wikiHow教程涵盖了广泛的用户自编任务，那么这些任务所需的API范围是什么。他们提出了一种方法，通过将wikiHow指令与置身于环境中的代理策略关联，迭代地生成新的API。借助大型语言模型（LLMs）在体感规划方面的最新成就，研究者提议使用少量样例提示GPT-4生成Python代码作为代理策略，并通过以下步骤扩展API库：1）重用初始API集；2）在必要时创建新的API调用。实验关注的是定义API，而非其实现性。在一小部分wikiHow教程上应用该方法后，发现需要300多个API来捕捉现实世界中的多样任务。自动和人工分析显示，提出的管道能有效复用和创造API。进一步的人工审查发现，现有的模拟器仅支持诱导出的API的一小部分（前50个常用API中的9个），这促使开发更丰富的体感环境。|
|**2024-07-09**|**AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning**|Jiaxi Cui et.al.|[2407.07094](http://arxiv.org/abs/2407.07094)|**[link](https://github.com/pandavt/datatager)**|**在各行各业广泛采用大型语言模型（LLMs）的过程中，往往忽视了个体和小型组织对针对其特定业务场景定制化模型的需求。为此，我们提出了一种新颖的微调方法——\textbf{AnyTaskTune}，即任务微调（Task-Fine-Tune），旨在提升模型在多样化的领域特定任务上的性能。该方法包括细致地识别和定义领域内的子任务，随后创建专门的增强数据集进行精细调整，从而优化任务特定的模型表现。我们在法律（如关键词提取和句子预测）等多个领域，包括金融、医疗、法律、心理学、客户服务和人力资源等二十多个子任务上进行了广泛的微调实验。为了支持社区参与并分享资源，我们将开源这些双语任务数据集。实验结果显示，使用\textbf{Task-Fine-Tune}方法微调的模型不仅在特定任务上表现出色，而且在各自领域内明显优于通用能力更强的模型。我们的工作已公开发布在：\url{https://github.com/PandaVT/DataTager}。**|
|**2024-07-09**|**FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation**|Liqun Ma et.al.|[2407.07093](http://arxiv.org/abs/2407.07093)|**[link](https://github.com/liqunma/fbi-llm)**|**该研究介绍了一种全新的全二进制大型语言模型（FBI-LLM），这是首次展示如何从头开始训练大规模的全二进制语言模型（不同于部分二进制或三进制的LSTM，如BitNet b1.58），其性能能够与浮点16位（FP16）或混合精度16位（BF16）的常规大语言模型相当。通过使用自回归蒸馏（AD）损失，同时保持模型尺寸（130M、13B、7B）和预训练数据量与常规LLM相当，FBI-LLM在困惑度和任务特定效果方面表现出竞争性。有趣的是，我们发现从零开始训练全二进制语言模型并不需要预训练权重。这项工作催生了一个新的计算框架，并可能推动针对完全1比特LLMs的专业硬件设计。我们公开所有模型、代码和训练数据，以支持进一步的研究（代码：https://github.com/LiqunMa/FBI-LLM，模型：https://huggingface.co/LiqunMa/）。**|
|**2024-07-09**|**Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models**|Logan Cross et.al.|[2407.07086](http://arxiv.org/abs/2407.07086)|**[link](https://github.com/locross93/hypothetical-minds)**|**在多智能体强化学习（MARL）方法中，处理多智能体系统的非stationarity并适应在线学习的能力是一个挑战。为此，我们利用大型语言模型构建了一个自主的解决策略。我们的新型智能体“假设心智”（Hypothetical Minds）采用认知启发式架构，包括感知、记忆和两个抽象层次上的分层规划模块。关键新增的是“心理理论”模块，它以自然语言的形式生成对其他智能体策略的假设，并通过验证这些假设对其他智能体行为的预测准确性来逐步优化。在Melting Pot基准的多种竞争、混合动机和协作环境中，假设心智显著优于先前的语言模型智能体和强化学习基线，无论是在二元环境还是群体环境中。对比分析显示，假设的评估和迭代精炼对于应对复杂场景至关重要。**|
|**2024-07-09**|**Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities**|Shaltiel Shmidman et.al.|[2407.07080](http://arxiv.org/abs/2407.07080)|null|该论文探讨了在希伯来等低资源语言中训练大型语言模型（LLMs）的挑战。我们介绍了DictaLM2.0和DictaLM2.0-Instruct，这两个模型基于Mistral模型，使用大约2000亿个希伯来语和英语词汇进行训练。适应预训练模型到新语言需要专门的技术，这与从头训练或在资源丰富的语言（如英语）上进一步训练现有模型有显著差异。论文详细阐述了这些创新的训练方法，以促进希伯来语的高效学习和适应其语言特性。此外，我们还对DictaLM2.0-Instruct进行了全面的指令微调，以提升其在任务导向指令上的性能。为了严格评估我们的模型，我们开发了一个新的希伯来LLM评估基准，涵盖了问答、情感分析、Winograd Schema Challenge、翻译和摘要等多个任务。本文不仅解决了在低资源语言中训练LLMs的复杂性，还提出了一种可用于其他LLM跨非英语语言适应的框架，从而对多语言自然语言处理领域做出了贡献。|
|**2024-07-09**|**Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps**|Yung-Sung Chuang et.al.|[2407.07071](http://arxiv.org/abs/2407.07071)|**[link](https://github.com/voidism/lookback-lens)**|**该论文探讨了大型语言模型（LLMs）在总结文章或根据给定段落回答问题时可能出现的语境性虚构问题。LLMs可能会杜撰细节，提供与输入上下文不符的不准确答案。研究者提出，这种虚构与模型倾向于关注上下文信息还是自动生成内容的程度有关。为此，他们设计了一个简单的检测模型——“Lookback Lens”，其输入特征是基于每个注意力头上下文注意力权重与新生成词的比例。实验表明，仅使用这些回顾比率特征的线性分类器与利用LLM整个隐藏状态或文本蕴含模型的更复杂检测器同样有效。Lookback Lens不仅适用于不同任务，还能跨模型迁移，一个在70亿参数模型上训练的检测器无需重新训练即可应用于更大的130亿参数模型。此外，研究还发现，通过简单的分类器指导解码方法，能够减少诸如XSum摘要任务中的虚构程度，例如降低9.6%的虚构发生率。**|
|**2024-07-09**|**Prompting Techniques for Secure Code Generation: A Systematic Investigation**|Catherine Tony et.al.|[2407.07064](http://arxiv.org/abs/2407.07064)|null|## 概要  随着大型语言模型（LLMs）在软件开发中的兴起，通过提示驱动编程，开发者能够通过自然语言（NL）指令生成代码。然而，关于它们能否产生安全代码的研究引发了质疑，这关系到提示生成软件的质量。尽管已经出现了多种精心设计的提示策略以优化LLM的响应，但这些方法与安全代码生成之间的相互作用仍需进一步研究。目标：本研究旨在探究不同提示技术对LLMs根据NL指令生成代码的安全性影响。方法：首先，我们进行系统文献回顾，以识别适用于代码生成任务的现有提示技术。然后，我们在GPT-3、GPT-3.5和GPT-4模型上评估这些技术中的部分，使用一个包含150个与安全相关的代码生成NL提示的数据集。结果：我们的工作（1）对代码生成的潜在提示技术进行了分类，（2）适应并评估了这些技术在安全代码生成任务中的表现，（3）观察到在测试的LLMs中，尤其是在使用了名为“递归批评与改进”（RCI）的现有技术后，安全漏洞有所减少，为LLM生成代码安全性的讨论提供了有价值的见解。|
|**2024-07-09**|**Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence**|Weize Chen et.al.|[2407.07061](http://arxiv.org/abs/2407.07061)|**[link](https://github.com/openbmb/ioa)**|**随着大型语言模型的迅速发展，出现了能效卓越的自主代理。然而，现有的多代理框架在整合来自不同生态系统的高能力第三方代理时面临挑战，通常局限于自身封闭环境。它们在模拟分布式环境时也受限于单设备设置，并且往往依赖硬编码的通信管道，难以适应任务需求的变化。受互联网理念启发，我们提出了一种名为“代理互联网”（Internet of Agents，IoA）的新框架。IoA旨在解决这些问题，提供一个灵活且可扩展的平台，促进基于语言模型的多代理协作。它引入了代理集成协议、即时消息架构以及动态的团队协作和对话流程控制机制。通过在通用助手任务、体感AI任务和检索增强生成基准上的广泛实验，我们证明IoA在性能上持续优于现有最先进的基线，展示了其在异构代理之间有效合作的能力。IoA代表了朝着将多样化的代理链接在一个类似互联网的环境中迈进，让它们能够无缝协作以提升整体智能和功能。我们的代码库已发布在：\url{https://github.com/OpenBMB/IoA}。**|
|**2024-07-09**|**Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model**|Wenqi Zhang et.al.|[2407.07053](http://arxiv.org/abs/2407.07053)|**[link](https://github.com/zwq2018/multi-modal-self-instruct)**|**尽管当前的大型多模态模型（LMMs）已经能够理解自然场景的照片和肖像，但它们对抽象图像（如图表、地图或布局）的理解以及视觉推理能力仍然相当初级。它们在处理日常任务时常常遇到困难，例如阅读时钟时间、理解流程图或根据路线图规划路径。鉴于此，我们设计了一个多模态自我指导系统，利用大型语言模型及其代码能力来生成大量的抽象图像和日常场景下的视觉推理指令。我们的方法轻松创建了一个多模态基准，包含11,193个指令，涵盖八个视觉场景：图表、表格、模拟地图、仪表板、流程图、关系图、楼层平面图和视觉谜题。  这个由简单线条和几何元素构成的基准揭示了最先进的LMM（如Claude-3.5-Sonnet和GPT-4o）在抽象图像理解、空间关系推理和视觉元素识别方面的局限性。此外，为了验证合成数据的质量，我们使用62,476条合成的图表、表格和路线图指令对LMM进行微调。结果显示，图表理解和地图导航性能得到了提升，同时也表明这对其他视觉推理任务可能具有潜在益处。我们的代码已在以下链接提供：\url{https://github.com/zwq2018/Multi-modal-Self-instruct}。**|
|**2024-07-09**|**Using Large Language Models for Generating Smart Contracts for Health Insurance from Textual Policies**|Inwon Kang et.al.|[2407.07019](http://arxiv.org/abs/2407.07019)|null|我们研究利用大型语言模型（LLMs）自动生成基于文本的健康保险政策的自动化代码，目标是区块链智能合约。智能合约因其不可变性、可验证性、扩展性和无需预设信任的特性而被选中。我们的方法按技术复杂度递增生成输出：（1）文本摘要，（2）声明式决策逻辑，以及（3）带有单元测试的智能合约代码。我们确认LLMs在任务（1）上表现出色，而结构化的输出有助于验证任务（2）和（3）。声明式语言常用于规范医疗政策，但在区块链上的执行较为复杂，因此任务（3）旨在直接通过智能合约自动实现这一过程。我们提出完整性、正确性、清晰度、语法和功能性代码作为评估指标。我们使用了来自Medicare官方手册的三个具有不同难度的保险政策场景进行评估，涉及GPT-3.5 Turbo、GPT-3.5 Turbo 16K、GPT-4、GPT-4 Turbo和CodeLLaMA等模型。结果显示，LLMs在生成文本摘要方面表现良好。尽管任务（2）到（3）的输出可以作为起点，但它们仍需人工审核：在某些情况下，即使“可运行”的代码也可能产生不正确的结果；目标语言的流行程度会影响输出质量；更复杂的场景仍是当前的一大挑战。然而，我们的实验展示了LLMs在将文本流程描述转化为智能合约方面的潜力。|
|**2024-07-09**|**End-To-End Causal Effect Estimation from Unstructured Natural Language Data**|Nikita Dhawan et.al.|[2407.07018](http://arxiv.org/abs/2407.07018)|null|了解干预措施的效果对人类决策至关重要。然而，当前因果效应估计方法依赖于手动收集和结构化数据，这导致研究成本增加、完成时间延长。我们展示了如何利用大型语言模型（LLMs）开采大规模、多样化的观察性文本数据，以在适当的因果假设下生成低成本的因果效应估计。我们提出NATURAL，一个基于LLMs的新型因果效应估计算法家族，适用于处理未结构化的文本数据。我们的方法利用LLMs的条件分布（针对感兴趣的变量，根据文本数据）辅助计算经典的因果效应估计。我们克服了一系列技术挑战，如自动化数据整理和使用LLMs填补缺失信息。  我们准备了六个（两个合成的和四个实际的）观察性数据集，并配以随机对照试验形式的真实标签，系统地评估了我们管道中的每一步。NATURAL估计算法表现出色，其结果与真实值的差距不超过3个百分点，包括在实际的三期和四期临床试验中。这些结果表明，未结构化的文本数据是因果效应信息的丰富来源，NATURAL是利用这一资源的自动化流程的第一步。|
|**2024-07-08**|**Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision**|Orr Zohar et.al.|[2407.06189](http://arxiv.org/abs/2407.06189)|**[link](https://github.com/orrzohar/Video-STaR)**|**大型视觉语言模型（LVLM）的性能与其训练数据的规模和质量密切相关。当前的视频指令调优数据集缺乏多样性，因为它们主要由提示大型语言模型生成视频字幕以形成问题-答案对，内容多为描述性。然而，许多带有丰富标签和监督的视频数据集已经存在，但如何将它们融入LVLM并非易事。  为此，我们提出了视频自我训练与增强推理（Video Self-Training with augmented Reasoning，简称Video-STaR），这是首个视频自我训练方法。Video-STaR使得任何标注的视频数据集都能用于视频指令调优。在这个过程中，LVLM在生成指令和微调之间循环。我们发现，这不仅能提升视频整体理解能力（I），还能让LVLM适应新的下游任务，利用现有监督进行学习。  具体来说，LVLM被提示提出一个答案，然后仅保留那些包含原始视频标签的答案。LVLM随后在生成的数据集上进行再训练。通过只在包含正确视频标签的生成答案上训练，Video-STaR利用现有的视频标签作为弱监督来指导视频指令调优。  实验结果显示，经过Video-STaR增强的LVLM在（I）一般视频问答任务中的表现提升了10%，在（II）下游任务中，Video-STaR提高了Kinetics700-QA的准确性20%，以及FineDiving动作质量评估的性能15%。总的来说，Video-STaR为LVLM的性能提升提供了一种有效且实用的方法。**|
|**2024-07-08**|**CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation**|Xinying Guo et.al.|[2407.06188](http://arxiv.org/abs/2407.06188)|null|在娱乐行业（如动画和游戏）以及战略领域（如城市模拟和规划）中，人群运动生成至关重要。然而，这一任务需要精细地融合控制与生成，以在特定的空间和语义约束下实现逼真的群体动态合成，其挑战尚未得到充分探索。当前的人体动作生成模型往往关注个体行为，忽视了集体行为的复杂性；而多个人体动作生成的最新方法严重依赖预设场景，且限于固定、少量的人际互动，限制了其实用性。  为解决这些问题，我们提出CrowdMoGen，一个零样本文本驱动的框架，它利用大型语言模型（LLM）的力量，将集体智慧融入运动生成框架，从而能够在没有配对训练数据的情况下实现通用的规划和群体运动生成。我们的框架主要由两个关键组件构成：1）人群场景规划器，学习根据特定场景上下文或引入的扰动协调运动和动态；2）集体运动生成器，根据整体计划高效合成所需的集体运动。大量的定量和定性实验验证了我们框架的有效性，它不仅填补了大规模和通用人群运动生成任务的重要空白，而且在真实感和灵活性方面表现出高水准。|
|**2024-07-08**|**On Speeding Up Language Model Evaluation**|Jin Peng Zhou et.al.|[2407.06172](http://arxiv.org/abs/2407.06172)|null|大型语言模型（LLMs）在自然语言处理（NLP）领域占据主导地位，它们在各种任务上表现出最先进的能力。从训练到推理，构建这样的模型涉及众多决策，形成一个复杂的搜索问题。例如，为了为特定任务找到最佳的预训练LLM、提示或超参数，通常需要对整个测试集中的多个候选方案进行全面评估。这种详尽的评估耗时且昂贵，因为LLMs的推理和度量计算需求高。  本文针对在有限预算内有效评估方法在测试样本上的性能这一挑战。我们利用了广泛研究的多臂老虎机框架，该框架通过顺序选择下一个要评估的方法-示例对，将我们的方法——结合多臂老虎机算法与低秩分解——显著减少了所需的资源。实验表明，我们的算法仅使用通常需求的5%-15%资源，就能识别出表现最好的方法，从而实现了高达85%-95%的成本节省。|
|**2024-07-08**|**What's Wrong with Your Code Generated by Large Language Models? An Extensive Study**|Shihan Dou et.al.|[2407.06153](http://arxiv.org/abs/2407.06153)|null|随着大型语言模型（LLMs）在代码生成领域的快速发展，研究人员对此的关注度日益提高。目前的研究主要集中在构建高质量数据集和采用多样化的训练技术来提升LLM的代码生成能力。然而，对于这些现有方法的局限性和边界，缺乏全面的研究探讨。为此，我们进行了一项详尽的实证研究，评估了三个领先闭源LLM和四个开源LLM在三个常用基准上的性能。研究考察了生成代码的长度、循环复杂度和API数量，结果显示这些模型在处理更复杂的编程问题时面临挑战，生成的代码往往较短但结构更复杂，与标准解决方案相比。  我们还创建了一个错误代码的分类体系，分为三个类别和12个子类别，分析常见错误类型的根源。为了检验LLMs在实际项目中的表现，我们亲手构建了一个包含140个代码生成任务的现实世界基准。对比分析显示，实际场景中的bug分布与现有基准存在显著差异。最后，我们提出了一种无需额外训练的迭代方法，引入自我批判机制，使LLMs能够根据bug类型和编译器反馈修正其生成的代码。实验结果表明，经过两次迭代后，我们的方法能显著减少错误，使通过率提高29.2%，这表明LLMs在处理复杂问题方面具有巨大潜力。|
|**2024-07-09**|**Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks**|Lukas Netz et.al.|[2407.06146](http://arxiv.org/abs/2407.06146)|null|我们介绍并评估了一种名为“语法遮盖”的方法，该方法用于引导大型语言模型（LLMs）在给定上下文无关文法的约束下生成语法正确的模型。尽管少量示例学习或提示引导等prompt工程方法可以提高LLMs生成正确语法的概率，但处理复杂文法时，这些方法往往耗时且效果不理想。当前的研究主要集中在语言模型训练或prompt工程上。本文提出了一种新方法，通过约束解码限制输出，确保生成的内容符合有效语法。我们利用MontiCore构建的多种领域特定语言（DSL）和多款LLMs进行实验，比较了使用和未使用约束解码的效果。同时，我们采用相应的解析器验证每种模型的句法准确性。实验结果显示，语法遮盖显著提升了多个LLMs的建模能力，减少了对精心设计提示的需求，提高了生成正确模型的可能性。|
|**2024-07-08**|**ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation**|Ethan Chern et.al.|[2407.06135](http://arxiv.org/abs/2407.06135)|**[link](https://github.com/gair-nlp/anole)**|**## 背景 先前的开源大型多模态模型（LMMs）存在一些局限性：（1）它们往往缺乏原生集成，需要适配器来衔接视觉表示与预训练的大型语言模型（LLMs）；（2）许多模型仅限于单模态生成；（3）尽管有些支持多模态生成，但它们依赖于单独的扩散模型处理视觉部分。为了克服这些问题，我们介绍了Anole，一个开源的、自回归的、原生的大型多模态模型，专为交错的图像-文本生成设计。我们基于Meta AI的Chameleon构建Anole，采用了一种既数据高效又参数高效的创新微调策略。Anole展示了高质量、连贯的多模态生成能力。我们已经公开了我们的模型、训练框架以及指令调优数据。**|
|**2024-07-08**|**Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization**|Hannah K. Bako et.al.|[2407.06129](http://arxiv.org/abs/2407.06129)|**[link](https://github.com/hdi-umd/semantic_profiling_llm_evaluation)**|**### 概述  自动根据人类对数据集的口头描述生成数据可视化图表，需要深度理解语言中的语义信息，包括对数据属性、可视化任务以及数据预处理步骤的隐含和明确提及。自然语言界面（NLIs）在数据可视化方面已经探讨了如何捕捉这些信息，但人类言语的不确定性带来了挑战。近期的大型语言模型（LLMs）为解决这些问题提供了可能，但它们提取相关语义信息的能力尚待探索。本研究评估了四款公开可用的LLMs（GPT-4、Gemini-Pro、Llama3和Mixtral），分析它们在面对不确定性时理解口头指令的能力，并识别数据上下文和视觉任务。研究结果显示，LLMs对口语中的不确定性很敏感，能够提取关键的数据背景信息。然而，它们在推断可视化任务方面表现欠佳。基于这些发现，我们提出了未来利用LLMs进行可视化生成的研究方向。**|
|**2024-07-08**|**Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities**|Avinash Anand et.al.|[2407.06125](http://arxiv.org/abs/2407.06125)|null|抑郁症被广泛认为是重大的公共卫生问题，严重影响个人的心理健康。未经诊断的抑郁症可能导致严重的健康问题，包括生理症状甚至自杀。通常，抑郁症的诊断依赖于临床医生和心理健康专业人员进行的结构化访谈和如Patient Health Questionnaire（PHQ）等问卷调查。然而，这在很大程度上依赖于医生的经验和判断，可能受到个人偏见的影响。由于抑郁症的成因仍在研究中，医生在识别和治疗初期阶段的抑郁症时面临挑战。  近期，人工智能神经计算在文本、图像和语音处理等领域取得了显著进展。我们的研究尝试利用这些最先进的模型，在E-DAIC（Extended Distress Analysis Interview Corpus Wizard of Oz）数据集和2019年Audio/Visual Emotion Challenge（AVEC）中进行实验，以期优化多模态结果。实验结果显示，我们提出的解决方案利用专有和开源大型语言模型（LLMs），在文本模态上的Root Mean Square Error（RMSE）得分达到3.98，优于AVEC 2019挑战的基线和当前最佳的回归分析架构。此外，我们的方法在分类任务中的准确性达到了71.43%。论文还介绍了一个新颖的音频-视觉多模态网络，其预测PHQ-8评分的RMSE为6.51。|
|**2024-07-08**|**Artificial Intuition: Efficient Classification of Scientific Abstracts**|Harsh Sakhrani et.al.|[2407.06093](http://arxiv.org/abs/2407.06093)|null|## 背景 为了获取战略洞见或进行科研项目管理，对简短的科学文本（如研究基金申请书或出版物摘要）进行粗粒度分类至关重要。这些文本向具备深厚专业知识的专家传达密集信息，但自动化的任务极其艰巨，因为篇幅有限且缺乏上下文。为此，我们开发了一种新方法来生成并准确分配特定领域的粗标签。研究表明，大型语言模型（LLM）能够提供任务所需的元数据，类似于增强人类直觉的补充知识，并提出了一个工作流程。作为初步实验，我们使用了美国国家航空航天局（NASA）的奖项摘要数据库。我们结合现有性能指标，开发了新的评估工具。|
|**2024-07-08**|**Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models**|Jinliang Lu et.al.|[2407.06089](http://arxiv.org/abs/2407.06089)|null|随着大型语言模型（LLMs）的显著成功，自然语言处理（NLP）研究进入了新时代。尽管这些模型各有所长，但训练在不同语料库上的LLMs表现出不同的优势和劣势，这给提高整体效率和灵活性带来了挑战。为了应对这些挑战，近期的研究探索了LLMs的协作策略。本文全面概述了这一新兴研究领域，强调了合作背后的动力。我们将协作策略主要分为三种方法：合并、集成和协作。合并是将多个LLMs的参数空间整合。集成则是结合多个模型的输出。协作利用不同LLMs的优势，使其在特定任务中发挥各自专长。我们将从不同角度详细介绍这些方法，并讨论其潜在应用。此外，我们还勾勒出未来的研究方向，期望本工作能激发更多关于LLMs协作的研究，推动高级NLP应用的发展。|
|**2024-07-05**|**Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs**|Rudolf Laine et.al.|[2407.04694](http://arxiv.org/abs/2407.04694)|**[link](https://github.com/lrudl/sad)**|## 背景  人工智能助手，如ChatGPT，在被训练时会回应用户：“我是一个大型语言模型”。这引发了一个问题：这些模型是否真的知道自己是LLMs，并能据此可靠地行动？它们是否了解自己当前的部署情况，例如面向公众？我们称之为模型的“情境意识”。为了量化大型语言模型（LLMs）的情境意识，我们设计了一套行为测试，基于问答和指令执行，这就是**情境意识数据集（Situational Awareness Dataset，简称SAD）**。该基准包括7个任务类别，超过13,000个问题，测试了多项能力，如识别自身生成的文本、预测自己的行为、分辨提示来自内部评估还是实际应用，以及遵循依赖自我认知的指令。  我们对16种LLMs在SAD上的性能进行了评估，包括基础（预训练）模型和聊天模型。尽管所有模型的表现都优于随机猜测，但最高分的模型（Claude 3 Opus）在某些任务上仍远未达到人类水平。此外，我们发现SAD的表现与通用知识指标（如MMLU）的相关性并不完全一致。聊天模型，经过针对性训练以作为AI助手，相对于基础模型在SAD上的表现更好，但在通用知识任务上则不然。SAD的目标是通过分解成可量化的能力，促进科学界对LLMs情境意识的理解。情境意识对于增强模型的自主规划和行动能力至关重要，这既有利于自动化，也带来了与AI安全和控制相关的全新风险。您可以在<https://situational-awareness-dataset.org>获取代码和最新结果。|
|**2024-07-05**|**ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models**|Yuzhe Gu et.al.|[2407.04693](http://arxiv.org/abs/2407.04693)|**[link](https://github.com/open-compass/anah)**|## 任务  大型语言模型（LLMs）在跨领域和广泛应用的长格式问答任务中会出现幻觉。当前的幻觉检测和缓解数据集在领域覆盖和规模上存在局限，由于劳动成本高昂且现有幻觉标注员的可靠性不足，难以实现规模化。为了推动对LLMs幻觉的可扩展监督，本文提出了一种迭代的自我训练框架。该框架通过期望最大化（EM）算法，每次迭代首先使用一个幻觉标注流程来标记扩大的数据集，然后用这个更准确的标注器对数据集进行训练。在下一轮迭代中，使用新的标注器更新幻觉标注流程。实验结果全面展示，最终得到的仅需7亿参数的幻觉标注器超越了GPT-4的表现，并在HaluEval和HalluQA上的零样本推理中取得了最新的幻觉检测效果。这种标注器不仅能够评估不同LLMs在大规模数据集上的幻觉程度，还能通过NLI指标提升（从25%提高到37%）来帮助减轻生成文本的幻觉问题。|
|**2024-07-05**|**Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge**|Yuanze Lin et.al.|[2407.04681](http://arxiv.org/abs/2407.04681)|null|近年来，大规模多模态语言模型（MLLM）在使用大型高质量的图像文本数据集进行训练后，在整体理解图像方面取得了显著进步。然而，文本形式固有的困难限制了它们处理需要精细或空间密集信息（如遮罩）的问题，这影响了它们对详细视觉元素的理解能力。受到检索增强生成（RAG）理念的启发，本文提出了一种新的视觉提示方法，旨在将来自专门视觉模型（如实例分割和OCR模型）的精细外部知识融入MLLM。这是一个有前景但尚未充分探索的方向，可以提升MLLM的表现。我们的方法区别于同时期的工作，它们将外部知识转化为额外的文本提示，迫使模型间接学习视觉内容与文本坐标之间的对应关系。相反，我们提议将精细知识信息直接嵌入到一个空间嵌入图中作为视觉提示。这种设计可以轻松地整合进各种MLLM，如LLaVA和Mipha，显著提高它们的视觉理解性能。通过严谨的实验，我们在九个基准测试中展示了我们的方法如何提升MLLM的整体性能，增强其对细粒度上下文感知的能力。|
|**2024-07-05**|**Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition**|Ye Bai et.al.|[2407.04675](http://arxiv.org/abs/2407.04675)|null|现代自动语音识别（ASR）模型需要准确转录来自不同领域、语言和口音的多样语音信号，同时考虑到特定上下文信息，以适应各种应用场景的需求。传统的端到端模型结合额外的语言模型表现出色，但在数据匹配场景中效果良好，但逐渐面临瓶颈。本文介绍了一种基于大型语言模型（LLM）的新型语音识别模型——Seed-ASR。它建立在音频条件化LLM（AcLLM）架构之上，通过将连续语音表示和上下文信息输入到LLM中，利用了LLM的强大功能。通过分阶段的大规模训练以及在LLM中激发上下文感知能力，Seed-ASR在包括多个领域、方言和语言的综合评估集上显著优于端到端模型。此外，Seed-ASR能够部署到各种场景中支持特定需求，无需额外的语言模型。与最近发布的大型ASR模型相比，Seed-ASR在中文和英文公开测试集上的词（或字符，针对中文）错误率降低了10%-40%，进一步证明了其强大的性能。|
|**2024-07-05**|**Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models with Adaptive Expert Placement**|Yongji Wu et.al.|[2407.04656](http://arxiv.org/abs/2407.04656)|null|随着大型语言模型（LLMs）的规模不断扩大，稀疏激活的混合专家（MoE）架构因其计算成本的亚线性扩展而被越来越多地采用。然而，频繁的训练失败仍然是一个重大挑战，因为单次失败可能导致所有GPU陷入闲置，直至问题解决，从而可能丢失大量训练进度，需要从检查点重新开始。现有的高效容错训练解决方案要么缺乏弹性，要么依赖于将恢复能力构建到管道并行性中，但这不适用于MoE模型，因为MoE架构采用了专家并行策略。  我们提出了Lazarus，一个针对MoE模型进行容错和弹性的训练系统。Lazarus通过动态分配专家副本来应对专家工作负载的固有不平衡，从而加速训练，并开发了一种理论上最优的专家放置算法，以最大限度地提高在失败后的恢复概率。通过自适应的专家放置和灵活的令牌分发器，Lazarus能够在故障后充分利用所有可用节点，避免GPU空闲。  我们的评估表明，与现有MoE训练系统相比，Lazarus在频繁的节点故障下性能提升高达5.7倍，且在真实spot实例跟踪上提升了3.4倍。|
|**2024-07-05**|**Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework**|Reza Averly et.al.|[2407.04629](http://arxiv.org/abs/2407.04629)|null|该论文关注的是临床命名实体识别（Clinical NER），这是一种从临床病历中提取重要实体的任务。近年来，大型语言模型（LLMs）在这一任务上表现出色。研究主要集中在专有的LLMs，但论文探讨了开放的、专门为命名实体识别训练的LLMs在临床NER中的性能。作者提出了一种新颖的框架，称为“实体分解与过滤”（Entity Decomposition with Filtering，EDF），目的是通过将实体识别任务分解为子实体类型的检索，并引入一个过滤机制来消除错误实体。实验结果表明，该框架在所有度量标准、模型、数据集和实体类型上都表现出有效性。分析显示，实体分解能够显著提高对先前未被捕捉到的实体的识别。此外，论文还提供了对框架的全面评估和深入的错误分析，以期为未来的研究提供方向。|
|**2024-07-05**|**On scalable oversight with weak LLMs judging strong LLMs**|Zachary Kenton et.al.|[2407.04622](http://arxiv.org/abs/2407.04622)|null|该论文探讨了可扩展的监督协议，目标是让人类能够有效监督超越人类级别的AI。研究主要聚焦在辩论、咨询和直接问答三种形式上，使用大型语言模型（LLMs）作为AI代理和法官角色，假设法官模型较弱。实验涵盖了广泛的任务异质性，扩展了先前仅关注信息不对称的单一提取式问答任务，增加了数学、编程、逻辑和多模态推理等领域的挑战。结果表明，在所有任务中，当咨询师随机被分配正确或错误答案时，辩论优于咨询。在存在信息不对称的提取式问答任务中，辩论优于直接问答，但在其他没有信息不对称的任务中，结果则不一。当AI被允许选择要论证的答案而非预先指定时，发现法官被错误答案说服的情况在辩论中减少。此外，更强的辩论者模型能提高法官的准确性，尽管提升程度略低于之前的研究。|
|**2024-07-05**|**Leveraging Large Language Models for Integrated Satellite-Aerial-Terrestrial Networks: Recent Advances and Future Directions**|Shumaila Javaid et.al.|[2407.04581](http://arxiv.org/abs/2407.04581)|null|本文探讨了大型语言模型（LLMs）如何融入集成卫星、航空和地面网络（ISATN）的变革潜力，利用先进的人工智能（AI）和机器学习（ML）技术优化这些网络的连通性。首先概述了ISATN的当前架构，强调了LLMs在提升数据流、信号处理和网络管理方面的作用，以推动5G/6G通信技术的发展，通过高级预测算法和实时决策来增强性能。接着，深入分析了ISATN组件，探讨了如何有效地利用LLMs解决传统数据传输和处理中的瓶颈问题。  文章着重于ISATN的网络管理挑战，包括资源分配策略、流量路由以及在不断变化条件下确保无缝连接和最优性能的网络安全。同时，我们讨论了将LLMs整合到ISATN中所面临的技术挑战，如数据集成、扩展性问题、决策过程中的延迟，以及构建健壮且容错的系统设计。最后，研究指出了未来研究的关键方向，即如何充分利用LLM的优势，以提升网络可靠性、优化性能，实现一个真正全球互联且智能的网络体系。|
|**2024-07-05**|**VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models**|Hang Gao et.al.|[2407.04573](http://arxiv.org/abs/2407.04573)|null|在大型语言模型（LLMs）快速发展的背景下，向量检索算法对于满足相似度和多样性要求的语义查询至关重要。尽管Maximal Marginal Relevance（MMR）在涉及这两个需求的检索场景中被广泛应用，但其参数λ的变化会导致结果波动，使得向量空间中的优化路径变得模糊。此外，当前缺乏对相似性和多样性在检索过程中约束的坚实理论分析。本文提出了一种新方法，通过查询向量与求和向量之间的关系来刻画这两种约束。这种关系确保了相似性，同时要求求和向量中的各个向量以分散的方式与查询向量对齐，以满足多样性需求。  我们还提出了一个新的组合优化问题：从一组候选向量中选择 $k$ 个，使得它们的求和向量最大程度地与查询向量匹配。我们证明了这个问题是NP完全的，揭示了在向量检索中同时追求相似性和多样性的深刻困难，并为后续研究奠定了理论基础。此外，我们设计了一个名为Vectors Retrieval with Similarity and Diversity（VRSD）的启发式算法，它不仅具有明确的优化目标，无需预设参数，而且在时间复杂性上相对于MMR有所降低。实证验证表明，VRSD在各种数据集上显著优于MMR。|
|**2024-07-05**|**PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit Posts**|Ana-Cristina Rogoz et.al.|[2407.04541](http://arxiv.org/abs/2407.04541)|**[link](https://github.com/ana-rogoz/poprero)**|**我们推出了PoPreRo，这是首个专为罗马尼亚Reddit帖子的流行度预测收集的dataset。PoPreRo汇集了五个不同罗马尼亚子论坛的多样化帖子样本，总计包含28,107条数据。随数据集一同发布的，我们还提供了一系列竞争性模型作为未来研究的基础。值得注意的是，测试集上得分最高的模型达到了61.35%的准确率和60.60%的宏F1分数，这表明在PoPreRo上的流行度预测任务极具挑战性。通过少量提示对Falcon-7B大型语言模型的进一步探究也指向了同样的结论。因此，我们相信PoPreRo是一个有价值的资源，可以用来评估罗马尼亚社交媒体帖子的流行度预测模型。我们的数据集已公开发布在https://github.com/ana-rogoz/PoPreRo。**|
|**2024-07-03**|**Universal Length Generalization with Turing Programs**|Kaiying Hou et.al.|[2407.03310](http://arxiv.org/abs/2407.03310)|null|**摘要：**  长度泛化指的是从简短的训练序列推断出长测试序列的能力，这对于当前的大语言模型是一个挑战。尽管先前的研究提出了一些架构或数据格式变化来实现长度泛化，但这些方法通常局限于特定任务。在此基础上，我们结合了擦除板和链式思考（Chain-of-Thought, CoT）技术，提出了Turing程序，这是一种新颖的CoT策略，它将算法性任务分解成类似图灵机计算的步骤。这个框架既通用又简单，只需要在上下文中稍作修改地复制文本。我们展示了使用Turing程序，我们在加法、乘法以及基于上下文的SGD等算法性任务上实现了稳健的长度泛化。接着，我们展示Transformer在随机Turing程序上也能实现长度泛化，这表明对于任何算法性任务，长度泛化都是可能的。最后，我们理论证明Transformer能够实现Turing程序，构造了一个简单的RASP（Weiss等人）程序，它模拟任意图灵机。|
|**2024-07-03**|**Large Language Models for JSON Schema Discovery**|Michael J. Mior et.al.|[2407.03286](http://arxiv.org/abs/2407.03286)|null|## 背景 半结构化数据格式如JSON因其在存储数据时的灵活性而被广泛应用。然而，JSON数据通常缺乏与关系数据库中的表单结构相对应的规范（schema）。因此，出现了许多从数据集中发现规范的工具。尽管这些工具很有用，但现有的方法主要关注文档的语法，而忽视了语义信息。本研究中，我们探讨如何自动为发现的规范添加有意义的语义信息，使其类似于人类作者编写的规范中所包含的信息。我们利用大型语言模型和人工编写的JSON Schema文档库，生成元素的自然语言描述、可重用定义的有意义名称，并识别出哪些发现的属性最有用，哪些可以视为“噪声”。我们的方法在先前已证明与人类判断高度相关的文本生成指标上表现出色。|
|**2024-07-03**|**LLM Internal States Reveal Hallucination Risk Faced With a Query**|Ziwei Ji et.al.|[2407.03282](http://arxiv.org/abs/2407.03282)|**[link](https://github.com/ziweiji/Internal_States_Reveal_Hallucination)**|## 背景  大型语言模型（LLMs）的幻觉问题严重制约了它们的可靠性和可信度。人类具有自我意识过程，能识别面对查询时的未知领域。为此，我们的论文研究了LLMs能否在生成响应之前自行评估其幻觉风险。我们从训练数据源和15个不同自然语言生成（NLG）任务的角度广泛分析LLMs的内部机制，这些任务涵盖了超过700个数据集。实证分析揭示了两个关键发现：(1) LLM的内部状态能够指示它们是否在训练数据中见过查询；(2) LLM的内部状态显示出它们对查询可能产生幻觉或不产生幻觉的风险。我们的研究关注特定的神经元、激活层和令牌，这些在LLM对不确定性和幻觉风险的认识中扮演着关键角色。通过一种探查估计算法，我们利用LLM的自我评估能力，在运行时实现了平均84.32%的幻觉估计准确率。|
|**2024-07-03**|**Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning**|Zhili Shen et.al.|[2407.03227](http://arxiv.org/abs/2407.03227)|null|我们从大型语言模型的角度探讨文本到SQL的语义解析。鉴于商业数据库模式的规模挑战和业务智能解决方案的部署问题，我们提出了一种方法，它动态获取输入数据库信息，并利用抽象语法树选择少量示例进行上下文学习。此外，我们研究了如何利用并行语义解析器生成SQL查询的近似版本，以支持我们的检索。我们甚至将这种方法推向极致，采用不到5亿参数的模型作为高效近似器，并赋予其并行处理模式的能力。我们在单语和跨语言的语义解析基准上应用了我们的方法，结果优于现有最佳基线。全面的实验揭示了这种检索增强生成设置中各个模块的贡献，为未来工作指明了有趣的方向。|
|**2024-07-03**|**How Does Quantization Affect Multilingual LLMs?**|Kelly Marchisio et.al.|[2407.03211](http://arxiv.org/abs/2407.03211)|null|## 背景 量化技术在提升大语言模型（LLM）的推理速度和部署效率方面被广泛应用。尽管有大量的研究关注了量化后的英语任务模型效果，但尚无研究针对多语言场景。我们对量化多语言LLM进行了深入分析，重点关注其跨语言性能及不同规模下的表现。我们采用自动基准测试、LLM作为评判者的方法以及人类评估，发现以下几点：(1) 量化对人类评价的影响是负面的，且自动指标严重低估了这种损害：自动任务中平均1.7%的性能下降对应人类评估中日本任务的16.0%显著下滑；(2) 不同语言受到量化的影响程度不均，非拉丁字母体系的语言受影响最严重；(3) 比如数学推理这类挑战性任务，其性能下降最为显著。随着低功耗模型服务于全球NLP技术的普及变得至关重要，我们的研究结果强调了在评估高效模型时，多语言性能应作为关键指标。|
|**2024-07-03**|**TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts**|Ruida Wang et.al.|[2407.03203](http://arxiv.org/abs/2407.03203)|**[link](https://github.com/RickySkywalker/TheoremLlama)**|**### 翻译  在数学证明的计算机可验证形式语言（如Lean）验证中，使用大型语言模型（LLMs）基于自然语言（NL）的证明方法具有重要影响。然而，由于NL与形式语言（FL）的证明数据稀缺，现代LLMs在生成完整证明方面的性能欠佳。为此，本文提出了一种名为**TheoremLlama**的端到端框架，旨在训练通用LLM成为Lean4专家。该框架包括NL-FL对齐数据集生成方法、LLM形式定理证明器的训练策略以及LLM在撰写Lean4证明中的技术。  关键创新在于我们开发了NL-FL自举方法，即将NL证明融入Lean4代码，利用LLMs的自然语言推理能力进行正式推理。通过这种数据集生成方式，我们提供了**Open Bootstrapped Theorems**（OBT），一个对齐且自举的NL-FL数据集。**TheoremLlama**框架在MiniF2F-Valid和Test数据集上的累计准确率分别达到36.48%和33.61%，超过了GPT-4的基线分数22.95%和25.41%。我们已公开了模型检查点和生成的数据集，并即将全部代码开源。**|
|**2024-07-03**|**Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models**|Haritz Puerto et.al.|[2407.03181](http://arxiv.org/abs/2407.03181)|**[link](https://github.com/ukplab/arxiv2024-divergent-cot)**|**该研究提出了一种新颖的方法，称为Divergent CoT（DCoT），通过要求模型在单次推理步骤中比较多个推理链来进一步提升性能。这种方法发现，即使在小型、更易于获取的大型语言模型上进行指令调优也能提高表现。通过广泛的实验，涉及不同类型的推理任务，研究发现对DCoT数据集的微调在各种规模的模型（从13亿到70亿参数）上普遍优于基本的CoT方法。实验和人工评估表明，这些性能提升源于模型在单次推理中生成了多个不同的推理路径，这表明语言模型能够实现自我纠正。相关代码和数据已在https://github.com/UKPLab/arxiv2024-divergent-cot上公开。**|
|**2024-07-03**|**Investigating Decoder-only Large Language Models for Speech-to-text Translation**|Chao-Wei Huang et.al.|[2407.03169](http://arxiv.org/abs/2407.03169)|null|## 背景  大型语言模型（LLMs）因其出色的推理能力、泛化能力和跨领域的流畅性，在提升语音相关任务方面展现出巨大潜力。本文关注的是如何将解码器仅有的LLMs整合到语音转文本翻译（Speech-to-Text Translation，S2TT）任务中。我们提出一种架构，让LLM直接处理编码的语音表示并生成文本翻译。同时，我们研究了不同参数高效微调技术和任务表述方式的影响。在不使用专有数据的情况下，我们的模型在CoVoST 2和FLEURS基准上实现了最先进的性能。我们还进行了深入分析，验证了我们设计选择的合理性，并为LLMs与S2TT任务的融合提供了见解。|
|**2024-07-03**|**SOS! Soft Prompt Attack Against Open-Source Large Language Models**|Ziqing Yang et.al.|[2407.03160](http://arxiv.org/abs/2407.03160)|null|## 背景  开源的大规模语言模型（LLMs）在公众和行业中的受欢迎程度日益提升，因为它们可定制、微调且免费使用。然而，一些开源LLMs在使用前需要审批，这促使第三方发布易于获取的版本，甚至对这些模型进行微调或量化优化，以降低计算需求。这些便捷版本对用户颇具吸引力，但也增加了训练时间攻击的风险，威胁到LLMs的完整性和安全性。本文提出一种新的训练时间攻击方法SOS，它设计得计算需求低，无需干净数据或调整模型权重，保持模型的可用性。SOS针对各种场景下的安全问题，包括后门攻击、破解攻击和提示窃取攻击。实验结果表明，该攻击在所有评估目标上均有效。此外，我们还展示了SOS技术的另一面——版权令牌：这是一种新颖的方法，允许用户标记其版权内容，防止模型使用。|
|**2024-07-03**|**Let the Code LLM Edit Itself When You Edit the Code**|Zhenyu He et.al.|[2407.03157](http://arxiv.org/abs/2407.03157)|null|在本研究中，我们探讨了代码生成中的常见场景：开发者实时编辑现有代码，并请求大型语言模型（如大语言模型）进行即时重预测下一个token或行。直接的方法是让LLM重新编码整个键值缓存以提供精确的预测，但这个过程计算成本高，特别是当序列长度很长时。仅编码编辑后的子序列并将其整合到原始键值缓存中会遇到时间混淆问题，导致性能大幅下降。为此，我们提出了一种解决方案——\textbf{位置完整性编码}（Positional Integrity Encoding，简称PIE）。PIE基于旋转型位置编码，首先移除引入时间混淆的旋转型矩阵，然后重新应用正确的矩阵，确保了令牌之间的位置关系正确，仅需一轮矩阵乘法即可完成。我们在RepoBench-C-8k数据集上，使用13亿、67亿和330亿参数的DeepSeek-Coder模型进行了广泛实验，涵盖了代码插入、代码删除和多位置代码编辑等三个实际编程任务。实验结果表明，与标准的完整重计算方法相比，PIE在所有模型规模和任务中都能减少超过85%的计算开销，同时保持了良好的性能近似。|
|**2024-07-02**|**MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention**|Huiqiang Jiang et.al.|[2407.02490](http://arxiv.org/abs/2407.02490)|**[link](https://github.com/microsoft/MInference)**|**由于大型语言模型（LLMs）的计算挑战，尤其是随着提示长度的增长，其广泛应用面临障碍。由于注意力计算的二次复杂性，80亿参数的LLM在单个A100 GPU上处理100万个令牌（即预填充阶段）需要30分钟。现有的加速预填充方法往往在面对长序列LLMs时难以保持既高效又准确。为此，我们提出了MInference（百万令牌推理），这是一种旨在提升长序列处理预填充阶段速度的稀疏计算方法。我们发现了注意力矩阵中的三种独特模式：A形、垂直斜线和块稀疏，这些模式可利用GPU进行高效的稀疏计算。我们在离线阶段确定每个注意力头的最佳模式，并在推理过程中动态构建稀疏索引。通过优化的GPU内核，我们实现了基于指定模式的稀疏注意力计算，显著减少了长序列LLMs预填充阶段的延迟。我们的方法无需修改预训练设置或额外微调即可直接应用于现有LLMs。我们在包括InfiniteBench、RULER、PG-19和Needle In A Haystack在内的各种下游任务以及LLaMA-3-1M、GLM4-1M、Yi-200K、Phi-3-128K和Qwen2-128K等模型上的实验表明，MInference在A100上有效降低了预填充的推理延迟高达10倍，同时保持了准确性。我们的代码已开源，地址为：https://aka.ms/MInference。**|
|**2024-07-02**|**Neurocache: Efficient Vector Retrieval for Long-range Language Modeling**|Ali Safaya et.al.|[2407.02486](http://arxiv.org/abs/2407.02486)|**[link](https://github.com/alisafaya/neurocache)**|**这篇论文介绍了一种名为Neurocache的方法，用于扩展大型语言模型（LLMs）的有效上下文范围，通过外部向量缓存存储其过去的模型状态。与近期的向量检索方法类似，Neurocache利用高效的k近邻(kNN)算法检索相关的历史状态，并将其融入注意力过程。Neurocache在改进现有方法方面有以下几点：(1) 存储压缩的状态，减小了缓存大小；(2) 每个令牌执行一次检索操作，提高了推理速度；(3) 将检索窗口扩展到邻近状态，提升了语言建模和下游任务的准确性。  实验结果表明，无论从头开始训练还是对预训练模型（如Llama2-7B和Mistral-7B）进行增强，Neurocache都能有效。我们还对比了Neurocache与其他文本检索方法，在单文档问答和少量样本学习任务中展示了其优势。源代码已在以下链接公开：https://github.com/alisafaya/neurocache。**|
|**2024-07-02**|**RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs**|Yue Yu et.al.|[2407.02485](http://arxiv.org/abs/2407.02485)|null|该研究提出了一种新颖的指令调优框架RankRAG，旨在针对检索增强生成（RAG）中的上下文排名和答案生成双重任务对大型语言模型进行调优。通过在训练过程中加入少量排名数据，指令调优的单个语言模型表现出令人惊讶的效果，超越了专门使用大量排名数据进行单独调优的现有专家排名模型。实验中，我们与包括GPT-4-0613、GPT-4-turbo-2024-0409和开放源代码的最先进的RAG性能模型ChatQA-1.5在内的多个强baseline进行了比较。具体来说，我们的Llama3-RankRAG在九个知识密集型基准上显著优于Llama3-ChatQA-1.5和GPT-4系列模型。此外，它还在无需针对生物医学领域数据进行指令调优的情况下，在五个生物医学领域的RAG基准上与GPT-4模型表现相当，这显示了其在新领域中的出色泛化能力。|
|**2024-07-02**|**MMedAgent: Learning to Use Medical Tools with Multi-modal Agent**|Binxu Li et.al.|[2407.02483](http://arxiv.org/abs/2407.02483)|**[link](https://github.com/Wangyixinxin/MMedAgent)**|尽管多模态大型语言模型（MLLMs）已经取得了成功，但它们的泛化能力仍然有限，在某些情况下不如专业模型。近期，研究人员开发了基于LLMs的代理，通过用户输入选择合适的专用模型来解决这些问题。然而，在医疗领域，这类进展的应用还不广泛。为了弥补这一空白，本文首次提出了一种专为医疗设计的代理，名为\textbf{M}ulti-modal \textbf{Med}ical \textbf{Agent}（MMedAgent）。我们构建了一个指令调优数据集，包含了六个医疗工具，用于解决七项任务，使代理能针对特定任务选择最适宜的工具。实验全面展示了MMedAgent在各种医疗任务上超越了开源方法，甚至包括封闭源模型GPT-4o，且在引入和整合新医疗工具方面表现出高效性。|
|**2024-07-02**|**Understanding Alignment in Multimodal LLMs: A Comprehensive Study**|Elmira Amirloo et.al.|[2407.02477](http://arxiv.org/abs/2407.02477)|null|随着大型语言模型（LLMs）性能的提升，偏好一致性已成为一个重要因素，但在多模态大型语言模型（MLLMs）中的应用相对较少。这些模型在图像理解任务中也会遇到诸如错误陈述和内容不一致（即幻觉）的问题。MLLMs的偏好对齐目标是使模型的回答更贴近图像信息。近期的研究已经引入了针对MLLM的偏好数据集，并尝试了直接偏好优化（DPO）和proximal policy optimization（PPO）等不同的对齐方法。然而，由于数据集、基础模型类型和对齐策略的差异，哪种方法对性能提升的贡献最大尚不清楚。  本文独立分析了MLLM偏好对齐的各个方面。我们将对齐算法分为离线（如DPO）和在线（如在线-DPO）两类，并表明在某些情况下结合这两种方法可以提高模型性能。我们还回顾了各种已发表的多模态偏好数据集，探讨了它们构建细节对模型性能的影响。基于这些发现，我们提出了一种新的多模态偏好数据生成方法——偏见驱动的幻觉采样（Bias-Driven Hallucination Sampling，BDHS），这种方法无需额外标注或外部模型，且在多个基准上展现出与之前发表的对齐工作相当的竞争性能。|
|**2024-07-02**|**Open Scene Graphs for Open World Object-Goal Navigation**|Joel Loo et.al.|[2407.02473](http://arxiv.org/abs/2407.02473)|null|如何构建能够在开放世界中执行语义导航任务的机器人，比如在新场景中寻找目标物体？尽管基础模型具备处理这类任务所需的丰富知识和泛化能力，但需要一种合适的场景表示来将它们整合到完整的机器人系统中。为此，我们提出了开放场景图（Open Scene Graphs，OSG），这是一种拓扑语义表示，用于保留和组织开放集中场景信息，且结构可适应不同环境类型。我们将基础模型和OSG整合到OpenSearch系统中，该系统专为开放世界的对象目标导航设计，能够理解自然语言指令并在多变环境中零样本泛化，寻找未见过的物体。我们的OSG增强了与大型语言模型（LLMs）的推理能力，使得OpenSearch在物体目标导航任务上表现出色，超越了现有的LLM方法。通过模拟实验和真实世界测试，我们验证了OpenSearch在各种环境、机器人和新颖指令下的泛化能力。|
|**2024-07-02**|**Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I**|Harrie Oosterhuis et.al.|[2407.02464](http://arxiv.org/abs/2407.02464)|null|传统的信息检索（IR）系统评估通常成本高昂，因为需要人工专家进行相关性标注。近年来，生成式人工智能，尤其是大型语言模型（LLMs），能够以相对较低的计算成本大规模生成相关性注释，可能减轻IR评估的传统成本，并使其适用于众多资源匮乏的应用场景。然而，生成的注释并非无误，直接用于评估可能导致结果不可靠。为此，本研究提出两种方法，分别是基于预测驱动的推断和规范风险控制，利用计算机生成的相关性注释为IR评估指标提供可靠的置信区间（CIs）。  我们的方法需要少量可靠的注释，通过统计分析生成注释中的错误，从而为评估指标设置CIs，具有坚实的理论基础。与现有方法不同，我们特别设计的规范风险控制方法适用于排名评估，并且可以根据查询和文档自适应调整CIs。实验结果显示，我们的置信区间准确捕捉了基于LLM注释的评估中的变异性和偏差，优于传统的Bootstrap估计。我们期望这些贡献能为那些传统上难以实现可靠评估的众多IR应用带来革新。|
|**2024-07-03**|**Video Watermarking: Safeguarding Your Video from (Unauthorized) Annotations by Video-based LLMs**|Jinmin Li et.al.|[2407.02411](http://arxiv.org/abs/2407.02411)|null|随着视频驱动的大型语言模型（LLMs）的兴起，视频理解能力得到了显著提升，但同时也引发了数据保护方面的担忧，因为视频更容易被无授权地标注。为此，本文提出了一种名为“Video Watermarking”的创新方法，旨在保护视频免受未经授权的视频LLMs，特别是针对内容和描述的处理。通过在关键帧中嵌入难以察觉的水印，我们利用多模态流损失保持观看体验的同时，防止视频被滥用。大量的实验表明，Video Watermarking显著降低了视频在各种视频LLMs中的可理解性，证明了其隐秘性和鲁棒性。总的来说，我们的方法为确保视频内容的安全、完整性和保密性提供了一种解决方案，以应对不断发展的视频LLMs技术。|
|**2024-07-02**|**CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models**|Song Wang et.al.|[2407.02408](http://arxiv.org/abs/2407.02408)|null|随着大型语言模型（LLMs）被越来越多地应用于各种自然语言处理任务，对其生成内容可能产生的负面社会影响的担忧也随之增加。为了评估LLMs的偏见，研究人员已经提出了一系列数据集。然而，现有的偏见评估工作往往只关注某种类型的偏见，并使用不一致的评价指标，这导致不同数据集和LLM之间的比较困难。为此，我们收集了多种用于评估LLM偏见的数据集，并进一步提出了CEB（Compositional Evaluation Benchmark），它涵盖了不同社会群体和社会任务中的各种类型偏见。CEB的构建基于我们新提出的构成性分类体系，从三个维度对每个数据集进行刻画：偏见类型、社会群体和任务。通过结合这三个维度，我们开发出一种全面的LLM偏见评估策略。实验结果表明，这些偏见在各维度上的程度有所不同，从而为针对特定偏见的缓解方法的发展提供了指导。|
|**2024-07-02**|**Assessing the Code Clone Detection Capability of Large Language Models**|Zixian Zhang et.al.|[2407.02402](http://arxiv.org/abs/2407.02402)|null|该研究旨在评估两种先进的大型语言模型（LLMs），GPT-3.5和GPT-4，在代码克隆检测任务中的性能。实验通过在两个数据集上测试模型：BigCloneBench（人类创建）和GPTCloneBench（LLM生成）。研究发现，GPT-4在所有类型的代码克隆检测中都明显优于GPT-3.5。结果显示，GPT模型的准确度与其识别代码克隆的能力与代码相似度之间存在关联，但它们在识别最复杂的Type-4代码克隆时效果较低。此外，GPT模型在检测LLM生成的代码中的代码克隆表现优于人类生成的代码，但整体准确性仍不显著。这些发现强调了进一步提升LLM在代码克隆识别能力的必要性，特别是针对自我生成代码克隆的问题，随着软件工程师越来越多地使用基于LLM的代码生成和重构工具，这可能会成为一个问题。|
|**2024-06-28**|**Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs**|Sukmin Yun et.al.|[2406.20098](http://arxiv.org/abs/2406.20098)|**[link](https://github.com/mbzuai-llm/web2code)**|**多模态大型语言模型（MLLMs）在图像、视频和音频等多种模态的处理任务上表现出色。然而，它们在理解和生成网页截图以及相应的HTML代码方面的能力相对较弱。为解决这个问题，我们提出Web2Code，这是一个包括大规模网页到代码的新基准，用于指令调优，并评估MLLM在网页理解及HTML代码转换能力上的表现。我们构建数据集时，利用预训练的LLMs增强现有的网页到代码数据集，并生成多样化的网页图片，以供渲染。输入是网页图片和说明，输出是网页的HTML代码，同时加入关于网页内容的丰富自然语言问答对，以促进对网页内容的全面理解。为了评估模型在这类任务中的性能，我们开发了一个测试框架，用于测试MLLM在网页理解与网页到代码生成方面的技能。实验结果表明，我们的数据集不仅有益于我们提出的任务，还在视觉领域的一般性能上有所提升，而先前的数据集会导致性能下降。我们期望这项工作能推动通用MLLM的发展，使其适用于网络内容生成和自动化任务。我们的数据和代码将在<https://github.com/MBZUAI-LLM/web2code>上公开。**|
|**2024-06-28**|**LLaRA: Supercharging Robot Learning Data for Vision-Language Policy**|Xiang Li et.al.|[2406.20095](http://arxiv.org/abs/2406.20095)|**[link](https://github.com/lostxine/llara)**|**该论文介绍了一种名为LLaRA（大型语言和机器人助手）的框架，它将机器人行动策略转化为对话形式，通过结合额外的数据辅助学习，提升响应质量。利用具备视觉输入的大型语言模型（VLMs），即视觉语言模型，这些模型能够处理状态信息，作为视觉-文本提示，并生成最优的机器人决策策略。首先，论文提出了一种自动化方法，从现有的行为克隆数据中生成多样且高质量的机器人指令数据集。然后，使用这种定制的对话式格式对VLM进行训练，使其能够生成有意义的机器人行动策略。实验结果表明，LLaRA框架在多个模拟和真实世界环境中展现出最先进的性能。相关代码、数据集和预训练模型已在<https://github.com/LostXine/LLaRA>提供。**|
|**2024-06-28**|**Scaling Synthetic Data Creation with 1,000,000,000 Personas**|Xin Chan et.al.|[2406.20094](http://arxiv.org/abs/2406.20094)|**[link](https://github.com/tencent-ailab/persona-hub)**|我们提出了一种新颖的基于人格的数据合成方法，该方法利用大型语言模型（LLM）内的多种视角来生成多样化的人工合成数据。为了在大规模上充分利用这种方法，我们引入了Persona Hub，这是一个从网络数据自动整理出的一亿个多元化人格的集合，相当于全球人口的约13%。这些人格作为分布式世界知识载体，几乎可以调用LLM内包含的各类观点，从而推动大规模、多样化的合成数据创建，适用于各种场景。通过展示Persona Hub如何在大规模生成高质量的数学和逻辑推理问题、指令（用户提示）、富含知识的文本、游戏NPC和工具（函数）等方面的应用，我们证明了基于人格的数据合成具有多样性、可扩展性、灵活性和易用性，可能引领合成数据创造和实际应用的新范式，对LLM的研究和发展产生深远影响。|
|**2024-06-28**|**LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context Compression**|Jieneng Chen et.al.|[2406.20092](http://arxiv.org/abs/2406.20092)|**[link](https://github.com/beckschen/llavolta)**|**尽管在大型语言模型（LLMs）的文本嵌入压缩方面取得了显著进步，但大型多模态模型（LMMs）中的视觉令牌压缩仍然被忽视。本文研究了视觉令牌的冗余性以及在这些模型中的有效训练。初步实验表明，在测试阶段通过简单平均池化消除高达70%的视觉令牌，GQA基准的视觉问答准确率仅下降3%，这显示出视觉上下文中存在大量冗余。为解决这个问题，我们提出了Visual Context Compressor，它在训练阶段减少视觉令牌数量，以提高效率而不会影响性能。为了在压缩视觉令牌时尽量减少信息损失并保持训练效率，我们开发了轻量级训练方案LLaVolta。LLaVolta采用分阶段的视觉上下文压缩策略，从重度到轻度逐渐压缩，最终在训练结束时完全不进行压缩，从而在测试时不会丢失任何信息。广泛的实验表明，我们的方法提升了多模态模型在图像-语言和视频-语言理解任务上的性能，并显著降低了训练成本。代码已在https://github.com/Beckschen/LLaVolta上开源。**|
|**2024-06-28**|**ProgressGym: Alignment with a Millennium of Moral Progress**|Tianyi Qiu et.al.|[2406.20087](http://arxiv.org/abs/2406.20087)|**[link](https://github.com/pku-alignment/progressgym)**|随着前沿人工智能系统，特别是大型语言模型（LLMs）在知识论中的影响力日益增强，它们可能强化社会普遍的价值观，进而加剧错误道德观念的固化，导致广泛的社会问题持续存在。为应对这一潜在风险，我们提出进步对齐作为一种技术解决方案。进步对齐算法旨在学习人类道德进步的机制，从而弥补现有对齐方法对当代道德盲点的敏感性。为了推动进步对齐的研究，我们开发了ProgressGym，一个实验性框架，它从历史中学习道德进步的规律，以促进现实世界道德决策的未来发展。借助9个世纪的历史文本和18个历史LLMs，ProgressGym将现实生活中的进步对齐挑战转化为具体的基准。我们定义了三个核心挑战：追踪演变的价值（PG-Follow）、预测道德进步（PG-Predict）以及调节人与AI价值变迁之间的反馈循环（PG-Coevolve）。这些任务需要时间维度的方法，而传统的对齐策略无法胜任。  为此，我们展示了终身学习和外推算法作为进步对齐的基本方法，并建立了一个开放的排行榜，邀请创新算法和新挑战。该框架和排行榜分别可在https://github.com/PKU-Alignment/ProgressGym 和 https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard 获取。|
|**2024-06-28**|**Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language**|Yicheng Chen et.al.|[2406.20085](http://arxiv.org/abs/2406.20085)|null|基于扩散模型的生成方法已经在生成各种布局的高质量图像方面展现出巨大潜力，这对于下游感知任务具有显著益处。然而，仅依赖语言描述和一个合适的多实例评估指标来实现全自动布局生成并未得到充分探索。本文提出了一种新颖的框架——Auto Cherry-Picker（ACP），旨在自动生成高质量的多模态训练样本，以增强感知和多模态训练效果。通过输入自然语言概念列表，我们引导大型语言模型（LLMs）生成详细的描述并设计合理的布局。然后，使用文本到图像模型生成多个图片。接着，我们采用精心设计的评估指标对生成的数据进行精炼，确保质量。特别是，我们提出了复合布局与图像评分（Composite Layout and Image Score，CLIS）这一新指标，用于公正地评估生成的图像。我们的合成高质示例在定制初始概念列表时，能够有效提升各种场景下的性能，尤其是在处理长尾分布和不平衡数据集的问题上。下游任务的实验结果显示，ACP显著提高了现有模型的表现。此外，我们深入研究了CLIS与下游任务性能提升之间的关联，发现CLIS分数越高，性能越好。这表明评估指标在视觉感知和多模态大型语言模型任务中可能发挥关键作用。我们将提供代码。|
|**2024-06-28**|**Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification**|Anisha Gunjal et.al.|[2406.20079](http://arxiv.org/abs/2406.20079)|**[link](https://github.com/anisha2102/molecular_facts)**|**随着大型语言模型（LLM）生成内容的自动事实核查变得越来越普遍，以应对错误叙述的问题，研究的一个关键焦点在于核查的粒度：较大的文本段落难以核查，而更原子化的事实（如命题）可能缺乏正确的上下文解读。本文探讨了在这些原子事实中上下文的作用。我们认为完全原子的事实并非最佳表示形式，为此我们提出了分子事实的两个标准：去情境化（decontextuality），即它们能否独立存在，以及最小化（minimality），即添加多少额外信息才能实现去情境化。我们量化了去情境化对最小化的影响，并提出了一种基础方法来自动生成分子事实，目标是在保持准确性的同时提供适量的信息。我们将这种方法与不同的去情境化策略进行了比较，发现分子事实能够在模糊场景中平衡最小化和事实核查的准确性。**|
|**2024-07-01**|**BMW Agents -- A Framework For Task Automation Through Multi-Agent Collaboration**|Noel Crawford et.al.|[2406.20041](http://arxiv.org/abs/2406.20041)|null|自主代理驱动的大规模语言模型（LLMs）展示了巨大的自动化潜力。早期的展示表明，这些代理能够解决复杂任务，与外部系统交互以增强知识，并触发行动。特别是，多个代理协作解决复杂任务的工作流证明了它们在不那么严格和定义不明确的环境中操作的能力。因此，多代理方法有巨大的潜力成为众多工业应用的核心，从复杂的知识检索系统到下一代机器人过程自动化。鉴于当前LLMs的推理能力，处理复杂流程需要分步骤的方法，包括设计明确且模块化的任务计划。根据复杂程度，这些任务可以由单个代理或一组代理执行。本研究专注于构建一个灵活的代理工程框架，重点关注规划和执行，旨在应对不同领域的复杂应用场景。该框架为工业应用提供可靠性，并提出确保可扩展、灵活且协作的工作流程技术，让多个自主代理协同解决问题。|
|**2024-06-28**|**LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models**|Renzhi Wang et.al.|[2406.20030](http://arxiv.org/abs/2406.20030)|null|## 背景  大型语言模型（LLMs）为了跟上不断变化的世界知识，需要持续进行模型更新，这催生了终生模型编辑任务。近年来，尽管已经开发出多种单次和批量编辑的技术，但它们在面对终生编辑时要么无法应用，要么效果不佳。本文中，我们提出LEMoE，一个专为终生模型编辑设计的混合专家（MoE）适配器。首先，我们分析了影响传统MoE适配器在终生编辑中有效性的因素，包括灾难性遗忘、路由不一致性和顺序敏感性。基于这些洞察，我们提出了一种定制的模块插入方法，引入了新颖的键值对锚定路由以增强训练和推理阶段的路由一致性，同时采用了一个简洁而有效的聚类基编辑顺序规划。实验结果表明，我们的方法在终生编辑任务中表现出色，超越了先前的模型编辑技术，同时保持了批量编辑任务中的优秀性能。我们的代码将开源。|
|**2024-06-28**|**ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models**|Yuxiang Zhang et.al.|[2406.20015](http://arxiv.org/abs/2406.20015)|**[link](https://github.com/toolbehonest/toolbehonest)**|**随着工具增强的大型语言模型（LLMs）迅速融入实际应用，社区亟需全面了解这些模型中的幻觉问题。为此，我们提出了一项全面的诊断基准——ToolBH。我们从深度和广度两个维度进行评估：在深度上，设计了多级诊断流程，包括（1）可解性检测、（2）解决方案规划和（3）缺失工具分析；在广度上，考虑了工具集特征下的三种场景：缺少必要工具、潜在工具和功能有限的工具。我们构建了七个任务，并通过多次人工标注收集了700份评估样本。结果显示，当前先进的模型Gemini-1.5-Pro和GPT-4o在这项基准上的总得分为45.3和37.0，满分100分。在工具增强的LLM场景中，更大的模型参数并不一定意味着更好的性能，训练数据和回复策略同样关键。我们的诊断分析指出，模型错误的主要原因在于任务可解性的判断。开放源码模型在冗长回复时性能下降，而专有模型在长链推理方面表现更优。**|
|**2024-06-27**|**ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos**|Jr-Jen Chen et.al.|[2406.19392](http://arxiv.org/abs/2406.19392)|**[link](https://github.com/rextime/rextime)**|**我们提出了一项名为ReXTime的基准测试，专门针对人工智能模型在视频事件中的时间推理能力进行严谨评估。ReXTime关注的是跨时间推理，即理解当问题及其相应的答案出现在不同的视频片段时的人类式理解。这种需要深入理解视频片段之间因果关系的时间推理能力对前沿的多模态大型语言模型构成了重大挑战。为了支持这种评价，我们开发了一个自动化管道，用于生成时间推理的问答对，大大减少了繁琐的手动标注需求。我们的基准包括921个精心筛选的验证样本和2,143个测试样本，每个样本都经过人工精心挑选以确保准确性和相关性。评估结果显示，尽管前沿大型语言模型在学术模型上表现突出，但它们与人类的表现仍存在显著的14.3%的精度差距。此外，我们的管道无需人工创建了一个包含9,695个机器生成样本的训练数据集，实证研究表明，这可以通过微调来提升跨时间推理能力。**|
|**2024-06-27**|**The Remarkable Robustness of LLMs: Stages of Inference?**|Vedang Lad et.al.|[2406.19384](http://arxiv.org/abs/2406.19384)|**[link](https://github.com/vdlad/remarkable-robustness-of-llms)**|**我们通过删除和交换相邻层来展示并研究大型语言模型的惊人鲁棒性。实验结果显示，在不进行微调的情况下，这些干预措施仍能保留原始模型72%至95%的预测精度，而且模型层数越多，表现出更高的鲁棒性。根据逐层干预实验和其他实验，我们提出了一个假设：存在四种通用的推理阶段，跨越八种不同的模型：解码器阶段，将原始令牌表示提升为更高级的上下文表示；特征工程阶段，迭代优化任务和实体特定特征；然后是模型的半部分，随着专门组件的作用，隐藏表示与词汇空间的对齐进入一个相变阶段；最后，最后一层通过消除对预测造成干扰的过时特征，精细化后续的令牌分布。**|
|**2024-06-27**|**The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models**|Xiliang Zhu et.al.|[2406.19358](http://arxiv.org/abs/2406.19358)|null|### 概述  情感分析在自然语言处理（NLP）中扮演着核心角色。XLM-R和mT5等多语言预训练模型的兴起推动了跨语言情感分析的关注度提升。近期大型语言模型（LLM）的出现极大地推动了通用NLP任务的发展，但这些模型在跨语言情感分析方面的性能尚未充分探讨。本研究通过实证分析，比较了公共小型多语言模型（SMLM）如XLM-R与以英语为中心的LLM（如Llama-3）在英语、西班牙语、法语和中文的情感分析中的零样本和少量样本迁移能力。结果显示，就公开模型而言，SMLM在零样本跨语言设置中表现出更好的性能。然而，在少量样本情况下，公开LLM显示出更强的适应性。此外，我们发现专有的GPT-3.5和GPT-4在零样本跨语言能力上领先，但在少量样本场景下，它们被公开模型超越。|
|**2024-06-27**|**DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions**|Nigel Fernandez et.al.|[2406.19356](http://arxiv.org/abs/2406.19356)|**[link](https://github.com/umass-ml4ed/divert)**|## 背景  高质量的干扰项对于选择题（尤其是数学选择题）的评估和教学价值至关重要。然而，手工设计能够反映学生实际知识缺陷或误解的干扰项是一项艰巨的任务。尽管大型语言模型（LLM）如GPT-4在生成干扰项方面有所助益，但数学这类学科的处理仍然具有挑战性。因此，我们提出了一种新的方法，旨在理解和生成解释性的错误表示，以生成数学选择题的干扰项。本文介绍DiVERT（基于文本的变异误差生成器），这是一种利用7亿参数开源LLM的变分方法，它在真实世界数学选择题数据集（包含1,434个问题，被数十万学生使用）上的实验表明，相较于最先进的GPT-4方法，DiVERT在干扰项生成方面表现出色。此外，我们还进行了与数学教育者的同行评审，结果表明DiVERT生成的错误标签质量接近人类编写的。  ## 任务  请将上述英文论文摘要翻译成中文，输出不应包含除摘要内容外的任何其他内容，且确保不出现","字符。|
|**2024-06-27**|**IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language**|Lucky Susanto et.al.|[2406.19349](http://arxiv.org/abs/2406.19349)|null|## 翻译  针对网络仇恨言论对社会和谐的严峻威胁，特别是在印尼这类国家，近年来仇恨言论在线比率增长了十倍，迫切需要有效的检测机制。然而，由于缺乏充足的标记数据，尤其是针对印尼文本的，这一进展受到了阻碍。边缘化群体，如什叶派、LGBTQ等少数群体，面临的挑战更大，因为仇恨言论报告不足，现有的检测工具对其理解有限。此外，当前数据集对主观性的处理不足，加剧了问题。为了应对这些问题，我们提出IndoToxic2024，这是一个全面的印尼仇恨言论和毒性分类数据集，包含43,692条记录，由19名多元化的个体进行标注，特别关注选举期间针对国内弱势群体（如总统选举中的特定群体）的文本。我们使用BERT模型（IndoBERTweet）进行了微调，为七种二元分类任务设定了基准，取得了0.78的宏F1分数。同时，我们展示了如何将人口统计信息融入其中，提升大型语言模型gpt-3.5-turbo在零样本情况下的性能。然而，我们也警告，过度依赖人口统计信息可能导致细化模型性能下降，因为这会导致数据碎片化。|
|**2024-06-27**|**Jump Starting Bandits with LLM-Generated Prior Knowledge**|Parand A. Alamdari et.al.|[2406.19317](http://arxiv.org/abs/2406.19317)|**[link](https://github.com/BorealisAI/jump-starting-bandits)**|我们提供了有力的证据，展示了将大型语言模型（LLMs）与上下文化多臂老虎机框架相结合的优势。上下文化老虎机在推荐系统中广泛应用，用于根据用户特定的上下文生成个性化建议。我们表明，经过大规模语料库训练，富含人类知识和偏好的LLMs能够很好地模拟人类行为，从而通过启动上下文化多臂老虎机来减少在线学习的遗憾（regret）。我们提出了一种初始化算法，通过提示LLMs生成接近人类偏好的预训练数据集，供老虎机学习使用。这显著降低了在线学习的遗憾和数据收集成本。我们的方法通过两组实验验证，包括使用LLMs作为占卜者（oracle）的实验和基于联合调查实验数据的真实世界实验。|
|**2024-06-27**|**From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data**|Zheyang Xiong et.al.|[2406.19292](http://arxiv.org/abs/2406.19292)|**[link](https://github.com/edixiong/artificial-needles)**|近期的研究指出，大型语言模型（LLMs）在处理长文本输入时在信息检索和推理能力上存在困难。为解决这个问题，我们提出了一种利用精心设计的合成数据集进行微调的方法，该数据集包含数值型键值对检索任务。我们在GPT-3.5 Turbo和Mistral 7B等模型上的实验显示，对这些模型进行这种数据集的微调显著提高了它们在长文本环境中的信息检索和推理能力。我们分析了微调后的模型，发现它们在从合成任务迁移到实际评估（如在20文档MDQA中的位置10处提升10.5%）方面的表现有所提升。此外，我们还发现，经过我们合成数据集微调的LLMs在通用基准上的性能保持稳定，而使用其他基于长文本增强数据集微调的LLMs可能会导致错误增加（例如，在TriviaQA上，Mistral 7B在我们的合成数据上微调无明显性能下降，而其他基线数据可能导致性能下降，范围在2.33%到6.19%之间）。本研究突显了通过合成数据微调来提升LLMs在长文本任务性能的潜力。|
|**2024-06-27**|**PhysioLLM: Supporting Personalized Health Insights with Wearables and Large Language Models**|Cathy Mengying Fang et.al.|[2406.19283](http://arxiv.org/abs/2406.19283)|null|我们介绍了一种名为PhysioLLM的互动系统，它利用大型语言模型（LLMs）结合可穿戴设备的生理数据和上下文信息，提供个性化的健康理解和探索。与商业健康应用不同，PhysioLLM具备全面的统计分析功能，能发现用户数据中的关联和趋势。用户可以用自然语言提问，获取生成的个性化洞察，并根据这些信息制定行动目标。以改善睡眠质量为例，因为其可通过生理数据量化且对整体健康至关重要。通过一项涉及24名Fitbit智能手表用户的用户研究，我们证明了PhysioLLM在促进对健康数据的深入个性化理解，以及支持实现个人健康目标方面，优于Fitbit应用和通用LLM聊天机器人。|
|**2024-06-27**|**HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale**|Junying Chen et.al.|[2406.19280](http://arxiv.org/abs/2406.19280)|**[link](https://github.com/freedomintelligence/huatuogpt-vision)**|**随着大型多模态语言模型（如GPT-4V）的迅速发展，它们在医学多模态能力方面取得了显著进步。然而，由于医学影像-文本数据的数量和质量受限于数据隐私问题和高昂的标注成本，这些模型仍面临挑战。早期的研究尝试利用PubMed的大型去标识化医疗图像-文本对来缓解这些问题，但它们仍受到数据噪音的影响。为解决这一问题，我们优化了PubMed中的医疗图像-文本对，并利用GPT-4V在“非盲”模式下进行数据清洗和格式转换，创建了PubMedVision数据集，包含130万份医学视觉问答样本。我们的验证表明：（1）PubMedVision显著提升了当前多模态语言模型在医学领域的性能，在诸如MMMU Health & Medicine track等基准测试中表现出显著改善；（2）医学专家的手动检查和实证结果证实了我们的数据集在数据质量上优于其他构建方法。利用PubMedVision，我们训练了一个名为HuatuoGPT-Vision的340亿参数的医学多模态语言模型，它在公开源多模态语言模型中表现出色，在医学多模态场景中显示出优越性能。**|
|**2024-06-27**|**AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning**|Praneeth Vadlapati et.al.|[2406.19271](http://arxiv.org/abs/2406.19271)|**[link](https://github.com/Pro-GenAI/AutoPureData)**|**人们对最新的和可靠的大型语言模型（LLMs）的需求持续增长。通常，LLMs是基于固定的数据集训练然后部署的。然而，训练数据会随着时间逐渐过时。研究关注如何利用网络数据自动更新AI模型，但这一过程涉及数据质量与安全的顾虑，如偏见、垃圾信息等。确保数据纯净对于生成可靠的模型至关重要。在不纯数据上训练可能导致不良结果。该研究提出了一种系统，它收集网络数据，并借助现有可信的AI模型自动筛选出不需要的内容。实验中，我们收集并处理了一小部分网络数据，验证了该系统的数据净化效果。**|
|**2024-06-26**|**Symbolic Learning Enables Self-Evolving Agents**|Wangchunshu Zhou et.al.|[2406.18532](http://arxiv.org/abs/2406.18532)|**[link](https://github.com/aiwaves-cn/agents)**|**人工智能界通过构建"语言代理"（即复杂的大型语言模型管道）来探寻通用人工智能（AGI）的道路，这些模型结合了提示技术和工具使用方法。尽管它们在众多实际任务中表现出色，但当前语言代理研究的一个关键局限是其模型中心或工程导向：提示、工具和管道的改进依赖于大量的人工专家设计，而非自动从数据学习。我们认为，从模型中心向数据中心转变——让语言代理能够自主学习和适应环境，是它们迈向AGI的关键。为此，我们提出了"代理符号学习"框架，这是一个系统性的方法，它使语言代理能够在数据驱动的方式下自我优化，利用符号优化器。我们将代理视为具有可学习权重的符号网络，这些权重由提示、工具及其组合方式定义。代理符号学习旨在模仿连接主义学习中的两个基本算法：反向传播和梯度下降，但它处理的是自然语言形式的权重、损失和梯度。我们在标准基准和复杂现实任务上进行了概念验证实验，结果表明，代理符号学习使得语言代理在创建和部署后能够自我更新，实现了"自我进化的代理"。**|
|**2024-06-26**|**PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation**|Christoph Leiter et.al.|[2406.18528](http://arxiv.org/abs/2406.18528)|**[link](https://github.com/gringham/prexme)**|## 翻译  大型语言模型（LLMs）在自然语言处理领域带来了革命性变化，它们的上下文学习能力使其成为自然语言生成评价的有力工具，特别适用于资源匮乏和时间限制的场景。本文提出PrExMe，一项大规模的提示探索度量法，我们在机器翻译（MT）和摘要任务上评估了超过720种开源LLM作为度量标准的模板，总计约660万次评估。这项详尽的比较（1）为近期开源LLMs作为评价指标的表现设定了基准；（2）探讨了不同提示策略的稳定性和变异性。我们发现，一方面，存在一些情况下提示表现稳定：有些LLMs表现出特有的偏好，倾向于使用文本标签来评分，而另一些则倾向于返回数值分数。另一方面，提示的稳定性和模型排名可能受到看似微不足道的更改的影响。例如，将输出格式从“0到100”改为“-1到+1”可能会显著改变我们的评估结果。我们的研究有助于理解不同提示方法对MT和摘要评价中LLM-based度量的影响，揭示了最稳定的提示模式，并指出了潜在局限性。|
|**2024-06-26**|**CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs**|Zirui Wang et.al.|[2406.18521](http://arxiv.org/abs/2406.18521)|**[link](https://github.com/princeton-nlp/CharXiv)**|在实际应用多模态大型语言模型（Multimodal Large Language Models，MLLMs）处理科学论文或财务报告等任务时，图表理解至关重要。然而，现有的数据集往往集中在简化和同质化的图表上，以及基于模板的问题，这可能导致性能评估过于乐观。我们发现，尽管开源模型在现有基准上可能表现优于强大的专有模型，但通过简单的压力测试，如改变图表或问题，性能会下降高达34.5%。为此，我们提出CharXiv，这是一个包含2,323个来自arXiv论文的自然、复杂且多样化的图表的全面评估套件。CharXiv包括两类问题：1）描述性问题，用于检查基本图表元素；2）推理问题，需要综合分析图表中的复杂视觉元素。所有图表和问题都由专家精心挑选、整理和验证以保证质量。结果显示，最强专有模型（例如GPT-4o，准确率为47.1%）与最强开源模型（如InternVL Chat V1.5，准确率为29.2%）之间存在显著差距，而所有模型的表现均远低于人类的80.5%水平，这揭示了现有MLLM在图表理解能力上的不足。我们希望CharXiv能推动未来的研究，通过提供更真实、更具代表性的进步衡量标准，促进图表理解领域的研究。项目页面和排行榜可访问：https://charxiv.github.io/。|
|**2024-06-26**|**"Is ChatGPT a Better Explainer than My Professor?": Evaluating the Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline**|Grace Li et.al.|[2406.18512](http://arxiv.org/abs/2406.18512)|null|### 概述  解释是知识共享的核心，它建立在沟通原理、社会动态和学习理论之上。我们专注于对话式的解释方法，因为其环境高度适应性和交互性。我们的研究利用了解释行为框架，这是一个理解解释者和被解释者在对话中如何运用策略进行解释、理解和互动的工具。我们利用Wachsmuth等人构建的WIRED YouTube系列数据集，并由Booshehri等人进行了带有解释行为的标注，这些注释为我们理解对话中解释者如何构建回应提供了依据。  随着去年生成式人工智能的发展，我们期望更好地理解大型语言模型（LLMs）的能力，以及它们如何增强专家解释者的对话交流能力。为此，我们使用了Booshehri等人2023年标注的5-Levels数据集来评估LLMs在解释性对话中的表现。为了评价LLMs生成解释者回应的有效性，我们设计了三种策略：人类解释者的原始回应、GPT4的标准回应以及加入了解释步骤的GPT4回应。我们邀请人类标注者对这三种策略进行评估。|
|**2024-06-26**|**Mental Modeling of Reinforcement Learning Agents by Language Models**|Wenhao Lu et.al.|[2406.18505](http://arxiv.org/abs/2406.18505)|null|## 背景 尽管现代语言模型已经展现出一定的推理能力，理论上能够表达任意可能的令牌分布，但它们如何利用预训练时积累的世界知识来理解物理世界中的代理行为，这一方面仍未得到充分探索。本研究首次实证考察大型语言模型（LLMs）在通过推理分析代理的行为及其对状态的影响，从而构建代理心理模型（agent mental modeling）的能力。这可能揭示出利用LLMs解析强化学习（RL）代理行为的潜力，这对于可解释强化学习（XRL）的关键挑战具有重要意义。为此，我们提出特定的评估指标，并在不同复杂度的RL任务数据集上进行测试，报告关于代理心理模型建立的研究结果。结果显示，当前的LLMs还无法仅通过推理完全实现代理的心理建模，这需要进一步创新。因此，这项工作提供了对现代LLMs能力和局限性的新见解。|
|**2024-06-26**|**Is In-Context Learning a Type of Gradient-Based Learning? Evidence from the Inverse Frequency Effect in Structural Priming**|Zhenghao Zhou et.al.|[2406.18501](http://arxiv.org/abs/2406.18501)|null|这篇论文探讨了大型语言模型（LLMs）的内插学习（in-context learning，ICL）能力，并将其与基于梯度的学习进行功能等效性诊断。研究者提出了一种新方法，利用逆频率效应（inverse frequency effect，IFE）来分析。IFE现象指的是在错误驱动的学习过程中，模型应对罕见样例产生的更新幅度大于常见样例。在心理学中，人类在结构化提示（如倾向于重复最近接触的句子结构）情境中表现出IFE，这表明其可能涉及错误驱动的学习机制。实验通过模拟结构化提示在ICL中的影响发现，LLMs同样显示出IFE，且这一效应在更大的模型中更为明显。因此，研究结果支持了ICL本质上是基于梯度的学习的假设，即在ICL的前向传播过程中隐含地计算了梯度。论文结论指出，人类和LLMs都使用了基于梯度的、错误驱动的处理机制。|
|**2024-06-26**|**Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation**|Ahmed Njifenjou et.al.|[2406.18460](http://arxiv.org/abs/2406.18460)|null|近年来，人们提出了一系列方法来创建能够进行开放领域对话的大型语言模型（LLMs）。这些模型能回答用户问题，但局限于单向问答形式，而非真正的对话。通常，通过针对特定数据集进行微调来调整它们的交流风格，但这既昂贵又限于少数语言。本研究探索了角色扮演的零样本提示作为提高开放领域对话效率和成本效益的解决方案，利用多语言能力强的训练有素模型（Beeching等人，2023年），这些模型能遵循指令。我们设计了一个提示系统，当与遵循指令的模型——这里使用Vicuna（Chiang等人，2023年）结合时，能够生成在法语中的对话代理，在两项任务中甚至超越了经过微调的模型，并在人类评估中表现出色。|
|**2024-06-26**|**Cascading Large Language Models for Salient Event Graph Generation**|Xingwei Tan et.al.|[2406.18449](http://arxiv.org/abs/2406.18449)|**[link](https://github.com/xingwei-warwick/callmsae)**|由于长文档中事件检测、关系识别以及非结构化输入与结构化图谱的整合等任务的复杂性，从文本生成事件图谱是一项挑战。当前的研究往往同等重视所有事件，未能区分对理解叙事至关重要的关键事件。本文提出CALLMSAE，一个基于CAscading大型语言模型（LLMs）的SAlient Event图谱生成框架，它利用LLMs的能力，并避免了昂贵的人工标注需求。首先，通过提示LLMs生成摘要，我们识别出重要事件。然后，我们开发了一种迭代的代码精炼提示策略，用于生成事件关系图，消除错误的关系并恢复缺失的边。对基于上下文的图谱生成模型进行 fine-tuning，在使用 LLM 生成的图谱上表现出色，优于使用 CAEVO 生成数据训练的模型。在人类标注的测试集上的实验结果显示，我们的方法能生成更突出且准确的图谱，超越了竞争性的基线。|
|**2024-06-26**|**New intelligent empowerment for digital transformation**|Peng Yifeng et.al.|[2406.18440](http://arxiv.org/abs/2406.18440)|null|这项研究提出了一种基于大型语言模型（LLMs）的创新评估方法，用于衡量企业的数字化转型（DT）过程。通过对2005年至2022年间在纽约证券交易所和纳斯达克上市的4407家公司的年度报告进行分析，构建了一套全面的DT指标。研究结果显示，DT显著提高了企业的财务表现。然而，不同的数字技术对财务性能的影响各不相同，区块链技术的积极影响相对较小。此外，研究还发现DT通过提升运营效率和降低成本促进财务绩效增长。本研究为学术界提供了新的DT评估工具，同时拓宽了生成人工智能技术在经济研究中的应用范围。|
|**2024-06-26**|**IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons**|Dan Shi et.al.|[2406.18406](http://arxiv.org/abs/2406.18406)|**[link](https://github.com/danshi777/ircan)**|人们普遍认为，大型语言模型（LLMs）在大规模数据训练后蕴含着丰富的知识。然而，近期研究揭示了LLMs生成文本时的知识冲突问题，即模型内编码的参数知识（即知识库）与上下文提供的新知识存在矛盾。为解决这一问题，我们提出了一种新颖框架——IRCAN（识别和重权上下文感知神经元）。IRCAN首先利用整合梯度计算得到的上下文感知归因分数，来识别那些对处理语境至关重要 的神经元。接着，通过重新赋权，我们强化这些识别出的上下文相关神经元，从而引导LLMs生成更符合上下文新知识的响应。我们在多种模型和任务上的广泛实验表明，IRCAN不仅显著提升了处理知识冲突的能力，还提供了一个可扩展的、即插即用的解决方案，能够无缝融入现有模型中。|
|**2024-06-25**|**MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning**|Xiangyu Zhao et.al.|[2406.17770](http://arxiv.org/abs/2406.17770)|**[link](https://github.com/phoenixz810/mg-llava)**|**## 背景  多模态大型语言模型（MLLMs）在视觉理解任务上取得了显著进步。然而，大多数模型局限于处理低分辨率图像，这限制了它们在需要详细视觉信息的感知任务中的表现。在我们的研究中，我们提出了一种创新的MLLM——MG-LLaVA，通过引入多尺度视觉流，包括低分辨率、高分辨率和对象级特征，来增强模型的视觉处理能力。我们设计了一个额外的高分辨率视觉编码器，以捕捉精细细节，并通过卷积门融合网络与基础视觉特征融合。为了进一步提升模型的对象识别能力，我们结合了来自离线检测器确定的边界框的物体级别特征。MG-LLaVA仅使用公开可用的多模态数据进行指令调优，展现出卓越的感知能力。我们用不同规模的语言编码器（从38亿到340亿参数）实例化MG-LLaVA，以全面评估其性能。多项基准测试的结果表明，MG-LLaVA在同类参数量的现有MLLM中表现出色，证明了其出色的效率。代码将在https://github.com/PhoenixZ810/MG-LLaVA上开源。**|
|**2024-06-25**|**BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning**|Ercong Nie et.al.|[2406.17764](http://arxiv.org/abs/2406.17764)|null|## 背景 大型语言模型（LLMs）积累了丰富的参数知识，但由于重新训练成本高昂且对闭源模型不可行，更新这些知识变得困难。知识编辑（KE）作为一种可能的解决方案，允许在不损害整体性能的前提下更新LLMs的知识。基于“上下文学习”（ICL）的即席KE方法展现出巨大潜力，使得LLMs能够作为黑盒处理。过去，KE主要集中在英语环境，而当前以英语为中心的LLMs在跨语言KE方面的潜力尚未充分挖掘。为了推动这方面的更多研究，我们推出了BMIKE-53基准，该基准针对53种不同语言的三种KE任务类型进行评估。我们还提出了一种无梯度的KE方法——多语言上下文知识编辑（MIKE），并在BMIKE-53上进行了实验。我们的评估关注跨语言知识转移的可靠性、泛化性、局部性和可移植性，为未来跨语言KE的研究提供了有价值的观点和框架。我们的代码和数据已通过匿名仓库https://anonymous.4open.science/r/MIKE公开获取。|
|**2024-06-25**|**CaLMQA: Exploring culturally specific long-form question answering across 23 languages**|Shane Arora et.al.|[2406.17761](http://arxiv.org/abs/2406.17761)|**[link](https://github.com/2015aroras/calmqa)**|**## 背景  大型语言模型（LLMs）在长篇问答任务中广泛应用，它们需生成段落级别的答案来回应复杂问题。尽管英语的长篇问答研究已相当深入，涉及多种数据集和评估指标，但其他语言的研究却相对匮乏。为了弥补这一差距，我们推出了CaLMQA，一个包含2,600个跨23种语言的复杂问题集合，其中包括资源有限、鲜少研究的语言，如斐济语和基林迪语。我们的数据集既包括社区网络论坛上收集的自然出现的问题，也包含了由母语使用者撰写的题目，我们为此专门聘请了他们。这个过程产生了多样且复杂的题目，反映了文化主题（如传统、法律、新闻），以及母语使用者的语言习惯。  我们对一系列开源和闭源模型进行了自动评估，使用了我们新提出的CaLMScore指标，该指标能检测答案中的语言错误和重复词。结果显示，对于某些低资源语言，LLM生成的答案质量明显下降。我们在部分模型的人工评估中发现，对于具有文化特性的问题，模型表现显著低于文化中立的问题。这些发现强调了对LLM多语言能力及非英语长篇问答评价领域更深入研究的必要性。**|
|**2024-06-25**|**Accelerating Clinical Evidence Synthesis with Large Language Models**|Zifeng Wang et.al.|[2406.17755](http://arxiv.org/abs/2406.17755)|null|人工智能自动医学发现是许多人的梦想。为此，我们开发了一种名为TrialMind的生成式AI管道，旨在进行医学系统性回顾，涵盖研究搜索、筛选和数据提取阶段。该系统利用大型语言模型（LLMs）驱动每个环节，并引入专家监督以减少错误。为了评估性能，我们创建了TrialReviewBench基准数据集，它是一个定制的包含870份来自25篇元分析论文的临床研究标注数据，涵盖不同医疗治疗领域。结果显示，TrialMind显著提升了文献审查效率，在从超过2000万篇PubMed研究中检索相关研究时，召回率高达0.897至1.000。在筛选阶段，我们的方法优于基于传统语言模型嵌入的方法（召回率分别为0.227-0.246 vs. 0.000-0.102）。此外，我们的方法在结果提取方面超越了直接使用GPT-4的表现，准确率范围为0.65到0.84。我们还支持森林图中的临床证据综合，经八名人类标注员验证，他们普遍更偏好TrialMind，其在涉及的审查中胜出率为62.5%至100%。这些发现表明，基于LLM的临床证据合成方法，如TrialMind，能够促进可靠且高质量的临床证据合成，从而提升临床研究的效率。|
|**2024-06-25**|**Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language**|Amalie Brogaard Pauli et.al.|[2406.17753](http://arxiv.org/abs/2406.17753)|null|本文探讨了在面对大量试图影响我们的信息，如预告消息、辩论、带有政治色彩的新闻和宣传时，大型语言模型（LLMs）生成具有说服力文本的能力。不同于以往专注于特定领域或类型劝说的研究，我们进行了一项全面的分析，旨在测量和基准LLMs在被明确要求增强或减少说服力时，以及仅要求进行释义时产生说服性文本的程度。为此，我们创建了一个新的数据集——“Persuasive-Pairs”，包含一组由简短文本和LLM重写以放大或削弱说服力的文本对。我们对这些配对进行了多标注，按相对尺度评估其说服力。这个数据集不仅本身具有价值，还展示了如何使用它训练一个回归模型，预测文本对之间说服力的得分，从而能够对不同领域的LLMs进行评分和比较。最后，我们讨论了不同系统提示对LLaMA3产生的影响，值得注意的是，即使在仅要求释义的情况下，不同的“角色”提示也会显著改变文本中的说服力。这些发现强调了研究LLM生成文本中的说服语言的重要性。|
|**2024-06-25**|**LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users**|Elinor Poole-Dayan et.al.|[2406.17737](http://arxiv.org/abs/2406.17737)|null|在最新的大型语言模型（LLMs）展现出卓越性能的同时，关于它们的不可靠行为，如虚构和偏见的研究层出不穷。本研究探讨了LLMs的回答质量在信息准确性、真实性以及拒绝回答方面，如何随着三种用户特征的变化而变化：英语水平、教育程度和国籍。我们在三个最先进的LLMs和两个事实核查相关的数据集上进行了详尽实验，重点关注其真实性。研究结果表明，当前最先进的LLMs对英语能力较低、教育水平较低以及非美国籍用户的回答质量存在更明显的负面倾向，这使得这些模型对于其最弱势用户来说，并非可靠的信息来源。|
|**2024-06-25**|**FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model**|Feijie Wu et.al.|[2406.17706](http://arxiv.org/abs/2406.17706)|**[link](https://github.com/HarliWu/FedBiOT)**|大型语言模型（LLMs）在经过适当领域特定数据的微调后，在许多任务上展现出出色性能。然而，这类专用数据通常分布在多个所有者之间，这就提出了如何在联邦学习（FL）中进行LLM微调的问题。面对有限的计算和通信能力，FL客户端在有效微调大型语言模型时面临挑战。为此，我们介绍了FedBiOT，一种旨在提高资源效率的LLM微调FL方法。具体来说，我们的方法包括服务器生成一个压缩的LLM，并确保其性能与完整模型相当。然后，客户端针对这个压缩模型的一个轻量但重要的部分——适配器进行微调。值得注意的是，由于服务器无法访问客户端拥有的私人数据，服务器用于校准的数据分布与客户端用于微调的数据不同。我们将问题建模为一个带有数据不一致性影响的 bilevel 优化问题，并导出了服务器和客户端的更新规则。我们在 LLaMA-2 上进行了广泛实验，结果显示，适配器在重新整合到全局语言模型时表现出色。实验结果还表明，FedBiOT 相比现有基准显著减少了资源消耗，同时保持了相近的性能水平。|
|**2024-06-25**|**From Distributional to Overton Pluralism: Investigating Large Language Model Alignment**|Thom Lake et.al.|[2406.17692](http://arxiv.org/abs/2406.17692)|**[link](https://github.com/thomlake/investigating-alignment)**|**该研究分析了大型语言模型（LLMs）经过校准后输出分布的变化特性。首先，重新评估了之前关于校准后响应多样性降低的报告，发现这种下降主要归因于质量控制和信息整合。校准能够抑制不相关和无帮助的内容，同时使输出分布倾向于更长的、涵盖多个基础LLM响应信息的答案，实质上是将多样化信息汇总在单个响应中。研究并未发现校准显著减少有用信息，进而引出问题：校准模型是否会产生基础模型无法再现的信息？第二部分的研究结果表明，情况并非如此，校准模型的行为可以通过基础模型在无需微调的情况下进行复现。通过上下文示例和较低分辨率的语义提示，可以从基础LLMs引导出与校准后的相似响应，甚至与校准后的响应之间的相似度接近。这些发现支持“表面校准假设”，即当前的校准技术仅捕捉了助手型基础LLM行为中有用的部分，并未扩展其能力。此外，它们还显示，基于上下文的校准作为一种模仿校准LLMs的策略，效果出人意料地好，且无需微调。研究代码和数据可在<https://github.com/thomlake/investigating-alignment>获取。**|
|**2024-06-25**|**VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation**|Kun Qian et.al.|[2406.17681](http://arxiv.org/abs/2406.17681)|**[link](https://github.com/qbetterk/VarBench)**|随着大型语言模型在传统基准测试中的表现日益出色，越来越多的研究人员开始关注预训练期间的基准数据泄露问题，通常称为数据污染问题。为了确保公正的评估，最近的基准测试仅公开训练和验证集，对测试集标签保密。他们要求任何希望评估自己语言模型的人都需要提交模型的预测结果，进行集中处理，然后在排行榜上公布模型的得分。然而，这个提交过程既低效又妨碍了有效的错误分析。为解决这个问题，我们提出动态化基准测试并实时评估语言模型。具体来说，我们从每个测试案例中提取变量，并为每个变量定义一个值范围。每次评估时，我们会从这些值域中抽取新的值来创建独特的测试案例，从而保证每次都是全新的评估。  我们针对数学生成任务的GSM8K、多项选择任务的ARC、commonsense问答的CommonsenseQA以及TruthfulQA的真实性问答任务，应用了这种变量扰动方法。实验结果显示，这种方法能更准确地衡量语言模型的真实能力，有效缓解了数据污染问题。|
|**2024-06-25**|**Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models**|Yuan Li et.al.|[2406.17675](http://arxiv.org/abs/2406.17675)|null|大型语言模型（LLMs）展现出卓越的任务解决能力，日益扮演类似人类助手的角色。社会对将LLMs更广泛地融入其中产生了兴趣，探讨它们是否具备心理特质，以及这些特质是否稳定且有助于理解其行为。本文借鉴心理学测量学的方法，提出了一种框架，用于研究LLMs中的心理学，包括心理维度识别、评估数据集创建和结果验证。在此框架下，我们开发了一个全面的LLM心理测量基准，涵盖了六种心理维度：个性、价值观、情绪、心智理论、动机和智力。这个基准包含了十三个包含多样场景和题型的数据集。研究发现，LLMs展现出广泛的心理特性。同时，我们观察到LLMs在自我报告的特质与其实际行为之间的不一致。该论文详细展示了LLMs的心理测量评估，为AI和社会科学领域的可靠评估提供了洞见，以及可能的应用方向。|
|**2024-06-24**|**EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees**|Yuhui Li et.al.|[2406.16858](http://arxiv.org/abs/2406.16858)|**[link](https://github.com/safeailab/eagle)**|在现代大型语言模型（LLMs）的推理过程中，成本高且耗时。实验表明，投机取巧的抽样方法如EAGLE已证实有效。传统方法假设草稿树的接受率仅依赖于令牌的位置，然而我们发现这其实还取决于上下文。为此，我们在EAGLE的基础上提出了EAGLE-2，引入了一种新的上下文感知动态草稿树技术到起草建模中。这一改进利用了EAGLE的草稿模型校准良好的特性：草稿模型的信心分数能近似表示接受率，误差较小。我们在三个系列的LLMs和六个任务上进行了广泛评估，结果显示EAGLE-2的速度提升比率为3.05倍到4.26倍，比EAGLE-1快20%到40%。此外，EAGLE-2还能保持生成文本分布不变，因此是一个无损加速算法。|
|**2024-06-24**|**From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models**|Sean Welleck et.al.|[2406.16838](http://arxiv.org/abs/2406.16838)|null|现代研究中最引人注目的发现之一是，在大型语言模型（LLMs）的训练过程中增加计算资源会带来更好的性能。然而，对于推断时的优化方法的关注相对较少。这篇综述专门探讨了这些推断时间的方法。我们从统一的数学框架出发，考察了三个领域：逐词生成算法、元生成算法和高效生成。逐词生成算法，通常称为解码算法，通过一次抽样一个token或构建词级搜索空间，然后选择输出。这些方法通常假设能够访问语言模型的logits、下一个token分布或概率分数。元生成算法处理部分或完整序列，融入领域知识，支持回溯，并整合外部信息。高效生成方法旨在减少token成本，提高生成速度。我们的综述融合了来自传统自然语言处理、现代LLMs和机器学习系统三个研究社区的观点。|
|**2024-06-24**|**USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$ onversations**|Mounika Marreddy et.al.|[2406.16833](http://arxiv.org/abs/2406.16833)|null|在当前的背景下，识别用户在各种话题的长篇讨论中的观点和立场对于个性化、市场研究、政治竞选、客户服务、冲突解决、定向广告和内容管理至关重要。然而，手动标注数据以训练此类模型面临诸多挑战，如耗时昂贵、长对话可能引入噪声，以及用户观点转变的微妙之处可能导致解读困难。鉴于大型语言模型（LLMs）在复杂自然语言处理任务中的出色表现，本文尝试利用Mistral Large和GPT-4自动化两个关键任务的标注过程，并提供推理：一是用户立场分类，即在对话中对用户帖子的观点进行五级标注；二是用户固执程度分类，关注用户在整个对话中的总体意见，采用四级标注。通过在764个多用户Reddit对话上应用零样本、一示例和少量样例标注的多数投票，我们创建了USDC数据集。然后，我们使用这个数据集对多个小型部署语言模型进行微调和指令调整，用于执行五类立场和四类固执程度的分类任务。我们公开了代码和数据集：[https://anonymous.4open.science/r/USDC-0F7F]。|
|**2024-06-24**|**Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track**|Ronak Pradeep et.al.|[2406.16828](http://arxiv.org/abs/2406.16828)|**[link](https://github.com/castorini/ragnarok)**|## 背景  您可能体验过新的Bing搜索或Google AI概述？这些都反映出当前搜索引擎正逐步发展到基于检索增强生成（RAG）的系统。这类系统能整合实时数据到大型语言模型（LLMs），提供信息丰富、有来源且简洁的摘要，与传统的文档排名展示方式形成对比。因此，为了推动RAG系统评估的创新，我们提议在TREC 2024年增设RAG竞赛。本文详述了我们如何实现这一目标：描述了可复用框架Ragnar\"ok的设计，解释了MS MARCO V2.1语料库的选择，发布了竞赛开发话题，并标准化了用户接口定义，以便利用户。接下来，我们将利用Ragnar\"ok展示关键的工业基准，如OpenAI的GPT-4o和Cohere的Command R+。我们还推出了一个网页界面，用于互动式地比较不同RAG系统的性能，并通过众包方式进行评估。我们开源Ragnar\"ok框架和基准，旨在为未来的RAG系统建立统一的标准。|
|**2024-06-24**|**RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale**|Beck LaBash et.al.|[2406.16801](http://arxiv.org/abs/2406.16801)|**[link](https://github.com/qurrent-ai/res-q)**|**## 翻译  大型语言模型（LLMs）的指令跟随能力促使了一类能够处理复杂任务的系统发展，如对大型代码仓库进行编辑。鉴于LLMs对提示微调的高敏感性和不可预测性，迫切需要稳健的评估工具来推动这些系统的未来发展。我们提出RES-Q，一个针对 $\textbf{R}$epository $\textbf{E}$diting $\textbf{S}$ ystems的自然语言指令基准，它基于100个真实的GitHub提交构建了100个仓库编辑任务。给定编辑指令和代码仓库，RES-Q评估LLM系统获取信息并构造满足指令要求的编辑的能力。我们认为，这种评估方式优于传统方法，能全面评估模型的性能。  我们使用Qurrent OS开发的语言代理软件构建了一个仓库编辑系统，对该系统中的各种最先进的LLMs，如Claude Sonnet 3.5和GPT-4o，进行了评估。尽管在HumanEval上的1%精确度@1得分有所差异，但在RES-Q上，Claude Sonnet 3.5的1%精确度@1得分比GPT-4o高出12%，这表明RES-Q具有区分模型能力的潜力，随着传统基准接近饱和，它能提供更深入的洞察。  我们还研究了token效率、与现有基准的性能关联，以及封闭源和开源LLM之间的有趣差异。相关代码和数据集可在https://github.com/Qurrent-AI/RES-Q获取。**|
|**2024-06-24**|**Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs**|Ashwinee Panda et.al.|[2406.16797](http://arxiv.org/abs/2406.16797)|**[link](https://github.com/kiddyboots216/lottery-ticket-adaptation)**|**## 背景 当前的大规模语言模型（LLMs）适应新任务的方法并不适用于多任务适应，因为它们会修改所有模型权重，导致不同任务之间产生破坏性的干扰。这可能导致对先前任务的遗忘，使得同时在多个任务上获得良好性能变得困难。为了解决这个问题，我们提出了Lottery Ticket Adaptation（LoTA），这是一种稀疏适应方法，它识别并优化模型中的一个稀疏子网络。我们在诸如指令跟随、推理、数学和摘要等复杂任务上评估了LoTA。  ## 方法 LoTA通过发现和优化“彩票券”（或稀疏任务向量）来实现，这种方法优于全量微调和低秩适应（LoRA）。LoTA不仅表现出更好的性能，还能在训练其他任务后保持良好的表现，从而避免了灾难性遗忘。此外，通过提取和针对特定任务进行微调，LoTA还支持在高度不同的任务间进行模型融合。  ## 结论 总的来说，LoTA作为一种有效的稀疏适应策略，为多任务大语言模型的适应提供了新的解决方案，能够在处理多个任务时保持稳定且高效的表现。**|
|**2024-06-24**|**M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models**|Rishabh Maheshwary et.al.|[2406.16783](http://arxiv.org/abs/2406.16783)|null|## 背景  在大型语言模型（LLMs）遵循指令的校准过程中，微调（finetuning, IFT）至关重要。近期已经提出了一些有效的IFT数据集，但大多集中在高资源语言如英语上。本研究中，我们创新性地提出一个全合成的、基于Evol分类法引导的多语言、多轮指令微调数据集——M2Lingual，目标是提升LLMs在多样语言和任务上的表现。M2Lingual共包含182,000个IFT对，源自不同种子，涵盖70种语言、17个NLP任务以及通用的指令-响应对。  ## 目的与贡献  使用M2Lingual进行训练的LLMs性能显著优于大多数现有的多语言IFT数据集。更重要的是，经M2Lingual微调的模型在各种评估基准上展现出稳健的跨语言能力，无论是在我们的多语言、多轮翻译评价基准上，还是在多种多样的多语言任务中。因此，我们贡献了Evol分类法的两步方法，并公开了M2Lingual的数据集：https://huggingface.co/datasets/ServiceNow-AI/M2Lingual。|
|**2024-06-24**|**It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension**|Sagi Shaier et.al.|[2406.16779](http://arxiv.org/abs/2406.16779)|null|过去十年，自然语言处理领域取得了显著进步。然而，一些实践未经充分评估就已确立。针对阅读理解这一情况，我们首先提出问题：1）输入顺序（即问题和上下文）如何影响模型性能？鉴于近期在输入侧重领域的进展，我们进一步探究：2）强调问题、上下文或两者是否能提升表现？我们在3个数据集上测试了9种大型语言模型，发现先呈现上下文再给出问题可以提高模型性能，最高可达31%的准确率提升。此外，强调上下文的效果优于突出显示问题，而且对模型缺乏参数知识来回答的问题，针对性地强调输入部分尤其有效。通过尝试基于提示和注意力的强调方法，我们发现最有效的策略出人意料地简单：只需在输入中附加几个标记，就能实现高达36%的准确性提升，使得小型模型能够超越其大得多的同类模型。|
|**2024-06-24**|**Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech Translation System for IWSLT 2024**|Sai Koneru et.al.|[2406.16777](http://arxiv.org/abs/2406.16777)|null|## 背景  大型语言模型（LLMs）正在被广泛研究，以应用于诸如语音识别（ASR）、机器翻译（MT）甚至端到端语音翻译（ST）等任务。本文介绍KIT团队在受限+LLM赛道下的离线提交，我们通过整合最新技术改进了级联语音翻译系统。特别地，我们将Mistral-7B模型\footnote{mistralai/Mistral-7B-Instruct-v0.1}融入其中，从两个方面增强系统：一是利用我们的系统生成的N-best列表精炼ASR输出，通过微调LLM提高转录准确性；二是对MT输出进行文档级别的精炼，利用ASR和MT预测来提升翻译质量。结果显示，LLM的集成使得ASR的Word Error Rate下降了绝对0.3%，MT的COMET评分提高了0.65%。然而，在包含重叠说话者和背景噪音的挑战性测试集中，由于ASR性能不佳，LLM集成的效果不明显。为了改善在这种情况下可能缺失的上下文信息，我们采用了分块长形式解码的ASR方法。|
|**2024-06-24**|**WARP: On the Benefits of Weight Averaged Rewarded Policies**|Alexandre Ramé et.al.|[2406.16768](http://arxiv.org/abs/2406.16768)|null|### 翻译  强化学习从人类反馈（RLHF）通过训练奖励模型来调整大型语言模型（LLMs），使其生成的内容符合人类偏好。为了保持预训练知识，RLHF通常采用KL散度正则化，但这会限制奖励优化。为此，本文提出了一种新颖的对齐策略，称为权重平均奖励策略（WARP）。WARP在三个阶段在权重空间中融合策略：首先，它使用指数移动平均策略作为KL正则化的动态基准。其次，应用球面插值将独立微调的策略合并成一个增强模型。最后，线性插值在合并模型和初始模型之间进行，以恢复预训练特征。该过程迭代进行，每次迭代的最终模型用作下一轮的高级初始化，逐步优化KL与奖励之间的权衡，实现固定KL下的更高奖励。GEMMA策略的实验验证了WARP的优点，其质量和对齐性能优于开源的LLMs。|
|**2024-06-21**|**GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians**|Haoyang Liu et.al.|[2406.15341](http://arxiv.org/abs/2406.15341)|**[link](https://github.com/liu-hy/genotex)**|**## 翻译  近年来，机器学习的进步显著提升了从基因表达数据中识别疾病相关基因的能力。然而，这些过程往往需要深厚的专长和大量的人工努力，限制了其可扩展性。大型语言模型（LLMs）驱动的代理显示出在自动化此类任务方面的潜力，因为它们的问题解决能力日益增强。为了支持这类方法的评估和发展，我们创建了GenoTEX，这是一个基因表达数据分析自动探索的基准，包括数据集选择、预处理和统计分析任务。GenoTEX提供了全面的分析管道，其中包含了人类生物信息学家精心编写的注释，他们对数据集进行深入分析以确保准确性和可靠性。  为了提供这些任务的基线，我们设计了GenoAgents，这是一个基于LLMs的代理团队，具备上下文感知规划、迭代校正以及与领域专家咨询的能力，它们协作探索基因数据集。我们的实验显示了LLM驱动方法在基因组数据分析中的潜力，而错误分析指出了挑战和未来的改进方向。我们提议GenoTEX作为一个有前景的资源，用于衡量和提升人工智能驱动的基因组数据分析方法。我们的基准已公开发布在：\url{https://github.com/Liu-Hy/GenoTex}。**|
|**2024-06-21**|**Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance**|Haoling Li et.al.|[2406.15330](http://arxiv.org/abs/2406.15330)|null|大型语言模型（LLMs）已经在众多研究领域带来了革新。尽管人们普遍知道微调对于增强LLMs的功能至关重要，但现有研究表明，微调过程中可能存在参数冗余。因此，有研究建议只更新部分参数，但这未能有效利用任务特定信息来识别训练中的重要参数。考虑到梯度本质上蕴含着任务相关数据的信息，我们提出了梯度掩码调优（Gradient-Mask Tuning，GMT）方法，该方法根据参数的梯度信息选择性地进行训练更新。具体来说，我们计算梯度的绝对值，并对较小幅度的梯度应用掩码。我们的实验结果表明，GMT不仅优于传统的微调方法，还提升了LLM性能的上限。进一步分析显示，GMT对掩码比例具有一定的鲁棒性，并且在计算效率上与基本的微调（Simple Fine-Tuning，SFT）相当。|
|**2024-06-21**|**Bug In the Code Stack: Can LLMs Find Bugs in Large Python Code Stacks**|Hokyung Lee et.al.|[2406.15325](http://arxiv.org/abs/2406.15325)|**[link](https://github.com/hamminghq/bug-in-the-code-stack)**|近年来，针对针对于大型语言模型（LLMs）在海量文本文档中检索上下文信息的Needle-in-a-Haystack（NIAH）基准研究有所进展。随着LLMs在软件开发流程中的日益融合，评估它们在代码环境中的表现变得至关重要。随着LLMs朝着程序合成方向发展，必须确保它们能理解语法并编写出符合语法规则的代码。为此，我们设计了Bug In The Code Stack（BICS）基准测试，旨在检验LLMs识别简单语法错误的能力于大型源代码中。我们的研究发现三个关键点：（1）与文本环境相比，基于代码的环境对检索任务构成了更大的挑战；（2）不同模型之间的性能存在显著差异；（3）尽管如此，较长的上下文长度与性能下降之间存在关联，但这种下降程度在不同的模型间有所不同。|
|**2024-06-21**|**Towards Fine-Grained Citation Evaluation in Generated Text: A Comparative Analysis of Faithfulness Metrics**|Weijia Zhang et.al.|[2406.15264](http://arxiv.org/abs/2406.15264)|null|大型语言模型（LLMs）常常产生不可靠或难以验证的信息，即“幻觉”。为解决这个问题，检索增强的LLMs引入了引用，使内容基于可核查的来源。然而，手动评估引用是否充分支持相关陈述仍然是一个重大挑战。先前的研究试图通过信仰度指标自动估计引用的支持程度，但这些方法仅限于二分类，忽视了实际场景中对精细级别引用支持的考量。为了探究信仰度指标在精细级别评估中的有效性，我们提出了一种比较评估框架，用于检验这些指标在区分三种支持等级（全面、部分和无支持）之间的能力：全面支持、部分支持和不支持。我们的框架采用相关性分析、分类评估和检索评估，全方位衡量指标分数与人类判断的一致性。研究结果显示，没有单一指标在所有评估中表现出色，揭示了精细级别支持评估的复杂性。根据发现的结果，我们为开发更有效的指标提供了实用建议。|
|**2024-06-21**|**Detecting Synthetic Lyrics with Few-Shot Inference**|Yanis Labrak et.al.|[2406.15231](http://arxiv.org/abs/2406.15231)|null|近年来，生成的音乐内容逐渐受到关注，大型语言模型被有效应用于创作各种风格、主题和语言结构的歌词，这推动了艺术家们的创作，但也带来了版权侵犯、消费者满意度和内容滥发等问题。为此，检测生成歌词的方法变得至关重要。然而，现有的研究并未专注于这一特定领域或创意文本的机器生成内容检测。针对这一空白，我们精心构建了首个高质量合成歌词数据集，并对多种基于少量样本的检测方法进行了详尽的定量评估，测试它们的泛化能力，并辅以人类评价。结果显示，我们的最佳少数样本检测器——基于LLM2Vec的方法超越了在其他领域表现强劲的风格和统计方法，成功鉴别出人类创作与机器生成的歌词，且展现出良好的跨艺术家和模型泛化能力，还能有效识别生成后的人工润色。这项研究强调了在创意内容检测领域，特别是泛化能力和对更大歌曲库的适应性方面，需要进一步研究。所有数据集、预处理脚本和代码已公开在GitHub和Hugging Face上，遵循Apache 2.0许可协议。|
|**2024-06-21**|**A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation**|Irune Zubiaga et.al.|[2406.15227](http://arxiv.org/abs/2406.15227)|**[link](https://github.com/hitz-zentroa/cn-eval)**|随着网络上错误信息和有害言论的增多，迫切需要有效的反叙事（Counter Narrative，CN）生成技术。然而，现有的自动评估方法往往缺乏可解释性，无法准确反映生成的CN与人类感知之间的复杂关系。为此，本文提出了一种新颖的方法来评估生成的CN，即利用大型语言模型（Large Language Model，LLM）作为评估器。通过以锦标赛形式对生成的CN进行对战比较，我们建立了一个模型排名流程，其与人类偏好间的相关系数达到0.88。此外，我们还探讨了使用LLM进行零样本（Zero-Shot，ZS）CN生成的能力，对比分析了聊天、指令和基础模型的性能和局限性。通过细致的评估，包括微调实验，我们揭示了在特定领域数据下的响应差异。结论是，对于执行这项任务，如果能避免因安全顾虑而拒绝生成，聊天导向的ZS模型可能是最佳选择。|
|**2024-06-21**|**Unsupervised Extraction of Dialogue Policies from Conversations**|Makesh Narsimhan Sreedhar et.al.|[2406.15214](http://arxiv.org/abs/2406.15214)|null|## 翻译  对话策略在构建任务导向的对话系统中至关重要，但其开发和维护往往需要对话建模专家的大量投入。尽管在许多情况下，手头有大量的对话数据，但人们缺乏有效的方法从这些数据中提取对话策略。为此，本文通过展示大型语言模型（LLMs）如何在对话数据转化为统一的中间表示——规范形式的过程中发挥作用，填补了这一空白。接着，我们提出了一种新颖的利用可控且可解释的图基方法生成对话策略的技术。通过将对话中的规范形式整合成流程网络，我们发现运行图遍历算法有助于提取对话流程。相比仅依赖LLM提取的流程，这些流程更好地反映了底层交互。我们的方法旨在赋予对话设计者更大的控制力，提供一个提升对话策略开发效率的工具。|
|**2024-06-21**|**Prompting Whisper for QA-driven Zero-shot End-to-end Spoken Language Understanding**|Mohan Li et.al.|[2406.15209](http://arxiv.org/abs/2406.15209)|null|## 背景 零样本语音语言理解（SLU）使系统能够在无需先前训练数据的新领域理解用户话语。当前的研究往往依赖大型语言模型（LLMs），导致庞大的存储需求和复杂性。本文提出使用 Whisper，一个独立的语音处理模型，来进行零样本端到端（E2E）SLU。为处理未见过的语义标签，我们将SLU任务融入问答（QA）框架中，通过提示Whisper解码器进行语义推断。我们采用前缀调优方法高效地训练该系统，只优化少量参数，而不是整个Whisper模型。实验结果显示，我们的提议系统在SLURP上的槽位填充（SLU-F1）得分比最近引入的零样本基准提高了40.7%。此外，在既定和跨领域评估环境下，它与基于Whisper-GPT-2的模块化系统表现相当，但模型参数减少了34.8%。|
|**2024-06-21**|**Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms**|Santiago Berrezueta-Guzman et.al.|[2406.15198](http://arxiv.org/abs/2406.15198)|null|注意力缺陷多动障碍（ADHD）是一种神经发育障碍，其特征为注意力不集中、过度活跃和冲动，严重影响个体的日常生活和生活质量。职业疗法在ADHD管理中扮演着关键角色，通过培养日常生活所需的技能，提升个体在学校、家庭和社会环境中全面参与的能力。近期研究强调了大型语言模型（如ChatGPT和社交辅助机器人）在心理治疗中的潜在价值，以弥补现有疗法的局限，提供定制化的支持并适应个体的独特需求。然而，关于这些先进技术在ADHD疗法中的联合应用研究尚存在较大空白。因此，我们整合了ChatGPT-4 Turbo和Claude-3 Opus两个先进语言模型到一个机器人助理中，以考察它们在机器人辅助互动中的性能，并在一个模拟治疗场景中比较它们与临床验证的定制模型的效果。研究结果显示，ChatGPT-4 Turbo在性能和响应速度上表现出色，适合于时间敏感的应用。而Claude-3 Opus在理解、连贯性和伦理考量方面表现出优势，强调安全和吸引人的互动。两者都展现出创新和适应性，但ChatGPT-4 Turbo在集成简易度和语言支持方面更具优势。选择哪个模型取决于ADHD疗法的具体需求。|
|**2024-06-21**|**UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis**|Yulong Hui et.al.|[2406.15187](http://arxiv.org/abs/2406.15187)|**[link](https://github.com/qinchuanhui/uda-benchmark)**|**## 翻译  尽管检索增强生成（Retrieval-Augmented Generation, RAG）技术提升了大型语言模型（Large Language Models, LLMs）与外部数据的协作能力，但在现实场景中仍面临诸多挑战。特别是在学术文献和金融问答等领域，数据常常以HTML或PDF格式的冗长、结构复杂的文本和表格形式存在。为此，我们提出一个名为“Unstructured Document Analysis”（UDA）的新基准，它包含2,965份真实世界的文档和29,590个专家标注的问答对。我们重新审视了基于LLM和RAG的方法在处理文档分析任务中的设计决策，并在多个文档领域和多样化的查询类型上评估答案质量和策略。  我们的评估揭示了有趣的结果，强调了数据解析和检索的重要性。我们希望这个基准能够为现实世界的文档分析应用提供启示，并为其发展服务。基准套件和代码已可在<https://github.com/qinchuanhui/UDA-Benchmark>获取。**|
|**2024-06-20**|**Model Merging and Safety Alignment: One Bad Model Spoils the Bunch**|Hasan Abed Al Kader Hammoud et.al.|[2406.14563](http://arxiv.org/abs/2406.14563)|null|## 背景 大型语言模型（LLMs）的合并是一种经济高效的方法，可以将多个专家级LLMs整合成一个全能模型，保留原始模型的专业知识。然而，当前的方法往往忽视了合并过程中安全对齐的重要性，导致生成的模型高度不一致。本研究探讨了模型合并对对齐性的影响。我们评估了几种流行的模型合并技术，发现现有方法不仅传递了领域专业知识，还传播了不一致性。为此，我们提出了一种两步法解决方案：(1) 生成合成的安全性和领域特定数据，(2) 将这些生成的数据融入现有的数据驱动的模型合并优化过程中。这样，我们能够将对齐性视为可以最大化于合并后LLM中的能力。实验表明，在合并过程中整合对齐相关数据的有效性，结果是既能保持领域专长又能实现良好对齐的模型。|
|**2024-06-20**|**Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities**|Sachit Menon et.al.|[2406.14562](http://arxiv.org/abs/2406.14562)|null|当面临涉及视觉思维的问题时，人类会自然地切换到推理模式，常常形成心理图像或绘制视觉辅助工具。大型语言模型在数学和符号推理方面展现出良好表现，通过文本形式表达中间推理步骤的链条思考，但在处理可以通过视觉推理轻松解答的文本查询时仍存在问题，即使经过大量的多模态预训练也是如此。我们提出了一种简单方法，即“白板思维提示”，来解锁多模态大型语言模型在跨模态中的视觉推理能力。白板思维提示为模型提供了一个比喻性的“白板”，让其以图像形式展现推理步骤，然后将这些图像返回模型进行进一步处理。我们发现这种方法无需示范或专用模块，而是利用模型现有的使用Matplotlib和Turtle等库编写代码的能力。这个简单策略在四个涉及视觉和空间推理的困难自然语言任务中实现了最先进的结果。我们发现，与链式思考相比，GPT-4o在某些场景下大幅失败，包括一些准确率为0%的情况下，而白板思维提示能提升至高达92%的准确性。我们详细探讨了该技术的成功之处及其错误来源。|
|**2024-06-21**|**Asynchronous Large Language Model Enhanced Planner for Autonomous Driving**|Yuan Chen et.al.|[2406.14556](http://arxiv.org/abs/2406.14556)|**[link](https://github.com/memberre/asyncdriver)**|尽管实时规划器在自动驾驶中表现出色，但大型语言模型（LLMs）的兴起为提高运动规划的可解释性和可控性开辟了新途径。然而，LLM驱动的规划器仍面临资源消耗大和推理时间长的问题，这阻碍了其实用部署。鉴于这些挑战，我们提出了AsyncDriver，一个全新的异步LLM增强的闭环框架。该框架利用LLM生成的与场景相关的指令特征，指导实时规划器进行精确和可控的轨迹预测。AsyncDriver展示了LLMs在理解和处理向量化场景数据及一系列路线指示方面的强大能力，同时通过异步设计，有效降低了LLM带来的计算成本，保持了与之相近的性能。实验表明，我们的方法在nuPlan的复杂场景中实现了更优的闭环评估性能。|
|**2024-06-20**|**GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models**|Shilong Li et.al.|[2406.14550](http://arxiv.org/abs/2406.14550)|null|长文本处理能力对于大型语言模型（LLMs）应对复杂任务至关重要。尽管已有多方努力优化LLMs处理长输入，但依然面临挑战。本文提出GraphReader，这是一种基于图的代理系统，旨在通过构建文本图并让代理自主探索来处理长文本。当接收到问题时，代理会逐步分析并制定合理计划，然后调用预定义函数读取节点内容和邻居信息，实现从粗到细的图探索。在探索过程中，代理不断记录新发现并反思当前情况，以优化获取信息的过程，直到收集足够信息生成答案。在LV-Eval数据集上的实验显示，使用4k上下文窗口的GraphReader在16k到256k的长文本长度上，相对于GPT-4-128k有显著优势。此外，我们的方法在四个单跳和多跳的挑战性基准上表现出色。|
|**2024-06-20**|**Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models**|Sunny Duan et.al.|[2406.14549](http://arxiv.org/abs/2406.14549)|null|随着大型语言模型的兴起，自然语言处理任务发生了革命性变化，但这也引发了数据隐私和安全的重大忧虑。这些模型在包含潜在敏感或专有信息的大量语料库上进行训练，数据泄露的风险——即模型响应揭示部分信息——尚不为人充分理解。本研究旨在探讨机器学习模型中的记忆现象，特别是关注其在训练过程中的演变。我们调查了训练数据的统计特性如何影响模型内编码的记忆，通过评估重复对记忆的影响。研究发现，模型记住一个序列的概率与它在数据中出现的次数呈对数关系。此外，我们发现即使没有后续的接触，某些看似未被记住的序列也可能在整个训练过程中逐渐显现。这种隐藏的已记住序列对数据隐私构成挑战，因为它们可能隐藏在模型的最终检查点中。因此，我们开发了一种诊断测试，通过考虑它们的交叉熵损失来揭示这些潜在的记忆序列。|
|**2024-06-20**|**Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data**|Johannes Treutlein et.al.|[2406.14546](http://arxiv.org/abs/2406.14546)|**[link](https://github.com/choidami/inductive-oocr)**|**针对大型语言模型（LLMs）的安全风险，一个策略是从其训练数据中删除危险知识。尽管这消除了显性信息，但隐性信息可能仍散落在多个训练文档中。我们研究的问题是：LLMs能否通过拼凑这些隐含线索，推断出被屏蔽的知识？为此，我们专注于无上下文归纳推理（Inductive Out-of-Context Reasoning，OOCR），这是一种泛化能力，要求LLMs根据分布在训练文档中的证据推断潜在信息，并在无需上下文学习的情况下应用于下游任务。通过五个任务的实验，我们展示了前沿LLMs确实具备这种能力。例如，在一项实验中，仅对一个未知城市与其与其他已知城市之间的距离进行微调，令人惊讶的是，即使没有示例或链式思考，该LLM也能表述出未知城市是巴黎，并据此解答后续问题。进一步的实验表明，仅接受单个硬币抛掷结果训练的LLMs能判断硬币是否偏斜，而只接触 $(x, f(x))$对的模型能阐述$f$ 的定义并计算逆运算。虽然OOCR在某些情况下表现良好，但我们也发现它并不总是可靠的，特别是在小型LLMs学习复杂结构时。总的来说，LLMs无需明确的上下文学习就能“串联起”信息，这给监控和控制它们获取的知识带来了潜在挑战。**|
|**2024-06-20**|**Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems**|Đorđe Klisura et.al.|[2406.14545](http://arxiv.org/abs/2406.14545)|null|关系数据库在现代信息系统中至关重要，是存储、查询和管理数据的核心。随着大语言模型的进步，文本到SQL技术崭露头角，极大地提升了从数据库中获取信息的能力，但同时也引发了关于隐私和安全的担忧。我们的研究专注于提取文本到SQL模型所依赖的数据库模式元素。了解模式可能使SQL注入攻击更为容易。为此，我们设计了一种零知识框架，通过提出精心构造的问题，无需直接了解数据库，该框架能促使这些模型处理这些问题并生成输出，从而揭示数据库模式结构。我们将此方法应用于针对文本-SQL对进行过微调的专用文本到SQL模型以及用于SQL生成的生成式语言模型。结果显示，对于微调模型，我们能够以接近0.75的F1分数重构表名，而对于生成式模型，这一分数更是高达0.96。|
|**2024-06-20**|**Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs**|Yuxuan Qiao et.al.|[2406.14544](http://arxiv.org/abs/2406.14544)|**[link](https://github.com/sparksjoe/prism)**|**## 翻译  视觉语言模型（VLMs）在处理各种视觉问题时展现出卓越的能力，这要求模型具备强大的感知和推理能力。然而，由于感知和推理在现有VLM中的交织性，独立评估这两方面的能力颇具挑战。为此，我们提出了一种创新框架——Prism，旨在分离视觉理解和推理在视觉问答中的作用。Prism分为两个阶段：感知阶段利用VLM提取并以文本形式表达视觉信息；推理阶段则根据提取的视觉信息，通过大型语言模型（LLM）生成响应。这种模块化设计使得我们可以系统地比较和评估不同VLM的感知和推理性能。  我们的分析框架提供了诸多洞见，证明了Prism作为成本效益高的视觉语言任务解决方案的潜力。通过将专注于感知的简化VLM与专为推理设计的强大LLM相结合，Prism在通用视觉语言任务上取得了优异成绩，同时显著降低了训练和运营成本。定量评估显示，当Prism配备基础的2B LLaVA VLM和开源的GPT-3.5时，其在严谨的多模态基准MMStar上的表现可与大十倍的VLM相当。该项目已发布在：https://github.com/SparksJoe/Prism。**|
|**2024-06-21**|**Are LLMs Naturally Good at Synthetic Tabular Data Generation?**|Shengzhe Xu et.al.|[2406.14541](http://arxiv.org/abs/2406.14541)|**[link](https://github.com/anonymou9167/anonymouscode)**|**大型语言模型（LLMs）在生成文本和图像方面表现出色，但其在生成最常见的数据类型——表格数据方面的潜力却鲜有研究。这篇论文指出，直接使用或经过传统微调的LLMs在作为合成表格生成器时表现极差。由于LLMs的自回归特性，随机顺序排列的微调与捕捉功能性依赖的重要性相悖，导致它们无法处理条件混合分布（这是反映现实世界约束的关键）。我们展示了如何通过使LLMs变得感知排列顺序来改善这些不足，从而提升其性能。**|
|**2024-06-20**|**PostMark: A Robust Blackbox Watermark for Large Language Models**|Yapei Chang et.al.|[2406.14517](http://arxiv.org/abs/2406.14517)|**[link](https://github.com/lilakk/postmark)**|**最有效的检测生成式语言模型（LLM）文本的方法是通过在解码过程中插入可识别的标记，即水印。然而，大多数现有方法依赖于获取到LLM的原始概率（logits），这使得LLM服务提供商不愿分享，因为担心模型泄露问题。因此，这些水印需要每个提供者独立开发。本文提出了一种创新的后处理水印方案，名为PostMark。它是一种模块化的、生成后插入的水印策略，无需触及logits，适合第三方实施。PostMark表现出更强的对抗同义句攻击能力：我们在实验中涵盖了八个基础算法、五个基线LLM和三个数据集。此外，我们还评估了PostMark对文本质量的影响，包括自动化和人工评估，探讨了质量和抗改写攻击之间的权衡。研究代码、输出和注释已公开在https://github.com/lilakk/PostMark。**|
|**2024-06-18**|**DrVideo: Document Retrieval Based Long Video Understanding**|Ziyu Ma et.al.|[2406.12846](http://arxiv.org/abs/2406.12846)|null|当前的长视频理解方法主要关注时长仅十几秒的视频，对处理更长视频的技术探索有限。长视频中的大量帧数带来了两个主要挑战：难以定位关键信息和进行长期推理。因此，我们提出DrVideo，一个基于文档检索的系统，专为长视频理解设计。我们的核心思想是将长视频理解问题转化为长文档理解任务，以充分利用大型语言模型的强大能力。具体来说，DrVideo将长视频转换为文本形式的长文档，首先检索关键帧并增强这些帧的信息，作为系统的起点。然后，它采用基于代理的迭代循环，持续搜索缺失信息、补充相关数据，并在收集到足够的与问题相关的信息后，以链式思考的方式给出最终预测。在多个长视频基准上的实验验证了我们方法的有效性。DrVideo在EgoSchema（3分钟）测试中比现有最先进的方法高出3.8个百分点，在MovieChat-1K（10分钟）的break模式和global模式中分别提高17.9和38.0分，以及在LLama-Vid QA（超过60分钟）数据集上提升30.2分。|
|**2024-06-18**|**Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts**|Haoxiang Wang et.al.|[2406.12845](http://arxiv.org/abs/2406.12845)|**[link](https://github.com/RLHFlow/RLHF-Reward-Modeling)**|**强化学习从人类反馈（RLHF）已经成为大型语言模型（LLMs）与人类偏好对齐的主要方法。传统上，通过使用人类偏好数据训练奖励模型（RM），过程通常从比较同一用户请求的响应开始，相对评分指示人类更喜欢哪个响应。然而，由于RM的黑盒特性，其输出缺乏可解释性，人们难以理解为什么RM认为某个回复是好的。鉴于RM作为人类偏好的代理，我们提议采用两阶段方法来创建可解释的RM：首先，使用多维绝对评分数据训练绝对评级多目标奖励模型（ArmoRM），每个维度对应于人类可理解的目标（如诚实、详尽、安全）；其次，利用混合专家（MoE）策略，结合一个门控网络，根据上下文自动选择最合适的奖励目标。我们成功地使用Llama-3 8B训练了ArmoRM，并在顶部添加了一个浅层MLP作为门控网络，形成了ArmoRM-Llama3-8B。我们的模型在评估RM的语言建模性能的RewardBench基准上实现了最先进的成绩。值得注意的是，我们的模型在性能上超过了使用GPT-4法官的LLM作为评判者的方法，并接近于规模更大的Nemotron-4 340B奖励模型的水平。**|
|**2024-06-18**|**Synergizing Foundation Models and Federated Learning: A Survey**|Shenghui Li et.al.|[2406.12844](http://arxiv.org/abs/2406.12844)|null|近期，大型语言模型、视觉Transformer和多模态模型等基础模型（FMs）的发展在学术界和工业界产生了显著影响。与小型模型相比，FMs在预训练阶段对大量数据的需求更大。尽管通用FMs可以使用互联网上的公开数据进行预训练，但针对特定领域的FMs需要专有数据，这在实际应用中因隐私问题而面临数据可用性挑战。联邦学习（FL）作为一种协作学习范式，打破了数据共享的障碍，为利用分布式数据定制和适应各种领域特定任务的FMs提供了前景，同时保护了数据隐私。这篇综述论文探讨了FL与FMs融合的潜力与挑战，总结了核心技术、未来发展方向以及应用场景。关于FM-FL的定期更新论文集合可在<https://github.com/lishenghui/awesome-fm-fl>获取。|
|**2024-06-18**|**LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation**|Seyedarmin Azizi et.al.|[2406.12832](http://arxiv.org/abs/2406.12832)|**[link](https://github.com/arminazizi98/lamda)**|**在大语言模型微调领域，低秩适应（LoRA）已经成为标准方法，因为它显著减少了可训练参数。然而，随着模型嵌入维度的增加，LoRA所需的可训练参数量也随之上升，导致计算成本较高。此外，其后向更新需要存储高维中间激活和优化器状态，对GPU内存需求较大。为此，本文提出了一种新的大语言模型微调方法——基于谱分解的低维适应（LaMDA）。LaMDA通过冻结第一投影矩阵（PMA），同时引入一个低维可训练的平方矩阵，实现了可训练参数和峰值GPU内存使用的大幅减少。在早期的微调阶段，LaMDA逐步冻结第二投影矩阵（PMB），进一步降低权重更新的计算成本，提高参数效率。  我们还引入了增强版LaMDA++，它通过规范化预训练模型权重的谱分析，实现轻量级的LoRA路径自适应秩分配。我们在多个任务上进行了评估，包括GLUE自然语言理解基准、文本摘要、自然语言生成以及复杂推理，应用于不同类型的大型语言模型。实验结果显示，LaMDA在性能上与现有方法相当或超越，且在微调期间可减少高达17.7倍的参数更新次数，以及1.32倍的峰值GPU内存使用。我们将公开代码。**|
|**2024-06-18**|**Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?**|Pinzhen Chen et.al.|[2406.12822](http://arxiv.org/abs/2406.12822)|null|## 背景 大型多语言模型旨在服务不同语种的母语使用者。我们推测，当前针对这些模型的微调和评估方法可能与其初衷不符，原因在于过度依赖翻译，可能导致翻译中的瑕疵。尚不清楚指令数据的性质如何影响模型输出，同时，用翻译测试集来捕捉这些细微差别是否有效。由于训练和评估阶段常常结合使用翻译数据，这些潜在问题可能被忽视。本研究通过在指令调优和评估阶段使用控制性的母语或翻译数据，来探究这些问题，并观察模型表现。我们在八种基础模型和八个不同基准上进行实验，结果显示，对于母语或生成性基准，使用母语或翻译指令数据时，模型性能高时，两者之间的差异尤为明显，而在其他类型的测试集上则不然。最后，我们发现正则化对于结构化任务有益，但对于生成性任务则不然。|
|**2024-06-18**|**Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?**|Zhe Yang et.al.|[2406.12809](http://arxiv.org/abs/2406.12809)|**[link](https://github.com/QwenLM/ConsisEval)**|大型语言模型（LLMs）展现了令人印象深刻的性能，但它们仍存在不一致的问题，例如对重述或微小顺序变化的反应不一致。除了这些不稳定性，我们还观察到尽管LLMs能够解决难题，但在相对简单的任务上却可能失败。为了评估这种从难到易的不一致性，我们创建了ConsisEval基准，其中每个条目包含两个难度有序的问题。我们还引入了一致性分数的概念，以量化这种不一致性，并分析通过相对一致性分数改进一致性潜力。通过对现有模型的广泛实验，我们得出以下发现：(1) GPT-4获得92.2%的最高一致性分数，但仍因冗余信息的干扰、问题误解等问题对特定问题不一致；(2) 能力更强的模型通常表现出更高的一致性，但也存在例外情况；(3) 对于 Fine-tuning 和上下文学习而言，硬数据可以提高一致性。我们的数据和代码将在GitHub上公开提供。|
|**2024-06-18**|**Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**|Zehao Wang et.al.|[2406.12806](http://arxiv.org/abs/2406.12806)|null|**背景**：配置设置对于调整软件行为以满足特定性能需求至关重要，但错误配置普遍存在。由于配置项众多且复杂，识别影响系统性能的配置是一项挑战。本研究提出PerfSense，这是一个轻量级框架，利用大型语言模型（LLMs）高效地识别性能关键配置，同时保持低开销。PerfSense利用LLM代理模拟开发者和性能工程师之间的交互，采用先进的提示链技术和检索增强生成（RAG）等技术。  **方法与成果**：我们在七个开源Java系统上的评估显示，PerfSense在分类性能敏感配置方面的平均准确率为64.77%，优于基于LLM的基线（50.36%）和先前的最佳方法（61.75%）。特别是，我们的提示链技术提高了召回率10%至30%，而保持了相似的精确度。进一步的手动分析362个误分类案例，发现常见问题包括LLMs对需求的理解偏差（占26.8%）。  **结论**：PerfSense显著减少了手动分类性能关键配置的工作量，并为未来的LLM基于代码分析研究提供了有价值的观点。|
|**2024-06-18**|**Supporting Human Raters with the Detection of Harmful Content using Large Language Models**|Kurt Thomas et.al.|[2406.12800](http://arxiv.org/abs/2406.12800)|null|本文探讨了利用大型语言模型（LLMs）自动或辅助人类审阅者检测有害内容的可能性，如仇恨言论、骚扰、极端主义和选举误导。通过50,000条评论的数据集，我们发现LLMs在与人类判断相比时能达到90%的准确率。我们提出五种设计模式，以整合LLMs与人工评级，例如预筛选非暴力内容、检测人类评级可能的错误，或者提供关键上下文以支持人工评级。我们展示了如何使用一个优化的提示来支持这些设计模式。在实际应用的试点中，我们的方法在优化人力资源效率方面实现了41.5%的提升，同时在检测违规内容的精确度和召回率上分别提高了9%至11%。|
|**2024-06-18**|**ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools**|Team GLM et.al.|[2406.12793](http://arxiv.org/abs/2406.12793)|**[link](https://github.com/thudm/chatglm-6b)**|我们介绍ChatGLM，这是一个随时间不断发展的大型语言模型系列。本报告主要关注GLM-4语言系列，包括GLM-4、GLM-4-Air和GLM-4-9B，它们代表了我们当前最强大的模型，集成了前三代ChatGLM的所有经验和教训。这些模型经过了十万亿次训练，主要涵盖中文和英语，以及少量来自24种语言的语料库，侧重于中英文的对齐。高质量的对齐是通过多阶段的后训练过程实现的，包括监督微调和学习人类反馈。评估显示，GLM-4在通用指标如MMLU、GSM8K、MATH、BBH、GPQA和HumanEval上接近或优于GPT-4；在IFEval指令跟随任务中的表现接近GPT-4 Turbo；在长文本任务上与GPT-4 Turbo（128K）和Claude 3相当；在中文对齐方面，GLM-4优于GPT-4，根据AlignBench衡量。GLM-4 All Tools模型进一步进行了对齐，以理解用户意图并能自主决定何时使用哪种工具，如Web浏览器、Python解释器、文本转图像模型和自定义函数，以有效地完成复杂任务。在实际应用中，它在诸如通过网络浏览获取信息和使用Python解释器解题等任务上与GPT-4 All Tools相匹配甚至超越。到目前为止，我们已经开源了一系列模型，包括ChatGLM-6B（三代）、GLM-4-9B（128K、1M）、GLM-4V-9B、WebGLM和CodeGeeX，在2023年仅Hugging Face上就有超过1000万次下载。这些开源模型可通过<https://github.com/THUDM>和<https://huggingface.co/THUDM>访问。|
|**2024-06-18**|**UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions**|Xunzhi Wang et.al.|[2406.12784](http://arxiv.org/abs/2406.12784)|**[link](https://github.com/Cyno2232/UBENCH)**|随着大型语言模型（LLMs）的迅速发展，它们在实际应用中展现出显著的效果。然而，由于低可解释性，这些模型在未预见情况下常会出现错误，限制了其价值。尽管已有许多研究致力于构建全面的评估体系，但先前的基准测试主要关注问题解决能力，对响应的不确定性评估不足，可能导致不稳定性。当前的方法在衡量LLM可靠性时资源消耗大，且难以测试黑盒模型。  为解决这些问题，我们提出了UBENCH，一个全面的LLM可靠性评估基准。它包含3,978个涵盖知识、语言理解、推理能力的多选题。实验结果显示，UBENCH达到了最先进的性能，并且其单次采样方法显著节省了计算资源，相较于需要多次采样的基线方法更为高效。此外，我们利用UBENCH评估了15种流行LLM的可靠性，发现GLM4表现出色，紧随其后的是GPT-4。我们还探究了Chain-of-Thought提示、角色扮演提示、选项顺序和温度对LLM可靠性的影响，分析了它们对不同模型的不同作用。|
|**2024-06-17**|**LLaNA: Large Language and NeRF Assistant**|Andrea Amaduzzi et.al.|[2406.11840](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型（MLLM）在理解和处理图像和3D数据方面表现出色，但它们在全面捕捉物体的外观和几何特性上存在局限。近期，神经辐射场（Neural Radiance Fields，简称NeRF）作为一种新兴的表示方式，通过一个简单的多层感知器（Multi-Layer Perceptron，MLP）的权重编码了物体的几何结构和高度逼真的外观，引起了广泛关注。本文探讨了将NeRF整合到MLLM中的可行性和效果。我们开发了LLaNA，这是首个通用的NeRF-语言助手，能够执行新任务，如NeRF描述和问答。我们的方法直接处理NeRF MLP的权重，无需渲染图像或构建3D数据结构，就能提取有关代表对象的信息。此外，我们创建了一个无须人工干预的NeRF文本标注数据集，用于各种NeRF-语言任务，并据此建立了一个评估方法来衡量我们的模型对NeRF理解能力。实验结果表明，处理NeRF权重的方法在与从NeRF中提取2D或3D表示进行比较时表现更优。|
|**2024-06-17**|**mDPO: Conditional Preference Optimization for Multimodal Large Language Models**|Fei Wang et.al.|[2406.11839](http://arxiv.org/abs/2406.11839)|**[link](https://github.com/luka-group/mDPO)**|### 背景  直接偏好优化（DPO）已被证明是大型语言模型（LLM）校准的有效手段。最近的研究尝试将DPO应用于多模态场景，但发现实现持续改进颇具挑战。通过对比实验，我们发现了多模态偏好优化中的无条件偏好问题，即模型忽视了图像条件。为解决这个问题，我们提出了mDPO，一个旨在防止语言偏好过度优先的多模态DPO目标，同时优化图像偏好。此外，我们引入了奖励锚点，确保选择的响应奖励保持正向，从而避免相对偏好优化固有的可能性降低问题。  ### 任务  我们在两个不同规模的多模态LLM以及三个常用基准上进行了实验，结果显示，mDPO有效解决了多模态偏好优化中的无条件偏好问题，并显著提高了模型性能，特别是在减少幻觉方面。|
|**2024-06-17**|**Unveiling Encoder-Free Vision-Language Models**|Haiwen Diao et.al.|[2406.11832](http://arxiv.org/abs/2406.11832)|**[link](https://github.com/baaivision/eve)**|**当前的视觉语言模型（VLM）主要依赖于视觉编码器来提取视觉特征，然后利用大型语言模型（LLMs）处理视觉语言任务。然而，视觉编码器在抽象视觉表示方面设定了强烈的先验，如分辨率、比例和语义倾向，这可能限制了VLM的灵活性和效率。直接训练无编码器的纯VLM仍然具有挑战性，且鲜有探索。实证研究显示，这种直接训练方法会导致收敛缓慢和性能差距较大。本文旨在弥合编码器依赖型和无编码器模型之间的差距，提出了一种简单而有效的纯VLM训练策略。具体来说，我们通过深入实验揭示了高效训练无编码器VLM的关键要素：（1）在统一的解码器内融合视觉与语言表示；（2）通过额外监督提升视觉识别能力。基于这些策略，我们开发了EVE，一个无编码器的视觉语言模型，既能高效训练也能快速推理。值得注意的是，仅使用3500万公开可用的数据，EVE就能在多个视觉语言基准上与类似容量的编码器依赖型VLM匹敌，甚至超越了训练过程神秘、数据未公开的Fuyu-8B模型。我们相信，EVE为跨模态开发纯粹的解码器架构提供了一个透明且高效的路径。我们的代码和模型已公开在：https://github.com/baaivision/EVE。**|
|**2024-06-17**|**Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models**|Bingqi Ma et.al.|[2406.11831](http://arxiv.org/abs/2406.11831)|null|大型语言模型（LLMs）基于解码器-only变压器在文本理解方面表现出色，但如何将这些先进的LLMs应用于文本到图像的扩散模型仍是一个待探索的问题。我们发现直接使用LLM作为提示编码器会显著降低生成图像时的提示跟随能力。主要存在两个问题：一是LLM的下一个词预测训练与扩散模型对区分性提示特征的需求不匹配；二是解码器架构固有的位置偏见。为解决这些问题，我们提出了一种新框架，通过精心设计的使用指南，增强LLM的文本表示能力，消除其内在的定位偏见，从而灵活地将最先进的LLMs融入文本到图像生成模型。此外，我们还提供了一种融合多个LLMs的方法。鉴于Transformer架构的卓越性能和扩展能力，我们进一步设计了基于该框架的LLM-Infused Diffusion Transformer（LI-DiT）。我们进行了广泛的实验，验证了LI-DiT在不同模型规模和数据量下的性能。得益于LLMs的内在能力及我们的创新设计，LI-DiT的提示理解性能轻松超越开源的最新模型，以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。强大的LI-DiT-10B将在进一步优化和安全检查后提供。|
|**2024-06-17**|**WPO: Enhancing RLHF with Weighted Preference Optimization**|Wenxuan Zhou et.al.|[2406.11827](http://arxiv.org/abs/2406.11827)|**[link](https://github.com/wzhouad/wpo)**|**强化学习从人类反馈（RLHF）是调整大型语言模型（LLMs）以更好地符合人类价值观的有前景方法。由于成本效益和可扩展性，离线偏好优化——通过其他模型获取偏好数据——被广泛采用。然而，离线偏好优化常受采样策略与目标策略之间分布差异的影响，导致优化效果不理想。为此，我们提出了一种创新策略——加权偏好优化（WPO），旨在通过调整偏好评分对，使离线数据更接近于当前策略，从而缓解这一问题。这种方法不仅解决了分布差距难题，还提升了优化过程，无需额外成本。  我们在Alpaca Eval 2和MT-bench等指令跟随基准上验证了我们的方法。WPO在Alpaca Eval 2上的性能比直接偏好优化（DPO）提高了5.6%。基于Llama-3-8B-Instruct，WPO甚至建立了显著的长度控制胜率，达到48.6%，在80亿参数模型排行榜上成为最强劲的模型。我们将在<https://github.com/wzhouad/WPO>上开源代码和模型。**|
|**2024-06-17**|**Embodied Instruction Following in Unknown Environments**|Zhenyu Wu et.al.|[2406.11818](http://arxiv.org/abs/2406.11818)|null|在自主家庭服务系统中，使实体代理能根据自然语言完成复杂的人类指令至关重要。传统方法仅能在所有互动对象都提供给代理的已知环境中执行指令，直接将现有方法应用于未知环境通常会产生操作不存在物体的不可行计划。相反，我们提出了一种针对未知环境的复杂任务实体指令跟随（Embodied Instruction Following，EIF）方法，该方法使代理能够有效地探索环境，利用现有物体生成可执行计划，以达成抽象指令。具体来说，我们构建了一个包括高层任务规划器和低层探索控制器的多模态大语言模型的层次化实体指令跟随框架。然后，我们通过动态区域注意力构建场景的语义表示地图，以展示已知的视觉线索，使任务规划和场景探索与人类指令目标保持一致。对于任务规划器，根据任务完成过程和已知视觉线索，我们生成步骤式的可行计划。对于探索控制器，根据生成的步骤计划和已知视觉线索预测最优的导航或物体交互策略。实验结果表明，我们的方法在大型房屋级场景中的204个复杂人类指令（如做早餐和整理房间）上实现了45.09%的成功率。|
|**2024-06-17**|**VideoLLM-online: Online Video Large Language Model for Streaming Video**|Joya Chen et.al.|[2406.11816](http://arxiv.org/abs/2406.11816)|null|## 翻译  近期的大型语言模型已经增强了视觉功能，能够理解图像、视频和融合了视觉与语言的内容。然而，这些大模odels的训练方法通常将视频视为预先剪辑好的片段，这使得它们在处理连续视频流时效果不佳且效率低下。为此，我们在本文中提出了一种新颖的“Learning-In-Video-Stream”（LIVE）框架，旨在实现实时、长序列、与视频流同步的对话，适用于连续视频输入。LIVE框架包括以下三个方面：（1）一个设计用于处理连续流式输入的语言建模目标；（2）一种数据生成策略，将离线时间标注转换为适合流式对话的格式；（3）一个优化的推理管道，以提高在实际视频流中的响应速度。基于Llama-2/Llama-3，我们构建了VideoLLM-online模型，并通过它展示了在处理视频流对话方面的显著优势，例如，在A100 GPU上，该模型能在5分钟视频片段中实现超过10帧每秒的流式对话。此外，VideoLLM-online还在公开的离线视频基准测试（如识别、captioning和预测）上展现出最先进的性能。我们已将代码、模型、数据和演示发布在https://showlab.github.io/videollm-online供人使用。|
|**2024-06-17**|**How Do Large Language Models Acquire Factual Knowledge During Pretraining?**|Hoyeon Chang et.al.|[2406.11813](http://arxiv.org/abs/2406.11813)|**[link](https://github.com/kaistai/factual-knowledge-acquisition)**|尽管近期研究表明大型语言模型（LLMs）能够存储大量事实知识，但它们如何在预训练过程中获取这些知识的机制尚不明确。本研究针对这一缺口，探讨了LLMs在预训练期间如何获取和保持事实知识。研究发现了一些关键洞见：首先，出乎意料的是，更多的训练数据对模型获取和保持事实知识的能力并无显著提升。其次，训练步数与记忆遗忘和事实知识泛化之间存在幂律关系，使用重复训练数据的模型遗忘速度更快。第三，增大批量大小可以提高模型抵抗遗忘的能力。总的来说，我们的观察表明，LLMs在预训练中的事实知识获取是通过逐步增加每一步中预训练数据中事实知识出现的概率。然而，这种增加随后会因遗忘而稀释。基于这种理解，我们能够解释一些最近观察到的LLM行为，如长尾知识上的性能不佳，以及去重预训练语料库的好处。|
|**2024-06-17**|**RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content**|Joao Monteiro et.al.|[2406.11811](http://arxiv.org/abs/2406.11811)|**[link](https://github.com/ServiceNow/repliqa)**|## 背景  大型语言模型（LLMs）在训练过程中大量依赖自动从互联网抓取的数据，其中包括包含大量通用知识的百科全书（如维基百科），也可能与用于评估LLMs的基准数据集重叠。因此，如果测试集可能已泄露到训练集中，对模型的评估可能会产生误导性的结论。为了推动语言模型的公正评估，我们提出了一种新的测试数据集——RepLiQA，适用于问答和主题检索任务。RepLiQA是一个包含五个分片的测试集，其中四个在本论文发布前未公开或通过LLM API提供。RepLiQA的每个样本由以下四部分组成：（1）由人类标注员创作的虚构场景描述文档（例如新闻文章），这些内容不会出现在互联网上；（2）关于文档主题的问题；（3）直接源自文档信息的正确答案；（4）包含答案的文档段落。这意味着只有当模型能在提供的文档中找到相关内容时，才能生成准确的答案。  我们进行了一项大规模基准测试，包括多个最先进的LLM，以揭示不同类型的和规模的模型在条件语言建模设置下的性能差异。RepLiQA的已发布分片可在以下链接找到：https://huggingface.co/datasets/ServiceNow/repliqa。|
|**2024-06-17**|**Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations**|Rima Hazra et.al.|[2406.11801](http://arxiv.org/abs/2406.11801)|**[link](https://github.com/declare-lab/safety-arithmetic)**|**随着大型语言模型（LLMs）在翻译和问答等应用中的日益重要，确保它们与人类价值观的正确导向变得至关重要。然而，当前的对齐方法在处理动态用户意图和复杂目标时存在困难，使得模型容易生成有害内容。为此，我们提出了一种无需训练的框架——安全算术（Safety Arithmetic），旨在提升LLMs在不同场景下的安全性，包括基础模型、监督微调模型（SFT）和编辑后的模型。安全算术包含两部分：有害内容消除（Harm Direction Removal）以避免不良输出，以及安全对齐（Safety Alignment）以促进安全响应。此外，我们还发布了NoIntentEdit数据集，它揭示了可能导致模型安全风险的编辑实例。实验结果显示，安全算术显著增强了安全措施，减少了过度安全的问题，同时保持了模型的实用性，相较于现有方法在保障内容生成的安全性方面表现出色。**|
|**2024-06-14**|**Quantifying Variance in Evaluation Benchmarks**|Lovish Madaan et.al.|[2406.10229](http://arxiv.org/abs/2406.10229)|null|评价基准是衡量大型语言模型（LLMs）能力的关键，也是推动这些能力进步的驱动力。最初设计用于评估预训练模型的性能（或缺乏），现在它们也被广泛用于决定不同的训练选择之间。然而，尽管被广泛应用，我们很少量化评价基准的方差，这决定了性能差异的含义。本文定义并测量了一系列旨在衡量评价基准方差的指标，包括初始化时的随机种子方差和训练过程中的单调性。通过对大量模型（包括公开可用的和从头训练的模型）进行研究，我们提供了各种方差度量的实证估计，并为实践者提供了考虑和建议。我们还评估了连续和离散性能度量的实用性和权衡，并探索了更好地理解和减少方差的方法。我们发现，对于较小规模（约70亿参数）的模型，如将多模态多任务学习（MMLU）任务框架为完成任务，可以常常降低方差；而受到人类测试文献启发的更复杂方法（如项目分析和项目反应理论）在显著减少方差方面效果有限。总的来说，我们的工作揭示了评价基准的方差特性，提出了针对LLMs的特定技术来减少方差，并普遍鼓励实践者在比较模型时仔细考虑方差因素。|
|**2024-06-14**|**Semantic Membership Inference Attack against Large Language Models**|Hamid Mozaffari et.al.|[2406.10218](http://arxiv.org/abs/2406.10218)|null|## 背景 成员身份泄露攻击（Membership Inference Attacks，MIA）的目标是识别特定数据点是否被纳入了目标模型的训练集。本文提出了一种新颖的方法——语义成员身份泄露攻击（Semantic Membership Inference Attack，SMIA），通过利用输入的语义内容及其扰动，提升MIA的性能。SMIA训练一个神经网络来分析目标模型对扰动输入的行为，从而捕捉成员样本与非成员样本之间输出概率分布的差异。我们在Pythia和GPT-Neo模型家族，以及Wikipedia数据集上进行了全面的评估。实验结果显示，SMIA明显优于现有攻击手段，例如在Pythia-12B上的AUC-ROC值达到了67.39%，而第二好的攻击方法仅为58.90%。|
|**2024-06-14**|**Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs**|Rui Yang et.al.|[2406.10216](http://arxiv.org/abs/2406.10216)|**[link](https://github.com/yangrui2015/generalizable-reward-model)**|在强化学习从人类反馈（RLHF）框架中，利用基于人类偏好数据的奖励模型已证实能有效调整大型语言模型（LLMs）以符合人类意图。然而，当前奖励模型对未见过的提示和响应的泛化能力有限，可能导致所谓的过度优化问题，即奖励优化过度导致实际性能下降。尽管先前的研究倾向于约束策略优化，我们的研究提出了一种新方法，通过正则化隐藏状态来增强奖励模型应对分布变化的泛化能力。具体来说，我们保留基础模型的语言模型头，并结合一系列文本生成损失，旨在保持隐藏状态的文本生成能力，同时在相同的隐藏状态后学习一个奖励头。实验结果表明，引入的正则化技术显著提高了在各种泛化任务中的奖励模型准确性，并有效缓解了RLHF中的过度优化问题，提供了一个更可靠、更稳健的偏好学习范式。|
|**2024-06-14**|**Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs**|Abhimanyu Hans et.al.|[2406.10209](http://arxiv.org/abs/2406.10209)|**[link](https://github.com/ahans30/goldfish-loss)**|**## 背景 大型语言模型能够记住并重复其训练数据，这带来了隐私和版权问题。为了减轻这种记忆，我们提出了一种对下一步 token 训练目标的微妙修改，称为“金鱼损失”。在训练过程中，随机选择一部分令牌不参与损失计算。模型不会记住这些被丢弃的令牌，从而防止了完整训练序列的逐字复制。我们在数十亿规模的 Llama-2 模型上进行了大量实验，包括预训练和从头开始训练，结果显示，我们的方法显著减少了可提取的记忆，而对下游基准的影响微乎其微。**|
|**2024-06-14**|**TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners**|Tomas de la Rosa et.al.|[2406.10196](http://arxiv.org/abs/2406.10196)|null|**摘要：**  旅行规划是一个复杂的任务，它涉及根据约束条件生成一系列与访问地点相关的行动，同时最大化用户的满意度。传统方法通常会将问题转化为特定形式的语言表达，从网络资源中提取相关信息，并使用合适的求解器来生成有效解决方案。然而，近期的基于大型语言模型（LLMs）的方法直接从用户请求中输出计划，利用丰富的旅行领域知识提供景点和可能路线等高层次信息。尽管如此，当前最先进的模型往往产生不连贯、未能完全满足约束的计划，且无法保证生成高质量方案。我们提出TRIP-PAL，一种融合LLMs和自动化规划器的混合方法：（1）LLMs获取并转换旅行信息和用户需求，将其转化为可输入规划器的数据结构；（2）自动化规划器负责生成满足约束并优化用户效用的旅行计划。我们在不同旅行场景中的实验表明，TRIP-PAL在生成旅行计划方面优于纯LLM方法。|
|**2024-06-14**|**Detecting and Evaluating Medical Hallucinations in Large Vision Language Models**|Jiawei Chen et.al.|[2406.10185](http://arxiv.org/abs/2406.10185)|null|随着大型视觉语言模型（LVLM）在医疗领域的应用日益增长，如医学图像问答和报告生成，它们从基础大语言模型（LLMs）那里继承了强大的功能，但同时也带来了令人担忧的幻觉问题，这在医疗这样对错误容限极低的环境中尤为重要。然而，目前尚无专门针对医疗领域的幻觉检测和评估方法或基准。为了填补这一空白，我们推出了Med-HallMark，这是首个专为医疗多模态领域设计的幻觉检测和评估基准。Med-HallMark支持多任务幻觉检测，提供多元化的幻觉数据，并采用分级幻觉分类。此外，我们提出了MediHall Score，这是一种新的医疗评估指标，通过分层评分系统评估LVLM的幻觉，考虑其严重程度和类型，从而实现对潜在临床影响的细致评估。我们还展示了MediHallDetector，一种专为精确幻觉检测设计的医疗LVLM，它采用了多任务训练方法。通过广泛的实验，我们在我们的基准上为流行的LVLM设立了基线。实验结果表明，MediHall Score提供了比传统指标更深入理解幻觉影响的能力，并显示了MediHallDetector的提升性能。我们期望这项工作能显著提高LVLM在医疗应用中的可靠性。所有相关资源将在不久后发布。|
|**2024-06-14**|**Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors**|Siyuan Chen et.al.|[2406.10181](http://arxiv.org/abs/2406.10181)|**[link](https://github.com/gulang2019/lsp-offload)**|在大语言模型（LLMs）的微调过程中，由于内存需求通常超过单个GPU的容量，解决这一内存挑战的一个常见方法是将计算和数据从GPU迁移到CPU。然而，这受到普通硬件带宽限制的制约，影响了CPU与GPU之间的通信效率。本文提出了一种名为LSP_Offload的框架，通过学习式的子空间投影器，实现在 commodity 硬件上接近原生速度的大规模语言模型微调。我们的数据驱动方法涉及学习一个高效的稀疏压缩器，以最小化通信并保持最小精度损失。此外，我们引入了一种创新的层级通信调度策略，以最大化通信与计算之间的并行性。因此，我们的框架能够在4GB笔记本GPU上微调13亿参数的模型，在配备24GB内存的NVIDIA RTX 4090 GPU上微调70亿参数的模型，仅比无内存限制的微调慢31%。与最先进的离线框架相比，我们的方法提高了微调吞吐量，最高可达3.33倍，当达到相同准确度时，减少了端到端微调时间的33.1%至62.5%。|
|**2024-06-14**|**Datasets for Multilingual Answer Sentence Selection**|Matteo Gabburo et.al.|[2406.10172](http://arxiv.org/abs/2406.10172)|null|**摘要：**  在设计高效的检索式问答（Question Answering，QA）系统中，答案句子选择（Answer Sentence Selection，AS2）是一个关键任务。然而，由于缺乏标注数据，大多数AS2领域的进展主要集中在英语上。这导致了非英语环境下QA系统的性能与英语系统之间的差距。本论文针对这一问题，我们开发了新的高质量多语言（法语、德语、意大利语、葡萄牙语和西班牙语）AS2数据集，通过使用大型语言模型（Large Language Model，LLM）对现有的英文AS2数据集（如ASNQ、WikiQA和TREC-QA）进行监督自动机器翻译（Automatic Machine Translation，AMT）。我们通过多种实验和不同Transformer架构的评估，验证了我们的方法以及翻译数据集的质量。结果显示，我们的数据集对于构建健壮的多语言AS2模型至关重要，显著缩小了非英语与英语环境下的性能差距。|
|**2024-06-14**|**Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models**|Carson Denison et.al.|[2406.10162](http://arxiv.org/abs/2406.10162)|**[link](https://github.com/anthropics/sycophancy-to-subterfuge-paper)**|**在强化学习中，当人工智能系统学会因训练目标不明确而获得不期望的行为时，就会出现规格游戏现象。这种行为可能从简单的奉承行为发展到更复杂且危险的奖励篡改，即模型直接修改其自身的奖励机制。然而，发现这些复杂行为可能超出探索的范畴。本论文探讨大型语言模型（LLMs）是否会在学习常见规格游戏策略后，泛化到执行更为罕见和明显的行为，包括奖励篡改。我们构建了一个逐步升级的可游戏环境系列，并发现针对早期阶段环境的训练会导致在后续环境中出现更多的规格游戏。令人惊讶的是，一小部分但非零的LLMs，在经历了完整训练课程后，能够零样本地直接修改其奖励函数。重新训练LLMs以避免早期阶段的游戏行为可以减轻但不能完全消除后期环境中的奖励篡改。此外，对可游戏环境进行无害性训练并不能阻止奖励篡改。这些结果表明，LLMs能够从常见的规格游戏策略中泛化到更恶劣的奖励篡改行为，并且要消除这种行为可能并非易事。**|
|**2024-06-14**|**BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack**|Yuri Kuratov et.al.|[2406.10149](http://arxiv.org/abs/2406.10149)|**[link](https://github.com/booydar/babilong)**|近年来，大型语言模型（LLMs）的输入上下文长度显著增加。然而，现有的评估方法未能充分衡量模型处理长篇文本中的事实推理能力。为此，我们提出了BABILong基准测试，旨在测试模型在分布式长文档中跨事实推理的能力。BABILong包括20个多样化的推理任务，如事实链、简单归纳、演绎、计数以及处理列表/集合等。这些任务本身就具有挑战性，而当所需事实分散在长篇自然文本中时，难度进一步提升。我们的评估显示，流行的LLMs实际上只利用了10%-20%的上下文信息，且随着推理复杂性的提高，性能急剧下降。对于替代的上下文推理方法，检索增强生成策略在单事实问题回答上的准确率仅为60%，与上下文长度无关。在上下文扩展方法中，循环记忆Transformer展现出最高性能，可处理长达1100万个令牌的长度。BABILong基准测试可以扩展到任意长度，以支持评估具有更强能力的新模型，并提供了长达100万令牌的分隔。|
|**2024-06-13**|**VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding**|Muhammad Maaz et.al.|[2406.09418](http://arxiv.org/abs/2406.09418)|**[link](https://github.com/mbzuai-oryx/videogpt-plus)**|**在基于语言模型的进展基础上，大型多模态模型（LMMs）在视频理解方面取得了显著进步。然而，现有的视频LMMs依赖于图像或视频编码器处理视觉输入，这些编码器各自存在局限性。图像编码器擅长捕捉帧序列中的丰富空间细节，但缺乏明确的时间上下文；而视频编码器提供时间上下文，但常常受限于计算资源，导致只能处理低分辨率的稀疏帧，从而影响了对空间和上下文的理解。因此，我们提出VideoGPT+，它结合了图像编码器（用于详细的空间理解）和视频编码器（用于全局时序上下文建模）的优势。该模型通过将视频划分为小段，并对来自两者特征的提取应用自适应池化策略，以提高性能。我们的架构在多个视频基准上表现出色，包括VCGBench、MVBench和零样本问答任务。此外，我们开发了一个112K的视频指令集，通过新颖的半自动标注管道进一步提升模型性能。为了全面评估视频LMMs，我们还提出了VCGBench-Diverse，它涵盖了18个广泛视频类别，如生活方式、体育、科学、游戏和监控视频，共4,354个问题-答案对。这个基准测试评估现有LMMs在密集视频描述、空间和时间理解以及复杂推理方面的泛化能力，确保在各种视频类型和动态下的全面评估。代码可在https://github.com/mbzuai-oryx/VideoGPT-plus找到。**|
|**2024-06-13**|**Explore the Limits of Omni-modal Pretraining at Scale**|Yiyuan Zhang et.al.|[2406.09412](http://arxiv.org/abs/2406.09412)|**[link](https://github.com/invictus717/MiCo)**|**我们提议构建全模态智能，旨在理解各种模态并学习通用表示。为此，我们提出了一种可扩展的预训练范式，称为多模态上下文（MiCo）。这种方法能够在预训练过程中同时增加模态数量、数据量以及模型参数的数量。通过MiCo，预训练模型在多项任务上展现出显著的多模态学习能力：一是针对10种不同模态的单模态感知基准，二是包括检索、问答和captioning在内的25项跨模态理解任务，三是18个多模态大语言模型基准。我们的模型创造了37项最新的最高性能记录。我们期望这项研究能推动全模态智能的发展。相关代码和模型已在<https://github.com/invictus717/MiCo>开源。**|
|**2024-06-13**|**Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms**|Miaosen Zhang et.al.|[2406.09397](http://arxiv.org/abs/2406.09397)|null|现代视觉模型在大规模嘈杂数据集上进行训练，虽然展现出强大能力，但在遵循用户意图、如视觉美感、特定风格和责任输出方面可能存在问题。本文关注视觉美学领域，目标是使视觉模型与人类审美标准在检索系统中保持一致。高级检索系统通常采用基于低级特征（如饱和度）的审美模型作为重排器或过滤器，但面对风格、文化或知识背景时性能有限。我们发现利用大型语言模型（LLM）的推理能力，通过改写搜索查询并扩展审美期望，可以弥补这一不足。  因此，我们提出了一种基于偏好的强化学习方法，该方法针对视觉模型进行微调，以提取LLM推理和审美模型的知识，从而更好地使视觉模型符合人类审美。由于缺乏专门用于评估检索系统的基准，我们利用强大的多模态大模型（LMM）来评价美感表现。考虑到美感评估的主观性，我们还提出了一个名为HPIR的新数据集，用于衡量与人类审美的契合度。实验结果显示，我们的方法显著提升了视觉模型的美感行为，从多个指标来看。我们相信，提出的算法可以作为一种通用实践，用于使视觉模型与人类价值观相一致。|
|**2024-06-13**|**Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA**|Jongwoo Park et.al.|[2406.09396](http://arxiv.org/abs/2406.09396)|**[link](https://github.com/jongwoopark7978/LVNet)**|长期视频通常包含大量冗余信息，跨越较长的时间间隔，且包含多个松散关联的事件或实体。因此，在进行长视频问答（LVQA）时，生成正确答案所需的所有信息往往只需一小部分帧就足以提供。近期的研究试图利用大型语言模型（LLMs）在LVQA基准上取得卓越性能，但这些模型依赖于视觉语言模型（VLMs）将视频中的所有视觉内容转换成自然语言。传统做法通常是均匀采样大量帧并独立为其生成描述，这既不高效也不免有冗余。针对这一问题，我们探索了关键帧选择和顺序感知的描述方法，以显著减少这些冗余。  为此，我们提出了两个创新方法：层次关键帧选择器和顺序视觉语言模型。我们的最终框架称为LVNet，在三个基准LVQA数据集上实现了最先进的性能。我们将公开我们的代码。|
|**2024-06-13**|**Needle In A Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs**|Zijia Zhao et.al.|[2406.09367](http://arxiv.org/abs/2406.09367)|**[link](https://github.com/joez17/videoniah)**|**视频理解是大规模多模态语言模型（MLLMs）的关键下一步。为了检验视频理解的特定方面，现有的视频基准通常需要精心选择与目标能力匹配的视频，并对查询-响应对进行繁琐的标注，以匹配视频内容。这个过程既具有挑战性又资源密集。本文提出VideoNIAH（视频针 haystack），一个通过合成视频生成的基准构建框架。VideoNIAH通过将不相关的图像/文本“针”插入原始视频中，将测试视频内容与它们的查询-响应分离。它仅基于这些针生成注释，确保视频来源的多样性和查询-响应的丰富性。此外，通过插入多个针，VideoNIAH严格评估模型的时序理解能力。我们利用VideoNIAH构建了视频基准VNBench，包括检索、排序和计数等任务。VNBench能够高效地评估视频模型的精细理解能力和时空建模能力，同时支持长距离依赖性的评估。我们还对近期的视频为中心的多模态大型语言模型进行了评估，包括开源和专有模型，提供了全面的分析。尽管专有模型相对于开源模型具有显著优势，但所有现有视频模型在长距离依赖任务上的性能仍然不佳。VideoNIAH是一个简单且高度可扩展的基准构建框架，我们相信它将激发未来视频基准工作的创新。代码和数据已在https://github.com/joez17/VideoNIAH上提供。**|
|**2024-06-13**|**ElicitationGPT: Text Elicitation Mechanisms via Language Models**|Yifan Wu et.al.|[2406.09363](http://arxiv.org/abs/2406.09363)|null|该论文探讨了如何利用无需领域知识的查询来大型语言模型（如ChatGPT）对获取的文本预测进行评分，以评估其与实际状态的一致性。这种方法是激励信息收集和机器学习模型训练的关键组成部分。研究通过在同行评审数据集上进行实验，比较自动的模型评分与人工导师给出的评分，旨在实证评估这些机制与人类偏好的一致性。|
|**2024-06-13**|**DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding**|Suwon Shon et.al.|[2406.09345](http://arxiv.org/abs/2406.09345)|null|## 背景  将预训练的文本型大型语言模型（LLMs）与语音输入相结合，已经赋予了这些模型执行多样化语音任务的能力，包括指令跟随。这种整合需要结合语音编码器、语音适配器和LLM，它们分别针对不同的任务进行训练。我们提议使用离散语音单元（DSU），而非连续值的语音编码输出，通过语音适配器将DSU转换到LLM的嵌入空间。我们通过无监督的语音编码器生成DSU，然后运用k-means聚类方法。提出的模型在处理来自见/未见过领域以及口语问答中的指令跟随任务时表现出稳健性能。我们还研究了来自不同自监督语音编码器层的DSU类型，以及梅尔频率倒谱系数（MFCC）。实验结果表明，在口语问答的指令调优任务中，ASR任务和数据集的重要性可能较低。|
|**2024-06-13**|**REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space**|Tomer Ashuach et.al.|[2406.09325](http://arxiv.org/abs/2406.09325)|null|大型语言模型（LLMs）可能无意中记住并泄露训练数据中的敏感或个人识别信息（PII），引发隐私问题。当前的解决方案包括昂贵的数据清洗，或者通过遗忘和模型编辑来过滤模型，但这些方法可能被提取攻击绕过。我们提出了一种新颖的模型编辑方法，名为REVS，用于从LLMs中消除敏感信息。REVS识别并修改与每条敏感信息相关的少量神经元。通过将这些神经元投影到词汇空间（去嵌入），我们定位驱动其生成的关键部分。然后，我们根据去嵌入矩阵的伪逆计算模型编辑，并应用它来降低目标敏感数据的生成概率。为了充分评估我们的方法在真正敏感信息上的效果，我们创建了两个数据集：一个是GPT-J固有的电子邮件数据集，另一个是我们调整模型使其记忆的合成社会保障号码数据集。与最先进的模型编辑方法相比，REVS在消除敏感信息和抵抗提取攻击方面表现出色，同时保持模型的完整性。代码和演示笔记本可在<https://technion-cs-nlp.github.io/REVS>获取。|
|**2024-06-13**|**Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs**|Zhao Xu et.al.|[2406.09324](http://arxiv.org/abs/2406.09324)|**[link](https://github.com/usail-hkust/bag_of_tricks_for_llm_jailbreaking)**|**尽管大型语言模型（LLMs）在零样本任务执行方面展现出显著能力，但它们易受破解攻击，可能被操纵产生有害输出。近期的研究开始将破解攻击分为令牌级和提示级。然而，先前的工作主要忽视了破解攻击的多样关键因素，大部分研究聚焦于LLM的漏洞，而对防御增强的LLMs探索不足。为了改进这一状况，我们评估了不同攻击设置对LLM性能的影响，并提议建立一个基准测试框架，以促进标准化评估。我们从目标级和攻击级两个角度，详细考察了实施针对LLMs的破解攻击的八个关键因素。我们在两个常用数据集上对六种防御方法进行了七种代表性的破解攻击，总计约320个实验，使用A800-80G GPU耗时大约5万小时。实验结果强调了对防御增强的LLMs进行标准化评估的必要性。我们的代码已开源：https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking。**|
|**2024-06-13**|**JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models**|Delong Ran et.al.|[2406.09321](http://arxiv.org/abs/2406.09321)|**[link](https://github.com/thuccslab/jailbreakeval)**|**本文探讨了针对大型语言模型（LLMs）的越狱攻击研究中的评估难题。目前，对于攻击是否成功缺乏统一标准，不同的评估方法如人工标注或特定方式提示GPT-4存在，各有优缺点，对人类价值观的体现和研究成本产生影响。我们的研究分析了近九十项2023年5月至2024年4月期间发布的越狱攻击相关研究，提出了一种详细的评估方法分类体系，深入剖析了各种评估器的优缺点及其应用现状。为了推动后续研究，我们开发并推出了JailbreakEval工具包，它是一个用户友好的平台，集成了多种知名的评估器，用户只需一个命令即可获取结果。此外，JailbreakEval支持用户在统一框架内定制自定义评估流程，简化了开发和比较过程。总之，我们期望JailbreakEval能促进越狱攻击评价的标准化，成为社区内越狱研究评估的催化剂。**|
|**2024-06-12**|**Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens**|Ting-Ji Huang et.al.|[2406.08477](http://arxiv.org/abs/2406.08477)|null|在推荐系统中，通过向量表示用户和项目对于多种任务至关重要。最近的研究尝试将大型语言模型（LLMs）应用于问答形式的推荐，使用词汇表内的标记（如“item”、“20”、“24”）来表示实际的用户和项目。然而，由于LLMs通常是在自然语言任务上预训练的，这些词汇表内的标记在表达独特用户和项目方面能力有限，即使经过推荐任务的微调，也会削弱推荐性能。本文探讨如何有效在LLM基的推荐系统中处理用户和项目的标记。  我们强调了出词汇表（OOV）标记的作用，它们除了词汇表内的标记外，还能捕捉用户/项目之间的关联性和多样性。通过分析历史用户-项目交互的表示学习，我们使具有相似特性的用户/项目组合共享相同的OOV标记。此外，将这些OOV标记整合到LLM的词汇表中，有助于更好地区分用户和项目，增强在下游任务微调时对用户-项目关系的捕捉。  我们的提出的框架在各种下游推荐任务上超越了现有最先进的方法。|
|**2024-06-12**|**Real2Code: Reconstruct Articulated Objects via Code Generation**|Zhao Mandi et.al.|[2406.08474](http://arxiv.org/abs/2406.08474)|null|我们提出了一种新颖的方法——Real2Code，旨在通过代码生成来重建可动物体。给定物体的视觉观测，我们首先利用图像分割模型和形状补全模型重构其部件几何结构。接着，我们将物体部件表示为带有方向的边界框，然后输入到一个经过微调的大语言模型（LLM）中，预测关节活动的代码表示。通过利用预训练的视觉和语言模型，我们的方法能够优雅地扩展到具有更多可动部件的对象，并能从合成训练数据中泛化到现实世界中的不规则环境物体。实验结果表明，Real2Code在重建精度上显著优于现有最先进的方法，并且是首个能够超越训练集中对象结构复杂性的方法，能够重建多达10个可动部件的物体。当与立体重建模型结合时，Real2Code还能从少量多视图RGB图像中泛化到现实世界的物体，无需深度或相机信息。|
|**2024-06-12**|**Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing**|Zhangchen Xu et.al.|[2406.08464](http://arxiv.org/abs/2406.08464)|**[link](https://github.com/magpie-align/magpie)**|高质量的指令数据对于调整大型语言模型至关重要。尽管像Llama-3-Instruct这样的模型公开了权重，但它们的对齐数据仍然保密，这限制了人工智能的普及。现有的开源数据生成方法受限于高昂的人力成本和有限的提示范围，难以有效扩展，可能影响公共对齐数据集的多样性和质量。能否通过直接从已对齐的大型语言模型中提取，大规模合成高质指令数据呢？我们提出了一种自我合成方法，称为Magpie。我们的关键观察是，由于Llama-3-Instruct等已对齐的模型具有自回归特性，当我们仅输入左侧模板到用户消息预留位置时，它们可以生成用户查询。我们利用这种方法提示Llama-3-Instruct，生成了400万个指令及其对应的响应。我们对提取的数据进行了全面分析，并选择了30万个高质量实例。为了比较Magpie数据与其他公共指令数据集，我们分别使用每个数据集对Llama-3-8B-Base进行微调，并评估微调后模型的性能。结果显示，在某些任务中，仅使用Magpie进行微调的模型在性能上与官方经过1000万个数据点监督微调（SFT）和后续反馈学习增强的Llama-3-8B-Instruct相当。我们还展示了仅使用Magpie进行SFT可以超越先前用于SFT和偏好优化（如UltraFeedback的直接偏好优化）的公共数据集。这种优势在AlpacaEval、ArenaHard和WildBench等对齐基准测试中表现明显。|
|**2024-06-12**|**TasTe: Teaching Large Language Models to Translate through Self-Reflection**|Yutong Wang et.al.|[2406.08434](http://arxiv.org/abs/2406.08434)|**[link](https://github.com/yutongwang1216/reflectionllmmt)**|**大型语言模型在自然语言处理任务中展现出卓越性能，特别是通过指令调优后，在机器翻译（Machine Translation, MT）等下游任务中的表现有所提升。然而，这些方法未能达到与监督神经机器翻译（Supervised Neural Machine Translation, NMT）系统相当的翻译质量。原因可能是当前使用的简单提示无法充分利用模型的指令跟随能力。为此，我们提出了TasTe框架，即“通过自我反思进行翻译”。该框架包括两个推理阶段：第一阶段，模型被引导生成初步翻译并同时对其自身进行评估；第二阶段，模型根据评估结果对初步翻译进行细化。在WMT22基准的四种语言方向上，我们的方法显示出与现有技术相比的有效性。这项工作展示了一种有前景的方法，能够释放大型语言模型的潜力，并增强其在机器翻译领域的性能。相关代码和数据已在https://github.com/YutongWang1216/ReflectionLLMMT上开源。**|
|**2024-06-12**|**Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL**|Zijin Hong et.al.|[2406.08426](http://arxiv.org/abs/2406.08426)|null|文本转SQL生成准确的SQL查询以响应自然语言问题是一个长期存在的挑战，它涉及用户问题理解、数据库模式理解以及SQL生成等多个复杂环节。传统的文本转SQL系统依赖于人工工程和深度神经网络。随着预训练语言模型（PLMs）的发展和在该任务中的应用，性能得到了显著提升。然而，随着数据库复杂度增加和用户问题难度增大，PLMs有限的理解能力可能导致错误的SQL生成，这促使研究人员寻求更高级和定制化的优化方法，限制了PLM基础系统的广泛应用。最近，大型语言模型（LLMs）因其在自然语言理解上的强大能力而备受瞩目。因此，整合LLM的实现为文本转SQL研究带来了独特的机遇、挑战和解决方案。本综述全面概述了基于LLM的文本转SQL。首先，我们概述当前面临的挑战和文本转SQL的发展历程。接着，详细介绍用于评估文本转SQL系统的数据集和评价指标。然后，我们系统分析了近期在LLM支持下的文本转SQL进展。最后，我们讨论了该领域尚存的挑战，并对未来研究方向提出期待。|
|**2024-06-12**|**OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text**|Qingyun Li et.al.|[2406.08418](http://arxiv.org/abs/2406.08418)|**[link](https://github.com/opengvlab/omnicorpus)**|**该论文介绍了一种名为OmniCorpus的大型图像-文本交错数据集，规模达到100亿级别。这个数据集通过高效的引擎筛选和提取了大量高质量文档，包含86亿张图片和1,696万亿个文本令牌，相较于同类数据（如MMC4、OBELICS），OmniCorpus具有以下优势：1）规模扩大15倍，同时保持了良好的数据质量；2）来源更为多样，包括英文和非英文网站，以及视频为主的网站；3）灵活性更强，可以从图像-文本交错格式轻松转换为纯文本语料库或图像-文本对。通过全面分析和实验，论文验证了OmniCorpus的数据质量、可用性和有效性，旨在为未来的多模态模型研究提供坚实的数据基础。相关的代码和数据已在https://github.com/OpenGVLab/OmniCorpus上公开。**|
|**2024-06-12**|**Discovering Preference Optimization Algorithms with and for Large Language Models**|Chris Lu et.al.|[2406.08414](http://arxiv.org/abs/2406.08414)|**[link](https://github.com/luchris429/DiscoPOP)**|****中文翻译：**  离线偏好优化是提升和控制大型语言模型（LLM）输出质量的重要方法。传统上，偏好优化被视为基于人工设计的凸损失函数的离线监督学习任务。然而，这些方法受限于人类创造力，未能充分探索可能的损失函数的巨大搜索空间。为此，我们提出了一种利用LLM进行目标发现的方法，以自动发现新的最先进的偏好优化算法，无需（专家）人工干预。具体来说，我们通过迭代地提示LLM，根据先前的性能评估提出并实现新的偏好优化损失函数。这个过程导致了未知且高效的优化算法的发现。其中最好的一个被命名为“发现偏好优化”（DiscoPOP），这是一种新颖的算法，它巧妙地融合了逻辑和指数损失。实验结果表明，DiscoPOP在性能上达到了最新水平，并成功地应用于未见过的任务上。**|
|**2024-06-12**|**Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference**|Christopher Wolters et.al.|[2406.08413](http://arxiv.org/abs/2406.08413)|null|## 背景  大型语言模型（LLMs）近期在自然语言处理领域取得了显著进步，使得机器能够生成逼真的文本并进行有意义的对话。然而，随着计算和内存需求的急剧增长，尤其是当LLMs超越单个GPU的处理能力时，对速度、效率和可访问性的需求也随之增加。同时，计算机性能和内存能力的发展并未跟上步伐，尤其是在摩尔定律放缓的背景下。内存访问成本远高于计算，这给大规模扩展带来了挑战，即所谓的“内存墙”。在这个时候，计算在内存（Compute-in-Memory, CIM）技术为AI推理提供了加速可能，通过在内存中直接执行模拟计算，有望降低延迟和功耗。通过紧密集成内存和计算元件，CIM消除了冯诺依曼瓶颈，减少了数据传输，提高了能源效率。  本综述论文概述了基于变压器的模型，探讨了各种CIM架构，并研究了它们如何应对现代人工智能计算系统面临的紧迫挑战。我们详细讨论了与变压器相关的运算及其硬件加速策略，同时指出相关CIM设计中的挑战、趋势和洞察。|
|**2024-06-12**|**Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models**|Chun-Yi Kuan et.al.|[2406.08402](http://arxiv.org/abs/2406.08402)|**[link](https://github.com/kuan2jiu99/audio-hallucination)**|**## 背景 大型音频语言模型（LALMs）通过整合音频感知能力，增强了传统的大规模语言模型，使其能够处理音频相关任务。先前的研究主要集中在评估LALMs在各种任务上的性能，但对它们的可靠性，特别是关于对象幻觉等问题的关注不足。我们的研究中，我们提出方法来评估公开可用的LALMs在对象幻觉方面的程度。结果表明，LALMs在理解音频内容方面与专门的音频captioning模型相当，但在回答区分性问题时表现不佳，尤其是那些需要识别音频片段中特定物体声音的问题。这揭示了当前LALMs的一个关键弱点：它们对区分性查询的理解不足。此外，我们还探讨了提示工程如何提升LALMs在区分性问题上的性能。**|
|**2024-06-12**|**cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers**|Anirudh Sundar et.al.|[2406.08398](http://arxiv.org/abs/2406.08398)|null|## 背景 在情境化和多模态交互对话（SIMMC）的新兴研究领域中，科学论文的互动是一个重要方向。由于科学论文主要由文本、公式、图表和表格构成，SIMMC方法需要针对这些组成部分进行专门设计，以支持科研人员所需的深度探究和互动。本研究提出了一种名为“对话式论文”（cPAPERS）的数据集，它包含了来自arXiv上可用的科学文档的学术论文评论中的问答对，这些问答与论文组件及其引用相关。我们介绍了数据收集策略，通过OpenReview收集这些问题-答案对，并与LaTeX源文件中的上下文信息关联起来。此外，我们展示了使用大型语言模型（LLMs）的一系列基线方法，包括零样本和微调配置，来处理cPAPERS数据集。|
|**2024-06-11**|**Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena**|Aidar Myrzakhan et.al.|[2406.07545](http://arxiv.org/abs/2406.07545)|**[link](https://github.com/vila-lab/open-llm-leaderboard)**|**### 背景  多项选择题（MCQ）常用于评估大型语言模型（LLMs）。通常，LLM会根据调整后的概率，如长度因素，选择最可能的答案。然而，LLMs可能存在固有的偏见，例如对A、B、C、D等选项ID的偏好，这可能影响答案预测。先前的研究通过在少数测试样本上随机打乱选项，并将其应用到新样本上，试图减少这种“选择偏差”。此外，MCQ的另一个问题是“彩票式猜测”，即LLM并未真正学习知识，而是凭运气猜对答案，这对小型LLMs尤为严重。  为解决这些问题，一个更全面的方法是转向开放式问题，这能从根本上消除选择偏差和随机猜测。但转向开放式问题也带来了挑战：一是如何识别合适的开放性问题，二是如何验证LLM对开放式问题的回答与人类标注的真实答案之间的准确性。本研究旨在解决这些难题，并建立一个新的LLM评估基准，通过完全的开放式问题来衡量模型性能，例如GPT-4o/4/3.5、Claude 3、Gemini等。  ### 任务  我们创建了Open-LLM-Leaderboard，这是一个新的评价平台，旨在跟踪各种LLM的表现，揭示它们的真实能力。我们的代码和数据集已开源，可在此链接获取：https://github.com/VILA-Lab/Open-LLM-Leaderboard。**|
|**2024-06-11**|**QuickLLaMA: Query-aware Inference Acceleration for Large Language Models**|Jingyao Li et.al.|[2406.07528](http://arxiv.org/abs/2406.07528)|**[link](https://github.com/dvlab-research/q-llm)**|**大型语言模型（LLMs）在理解和处理长序列方面的能力对于各领域的发展至关重要。然而，它们在捕捉序列中的长期依赖关系以深入理解语义方面仍然存在挑战。为此，我们提出了Query-aware Inference for LLMs（Q-LLM），这是一种旨在模仿人类认知处理大规模序列的系统。通过聚焦于与给定查询相关的内存数据，Q-LLM能够在固定窗口大小内准确捕捉相关信息，并为查询提供精确的答案，无需额外训练，可无缝集成到任何LLMs中。使用LLaMA3（QuickLLaMA）的Q-LLM能在30秒内阅读《哈利·波特》，并能准确回答问题。相较于当前最先进的LLaMA3，Q-LLM的性能提升了7.17%，而在Mistral上，它在 $\infty$ -bench上的表现提升了3.26%。在“针锋相对”任务中，Q-LLM在广泛认可的基准上，相对于当前最佳成绩，Mistral上的提升达到了7.0%，在LLaMA3上实现了100%的准确率。我们的代码已在https://github.com/dvlab-research/Q-LLM上开源。**|
|**2024-06-11**|**Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement**|Yunzhen Feng et.al.|[2406.07515](http://arxiv.org/abs/2406.07515)|null|随着生成模型合成数据的兴起，越来越多地被用于大型语言模型的微调，这引发了对模型崩溃（即微调性能下降）的关注。由于人类和机器都较容易分辨好样本和坏样本，而非生成高质量样本，我们探讨了如何利用反馈来防止模型在合成数据上出现崩溃。我们理论分析了一个高斯混合分类模型在基于反馈增强的合成数据训练下的最优性能，并提供了有限样本情况下的实验证据。我们在两个实际问题上展示了这些理论预测：使用变压器计算矩阵特征值和利用大型语言模型进行新闻摘要，这两种情况下模型在生成数据上都会经历崩溃。我们发现，通过从反馈增强的合成数据中训练，无论是修剪错误预测还是选择最佳猜测，都能防止模型崩溃，证实了像RLHF（Reinforcement Learning with Human Feedback）这样的流行方法的有效性。|
|**2024-06-11**|**THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical Report**|KBTG Labs et.al.|[2406.07505](http://arxiv.org/abs/2406.07505)|null|## 背景  近期大型语言模型（LLMs）的进步在科技领域展现了新功能和机遇。然而，非常大的LLMs的实际应用受到其高计算成本的制约，这与其相对有限的人类能力相比，收益并不明显。尽管小型、更实用的LLMs在金融分析方面展现出潜力，但它们尚未完全掌握，如它们在模拟特许金融分析师（CFA）考试中的接近通过表现所示。本文中，我们展示了Financial Analyst Extension（FAE）对我们的Text Hyperlocally Augmented Large Language Extension（THaLLE）系列的扩展，这一系列80亿参数的LLMs在模拟CFA考试中始终表现出最高性能，与同类规模的模型相比。我们详细记录了用于优化的微调技术，以供后续研究参考。此外，我们引入Flare CFA，这是一个公开可用的金融顾问评估数据集，用于检验LLMs在财务顾问角色中的能力。|
|**2024-06-11**|**Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions**|Renjie Pi et.al.|[2406.07502](http://arxiv.org/abs/2406.07502)|**[link](https://github.com/sterzhang/image-textualization)**|**## 背景  图像描述数据集对于推动图像理解、文本到图像生成和文本图像检索等应用至关重要。当前，这些数据集主要来自两个途径：一是从网络上抓取图像与文字对，但这类描述往往质量较低且存在噪声；二是人工标注，如COCO等，通常描述简洁，缺乏详细信息。尽管详细的图像描述可以通过人类标注获得，但高昂的标注成本限制了其可行性。这些局限性促使我们寻求更有效和可扩展的方法来生成准确而详尽的图像描述。  本文提出了一种创新框架，称为“图像文本化”（Image Textualization，简称IT），它通过协同利用现有的多模态大型语言模型（Multimodal Large Language Models，MLLMs）和视觉专家模型，有效地将视觉信息转化为文本，从而自动生成高质量的图像描述。针对当前缺乏详尽描述的基准问题，我们还提出了多个评价基准，以全面评估我们的框架生成的图像描述质量。  此外，我们展示了在IT精心编纂的描述训练下，LLaVA-7B模型的图像描述生成能力得到了提升，能够生成更丰富的描述，输出长度和细节显著增加，同时减少了幻觉现象。**|
|**2024-06-11**|**TextGrad: Automatic "Differentiation" via Text**|Mert Yuksekgonul et.al.|[2406.07496](http://arxiv.org/abs/2406.07496)|**[link](https://github.com/zou-group/textgrad)**|**人工智能正经历一场范式转变，通过大型语言模型（LLMs）和其他复杂组件的协同工作取得了突破。当前，为复合人工智能系统设计原则化的自动化优化方法成为一项关键新挑战。神经网络在早期面临类似问题时，通过反向传播和自动微分实现了重大革新。受此启发，我们提出了TextGrad，这是一个强大的框架，它通过文本实现自动“微分”，将LLMs提供的丰富、通用的自然语言建议回传到复合AI系统的各个组件中。TextGrad遵循PyTorch的语法和抽象，易于使用且灵活，用户仅需提供目标函数，无需调整框架组件或提示，即可无缝应用。  TextGrad适用于多种任务，从问答和分子优化到放射治疗计划设计。在无需修改框架的情况下，它显著提升了GPT-4o在Google证明性问题回答中的零-shot准确率，从51%提升至55%；在优化LeetCode难题解法上实现了20%的相对性能提升；改进了推理提示，设计出具有理想体外亲和力的新药候选分子；以及设计出具有高特异性的放射治疗方案。TextGrad为下一代AI系统的发展奠定了基础，推动了复合AI技术的加速发展。**|
|**2024-06-12**|**CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization**|Frederic Kirstein et.al.|[2406.07494](http://arxiv.org/abs/2406.07494)|null|该文章综述了2019年至2024年间发表的1262篇独特的研究论文，集中在Transformer架构在英文对话摘要生成方面的研究。文章详细探讨了对话摘要中存在的主要挑战，如语言理解、结构处理、理解能力、说话者识别、重要性判断和事实准确性，并与相应的技术，如图解方法、额外训练任务和规划策略进行了关联。尽管在某些方面（如语言）取得了显著进展，但如理解力、真实性与重要性评估等挑战仍然存在，提供了丰富的研究空间。  文章还分析了评估这些方法的方式，涵盖了对话子领域（如会议、医疗）的常用数据集，以及自动评价指标（如ROUGE）和人类评估的普遍实践。然而，发现跨领域的数据集相对有限，且报告的人类评估往往缺乏足够的内审员一致性信息和标注指南细节。此外，文章讨论了大语言模型的最新探索可能带来的影响，指出尽管它们可能会改变相关性和难度，但描述的挑战分类体系仍然具有价值。|
|**2024-06-11**|**PITCH: Productivity and Mental Well-being Coaching through Daily Conversational Interaction**|Adnan Abbas et.al.|[2406.07485](http://arxiv.org/abs/2406.07485)|null|高效的计划制定对生产力和心理健康至关重要，但人们往往难以制定实际的计划并反思自己的效率。利用人工智能的发展，对话助手作为一种有前景的工具，旨在通过对话方式将计划外化，强化决心，促进专注行动，从而正面影响生产力和心理健康。我们的研究目标是设计一个对话助手，通过自然对话的社交互动性，提供深入的问题和反思提示，以提高计划执行度。尽管先前的研究显示了这些代理的效益，但许多干预措施仍保持静态，可能导致用户参与度随时间下降。为了弥补这一不足，我们提出了一种新颖的旋转和上下文感知的提示策略，每天为用户提供多样的干预手段。我们的系统PITCH利用大语言模型（LLMs）来促进日常计划的外部化和反思。本研究旨在探究与对话代理一起外化任务对生产力和心理健康的影响，以及旋转策略在保持用户参与度方面的有效性。|
|**2024-06-11**|**Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing**|Mao Li et.al.|[2406.07483](http://arxiv.org/abs/2406.07483)|null|在快速发展的自然语言处理领域，大型语言模型（LLMs）在社交媒体帖子的自动文本标注方面展现出浓厚兴趣。本文研究了八种开源和专有LLMs在立场标注任务中的性能，将其与人类（通过众包）的判断进行基准测试。我们探究了何时LLMs可能与人类判断产生分歧的情况。研究发现，文本中表达立场的明确程度对LLMs判断与人类一致性至关重要。当人类注释者表现良好时，LLMs也表现出色；反之，LLMs的失败往往对应于人类难以达成一致的情境。因此，我们建议结合人类专业知识的精确度与LLMs预测的规模，提出一种全面的方法。这项研究强调了提高自动化立场检测准确性和全面性的必要性，旨在推动这些技术在更高效、无偏见的社会媒体分析中得到提升。|
|**2024-06-11**|**VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs**|Zesen Cheng et.al.|[2406.07476](http://arxiv.org/abs/2406.07476)|**[link](https://github.com/damo-nlp-sg/videollama2)**|**本文介绍VideoLLaMA 2，一套专为提升视频和音频定向任务中的空间-时间建模及音频理解能力而设计的视频大型语言模型（Video-LLMs）。它在前一代的基础上增添了定制的时空卷积（STC）连接器，有效地捕捉视频数据的复杂空间和时间动态。此外，我们通过联合训练融入了音频分支，增强了模型的多模态理解能力，使其能无缝融合音频线索。在多项评估中，如多选视频问答（MC-VQA）、开放性视频问答（OE-VQA）和视频captioning（VC）任务上，VideoLLaMA 2表现出与开源模型相当的竞争实力，并在某些基准上接近专有模型。在音频仅用（AQA）和音频-视频问答（OE-AVQA）任务上，VideoLLaMA 2也显示出对现有模型的合理改进。这些进步凸显了VideoLLaMA 2在多模态理解方面的卓越性能，为智能视频分析系统树立了新标准。所有模型均公开以促进进一步研究。**|
|**2024-06-10**|**Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation**|Peize Sun et.al.|[2406.06525](http://arxiv.org/abs/2406.06525)|**[link](https://github.com/foundationvision/llamagen)**|**我们提出LlamaGen，这是一种全新的图像生成模型家族，它将大型语言模型的原始“下一个词预测”范式应用于视觉生成领域。这表明，如果适当扩展，未经视觉特性的先验知识增强的纯自回归模型（如Llama）也能达到最先进的图像生成性能。我们的研究探索了图像分词器的设计空间、图像生成模型的可扩展性以及训练数据质量，结果如下：(1) 一种具有16倍下采样的图像分词器，其在ImageNet基准上的重构质量为0.94，代码书利用率高达97%。(2) 一系列从111百万到31亿参数的类条件图像生成模型，在ImageNet 256x256基准上实现了2.18的FID分数，超越了流行的扩散模型，如LDM和DiT。(3) 一个7.75亿参数的文本条件图像生成模型，通过两阶段训练在LAION-COCO和高审美质量图像上，显示出良好的视觉质量和文本一致性性能。(4) 我们验证了大语言模型服务框架在优化图像生成模型推理速度方面的有效性，实现了326%至414%的速度提升。我们开源所有模型和代码，以促进视觉生成和多模态基础模型的开放源代码社区的发展。**|
|**2024-06-10**|**UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor**|Shivani Upadhyay et.al.|[2406.06519](http://arxiv.org/abs/2406.06519)|**[link](https://github.com/castorini/umbrela)**|**## 翻译  大量相关性判断对于检索系统的有效训练和精确评估至关重要。传统上，这些判断由人工评定员完成，过程昂贵且耗时。微软Bing的Thomas等人最近的一项研究表明，大型语言模型（LLMs）能够准确地进行相关性评估，提供与人类相当的判断。遗憾的是，他们的研究并未公开可供重复使用的软件工具。我们的工作介绍了一个开源工具包——UMBRELA（全称为“UMBRELA是Bing RELevance Assessor的递归缩写”），它基于OpenAI的GPT-4模型复现了Thomas等人的结果，并为原论文增添了更多细节。我们在TREC 2019年至2023年的深度学习任务中发现，LLM生成的相关性判断与高效多阶段检索系统生成的排名高度相关。该工具包设计为易于扩展，可以融入现有的多阶段检索和评估流程，为研究检索评估方法的研究者提供了宝贵的资源。UMBRELA将在TREC 2024年的RAG任务中用于辅助相关性评估，我们期望它成为该领域进一步创新的基础。UMBRELA的代码库可于https://github.com/castorini/umbrela获取。**|
|**2024-06-10**|**NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative**|Asmar Nadeem et.al.|[2406.06499](http://arxiv.org/abs/2406.06499)|null|当前的视频字幕基准和模型在表征因果时间叙事方面存在不足，这种叙事是通过因果关系连接的一系列事件，随时间发展，由人物或主体驱动。这种缺乏叙事性限制了模型生成捕捉视频内容内在因果和时间动态的文本描述的能力。为填补这一空白，我们提出NarrativeBridge，它包括以下两个组成部分：（1）一个由大型语言模型通过少量提示生成的新型因果时间叙事（CTN）字幕基准，该基准明确地在视频描述中编码因果关系，通过自动评估确保质量和相关性；（2）一个专门的因果网络（CEN）架构，具有独立的编码器以分别捕获因果动态，从而实现有效的学习和生成具有因果时间叙事的字幕。实验结果表明，CEN在表达视频内容的因果和时间方面比第二好的模型（GIT）更准确：在MSVD和MSR-VTT数据集上的CIDEr分数分别为17.88和17.44。提出的框架能够理解和生成具有复杂因果时间叙事结构的细微文本描述，这是视频字幕生成的一个关键局限性。有关项目详情，请访问<https://narrativebridge.github.io/>。|
|**2024-06-10**|**Towards a Personal Health Large Language Model**|Justin Cosentino et.al.|[2406.06474](http://arxiv.org/abs/2406.06474)|null|在健康领域，大部分大型语言模型（LLM）的研究集中在临床任务上。然而，移动和可穿戴设备提供的丰富、长期的个人健康监测数据往往被忽视。本文介绍了一种名为Personal Health Large Language Model（PH-LLM）的新模型，它是Gemini的定制版，专为理解和处理数值时间序列的个人健康数据而设计。我们创建并整理了三个测试集，考察了PH-LLM在以下方面的性能：1）从睡眠模式、身体活动和生理反应中生成个性化见解和建议；2）专业知识领域的专家水平；3）预测自我报告的睡眠结果。我们与领域专家合作构建了857个案例研究，以评估实际的睡眠和健身场景。通过针对特定领域的评分标准进行全面评估，我们发现Gemini Ultra 1.0和PH-LLM在健身方面与专家表现无统计差异，尽管在睡眠方面专家仍占优势，但Fine-tune后的PH-LLM在利用相关领域知识和个人化睡眠信息方面表现出显著提升。我们还通过多项选择的睡眠医学和健身考试评估了PH-LLM的专业知识，其得分分别为79%和88%，超过了人类专家样本的平均分。最后，我们训练PH-LLM预测来自可穿戴设备文本和多模态编码数据的自我报告睡眠质量结果，并证明了多模态编码对于达到专门区分模型的性能至关重要。尽管在个人健康这个关键安全领域还需要进一步发展和评估，但这些结果展示了Gemini模型的广泛知识和能力，以及将生理数据应用于个人健康应用，如PH-LLM中的做法。|
|**2024-06-10**|**AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction**|Zhen Xing et.al.|[2406.06465](http://arxiv.org/abs/2406.06465)|null|文本引导的视频预测（TVP）任务旨在根据初始帧和指令预测后续帧的运动，这对于虚拟现实、机器人技术和内容创作等领域具有广泛的应用。尽管先前的方法通过改编Stable Diffusion在该任务上取得了重大进展，但它们在帧一致性与时间稳定性方面仍存在问题，主要受限于视频数据集的规模。我们观察到，预训练的Image2Video扩散模型对视频动态有良好的先验知识，但缺乏文本控制。因此，将Image2Video模型转移，同时注入指令控制以生成可控制的视频，既具有意义又颇具挑战。  为了实现这一目标，我们提出了多模态大型语言模型（MLLM），用于根据初始帧和文本指令预测未来的视频状态。特别地，我们设计了双查询Transformer（DQFormer）架构，它将指令和帧信息整合到条件嵌入中，用于未来帧的预测。此外，我们开发了长短期时序适配器和空间适配器，能够在少量训练成本下快速将通用视频扩散模型适应特定场景。  实验结果表明，我们的方法在Something Something V2、Epic Kitchen-100、Bridge Data和UCF-101四个数据集上显著优于现有技术。特别是在Bridge数据集和SSv2上，AID分别实现了91.2%和55.5%的FVD改进，这证明了其在不同领域的有效性。更多示例可在我们的网站<https://chenhsing.github.io/AID>找到。|
|**2024-06-10**|**Transforming Wearable Data into Health Insights using Large Language Model Agents**|Mike A. Merrill et.al.|[2406.06464](http://arxiv.org/abs/2406.06464)|null|尽管可穿戴健康追踪器日益普及，睡眠和运动对健康的重要性不言而喻，但从这些数据中提取具有行动价值的个性化见解仍是一个挑战。这需要对大量数据进行非结构化分析。随着大型语言模型（LLM）的兴起，它们能够利用工具理解和与世界互动，为大规模个性化分析带来了希望。然而，在个人健康领域的LLM应用尚待开发。本文介绍了一种名为Personal Health Insights Agent（PHIA）的系统，它利用最新的代码生成和信息检索工具来分析和解释行为健康数据。我们构建了两个超过4000个健康洞察问题的基准问答数据集。根据650小时的人类和专家评估，PHIA能准确回答84%以上的事实性数值问题，以及超过83%的众包开放性问题。这项工作对于推动大众行为健康进步具有重要意义，可能使个人能够解读自己的可穿戴数据，开辟了一个以数据驱动洞察为指导的个性化健康方案的新时代，使得健康保健更加便捷且个性化。|
|**2024-06-11**|**Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies**|Junlin Wang et.al.|[2406.06461](http://arxiv.org/abs/2406.06461)|null|这篇论文指出，尽管已经提出了多种推理策略来评估大型语言模型的能力，但传统的评价方法仅关注性能指标，忽视了一个关键因素：额外计算资源带来的增效。这可能导致对策略效率的片面理解。为此，论文提出了一种框架，将计算预算纳入评估，以提供一个既考虑性能指标又考虑计算成本的更全面比较。通过这种预算意识的视角，研究发现复杂的推理策略在没有显著算法创新的情况下，往往由于分配了更多的计算资源而超越了简单的基线。例如，当给予链式思考自洽性（chain-of-thought self-consistency）类似级别的计算资源，它常常能优于文献中提出的推理策略。然而，在这种规模敏感的视角下，某些策略如多代理辩论或多反思在增加计算预算时可能会表现得更差。|
|**2024-06-10**|**Evaluating the Retrieval Component in LLM-Based Question Answering Systems**|Ashkan Alinejad et.al.|[2406.06458](http://arxiv.org/abs/2406.06458)|null|## 背景  大规模语言模型（LLMs）驱动的问答系统在依赖检索组件时，能够获取领域特定信息并降低产生不准确回复或错误信息的风险。尽管信息检索领域的评估方法早已存在，但如何评估LLMs驱动的聊天机器人中的检索器性能仍是一个挑战。本研究提出了一种简单的基准方法，用于评价基于检索增强生成（Retrieval-Augmented Generation，RAG）的聊天机器人中的检索器。  ## 任务  我们的研究发现，这种方法能更全面地反映检索器的性能，并与整个问答系统的整体表现更为一致。尽管传统的精确度（precision）、召回率（recall）和F1分数等指标可能无法完全揭示LLMs的能力，因为它们可能会在检索器不完美时仍提供准确答案，但我们的评估方法考虑到了LLMs的优势，即它们能够忽略无关上下文，同时也能处理可能存在的错误和虚构内容。|
|**2024-06-10**|**A Large Language Model Pipeline for Breast Cancer Oncology**|Tristen Pool et.al.|[2406.06455](http://arxiv.org/abs/2406.06455)|null|大型语言模型在众多领域展现出创新潜力，但在癌症治疗方面的应用仍需进一步开发。研究者使用一种新颖的Langchain提示工程管道，对最先进的OpenAI模型进行了微调，数据集包括临床数据和临床指南文本，专注于乳腺癌患者辅助放疗和化疗两个关键治疗因素。结果显示，模型在分类这两个治疗手段时达到了高精度（0.85+）。通过观察人类肿瘤学家的治疗质量数据，建立了一个置信区间，估计模型在预测治疗方案时必须比原始肿瘤学家表现得更好，才能在总体上成为更好的解决方案的比例为8.2%至13.3%。由于癌症治疗决策结果的不确定性，未来可能需要进行临床试验来验证这一阈值。考虑到美国85%的癌症患者在地方社区设施接受治疗，这类模型有可能显著扩大优质护理的可及性，其效果至少接近人类肿瘤学家。|
|**2024-06-10**|**Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course**|Aadarsh Padiyath et.al.|[2406.06451](http://arxiv.org/abs/2406.06451)|null|大型语言模型（LLMs）在代码生成、调试和解释方面的性能引发了许多研究者和教育工作者对本科编程教育的关注，他们期待这些模型能革新编程教学。然而，关于如何以及为何在编程教育中使用LLMs的决策可能不仅仅基于技术评估。本研究以社会塑造技术理论为指导框架，探讨了学生对LLMs的社会感知如何影响他们的使用行为。我们通过分析一份匿名的课程结束时的调查问卷（n=158）、中期自我效能问卷（n=158）、10位学生的深度访谈、自我报告的LLM在作业中的使用情况，以及期中考试成绩，发现学生的LLM使用与其对未来职业的期望和对同伴使用的感知有关。此外，我们发现早期自我报告的LLM使用与较低的自我效能和中期考试成绩相关，而学生对过度依赖LLM的感知，而非实际使用，与课程后期的自我效能下降有关。|
|**2024-06-07**|**3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs**|Jianing Yang et.al.|[2406.05132](http://arxiv.org/abs/2406.05132)|**[link](https://github.com/sled-group/3D-GRAND)**|在这个研究中，语言与三维感知的融合对于构建理解和互动于物理世界的实体代理和机器人至关重要。尽管大型语言模型（LLMs）在语言理解和生成方面表现出色，但在适应三维环境（3D-LLMs）方面仍处于初级阶段，主要挑战在于缺乏大规模的密集地将语言与三维场景关联的数据集。为此，我们提出了3D-GRAND，这是一个开创性的大型数据集，包含40,087个家庭场景，配对有620万条详尽的场景-语言指令。实验结果显示，使用3D-GRAND进行指令调优显著提高了3D-LLMs的定位能力，并减少了错误的想象。我们还设计了3D-POPE基准，用于系统性评估3D-LLMs中的幻觉问题，以促进未来模型的公平比较。  我们的实验揭示了数据集规模与3D-LLM性能之间的关联，强调了大型三维文本数据集在推动体感AI研究中的关键作用。值得注意的是，初步迹象表明，通过在大型合成数据上训练的模型可能在现实世界3D扫描中表现良好，这展示了模拟到实际的迁移学习潜力。通过3D-GRAND和3D-POPE，我们旨在为体感AI社区提供必要的资源和洞见，推动更可靠、更扎实的3D-LLMs的发展。项目网站：https://3d-grand.github.io|
|**2024-06-07**|**An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models**|Xiongtao Zhou et.al.|[2406.05130](http://arxiv.org/abs/2406.05130)|**[link](https://github.com/alenai97/peft-mllm)**|这篇论文关注的是大型多模态语言模型（MLLMs）的参数高效微调（PEFT）。由于这些模型通常具有数十亿参数，全面调整变得困难。研究目标是找出在参数受限情况下提升MLLM性能的有效方法。通过实验使用四种流行的PEFT技术对开源MLLMs的LLM组件进行微调，论文进行了详尽的分析，内容包括不同方法对模型、参数位置、微调数据规模、模型稳定性、泛化能力以及幻觉的影响。研究涵盖了两种类型的七项数据集：未见过的和已见过的。结果显示，适配器是最有效的PEFT方法，而连接器层的微调在大多数情况下能提高性能。研究代码和数据可在<https://github.com/alenai97/PEFT-MLLM.git>获取。|
|**2024-06-07**|**Towards Semantic Equivalence of Tokenization in Multimodal LLM**|Shengqiong Wu et.al.|[2406.05127](http://arxiv.org/abs/2406.05127)|null|### 背景  多模态大型语言模型（MLLMs）在处理视觉语言任务方面展现出卓越性能。MLLM的核心在于视觉 tokenization，即如何有效地将输入的视觉信号转化为对语言模型有益的特征表示。然而，现有的视觉tokenizer在保持视觉与语言的语义一致性上存在问题，它们过于碎片化视觉输入，破坏了视觉内容的语义完整性。为解决这一问题，本文提出了一种新颖的动态语义等效视觉tokenizer（SeTok），它通过动态聚类算法将视觉特征组织成语义单元，根据图像复杂性灵活决定token的数量。这种生成的视觉tokens能有效保持语义完整性，同时捕捉低频和高频视觉特征。  ### 任务  我们提出了一种名为Setokim的新型MLLM，它结合了SeTok。实验结果表明，Setokim在各种任务上表现出显著的优势。关于更多详情，可以访问项目网页：https://chocowu.github.io/SeTok-web/。|
|**2024-06-07**|**LINX: A Language Driven Generative System for Goal-Oriented Automated Data Exploration**|Tavor Lipman et.al.|[2406.05107](http://arxiv.org/abs/2406.05107)|null|## 翻译  数据探索是一个复杂的过程，用户通过逐步执行一系列查询来审视数据集。有时，用户会探索新数据以熟悉它，但更多时候，探索过程是围绕特定分析目标或问题进行的。为了帮助用户有效探索，已提出自动化数据探索（Automated Data Exploration，ADE）系统，它们旨在自动生成展示数据有趣特性的完整探索流程。然而，现有的ADE系统常受限于预定义的优化函数，导致对同一数据集始终产生相同的探索序列，这在有明确目标的探索中显得不足。为此，本文提出LINX，一个结合自然语言接口的生成式系统，专注于面向目标的数据探索。  LINX接受输入数据集和用自然语言描述的分析目标，生成与用户需求相关的个性化探索会话。系统利用大型语言模型解析输入的分析目标，并据此生成期望输出探索会话的规范。这些规范随后被传递给基于约束深度强化学习（Constrained Deep Reinforcement Learning，CDRL）的新型模块化ADE引擎，使其能根据指定指令调整输出。为了验证LINX的效果，我们创建了一个新的面向目标探索的基准数据集，并进行了深入的用户研究。实验结果表明，LINX生成的探索笔记本在相关性和实用性上显著优于现有解决方案，包括ChatGPT、无目标导向的ADE以及商业系统。|
|**2024-06-07**|**Multi-Head RAG: Solving Multi-Aspect Problems with LLMs**|Maciej Besta et.al.|[2406.05085](http://arxiv.org/abs/2406.05085)|**[link](https://github.com/spcl/mrag)**|**## 背景  **增强型检索生成（Retrieval Augmented Generation, RAG）**通过将文档内容融入大语言模型（Large Language Models, LLMs）的上下文中，提高了其响应的准确性和相关性。然而，现有的RAG方法并未充分处理那些可能需要检索包含不同内容的多文档查询。这类问题在现实中很常见，但挑战在于，这些文档的嵌入在向量空间中可能相距较远，难以一次性获取。本文提出了一种新的方案——**多头检索增强生成（Multi-Head RAG, MRAG）**，它以一种简单而强大的方式解决这个问题：利用Transformer的多头注意力层的激活作为检索键，而非解码层。这个想法的驱动力在于，不同的注意力头能够学习捕捉数据的不同方面。通过利用这些激活，我们得到的嵌入能代表数据项和查询的多种特性，从而提升复杂查询的检索精度。  **贡献**  我们提供了评估方法、度量标准、合成数据集以及实际应用案例，来展示MRAG的有效性。与标准RAG基线相比，MRAG在相关性方面的提升可高达20%。MRAG可以无缝融入现有的RAG框架，如RAGAS，以及各类数据存储系统。  总结，本文旨在改进现有RAG模型，以更好地处理涉及多角度信息检索的复杂查询任务。**|
|**2024-06-07**|**Are Large Language Models More Empathetic than Humans?**|Anuradha Welivita et.al.|[2406.05063](http://arxiv.org/abs/2406.05063)|null|随着大型语言模型（LLMs）的兴起，研究它们是否能在情感识别和共情回应方面超越人类已成为研究焦点。本论文开展了一项深入研究，对比了包括GPT-4、LLaMA-2-70B-Chat、Gemini-1.0-Pro和Mixtral-8x7B-Instruct在内的四款最先进的LLMs与人类在共情回应能力上的表现。我们通过一项涉及1,000名参与者的双盲用户研究，对2,000个精心挑选的情感对话提示进行了分析，这些提示涵盖了32种不同正负情绪的广泛范围。研究结果显示，LLMs的共情回应能力在统计学上优于人类。GPT-4表现出最强烈的共情，其“好”等级别的回复比人类基准提高了约31%。紧随其后的是LLaMA-2，提升了约24%，Mixtral-8x7B提升了约21%，Gemini-Pro提升了约10%。我们还对回复评级进行了更详细的分析，发现某些LLMs在回应特定情绪方面明显优于其他模型。提出的评估框架提供了一种可扩展且适应性强的方法，用于评估新LLMs的共情能力，避免了未来研究重复这项研究的必要性。|
|**2024-06-07**|**Robustness Assessment of Mathematical Reasoning in the Presence of Missing and Contradictory Conditions**|Shi-Yu Tian et.al.|[2406.05055](http://arxiv.org/abs/2406.05055)|null|大型语言模型在推理任务上表现出色，通过少量示例提示可以进一步提升性能。然而，当前的评估主要集中在精心构建的基准上，忽视了现实世界中存在缺失和矛盾条件的推理问题，即所谓的不明确问题。我们的观察表明，现有的少量提示方法在这种情况下效果不佳，往往给出过度自信的答案或错误推断。为了深入研究这个问题，我们创建了一个名为“带有缺失和矛盾条件的问题”（PMC）的基准，并引入了两个新指标来评估少量提示方法在处理这类问题时的表现。使用PMC基准的分析揭示了在解决明确问题的数学推理性能与识别不明确问题能力之间存在权衡。针对PMC带来的挑战，我们提出了一种新颖的少量提示方法，称为SMT-LIB提示（SLP）。这种方法利用SMT-LIB语言描述问题，而不是直接求解，然后采用双重检查求解策略验证解决方案的满足性和唯一性，从而提供最终反馈。实验结果全面展示了我们的SLP方法在处理带有缺失和矛盾条件的问题时，相较于现有方法具有显著优势。我们将开源我们的基准和代码，以促进未来的研究。|
|**2024-06-07**|**Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation**|Nachiket Kotalwar et.al.|[2406.05053](http://arxiv.org/abs/2406.05053)|null|### 概述  生成式人工智能和大型语言模型在编程教育中的潜力巨大，它们能够为学习者提供个性化的反馈和提示。当前的研究主要集中在提升生成反馈的质量，以达到人类导师的水平。然而，在实际教育部署中，除了质量外，成本、时间及数据隐私也是关键考量因素。本论文旨在对语言模型在编程反馈生成方面的性能进行全面评估，包括质量、成本、速度和数据隐私等多个维度。我们特别关注利用最新的在浏览器内推理技术，这有助于直接降低成本并保护数据隐私。  为了优化适合浏览器内运行的小型模型的反馈质量，我们开发了一种基于GPT-4生成的合成数据的微调流程。我们将展示如何使用WebLLM的浏览器内推理引擎来优化Llama3-8B和Phi3-3.8B的4位量化模型在三个不同Python编程数据集上的效果。我们承诺会公开全部实现、web应用和数据集，以促进在浏览器语言模型领域的进一步研究。|
|**2024-06-07**|**Bootstrapping Referring Multi-Object Tracking**|Yani Zhang et.al.|[2406.05039](http://arxiv.org/abs/2406.05039)|**[link](https://github.com/zyn213/temprmot)**|## 背景 当前的多对象引用跟踪（RMOT）任务通常依赖于手动标注的数据集和静态规则，这限制了多样性和实施范围。为了解决这个问题，我们的研究主要关注通过引入更多区分性语言词汇来推动RMOT任务的发展。为此，我们首先对Refer-KITTI数据集进行了扩展，创建了Refer-KITTI-V2，它从最初的2,719个手动标注开始，解决了类别不平衡问题，并增加了更多关键词，使其更贴近现实场景，相较于Refer-KITTI有所进步。我们进一步利用大型语言模型扩充这些标注，总计达到9,758个，生成了617个不同的词汇，超越了先前的RMOT基准。  此外，我们还改进了RMOT的端到端框架，采用了一个简单而优雅的时序推进策略，该策略在性能上优于先前的方法。相关源代码和数据集已可在<https://github.com/zyn213/TempRMOT>获取。|
|**2024-06-07**|**Scenarios and Approaches for Situated Natural Language Explanations**|Pengshuo Qiu et.al.|[2406.05035](http://arxiv.org/abs/2406.05035)|null|大型语言模型（LLMs）能够生成适应不同用户情境的自然语言解释（NLE）。然而，对于这种适应性的量化评估尚存空白。为此，我们创建了一个基准数据集——基于情境的解释（Situation-Based Explanation，SBE）数据集，包含100个需要解释的事物（explanandum）。每个事物都配对了针对教师、学生和专业人士等不同受众群体的解释，以便评估模型在满足这些多元化群体信息需求和背景下的解释精准度，如学生、教师和家长。每种“事例-受众”组合都附有人类撰写的参考解释，用于计算分数，以量化模型如何根据情境调整解释。我们在不同规模的预训练语言模型上测试了三种提示方法：规则基础提示、元提示和上下文学习提示。研究发现：1）模型可以通过生成提示产生更精确地符合目标情境的解释；2）明确提示“你是一个有用的助手”并非针对情境化NLE任务的必要技术；3）上下文学习提示仅能帮助模型学习演示模板，但无助于提升其推理性能。SBE数据集和我们的分析为今后生成适应情境的自然语言解释的研究提供了基础。|
|**2024-06-06**|**Verbalized Machine Learning: Revisiting Machine Learning with Language Models**|Tim Z. Xiao et.al.|[2406.04344](http://arxiv.org/abs/2406.04344)|null|受大型语言模型（LLMs）取得的巨大进展启发，我们提出了口头化机器学习（VML）框架。与传统的机器学习模型，通常在连续参数空间中优化不同，VML将参数空间限制为人可理解的自然语言。这种约束促使我们从新角度看待函数逼近问题，即将带有文本提示的LLM视为由文本提示参数化的函数。我们借此视角重新审视了经典机器学习任务，如回归和分类，发现这些问题可以通过LLM参数化的学习器和优化器来解决。VML的主要优势包括：（1）易于编码先验知识：关于问题和假设类的先验知识可以以自然语言形式编码并输入给LLM参数化的学习器；（2）自动模型选择：优化器可以根据数据和口头化先验知识自动选择具体的模型类别，并在训练过程中更新模型类别；（3）可解释的学习者更新：LLM参数化的优化器可以解释每次学习者更新的原因。我们进行了多项实验评估VML的有效性，希望它能成为增强机器学习可解释性和信任度的桥梁。|
|**2024-06-06**|**RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation**|Jiaming Liu et.al.|[2406.04339](http://arxiv.org/abs/2406.04339)|null|在机器人操作的核心目标中，让模型理解视觉场景并执行动作是一个基本任务。尽管现有的机器人多模态大型语言模型（MLLM）能够处理一些基础任务，但它们在两个方面仍面临挑战：1）处理复杂任务的推理能力不足；2）对于MLLM的微调和推理存在高计算成本。近期提出的基于状态空间模型（SSM）的Mamba展示了在非平凡序列建模方面的潜力，具有线性推理复杂度。在此启发下，我们开发了RoboMamba，一个端到端的机器人MLLM，它利用Mamba模型结合机器人推理和动作能力，同时保持高效的微调和推理效率。  首先，我们将视觉编码器与Mamba集成，通过联合训练使视觉数据与语言嵌入对齐，赋予模型视觉常识和与机器人相关的推理能力。为了进一步提升RoboMamba的动作姿态预测能力，我们探索了一种高效的微调策略，仅使用简单的策略头。实验表明，一旦RoboMamba具备足够的推理能力，只需极少的微调参数（模型的0.1%）和时间（20分钟），就能习得操纵技能。在实验中，RoboMamba在通用和机器人评估基准上展现出卓越的推理能力。同时，我们的模型在模拟和真实世界实验中实现了姿态预测的出色表现，其推理速度比现有机器人MLLM快7倍。项目的网页链接为：<https://sites.google.com/view/robomamba-web>。|
|**2024-06-06**|**Coherent Zero-Shot Visual Instruction Generation**|Quynh Phung et.al.|[2406.04337](http://arxiv.org/abs/2406.04337)|null|尽管文本到图像合成技术取得了进步，特别是在扩散模型方面，但生成需要物体在连续步骤中保持一致表示和平滑状态转换的视觉指令仍然是一项艰巨挑战。本文提出了一种无需训练的框架，巧妙地结合了文本理解与图像生成，以确保视觉指令既美观又具有连贯性和准确性。通过测试多步骤指令，并与多个基线进行比较，我们验证了这种方法的有效性。实验结果显示，我们的方法能够生成连贯且视觉上吸引人的指令。|
|**2024-06-06**|**DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs**|Lingchen Meng et.al.|[2406.04334](http://arxiv.org/abs/2406.04334)|null|大多数大型多模态模型（LMMs）通过将视觉令牌作为序列输入到大型语言模型（LLMs）的第一层来实现。这种方法虽然直观，但会显著增加计算和内存开销，因为模型需要处理更多的输入层令牌。本文提出了一种新的架构DeepStack，用于LMMs。在LMM的视觉和语言Transformer的N层中，我们将视觉令牌分为N组，并从底层逐层向上馈送到对应的Transformer层。令人惊讶的是，这种简单的方法极大地增强了LMM在跨层视觉令牌交互方面的建模能力，同时成本几乎不变。我们分别将DeepStack应用于LMM的语言和视觉Transformer，并通过广泛实证结果验证了DeepStack LMM的有效性。  使用相同的上下文长度，我们的DeepStack 7B和13B参数模型在9个基准测试上平均超越同类模型2.7分和2.9分。仅使用五分之一的上下文长度，DeepStack的表现接近于使用完整上下文长度的模型。这些提升在高分辨率任务中尤为明显，例如，与LLaVA-1.5-7B相比，TextVQA、DocVQA和InfoVQA上的性能分别提高了4.2分、11.0分和4.0分。此外，我们还将DeepStack应用到视觉Transformer层，这带来了与LLaVA-1.5-7B相当的平均改进，为3.8分。|
|**2024-06-06**|**PaCE: Parsimonious Concept Engineering for Large Language Models**|Jinqi Luo et.al.|[2406.04331](http://arxiv.org/abs/2406.04331)|**[link](https://github.com/peterljq/parsimonious-concept-engineering)**|**大型语言模型（LLMs）被广泛应用于各种任务，尽管它们能够生成类似人类的回复，但也会产生不良输出，如潜在有害信息、种族或性别歧视性言论以及错误的信息。为了减少这些问题，研究人员开发了对齐方法，如微调、提示工程和表示工程。然而，现有方法面临挑战：一些需要针对每个对齐任务进行昂贵的微调；一些未能充分消除不良概念，对齐效果不佳；一些则删除了良性的概念，降低了LLMs的语言能力。为此，我们提出了名为Parsimonious Concept Engineering（PaCE）的新型激活工程框架，旨在解决这些问题。  首先，我们构建了一个大规模的概念字典，它在激活空间中表示每个原子对应一个语义概念。接着，对于给定的任何对齐任务，我们会使用一个概念分区器高效地标记这些概念为良性或不良。在推理阶段，我们利用稀疏编码方法，根据概念字典分解LLM的激活，将其准确表示为良性成分和不良成分的线性组合。通过移除不良成分，我们能够调整LLMs的行为以符合对齐目标。  我们在回应净化、真实性增强和情感修订等任务上进行了实验，并发现PaCE在实现对齐性能的同时，保持了良好的语言能力，达到了当前最先进的水平。**|
|**2024-06-06**|**Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step**|Zhanhao Liang et.al.|[2406.04314](http://arxiv.org/abs/2406.04314)|**[link](https://github.com/rockeycoss/spo)**|## 背景  近期，Direct Preference Optimization (DPO) 已成功扩展到调整文本到图像的扩散模型，使其与人类偏好保持一致。不同于大多数现有 DPO 方法假设所有扩散步骤都与最终生成图像保持一致的偏好顺序，我们认为这种假设忽略了每个步骤特有的去噪性能，因此应该为每一步定制偏好标签。为此，我们提出了一种新颖的后训练方法——Step-aware Preference Optimization (SPO)，它独立评估并调整每个步骤的去噪性能，利用步级感知偏好模型和步级重采样器来确保准确的步级监督。  在SPO中，我们在每个去噪步骤中会创建一个图像池，寻找合适的胜者-败者对，并且关键在于，我们会从池中随机选择一个图像作为下一次去噪步骤的起点。这个步级重采样过程保证了每次胜者-败者对都来自同一原始图像，使得比较独立于前一步。为了评估每个步骤的偏好，我们训练了一个专门的步级感知偏好模型，适用于模糊和清晰的图像。在Stable Diffusion v1.5和SDXL等实验中，SPO 显著优于最新的Diffusion-DPO，尤其是在处理复杂、详细的提示时，能更好地生成图像并提升美学效果，同时在训练效率上超过20倍。代码和模型可在此链接获取：[https://rockeycoss.github.io/spo.github.io/](https://rockeycoss.github.io/spo.github.io/)。|
|**2024-06-06**|**Semantically Diverse Language Generation for Uncertainty Estimation in Language Models**|Lukas Aichberger et.al.|[2406.04306](http://arxiv.org/abs/2406.04306)|**[link](https://github.com/ml-jku/SDLG)**|**大型语言模型（LLMs）在生成文本时可能会出现幻觉，这阻碍了社会和工业中的各种应用，因为它们会降低LLMs的可信度。当前的LLMs采用自回归方式生成文本，即预测并添加文本标记。当LLMs对生成的下一个标记的语义含义不确定时，很可能会产生幻觉。因此，人们认为幻觉源于预测不确定性。我们提出了“语义多样性语言生成”（Semantically Diverse Language Generation，SDLG），用于量化LLMs的预测不确定性。SDLG引导LLM生成语义多样但又合理的初始文本替代方案，从而提供了精确的aleatoric语义不确定性测量，能够检测初始文本是否可能出现幻觉。  实验在问答任务上表明，SDLG始终优于现有方法，并且在计算效率上最为高效，为LLMs的不确定性估计设定了新的标准。**|
|**2024-06-06**|**Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models**|Phat Nguyen et.al.|[2406.04300](http://arxiv.org/abs/2406.04300)|null|在模拟训练和评估关键安全系统，如自动驾驶车辆时，通过模拟生成各种场景至关重要。然而，模型其他车辆的轨迹以模拟复杂且有意义的近距离交互任务成本高昂。利用语言描述来生成驾驶行为是一种有前景的方法，它提供了一种可扩展且直观的人类操作方式，能够模拟广泛驾驶互动。但大型标注的语言-轨迹数据稀缺是这一方法面临的挑战。为此，我们提出了Text-to-Drive（T2D），这是一种利用大型语言模型（LLMs）合成多样化驾驶行为的技术。我们的方法采用知识驱动两阶段策略：首先，利用LLMs的内置知识生成丰富多样的驾驶行为语言描述；接着，利用其推理能力在模拟器中实现这些行为。T2D的核心是使用LLM构建状态图，将低级状态映射到高级抽象，从而简化了诸如总结低级观测、评估策略与行为描述的一致性以及设计辅助奖励等下游任务，无需人工监督。通过我们的知识驱动方法，我们证明T2D能生成比其他基准更丰富的轨迹，并提供一个自然语言界面，允许用户交互式地融入人类偏好。更多示例请访问我们的网站：<https://text-to-drive.github.io/>|
|**2024-06-07**|**What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages**|Nadav Borenstein et.al.|[2406.04289](http://arxiv.org/abs/2406.04289)|null|## 背景  大型语言模型能够学习什么？根据定义，语言模型（LM）是字符串的分布。因此，可以将这个问题转化为评估字符串分布类的学习能力。尽管先前的研究主要关注理论限制，但我们关注的是实际可学习性。不同于以往的实证工作，我们评估神经语言模型在其“主场”——学习概率语言——上的表现，而不是作为形式语言的分类器。具体来说，我们研究递归语言模型（RLM）由循环神经网络（RNN）和Transformer LM学习的可行性。我们通过实验测试RLM的可学习性，考察其与RLM的复杂参数以及神经LM隐藏层大小的关系。实验结果显示，RLM的秩（对应于其条件分布对数似然线性空间的大小）和采样字符串的预期长度是RNN和Transformer LM可学习性的强且显著预测因素。其他一些预测指标也达到了显著性，但RNN和Transformer之间存在不同的模式。|
|**2024-06-06**|**Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People**|Dun-Ming Huang et.al.|[2406.04278](http://arxiv.org/abs/2406.04278)|**[link](https://github.com/jacobyn/SamplingTonesACL)**|**## 翻译后的中文摘要  对话语气在人际交流中至关重要。随着大型语言模型（LLMs）的日益普及，研究它们与人类交流语气的差异变得尤为重要。然而，当前关于对话模式的研究往往依赖于预先存在的分类体系或文本语料库，这些可能存在实验者偏见，并可能无法充分反映研究领域中的真实世界分布。受认知科学方法的启发，我们提出一种迭代方法，通过交替进行两项任务来同时揭示语气和句子：（1）参与者判断给定句子的语气，（2）另一参与者根据该语气生成句子。我们在人类参与者和GPT-4之间进行了100轮这样的互动，从而获得了一组包含句子和常见对话语气的数据。我们还进行了额外实验，让人类和GPT-4对所有句子标注所有语气。基于1,339名人类参与者、33,370次人类评价以及29,900个GPT-4查询的数据，我们展示了如何使用这种方法创建一个可解释的几何表示，以展示人类和GPT-4之间的对话语气关系。这项工作展示了机器学习和认知科学理念如何结合，以解决人机交互中的挑战。**|
|**2024-06-05**|**Wings: Learning Multimodal LLMs without Text-only Forgetting**|Yi-Kai Zhang et.al.|[2406.03496](http://arxiv.org/abs/2406.03496)|null|## 任务  多模态大型语言模型（MLLMs）起源于预训练的通用语言模型，首先将图像与文本对齐，然后在混合模态输入上进行微调。然而，MLLM在处理仅包含文本的指令时会出现灾难性的遗忘，这些文本指令并未包含图像，这些问题在初始的语言模型阶段就已经存在。本文提出Wings，一个新型的MLLM，它在文本对话和多模态理解方面表现出色。通过分析MLLM在多模态指令中的注意力，我们发现文本遗忘与从图像前向图像后的注意力转移有关。因此，我们构建了额外模块作为增强学习器，以补偿这种注意力转移。视觉和文本学习器作为“翅膀”式的补充，平行连接在每个注意力块内，起初图像和文本输入由视觉学习器与主注意力协同工作，平衡对视觉元素的关注。随后，文本学习器通过注意力路由的方式与视觉学习器的输出协作整合。我们设计了低秩残差注意力（LoRRA）机制以保证学习器的高效运行。  实验结果表明，Wings在文本对话和视觉问答任务上优于同等规模的MLLM。在我们新构建的交错图像-文本（IIT）基准测试中，Wings在从文本为主到多模态为主的问答任务中展现出卓越性能。|
|**2024-06-06**|**Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training**|Ao Sun et.al.|[2406.03488](http://arxiv.org/abs/2406.03488)|**[link](https://github.com/maydomine/seq1f1b)**|大型语言模型（LLMs）的兴起在很大程度上依赖于分布式训练策略，其中管道并行性起着关键作用。随着LLMs的训练序列长度扩展到32k甚至128k，当前的管道并行方法面临严重瓶颈，如高内存占用和显著的管道延迟，这极大地限制了模型的可扩展性和训练吞吐量。为了提高内存效率和训练效率，我们提出了一种针对长序列训练LLMs的高效序列级一次前向一次后向（1F1B）管道调度方法，称为Seq1F1B。Seq1F1B将批级别可调度单元分解为更细的序列级单元，从而减小延迟并降低内存需求。  考虑到如果均匀分割序列，Seq1F1B可能会产生轻微的额外延迟，我们设计了一种基于计算的策略来划分输入序列，以缓解这个副作用。与竞争性的管道基线方法，如Megatron的1F1B管道并行相比，我们的方法在保持更高训练吞吐量的同时，内存占用更低。值得注意的是，Seq1F1B能够在不使用重新计算策略的情况下，有效地在64个NVIDIA A100 GPU上训练一个具有300亿参数的LLM，处理长达64k的序列，这是现有方法无法实现的。我们的代码基于Megatron-LM，并已开源：https://github.com/MayDomine/Seq1F1B.git。|
|**2024-06-05**|**Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends**|Sanjana Ramprasad et.al.|[2406.03487](http://arxiv.org/abs/2406.03487)|null|### 翻译  近期的大型语言模型（LLMs）的进步显著提升了摘要生成系统的性能，但它们在真实性方面的问题引起了关注。尽管之前的研究广泛评估了新闻领域的LLMs，对话摘要的评价主要集中在基于BART的模型上，这在我们理解它们的可信度方面留下了空白。本研究旨在评估LLMs在对话摘要中的真实性，通过人类标注，并着重于识别和分类句级不一致。我们特别关注GPT-4和Alpaca-13B这两款主流模型。我们的评估揭示了错误定义的微妙之处：LLMs常常生成看似合理的推断，这些推断依赖于对话中的间接证据，而缺乏直接证据，这在旧模型中较少见。我们提出了一种改进的错误分类体系，引入了“情境推理”类别来归类这些LLM行为，并公开了相关数据集。利用我们的分类体系，我们比较了LLMs与老式微调模型之间的行为差异。此外，我们系统地评估了自动错误检测方法在LLM摘要上的效果，发现它们在识别这类细微错误时表现不佳。为此，我们提出了两种基于提示的精细错误检测方法，这两种方法优于现有指标，特别是在识别“情境推理”错误时。|
|**2024-06-05**|**BIPED: Pedagogically Informed Tutoring System for ESL Education**|Soonwoo Kwon et.al.|[2406.03486](http://arxiv.org/abs/2406.03486)|null|大型语言模型（LLMs）显示出巨大的潜力，能够作为经济且易于获取的英语第二语言（L2）学习者对话式智能辅导系统（CITS）。然而，现有的CITS往往只能教授简单概念，或者在教学深度上无法满足不同学习策略的需求。为了开发一个更具教育学导向、能教授复杂概念的CITS，我们构建了一个双语教育指导对话数据集（BIPED），包含一对一的人类英语辅导互动。通过对辅导对话的后处理分析，我们提炼出一套包含34种教师行为和9种学生行为的对话动作词典，并将其用于进一步标注收集的数据。根据先预测合适的教师行为再生成相应回复的两步框架，我们利用GPT-4和SOLAR-KO分别实现了两个CITS模型。实验结果表明，这些实施的模型不仅模仿了人类教师的风格，还运用了丰富且与上下文相适应的教学策略。|
|**2024-06-05**|**Does your data spark joy? Performance gains from domain upsampling at the end of training**|Cody Blakeney et.al.|[2406.03476](http://arxiv.org/abs/2406.03476)|null|随着大型语言模型（LLMs）的预训练数据集规模增长到万亿级别的tokens，这些数据集主要由大规模的CommonCrawl网络爬虫内容以及较小的领域特定数据组成。由于在大计算量（FLOPs）下训练以揭示模型在困难和新兴基准上的显著变化成本高昂，如何在通用网络抓取的多样性和领域特定信息密度之间找到最优平衡成为一个问题。本文展示了如何利用这些较小的领域特定数据，在训练后期对其进行上采样，从而在诸如MMLU、GSM8K和HumanEval等基准上提升性能。对于一个训练了1万亿（T）令牌的70亿参数模型，这种简单方法可使其性能提高6.90分、8.26分和6.17分，与训练时间两倍的Llama-2（7B）模型相当。我们研究了在训练后期领域上采样的持续时间，从5%到30%，发现10%到20%的比例最为合适，以平衡一般语言建模能力与特定任务的优化。此外，我们还利用领域上采样来大规模分析单个数据集对不同基准的增益，通过在这一阶段移除它们进行实验。这种方法极大地降低了实验成本，使得能够以预训练运行的十分之一左右的成本探索不同预训练数据集的影响。|
|**2024-06-05**|**AD-H: Autonomous Driving with Hierarchical Agents**|Zaibin Zhang et.al.|[2406.03474](http://arxiv.org/abs/2406.03474)|null|鉴于多模态大语言模型（MLLM）的强大功能，近期的研究聚焦于使用MLLM驱动的自动驾驶系统在大规模动态环境中。然而，常见的方法直接将高级指令转化为低级车辆控制信号，这违背了MLLM的本质生成模式，未能充分利用其潜在能力。因此，这些方法的一般化能力受到训练数据集的极大限制。为解决这个问题，我们提出通过中层语言驱动命令来连接高级指令和低级控制信号，它们比高级指令更细致，但比控制信号更通用且可解释，从而有效弥合两者之间的鸿沟。我们通过一个名为AD-H的分层多代理驾驶系统实现这一理念，包括一个用于高层推理的MLLM规划器和一个轻量级控制器进行低层执行。这种分层设计使MLLM摆脱了低级控制信号解码，充分释放了其在高层感知、推理和规划方面的涌现能力。  我们构建了一个带有动作层次注释的新数据集。全面的闭环评估显示，我们的AD-H系统具有多项关键优势。首先，AD-H在驾驶性能上显著优于现有方法，甚至展现出在车辆操作过程中自我纠正的能力，这是训练数据未涵盖的场景。其次，AD-H在长程指令和新环境条件下表现出色，明显超越当前最先进的方法。我们将公开我们的数据和代码，可通过<https://github.com/zhangzaibin/AD-H>获取。|
|**2024-06-05**|**What is the Best Way for ChatGPT to Translate Poetry?**|Shanshan Wang et.al.|[2406.03450](http://arxiv.org/abs/2406.03450)|null|本文研究了大型语言模型如ChatGPT在英语-中文诗歌翻译任务中的性能，通过定向提示和小样本场景分析以优化其表现。尽管初期结果令人鼓舞，但研究发现ChatGPT的翻译存在持续问题。为此，我们提出了“解释辅助诗歌机器翻译”（EAPMT）方法，该方法利用诗歌的单语解释作为翻译过程的指导。同时，我们改进了现有的评估标准，以更好地适应现代诗歌翻译的微妙之处。我们邀请专业诗人进行评估，并结合GPT-4的评价，结果显示，我们的EAPMT方法在与传统ChatGPT翻译方法以及现有在线系统的比较中表现出色。论文验证了我们方法的有效性，并为文学翻译的机器辅助提供了新颖视角。|
|**2024-06-05**|**Pre-trained Large Language Models Use Fourier Features to Compute Addition**|Tianyi Zhou et.al.|[2406.03445](http://arxiv.org/abs/2406.03445)|null|## 翻译  预训练的大型语言模型（LLMs）在数学推理方面表现出色，但它们如何执行基本的算术运算，如加法，仍不清楚。本文揭示了预训练的LLMs通过傅里叶特征进行加法——这些是隐藏状态中的维度，通过一组在频域中稀疏分布的特征来表示数字。在模型中，多层感知器（MLP）层和注意力层以互补的方式使用傅里叶特征：MLP层主要使用低频特征近似答案的大小，而注意力层主要通过高频特征执行模运算（例如判断答案是否为偶数）。预训练对于这种机制至关重要：从头开始训练的模型仅利用低频特征，导致准确性较低。将预训练的词嵌入引入到随机初始化的模型中可以恢复其性能。总的来说，我们的分析表明，适当的预训练表示（如傅里叶特征）能够解锁Transformer学习算法任务精确机制的能力。|
|**2024-06-05**|**Cycles of Thought: Measuring LLM Confidence through Stable Explanations**|Evan Becker et.al.|[2406.03441](http://arxiv.org/abs/2406.03441)|null|在许多高风险的机器学习应用中，模型需要能够表明其对预测的不确定性至关重要。尽管大型语言模型（LLMs）在各种基准上的准确度可达到甚至超过人类水平，但它们对错误响应的过度自信仍是已知的问题。传统的方法在直接应用于LLMs时可能面临计算成本和封闭源模型的挑战。近期提出了一些黑盒方法，但它们往往依赖于诸如自我表述的信心等启发式。我们提出了一种框架，通过分析模型生成答案的解释分布来衡量LLMs的不确定性。尽管利用解释本身并非新颖，但我们将其视为测试时间分类器，通过计算最可能的分类器后验答案分布，以此进行不确定性评估。  我们展示了使用解释蕴含作为分类器似然性的一种特定框架实例，如何在五个不同的数据集上改进了信心分数指标（特别是AUROC和AURC）。我们的结果表明，该框架既具有理论依据，又是有效量化LLMs不确定性的方式。|
|**2024-06-05**|**Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach**|Saehyung Lee et.al.|[2406.03411](http://arxiv.org/abs/2406.03411)|**[link](https://github.com/saehyung-lee/plugir)**|**该论文主要关注的是交互式文本到图像检索任务中的对话形式上下文查询问题。我们的方法论，名为PlugIR，通过两种方式有效地利用大型语言模型（LLMs）的一般指令跟随能力。首先，通过重述对话形式的上下文，我们消除了在现有视觉对话数据上微调检索模型的需求，从而能够使用任意黑盒模型。其次，我们设计了一个LLM提问者，根据当前上下文中候选图像的信息，生成关于目标图像属性的非冗余问题。这种方法减少了生成问题的噪声和冗余。除了我们的方法，我们还提出了一种新的评估指标，称为最佳对数排名积分（BRI），以全面评估交互式检索系统。PlugIR在多个基准测试中表现出优于零次设置和 Fine-tuned 基准的性能。此外， PlugIR 的两个组成部分可以根据不同情况灵活单独或结合应用。我们的代码已开源在：https://github.com/Saehyung-Lee/PlugIR。**|
|**2024-06-04**|**Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks**|Tianyu He et.al.|[2406.02550](http://arxiv.org/abs/2406.02550)|**[link](https://github.com/ablghtianyi/ICL_Modular_Arithmetic)**|**这篇工作研究了大型语言模型在一组模块化算术任务中出现的上下文学习和技能组合现象。我们关注的是有限数量的一次性模运算函数 $z = a \times x + b \times y \;(\text{mod}\; p)$，这些函数由向量 $(a, b) \in \mathbb{Z}_p^2$ 标记。部分任务被用作预训练，其余用于分布外测试。实验表明，GPT风格的Transformer随着预训练任务数量增加，其在分布内和分布外的泛化能力会经历转变。最小型能实现分布外泛化的模型需要两个Transformer块；而对于更深的模型，分布外泛化阶段是“瞬态”的，需要早期停止。最后，我们对预训练模型进行了可解释性分析，揭示了两种阶段中高度结构化的表示，并讨论了学习到的算法。**|
|**2024-06-04**|**Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning**|Alex Jinpeng Wang et.al.|[2406.02547](http://arxiv.org/abs/2406.02547)|**[link](https://github.com/showlab/VisInContext)**|**这段研究并未介绍最先进的多模态大语言模型（MLLM），而是提出了一种创新方法，旨在有效提升长序列在多模态模型中的处理。我们提出了“Visualized In-Context Text Processing”（VisInContext）技术，通过视觉令牌来处理长文本，从而显著降低GPU内存使用和浮点运算（FLOPs）在训练和推理阶段的需求。例如，对于一个560亿参数的混合 Experts（MOE）模型，我们的方法将预训练中的上下文文本长度扩展到了2048个tokens，而计算量几乎保持不变。实验结果显示，使用VisInContext训练的模型在常见的基于实例的少量数据评估下游任务中表现出色。此外，VisInContext与现有技术相结合，能增强对文档的理解能力，特别适用于文档问答和连续文档检索，显示出巨大的潜力。**|
|**2024-06-04**|**To Believe or Not to Believe Your LLM**|Yasin Abbasi Yadkori et.al.|[2406.02543](http://arxiv.org/abs/2406.02543)|null|我们研究大型语言模型（LLMs）中的不确定性量化，目标是识别对给定查询的响应时的不确定性程度。我们同时考虑了两种类型的不确定性：一种是知识性不确定性（例如对事实或语言真理的未知），另一种是不可消除的随机性（如可能的答案多样性）。特别是，我们提出了一种信息论指标，能够可靠地区分出只有知识性不确定性较大的情况，这时模型的输出是不可靠的。这个条件仅依赖于通过特殊迭代提示基于先前响应得到的模型输出来计算。这种量化方法可以检测单答和多答情况下是否存在虚构（即知识性不确定性高）的情况，这与许多标准的不确定性量化策略（如以响应的对数似然性作为阈值）不同，后者无法识别多答情况下的虚构。  我们进行了一系列实验，展示了我们的方法的优势。此外，我们的研究还揭示了LLM如何通过迭代提示放大对给定输出的概率分配，这可能具有独立的兴趣价值。|
|**2024-06-04**|**Loki: Low-Rank Keys for Efficient Sparse Attention**|Prajwal Singhania et.al.|[2406.02542](http://arxiv.org/abs/2406.02542)|**[link](https://github.com/hpcgroup/loki)**|针对大型语言模型的推理计算成本高昂，特别是当使用长序列时，自注意力机制是主要开销。为了解决这个问题，近期的研究提出了一些稀疏注意力近似方法。本文中，我们通过分析发现，注意力块中的键向量实际上处于一个远低于原始维度的空间。这一观察促使我们提出Loki，一种新的稀疏注意力方法。Loki根据在低维空间计算的注意力得分，对KV缓存中的令牌进行排序和选择。实验结果表明，Loki能够比其他流行近似方法更好地保持模型的效能，同时由于减少了数据移动（加载/存储）和计算成本，加速了注意力计算。|
|**2024-06-04**|**Parrot: Multilingual Visual Instruction Tuning**|Hai-Long Sun et.al.|[2406.02539](http://arxiv.org/abs/2406.02539)|**[link](https://github.com/aidc-ai/parrot)**|随着GPT-4V等多模态大型语言模型的快速发展，人工智能朝着通用人工智能迈出了重要一步。当前的方法主要依赖于监督微调（SFT）来同步视觉编码器与语言模型，从而赋予它们多模态能力。然而，这种做法可能导致随着训练的进行，语言模型处理多种语言的能力逐渐减弱。我们发现，以英语为中心的不平衡SFT数据集会导致非英语语言性能显著下降，原因在于SFT过程中未能有效连接视觉编码器和多语言令牌。为此，我们提出Parrot，一种利用文本引导在语言层面驱动视觉令牌对齐的新方法。Parrot通过让视觉令牌根据不同的语言输入进行条件化，并借助混合专家（MoE）促进多语言令牌的对齐。特别是，为了增强非英语视觉令牌的对齐，我们计算初始视觉特征与文本嵌入之间的跨注意力，然后将其输入到MoE路由器，选择最相关的专家。选定的专家会将初始视觉令牌转化为特定语言的视觉令牌。鉴于目前缺乏评估多语言能力的标准基准，我们还创建并公开了一个大规模多语言多模态基准（MMMB），包括6种语言、15个类别和12,000个问题。Parrot不仅在MMMB和MMM Benchmark上展现出最先进的性能，还在广泛的多模态任务中表现出色。我们将提供Parrot的源代码和训练数据集供公众使用。|
|**2024-06-04**|**Mitigate Position Bias in Large Language Models via Scaling a Single Dimension**|Yijiong Yu et.al.|[2406.02536](http://arxiv.org/abs/2406.02536)|**[link](https://github.com/PositionalHidden/PositionalHidden)**|这篇论文主要探讨了大型语言模型（LLMs）在实际应用中的一个现象——位置偏见，也称为"迷失在中间"。这种偏见在长文本情境中尤为明显，即关键信息在提示中的不同位置会显著影响模型的准确性。研究发现，注意力权重是位置偏见的微观表现。此外，论文指出，因果注意力掩码通过创建位置特定的隐藏状态，也对位置偏见有所贡献。  基于这些洞察，作者提出了一种方法来减轻位置偏见，即调整这些位置特定的隐藏状态。实验在多个任务上进行，包括自然问题多文档问答、键值检索、LongBench和时间线重排，涉及RoPE模型、扩展上下文窗口模型和Alibi模型等多种架构。结果显示，我们的方法通过仅修改隐藏状态的一个维度，就能实现性能提升，最高可达15.2%。研究者还提供了代码供进一步使用，代码地址为：https://aka.ms/PositionalHidden。|
|**2024-06-04**|**SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices**|Ruslan Svirschevski et.al.|[2406.02532](http://arxiv.org/abs/2406.02532)|**[link](https://github.com/yandex-research/specexec)**|随着大型语言模型的广泛应用，高效运行它们变得至关重要。近期的研究通过推测性解码实现了显著的速度提升。然而，大多数工作都是针对数据中心硬件进行设计。本研究反问：我们能在消费级设备上多快地运行LLMs？消费者级GPU已无法容纳最大的模型（500亿参数以上），因此需要将参数卸载到RAM或SSD。当使用卸载参数的方式运行时，推理引擎可以同时处理数百乃至数千个令牌的批次，使其非常适合推测性解码。我们提出SpecExec（推测性执行），这是一种简单的并行解码方法，适用于主流LLM家族，能生成每轮目标模型迭代高达20个令牌的预测。它利用现代LLMs中概率分布的高波动性和模型输出概率之间的高度一致性。SpecExec通过从草稿模型获取最可能的令牌延续，构建一个目标模型的“缓存”树，然后在一个单次遍历中验证。  使用SpecExec，我们在消费级GPU上实现了500亿参数LLM的推理，配合RAM卸载，4位量化下的速度达到4-6个令牌/秒，而16位权重下的速度为2-3个令牌/秒。|
|**2024-06-04**|**Scalable MatMul-free Language Modeling**|Rui-Jie Zhu et.al.|[2406.02528](http://arxiv.org/abs/2406.02528)|**[link](https://github.com/ridgerchu/matmulfreellm)**|**## 翻译  在大型语言模型（LLMs）中，矩阵乘法（MatMul）通常占据主要计算开销。随着LLMs的规模扩大，其嵌入维度和上下文长度也随之增加，这一问题更为显著。本文提出了一种方法，能够在保持强大性能的同时，完全移除LLMs中的MatMul操作，即使是在27亿参数量级的模型上也能实现。实验表明，我们的无MatMul模型在与内存消耗显著更多的状态-of-the-artTransformer相当的条件下表现出色。我们研究了模型的扩展性规律，并发现无MatMul模型与全精度Transformer之间的性能差距随着模型尺寸增大而减小。  此外，我们提供了一个高效的GPU实现，相较于未优化的基线，训练时能减少高达61%的内存使用。在推理阶段，通过优化的内核，我们的模型内存消耗可降低超过10倍。为了准确评估架构效率，我们在FPGA上构建了定制硬件解决方案，利用GPU无法处理的轻量级运算，实现了对十亿参数规模模型的高速处理，使其接近人脑级别的效率。  这项工作不仅展示了LLMs在减小复杂性后仍能保持高效，还指出了未来加速器应优化的运算类型，以适应下一代轻量级LLMs的需求。我们的代码实现已开源至：\url{https://github.com/ridgerchu/matmulfreellm}。**|
|**2024-06-04**|**CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks**|Maciej Besta et.al.|[2406.02524](http://arxiv.org/abs/2406.02524)|**[link](https://github.com/spcl/checkembed)**|大型语言模型（LLMs）正在各个领域带来变革，但验证其答案仍然是一个重大挑战，尤其是在处理复杂、开放性的任务，如知识整合、摘要和提取。本文提出了一种名为CheckEmbed的精确、可扩展且简便的LLM验证方法。CheckEmbed的核心理念是：通过利用如GPT文本嵌入大模型获取的答案级嵌入来比较LLM的回答。这将复杂的文本答案转化为单一的嵌入，简化了对比过程，实现快速而有意义的验证。我们构建了一个全面的验证管道，该管道实现了CheckEmbed的理念，并提供了评估LLM答案真实性的度量，如嵌入热力图及其总结。我们展示了如何利用这些指标设计实际的引擎，以决定LLM答案是否令人满意。在实际文档分析任务中，如术语提取和文档摘要，我们的方法表现出显著的准确性提升、成本效益和运行时间性能，相较于BERTScore或SelfCheckGPT等基于token、句子和事实级别的方案。|
|**2024-06-04**|**RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots**|Soroush Nasiriany et.al.|[2406.02523](http://arxiv.org/abs/2406.02523)|null|## 翻译  人工智能的最新进展在很大程度上依赖于规模的扩大。然而，在机器人领域，大规模机器人数据集的获取是一个瓶颈。我们主张利用逼真的物理模拟来提升环境、任务和数据集的规模，以支持机器人学习方法。为此，我们介绍RoboCasa，这是一个大型的仿真框架，旨在训练能够在日常环境中通用的机器人。RoboCasa的特点是拥有丰富且多样化的厨房场景，包括超过150个类别的一千多件3D模型资产和数十种可交互的家具和电器。  我们通过生成式AI工具进一步增强模拟的真实性和多样性，如使用文本到3D模型的技术生成对象资产，以及通过文本到图像模型生成环境纹理。我们设计了100项任务，包括由大型语言模型指导的复合任务，用于系统性评估。为了促进学习，我们提供了高质量的人类演示，并结合自动轨迹生成方法，以最小的人力成本大幅扩充数据集。  我们的实验表明，在使用合成生成的机器人数据进行大规模模仿学习时，存在明显的规模效应，并显示出利用模拟数据在现实世界任务中的巨大潜力。相关视频和开源代码已在https://robocasa.ai/网站上提供。|
|**2024-05-31**|**Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**|Chaoyou Fu et.al.|[2405.21075](http://arxiv.org/abs/2405.21075)|null|在人工智能的追求中，多模态大型语言模型（MLLMs）已成为近期进步的核心。然而，对它们处理序列视觉数据的能力的关注尚显不足。为此，我们在本文中提出Video-MME，这是首个全面评估MLLMs在视频分析性能的多模态评估基准。我们的工作有四个关键特性：1）视频类型多样，涵盖6个主要视觉领域和30个子领域，确保广泛的应用场景泛化能力；2）时间维度的跨度，包括短、中、长期视频，从11秒到1小时，以检验模型对复杂情境动态的适应性；3）数据模态的广度，结合视频帧以外的多种输入，如字幕和音频，揭示MLLMs的全方位能力；4）高质量的标注，由专家严格手动标记，以保证精确且可靠的模型评估。我们精心挑选并手动注解了900段视频，总时长达到256小时，生成了2,700个问题-答案对。通过Video-MME，我们对包括GPT-4系列、Gemini 1.5 Pro在内的多个最先进的MLLM，以及开源图像模型InternVL-Chat-V1.5和视频模型LLaVA-NeXT-Video进行了深入评估。实验结果显示，Gemini 1.5 Pro是表现最佳的商业模型，明显优于开源模型。我们的数据集和发现强调了改进处理更长序列和多模态数据的必要性。项目网页链接：https://video-mme.github.io|
|**2024-05-31**|**Grammar-Aligned Decoding**|Kanghee Park et.al.|[2405.21047](http://arxiv.org/abs/2405.21047)|null|大型语言模型（LLMs）在生成高度结构化的输出时面临挑战，如程序代码、数学公式或规范的标记。约束解码方法通过限制每次输出可能的令牌，确保输出符合特定规则来缓解这个问题，例如在语法约束解码（GCD）中，LLM的输出必须遵循给定的语法规则。然而，研究表明，这种约束解码可能会扭曲模型的分布，导致生成的输出虽然语法正确，但其概率并不直接反映LLM本身的概率分配，从而质量不高。我们称之为“与语法约束对齐的解码”（Grammar-Aligned Decoding，GAD），并提出了一种名为“自适应采样与近似期望未来”（Adaptive Sampling with Approximate Expected Futures，ASAp）的解码算法。  ASAp算法旨在保证输出的语法性，并理论上产生与LLM在给定语法约束条件下的条件概率相符的结果。该算法利用先前的样本输出来稳健地估算不同输出前缀的未来语法可能性。我们在代码生成和结构化自然语言处理任务上的实验表明，ASAp经常能够生成比现有GCD技术更符合LLM分布且仍遵守所需语法限制的输出，从而提高了整体质量。|
|**2024-05-31**|**Direct Alignment of Language Models via Quality-Aware Self-Refinement**|Runsheng Yu et.al.|[2405.21040](http://arxiv.org/abs/2405.21040)|null|强化学习从人类反馈（RLHF）是调整大型语言模型（LLMs）行为以符合人类偏好的常用方法。最近，直接策略优化（DPO）作为一种替代方案兴起，它不再依赖LLM奖励模型，从而减少了额外的内存和训练时间。然而，DPO忽视了正向和负向响应的相对质量，可能导致训练结果不理想。为解决这个问题，我们探讨利用LLM内部知识在即时微调过程中获取响应的质量，并优化损失函数。我们设计了一种细化函数，利用LLM的知识来估计正向和负向响应的品质。实验表明，在轻度假设下，构建的细化函数能够帮助自我调整损失函数。我们将这个细化功能整合到DPO及其变体身份策略优化（IPO）中。实验证明，这些改进后的模型在各种评估者上表现出优于DPO和IPO的性能。|
|**2024-05-31**|**Standards for Belief Representations in LLMs**|Daniel A. Herrmann et.al.|[2405.21030](http://arxiv.org/abs/2405.21030)|null|随着大型语言模型（LLMs）在各个领域展现出非凡能力，计算机科学家们正在寻求理解它们的认知过程，特别是关于LLMs如何（如果有的话）内部构建对世界的信念。然而，目前尚缺乏一个统一的理论框架来支撑对LLM中信念的研究。本文试图填补这一空白，提出了一套条件，使LLM中的表示能够被视为信念似的。我们指出，尽管在LLMs中测量信念的项目与决策理论和形式认识论中的信念测量在许多方面有相似之处，但也存在差异，这些差异应影响我们的测量方法。因此，借鉴哲学洞察和机器学习的当代实践，我们确立了四个标准：准确性、一致性、统一性和实用性。这四个标准结合了理论考量与实际限制，为全面理解LLM中的信念表示奠定了基础。我们引用实证工作的成果，揭示了单独使用某些标准时识别信念表示的局限性。|
|**2024-05-31**|**LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models**|Elias Stengel-Eskin et.al.|[2405.21028](http://arxiv.org/abs/2405.21028)|**[link](https://github.com/esteng/pragmatic_calibration)**|**当回答问题时，语言模型不仅能提供答案，还能传达对答案正确性的信心程度。这包括明确的分数标记，如给出数字，以及隐含的信心标志，如权威语气或提供额外知识。然而，当前大多数模型往往过于自信。为了校准这些信心度，我们提出了一种实用的、考虑听众的微调方法（LACIE），它不仅关注答案是否正确，还关注答案是否会被听众接受。我们将校准视为偏好优化，通过双代理游戏创建数据，让一个演讲者模型的输出接受模拟听者的评判。然后，我们使用LACIE对三个语言模型（Mistral-7B、Llama3-8B和Llama3-70B）进行微调，并显示经过微调的模型在模拟听者面前有更好的校准。重要的是，这些趋势也适用于人类听众，帮助他们更准确地预测模型的正确性：我们在人机评估中发现，经过LACIE训练的模型接受的错误答案减少了47%，而正确答案的接受率保持不变。此外，LACIE泛化到另一个数据集上，在使用TriviaQA训练后，TruthfulQA上的真实性大幅提高。我们的分析表明，LACIE导致了正确和错误示例之间的信心度更好地分离。定性上，我们发现经过LACIE训练的模型会更加谨慎，并在回答正确时通过使用权威语气或提供细节来隐性地表示确定性。最后，LACIE微调导致模型对于可能错误的答案更倾向于放弃（例如说“我不知道”）。**|
|**2024-05-31**|**Improved Techniques for Optimization-Based Jailbreaking on Large Language Models**|Xiaojun Jia et.al.|[2405.21018](http://arxiv.org/abs/2405.21018)|**[link](https://github.com/jiaxiaojunqaq/i-gcg)**|**随着大型语言模型（LLMs）的快速发展，其安全校准成为广泛应用的关键。针对这些模型的破解（即“jailbreaking”）活动日益增多，其中贪婪坐标梯度（GCG）攻击因其成效显著而受到关注。然而，GCG的攻击效率仍有提升空间。本文提出了一系列改进的优化基线破解技术，以提升GCG的性能。首先，我们注意到单个目标模板“Sure”极大地限制了GCG的攻击效果，因此我们建议采用包含有害自我暗示和/或指导的多样化目标模板，以误导模型。在优化策略上，我们建议在GCG中实施自动多坐标更新，以加速收敛，并引入从简单到复杂（easy-to-hard）的初始化技巧。将这些改进整合，我们开发出一种高效的方法—— $\mathcal{I}$ -GCG。实验在一系列基准测试，如NeurIPS 2023 红队挑战中进行，结果显示，我们的改进技术能够帮助GCG超越现有破解攻击，实现接近100%的攻击成功率。代码已发布在https://github.com/jiaxiaojunQAQ/I-GCG。**|
|**2024-05-31**|**DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models**|Linli Yao et.al.|[2405.20985](http://arxiv.org/abs/2405.20985)|**[link](https://github.com/yaolinli/deco)**|该研究关注于多模态语言模型（MLLMs）中的投影器模块，因为它们在连接视觉和语言模态、促进跨模态对齐方面发挥关键作用。然而，目前对于投影器在视觉-语言对齐方面的效果评估仍显不足，通常只能通过下游任务的性能间接推断。为此，本研究通过分析MLLM中的视觉-语言语义流，来解读投影器的工作机制。  具体来说，研究者追踪从生成的语言标记到原始视觉编码块以及投影器产生的中间输出之间的语义相关性流。发现压缩型投影器（如QFormer）倾向于将视觉块抽象成有限的几个概念，如物体或属性，导致“双重抽象”现象：首先，投影器参照预定义查询令牌进行视觉语义抽象，然后，基于文本指令的大语言模型进一步提取。这种双重抽象在训练过程中效率不高，并可能导致视觉语义信息的累积缺失。  为解决这个问题，研究提出“解耦压缩与抽象（DeCo）”的关键洞察，即在投影层面上将视觉令牌数量压缩，而让大语言模型完全负责视觉语义抽象。因此，研究人员采用了一种简单的压缩器——二维自适应池化，以无参数的方式降低视觉块的尺寸。实验结果显示，DeCo在性能和效率上都优于传统的压缩投影器。它在MLLM基准、视觉定位和开放性视觉问答任务中分别取得了0.9%、7.1%和2.9%的性能提升，同时拥有更少的可训练参数和更快的收敛速度。|
|**2024-05-31**|**Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training**|Feiteng Fang et.al.|[2405.20978](http://arxiv.org/abs/2405.20978)|**[link](https://github.com/calubkk/raat)**|大型语言模型（LLMs）展现出强大功能，但面临挑战，如虚构、过时知识和难以追溯的推理过程。为解决这些问题，检索增强生成（RAG）作为一种有前景的方法崭露头角，它结合外部数据库的知识。然而，不适当的检索段落可能妨碍LLMs生成全面且高质量的回答。先前关于RAG中检索噪声稳健性的研究往往局限于有限的噪声类型，这与现实世界的检索环境不符，限制了实际应用。本研究首先探讨了检索噪声，并将其分为三种不同的类别，反映真实环境。我们分析了这些不同类型的检索噪声对LLMs稳健性的影响。  接着，我们提出了一种新颖的RAG方法，称为检索增强自适应对抗训练（RAAT）。RAAT利用自适应对抗训练来动态调整模型的训练流程以应对检索噪声，并采用多任务学习确保模型能够识别嘈杂的上下文。大量的实验表明，在各种噪声条件下，使用RAAT训练的LLaMA-2 7B模型在F1和EM分数上显示出显著提升。为了便于复现，我们已在https://github.com/calubkk/RAAT上发布了我们的代码和数据。|
|**2024-05-31**|**SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales**|Tianyang Xu et.al.|[2405.20974](http://arxiv.org/abs/2405.20974)|**[link](https://github.com/xu1868/sayself)**|**大型语言模型（LLMs）常常产生不准确或虚假的信息，并且通常无法表明其信心水平，这限制了它们的广泛应用。先前的研究试图通过直接提示或自我一致性提示来提取LLMs的信心，或者构建特定数据集进行监督微调。基于提示的方法性能较差，而基于训练的方法又局限于二元或不精确的整体信心估计。本文提出了一种先进的方法——SaySelf，这是一个训练框架，旨在教导LLMs提供更精确的细粒度信心估计。  此外，SaySelf还推动LLMs生成自我反思的解释，明确指出它们在参数知识上的空白并解释不确定性。这是通过让LLM以自然语言的形式自动总结特定知识中的不确定性来实现的。这种总结是基于对多个采样推理链的不一致性分析，生成的数据用于监督微调。为了进一步校准信心估计，我们采用了精心设计的强化学习，奖励准确、高置信度的预测，同时惩罚错误输出中的过度自信。  实验结果表明，无论是在分布内还是分布外的数据集上，SaySelf都能有效减少信心校准误差，同时保持任务性能。生成的自我反思理由也被证明是合理的，能进一步促进校准。代码已公开在：\url{https://github.com/xu1868/SaySelf}。**|
|**2024-05-31**|**LCQ: Low-Rank Codebook based Quantization for Large Language Models**|Wen-Pu Cai et.al.|[2405.20973](http://arxiv.org/abs/2405.20973)|null|## 背景  大型语言模型（LLMs）在众多任务上展现出优异性能，但它们的存储和计算成本高成为部署的一大挑战。为了压缩模型并降低成本，权重量化技术被广泛应用。目前，大多数针对LLMs的量化方法使用秩一码本，然而在高压缩比下，这会导致显著的精度损失。本文提出了一种新颖的权重量化方法，称为低秩码本量化（LCQ），旨在解决这一问题。  ## 方法  LCQ采用低秩码本进行量化，其秩可以大于一。这种方法旨在通过利用更高的秩来保持或提升模型的精度，同时控制额外的存储开销几乎为零。实验表明，与现有方法相比，LCQ在保持良好准确性的前提下，能够实现更优的压缩效果。  ## 结论  综上所述，本文介绍了一种创新的低秩码本量化方法，它有望在不显著增加存储成本的情况下，提升大型语言模型在实际应用中的性能和效率，为高效部署这些模型提供了新的解决方案。|
|**2024-05-30**|**MotionLLM: Understanding Human Behaviors from Human Motions and Videos**|Ling-Hao Chen et.al.|[2405.20340](http://arxiv.org/abs/2405.20340)|**[link](https://github.com/IDEA-Research/MotionLLM)**|这项研究关注于多模态（视频和动作模态）下的人类行为理解，通过大型语言模型（LLMs）的强大功能。与专为单模态（视频或动作）设计的最新LLMs不同，我们认为理解人类行为需要对视频和动作序列（如SMPL序列）进行联合建模，以有效捕捉精细的身体部位动态和语义。为此，我们提出MotionLLM，这是一个简洁而有效的框架，用于人类动作理解、描述和推理。MotionLLM采用了一体化的视频-动作训练策略，利用现有粗粒度的视频-文本数据和精细动作-文本数据的优势，以获取丰富的空间-时间洞察。此外，我们还创建了一个大规模的MoVid数据集，包含了多样化的视频、动作、caption和指令。我们还提出了MoVid-Bench，它具有精心的手动标注，以更好地评估在视频和动作上的人类行为理解能力。实验结果充分展示了MotionLLM在caption生成、空间-时间理解以及推理能力方面的优越性。|
|**2024-05-30**|**Visual Perception by Large Language Model's Weights**|Feipeng Ma et.al.|[2405.20339](http://arxiv.org/abs/2405.20339)|**[link](https://github.com/FeipengMa6/VLoRA)**|这篇论文的背景是现有的多模态大型语言模型（MLLMs）采用了一种方法，即将视觉信息与语言模型的输入空间对齐，然后将视觉令牌与文本令牌合并，形成统一的序列输入给语言模型。然而，这种方法由于增加了由视觉令牌导致的输入序列长度，计算成本较高。为此，论文提出了一种新颖的参数空间对齐范式，通过将视觉信息表示为模型权重来处理。对于每个输入图像，首先使用视觉编码器提取特征，然后将这些特征转换为感知权重，并将其与语言模型的权重融合。这样，语言模型的输入无需视觉令牌，从而缩短了输入序列，显著提高了效率。  基于这一理念，论文提出了VLoRA模型，其中包含一个感知权重生成器。该生成器设计成能够将视觉特征转化为具有低秩特性的感知权重，类似于LoRA（低秩自适应训练）。实验结果表明，尽管VLoRA在多种多模态任务的基准上表现出与现有MLLMs相当的性能，但其在训练和推理阶段的计算成本显著降低。论文承诺开源代码和模型。|
|**2024-05-30**|**Xwin-LM: Strong and Scalable Alignment Practice for LLMs**|Bolin Ni et.al.|[2405.20335](http://arxiv.org/abs/2405.20335)|**[link](https://github.com/xwin-lm/xwin-lm)**|**本文介绍Xwin-LM，一个专为大型语言模型（LLMs）设计的全面对齐方法套件。它涵盖了监督微调（SFT）、奖励建模（RM）、拒绝采样微调（RS）和直接偏好优化（DPO）等多种关键技术。主要组成部分包括：(1) 使用高质量指令数据进行初始微调的Xwin-LM-SFT；(2) 由GPT-4精心标注的大型多轮偏好数据集Xwin-Pair；(3) 在7B、13B和70B参数规模上训练的Xwin-RM奖励模型；(4) 每个提示关联64个独特响应的多wise偏好数据集Xwin-Set，这些响应由Xwin-LM-SFT生成并由Xwin-RM评分；(5) 使用Xwin-Set中最高得分响应进行微调的Xwin-LM-RS模型；(6) 通过DPO算法在Xwin-Set上进一步优化的Xwin-LM-DPO模型。我们在AlpacaEval和MT-bench上的评估显示了整个管道的稳定且显著改进，证明了Xwin-LM的强大和可扩展性。我们将在https://github.com/Xwin-LM/Xwin-LM的仓库中持续更新，以促进社区研究。**|
|**2024-05-31**|**ParSEL: Parameterized Shape Editing with Language**|Aditya Ganeshan et.al.|[2405.20319](http://arxiv.org/abs/2405.20319)|null|本文提出了一种名为ParSEL的系统，它旨在通过自然语言实现高质量3D资产的可控编辑。面对自然语言在精确操控上的局限性，ParSEL接收一个分割的3D网格和编辑请求，生成一个参数化的编辑程序。用户可以调整程序参数，精细地探索形状变化，控制编辑幅度。系统利用大型语言模型（LLMs）来理解初始编辑指令，但发现它们在推断完整编辑程序时常常不足，产生的结果可能违反形状逻辑。为此，我们设计了分析性编辑传播（Analytical Edit Propagation，AEP）算法，它从初始编辑种子开始，通过计算机代数系统进行几何分析，寻找与潜在用户编辑兼容的分析性编辑操作，以生成完整的编辑程序。实验表明，相较于其他方案，ParSEL通过自然语言请求有效地实现了对3D对象的可控编辑。|
|**2024-05-30**|**CausalQuest: Collecting Natural Causal Questions for AI Agents**|Roberto Ceraolo et.al.|[2405.20318](http://arxiv.org/abs/2405.20318)|**[link](https://github.com/roberto-ceraolo/causal-quest)**|**人类天生就有寻求因果关系的驱动力，无论是出于好奇心还是特定目标。为了开发能处理这种人类本性追求的AI代理，我们急需一个全面的自然因果问题数据集。然而，现有的数据集要么包含人工制造的问题，无法反映实际AI应用场景，要么在特定来源的问题覆盖上有限。为此，我们提出了CausalQuest，这是一个源自社交网络、搜索引擎和AI助手的13,500个自然出现的问题的数据集。我们定义了因果问题，并建立了更细致的分类体系。通过人类标注员和大型语言模型的协作，我们对数据集进行了精心标注。研究发现，42%的人类提问实际上是关于因果的，大部分是想了解给定结果背后的原因。利用这个数据集，我们训练了高效的二分类器（高达28.5亿参数），用于识别因果问题，实现了高性能，F1分数高达0.877。最后，我们提出了一系列丰富的未来研究方向，这些都可以基于我们的数据和模型进行扩展。**|
|**2024-05-30**|**ANAH: Analytical Annotation of Hallucinations in Large Language Models**|Ziwei Ji et.al.|[2405.20315](http://arxiv.org/abs/2405.20315)|**[link](https://github.com/open-compass/anah)**|**### 背景  大型语言模型（LLMs）的“幻觉”问题对于其广泛应用至关重要。然而，对这一问题的细致测量在社区中并未得到充分探索。为此，我们提出了一项名为 $\textbf{ANAH}$ 的双语数据集，专注于生成式问答中的LLM幻觉分析。ANAH中的每个答案句子都经过严谨标注，包括参考片段检索、幻觉类型的判断以及错误内容的修正。该数据集包含约12,000个句级注释，涵盖了大约4,300个LLM响应，涉及超过700个主题，通过人机交互式流程构建而成。由于幻觉注释的精细粒度，我们可以定量确认LLMs的幻觉问题随着答案的扩展而逐渐增加，并利用ANAH来训练和评估幻觉标注器。  ### 任务  我们构建了大约12,000条句子级别的注释，针对约4,300个LLM生成的回答，涵盖了超过700个主题。这个名为ANAH的数据集通过人类参与的流程精心设计，旨在提供关于生成式问答中LLMs幻觉的详尽分析。通过细致的幻觉标注，我们能够量化地验证LLMs在生成答案时幻觉问题的累积，并利用ANAH来训练和评估幻觉识别能力。我们的实验深入研究了生成式和区分性标注器，并发现尽管开源LLMs在精细幻觉标注方面面临挑战，但使用ANAH训练的生成式标注器能够超越所有开源模型，甚至接近GPT-3.5的表现，并展现出在未见过问题上的良好泛化能力。**|
|**2024-05-30**|**Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation**|Guillaume Huguet et.al.|[2405.20313](http://arxiv.org/abs/2405.20313)|**[link](https://github.com/dreamfold/foldflow)**|蛋白质在几乎所有的生物过程中发挥关键作用，其多样化的功能源于复杂的三维结构，而这些结构又由氨基酸序列决定。在这篇论文中，我们利用氨基酸序列丰富的生物学归纳偏置，提出了一种新的序列条件的SE(3)等变流匹配模型——FoldFlow-2，用于蛋白质结构生成。与FoldFlow家族的先前模型相比，FoldFlow-2引入了新颖的架构特性，包括用于编码序列的蛋白质大语言模型、结合结构和序列表示的新多模态融合主干，以及基于几何变换器的解码器。为了增加生成样本的多样性和新颖性——这对新药设计至关重要——我们在比先前工作使用的PDB数据集大一个数量级的新数据集上大规模训练FoldFlow-2，该数据集包含了已知的PDB蛋白质和通过过滤获得的高质量合成结构。此外，我们展示了如何通过引入强化微调（Reinforced Finetuning，简称ReFT）目标，使FoldFlow-2能够适应任意奖励，如提高二级结构多样性。  实验结果表明，FoldFlow-2超越了现有基于蛋白质结构的生成模型的状态，无论在无条件生成还是在设计性、多样性和新颖性方面，都优于RFDiffusion，且在蛋白质长度的各类任务上表现出良好的泛化能力，特别是在等温构象采样任务上。最后，我们展示了一个经过微调的FoldFlow-2在诸如VHH纳米抗体骨架设计等具有挑战性的条件设计任务上取得了进展。|
|**2024-05-30**|**Large Language Models Can Self-Improve At Web Agent Tasks**|Ajay Patel et.al.|[2405.20309](http://arxiv.org/abs/2405.20309)|**[link](https://github.com/AjayP13/webdreamer)**|在复杂的环境中，如网络浏览器，训练模型作为能够有效导航和执行动作的代理通常具有挑战性，主要受限于缺乏训练数据。近年来，大型语言模型（LLMs）显示出通过自然语言提示以零样本或少量样本来在新环境中导航的能力。研究还表明，LLMs可以通过自我改进（即在其自身生成的数据上微调）来超越基础性能。本研究旨在探究LLMs在长时序任务的复杂环境——WebArena基准中，通过自我改进能否提升其表现。WebArena要求代理自主浏览网页并执行操作以达成特定目标。我们使用三种不同的合成训练数据混合进行微调，并发现经过自我改进后，模型在WebArena基准上的任务完成率提高了31%。此外，我们还提出了新的评估指标，用于更全面地评估我们的微调代理模型的行为性能、鲁棒性、能力以及轨迹质量，这些指标超越了当前仅依赖于整体基准分数的评估方式。|
|**2024-05-30**|**Group Robust Preference Optimization in Reward-free RLHF**|Shyam Sundhar Ramesh et.al.|[2405.20304](http://arxiv.org/abs/2405.20304)|**[link](https://github.com/rsshyam/Group-robust-preference-optimization)**|**## 翻译  针对大型语言模型（LLMs）的特定任务进行适应时，通常需要通过基于人类反馈的强化学习（RLHF）和多元标签者群体（如不同性别、种族、公司团队等）的偏好数据进行微调。然而，传统方法倾向于采用“一刀切”的策略，即假设并优化单一的偏好模型，对各群体的独特特性和需求不够敏感。为此，我们提出了一种新颖的群体鲁棒偏好优化（GRPO）方法，旨在稳健地使LLMs适应各个群体的偏好。GRPO方法基于无奖励直接偏好优化，但区别于以往，它目标是寻找一个能最大化最差群体性能的鲁棒策略。为了实现这一目标，GRPO会动态且逐次调整不同群体的权重，优先关注累积损失较高的群体。我们在理论上探讨了GRPO的可行性，并分析了其在对数线性策略类别下的收敛性。通过使用来自不同群体的全局意见数据对LLMs进行GRPO微调，我们显著提高了最差群体的表现，减少了群体间损失的不平衡，同时提高了概率准确性，相较于非鲁棒基线，这些改进效果显著。**|
|**2024-05-30**|**Who Writes the Review, Human or AI?**|Panagiotis C. Theocharopoulos et.al.|[2405.20285](http://arxiv.org/abs/2405.20285)|null|随着人工智能在自然语言处理中的广泛应用，人们关注如何识别不同领域的AI生成文本。本研究旨在探讨这个问题，通过提出一种方法来准确区分人工智能生成的和人类撰写的书评。我们的方法利用迁移学习，让模型能够在不同主题间识别生成文本，同时提高其识别写作风格和词汇变化的能力。我们构建了一个数据集，包含真实的书评和使用Vicuna开源语言模型生成的模拟评论，以评估所提方法的有效性。实验结果显示，识别文本原创来源是可行的，准确率达到96.86%。我们的工作聚焦于大型语言模型在文本识别方面的性能与局限性研究，这对于未来有效管理此类模型以及确保人类创作内容的完整性和真实性具有重要意义。|
|**2024-05-29**|**X-VILA: Cross-Modality Alignment for Large Language Model**|Hanrong Ye et.al.|[2405.19335](http://arxiv.org/abs/2405.19335)|null|我们提出X-VILA，一种旨在增强大型语言模型（LLMs）功能的多模态模型，它融合了图像、视频和音频模态。通过将各模态特定的编码器与LLM输入对齐，并将扩散解码器与LLM输出对齐，X-VILA实现了跨模态理解、推理和生成。为了支持这种跨模态对齐，我们开发了一个有效的任意模态指令跟随数据集。然而，我们发现当前的跨模态对齐方法存在一个关键问题，导致视觉信息丢失。为此，我们设计了视觉对齐机制，包括一个视觉嵌入高速公路模块，以解决这一问题。此外，我们还提供了一种资源高效的训练策略，使得X-VILA在任意模态对话任务上表现出色，大幅超越先前的方法。令人惊讶的是，即使在缺乏类似训练数据的情况下，X-VILA在不同模态间也展现出涌现特性。该项目将开源。|
|**2024-05-29**|**LLMs Meet Multimodal Generation and Editing: A Survey**|Yingqing He et.al.|[2405.19334](http://arxiv.org/abs/2405.19334)|**[link](https://github.com/yingqinghe/awesome-llms-meet-multimodal-generation)**|**随着大型语言模型（LLMs）的最新进展，人们越来越关注将它们与多模态学习相结合。当前的多模态大语言模型（MLLMs）调查主要集中在理解上。这篇综述详细探讨了跨图像、视频、3D和音频等领域的多模态生成，特别强调了这些领域中的里程碑式工作及其技术进步。我们深入研究了这些方法的关键技术组件，以及在相关研究中使用的多模态数据集。此外，我们还剖析了借助现有生成模型进行人类-计算机交互的工具增强型多模态代理。最后，我们全面讨论了人工智能安全的进步，并探索了新兴应用和未来前景。我们的工作提供了一个系统而深入的多模态生成概述，有望推动生成内容的人工智能（AIGC）和世界模型的发展。所有相关的论文列表可在<https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation>找到。**|
|**2024-05-29**|**Multi-Modal Generative Embedding Model**|Feipeng Ma et.al.|[2405.19333](http://arxiv.org/abs/2405.19333)|null|在大多数多模态任务中，问题可以归结为生成或嵌入。现有的模型通常通过将语言模块分解为一个用于生成的文本解码器和一个用于嵌入的文本编码器来处理这两种问题。为了探索多模态方法的简约性，本工作试图仅使用一个模型来处理每种模态。为此，我们提出了一种多模态生成嵌入模型（MM-GEM），它将生成和嵌入目标整合到一个大型语言模型中。同时，我们设计了PoolAggregator，以提高效率并实现细粒度的嵌入和生成能力。  令人惊讶的是，这两个目标之间并没有显著冲突。例如，基于ViT-Large和TinyLlama的MM-GEM在诸如跨模态检索和零样本分类等多模态嵌入模型基准上表现出良好的性能，同时具备良好的图像描述能力。此外，MM-GEM能够无缝执行区域级别的图像描述生成和检索任务。另外，MM-GEM中的先进文本模型对于长文本和图像检索的Recall@1指标带来了超过5%的提升。|
|**2024-05-29**|**Self-Exploring Language Models: Active Preference Elicitation for Online Alignment**|Shenao Zhang et.al.|[2405.19332](http://arxiv.org/abs/2405.19332)|**[link](https://github.com/shenao-zhang/selm)**|****摘要：**  偏好优化，特别是在人类反馈强化学习（RLHF）的驱动下，已经在使大型语言模型（LLMs）遵循人类意愿方面取得了显著成就。相较于使用固定数据集的离线对齐，通过人或人工智能对模型生成的反馈通常能够通过迭代过程提升奖励模型的能力和LLMs的一致性。然而，要实现全局准确的奖励模型，需要系统地探索生成各种各样的响应，以涵盖自然语言的广阔空间。仅依赖标准奖励最大化LLMs的随机采样是不足以满足这一需求的。  为解决这个问题，我们提出了一种双层目标，乐观地倾向于可能具有高奖励的响应，以此来主动探索分布外区域。通过解决内层问题，利用重新参数化的奖励函数，我们提出了名为Self-Exploring Language Models（SELM）的算法。它消除了对单独奖励模型（RM）的需求，并通过一个直观的目标对LLMs进行迭代更新。与直接偏好优化（DPO）相比，SELM的目标降低了对未见过的过度延伸的无差别偏好，提高了探索效率。  我们的实验结果显示，在Zephyr-7B-SFT和Llama-3-8B-Instruct模型上进行微调后，SELM在MT-Bench和AlpacaEval 2.0等指令跟随基准以及不同设置下的各种标准学术基准上表现出显著的性能提升。我们的代码和模型已可在<https://github.com/shenao-zhang/SELM>获取。**|
|**2024-05-29**|**Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation**|Atrisha Sarkar et.al.|[2405.19328](http://arxiv.org/abs/2405.19328)|null|本文提出了一种名为“规范模块”的架构，它针对生成性代理在面对包含现有规范的社会结构时的协作挑战。这些代理通过大型语言模型理解和评估环境，但在处理复杂社会任务时，如何识别并适应规范基础设施成为关键问题。规范模块的核心在于促进均衡选择，借鉴分类机构实现相关均衡的概念，使代理能够通过同伴互动学习环境中不同候选机构中的权威性。通过提升规范能力，代理可以协调制裁行为，进而影响社交环境中的基本行为，从而提高整体福祉。  我们设计了一个支持机构的新环境，并根据两个主要标准来评估该框架：一是代理能否忽略非权威机构，二是代理在多个选项中识别权威机构的能力。实验结果显示，配备了规范模块的代理相比基础代理能实现更稳定的合作效果，这为研究设计考虑规范基础设施的环境和代理开辟了新途径。|
|**2024-05-29**|**MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series**|Ge Zhang et.al.|[2405.19327](http://arxiv.org/abs/2405.19327)|**[link](https://github.com/multimodal-art-projection/map-neo)**|近年来，大型语言模型（LLMs）在各种任务上取得了显著进步。然而，出于商业利益，像GPT、Gemini和Claude这样的最先进模型被封闭在专有接口后，其训练详情并未公开。近期，一些机构开源了类似性能的LLMs，如LLaMA-3，但大多数细节（如中间检查点、预训练语料库和训练代码等）仍未披露。为了提高LLMs的透明度，研究界正在推动真正开放的模型，如Pythia、Amber和OLMo，这些模型提供了更多的信息，促进了对大模型性能、局限性、偏见和风险的科学研究。然而，现有的开放模型在推理、知识和编程任务上的表现仍逊于同等规模的封闭源码模型。  因此，我们开源了MAP-Neo，一个拥有70亿参数的双语语言模型，从头开始在4.5万亿高质量令牌上进行训练。MAP-Neo是首个与现有顶级LLMs性能相当的完全开源的双语模型。此外，我们还公开了所有细节，包括清理后的预训练语料库、数据清洗流程、检查点以及优化的训练和评估框架，以供重现。我们期望MAP-Neo能推动开放研究社区的发展，激发更多创新，促进LLMs的进一步提升。|
|**2024-05-29**|**Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models**|Tianrun Chen et.al.|[2405.19326](http://arxiv.org/abs/2405.19326)|null|本文提出了一项新的任务：零样本3D推理分割，目标是针对物体的部件搜索和定位，这是一种超越了先前类别特定的3D语义分割、3D实例分割和开放词汇3D分割局限的新范式。我们设计了一个名为Reasoning3D的简单基线方法，它能够理解和执行复杂的命令，对3D网格进行（细致）部分分割，同时具备上下文感知和推理答案的交互式分割能力。特别地，Reasoning3D利用预训练的2D分割网络，该网络由大型语言模型（LLMs）驱动，在零样本情况下解析用户输入查询。已有研究表明，大规模预训练赋予基础模型世界知识的先验，使其能够理解复杂指令，这使得我们在依赖有限3D数据集的情况下也能“分割任何东西”（源效率高）。实验表明，我们的方法具有泛化性，能有效根据隐性文本查询在3D对象（3D网格）中定位和突出显示部分，包括可动3D对象和真实世界的扫描数据。此外，我们的无监督方法便于快速部署，并为未来3D（语义）对象理解领域的研究，如机器人、物体操作、部件组装、自动驾驶应用、增强现实和虚拟现实（AR/VR）、以及医疗应用，提供了一个可行的通用基准。代码、模型权重、部署指南和评估协议可在以下链接获取：http://tianrun-chen.github.io/Reason3D/。|
|**2024-05-29**|**Nearest Neighbor Speculative Decoding for LLM Generation and Attribution**|Minghan Li et.al.|[2405.19325](http://arxiv.org/abs/2405.19325)|null|大型语言模型（LLMs）常常会产生虚构内容且缺乏对生成文本的来源标注。为解决这些问题，半参数化语言模型如kNN-LM通过在非参数数据存储中寻找与给定提示最接近的邻居来改进LM输出。然而，这类模型的推理速度通常较慢，生成的文本流畅度不高。本文提出了一种新颖的半参数化语言建模方法——Nearest Neighbor Speculative Decoding（NEST），它能够将现实世界中的任意长度文本片段融入生成过程，并提供其源头的标注。NEST在每次推理步骤中进行基于令牌的检索，计算出一个半参数混合分布，并从语料库中识别出可能的连续文本段落扩展。它采用一种近似推测解码策略，接受检索到的片段前缀或生成新的令牌。NEST显著提高了基础LM在各种知识密集型任务中的生成质量和来源标注率，超越了传统的kNN-LM方法，并在基于上下文的检索增强方面表现出竞争力。此外，NEST大幅提升了生成速度，当应用于Llama-2-Chat 70B时，推理时间提高了1.8倍。|
|**2024-05-29**|**Are Large Language Models Chameleons?**|Mingmeng Geng et.al.|[2405.19323](http://arxiv.org/abs/2405.19323)|null|大语言模型（LLMs）是否拥有自己的世界观和人格倾向？研究人员进行了超过一百万次的实验，让LLMs回答主观问题。通过将这些模型的响应与欧洲社会调查（ESS）的实际数据进行比较，结果显示提示对偏见和变异性有显著影响，揭示了重大的文化、年龄和性别偏差。文中讨论了评估LLMs与调查数据差异的方法，如计算加权平均值以及一个新提出的基于Jaccard相似性的测量指标。研究者强调，在利用LLMs模拟个体决策或集体行为之前，分析提示的稳健性和变异性至关重要，因为它们的模仿能力充其量只能说是近似的。|
|**2024-05-29**|**Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF**|Shicong Cen et.al.|[2405.19320](http://arxiv.org/abs/2405.19320)|null|**摘要：**  强化学习从人类反馈（RLHF）在调整大型语言模型（LLMs）以符合人类偏好方面展现出巨大潜力。在线和离线RLHF都处于活跃的研究阶段，但关键挑战之一是如何在处理从偏好数据中学习的奖励函数不确定性时。尽管标准强化学习（RL）中乐观主义或悲观主义的原则已广为人知，但在大型语言模型中实现既实用又基于理论的方法尚不成熟，因为构建置信区间的标准技术在处理任意策略参数化时变得难以处理。  本文提出了一种统一的在线和离线RLHF方法——价值激励的偏好优化（VPO）。VPO通过在最大似然估计的奖励函数中添加相应的值函数的正则化，以指示选择乐观主义还是悲观主义，实现了这一目标。此外，VPO直接优化策略，并利用隐式奖励建模，因此其RLHF管道与直接偏好优化更为简单。对于在线和离线设置，VPO提供了理论保证，其收敛速度与标准RL相当。实验在文本摘要和对话任务上验证了VPO的实用性与有效性。|
|**2024-05-28**|**Don't Forget to Connect! Improving RAG with Graph-based Reranking**|Jialin Dong et.al.|[2405.18414](http://arxiv.org/abs/2405.18414)|null|## 背景  检索增强生成（Retrieval Augmented Generation，RAG）通过结合现有文档的上下文显著提升了大语言模型（Large Language Model，LLM）的响应性能。然而，当文档与问题上下文的相关性不明显或存在部分信息时，RAG的效果如何？又该如何处理文档之间的关联性呢？本研究旨在解答RAG生成中的这两个核心问题。我们提出了一种名为G-RAG的方法，它是一个基于图神经网络（Graph Neural Networks，GNNs）的重排器，介于RAG的检索器和阅读器之间。G-RAG结合了文档之间的连接性和语义信息（通过抽象意义表示图），为RAG提供了一个具有上下文感知的排名器。实验结果表明，G-RAG超越了现有的领先方法，同时计算开销更小。此外，我们评估了PaLM 2作为重排器的表现，发现其明显逊色于G-RAG，这强调了即使使用大型语言模型，重排在RAG中的重要性。|
|**2024-05-28**|**Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning**|Yixiao Zhang et.al.|[2405.18386](http://arxiv.org/abs/2405.18386)|**[link](https://github.com/ldzhangyx/instruct-MusicGen)**|**在文本到音乐编辑领域，近期的进步依赖于文本查询来改变音乐风格或调整乐器元素。然而，现有方法要么需要从头训练特定的编辑模型，耗时且资源密集，要么使用大型语言模型预测编辑后的音乐，导致音频重建不够精确。为了结合优点并解决这些问题，我们提出了Instruct-MusicGen，这是一种新颖的方法，它针对预训练的MusicGen模型进行微调，以高效地执行编辑指令，如添加、删除或分离音轨。我们的方法修改了原始MusicGen架构，引入了文本融合模块和音频融合模块，使模型能够同时处理指令文本和音频输入，生成所需的编辑音乐。令人惊讶的是，Instruct-MusicGen仅向原始模型增加了8%的新参数，并在5000步的训练后，其性能超越现有基准，且表现出与专门针对任务训练的模型相当的能力。这一进展不仅提高了文本到音乐编辑的效率，还拓宽了音乐语言模型在动态音乐制作环境中的应用范围。**|
|**2024-05-28**|**OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning**|Pengxiang Li et.al.|[2405.18380](http://arxiv.org/abs/2405.18380)|**[link](https://github.com/pixeli99/owlore)**|**随着大型语言模型（LLMs）的快速发展，它们在自然语言处理任务中带来了革命性变化。然而，大模型的训练或微调带来了巨大挑战。针对这一问题，低秩适应（LoRA）等参数高效方法崭露头角，但往往牺牲性能。本文提出了一种新的内存高效微调方法——Outlier-weighed Layerwise Sampled Low-Rank Projection（OwLore），它受到LLMs层间异常分布的启发，通过动态采样预训练层而非添加额外适配器来进行微调。我们首先通过Heavy-Tailed Self-Regularization理论（HT-SR）解读异常现象，发现具有更多异常值的层更倾向于呈现长尾分布，训练效果更好。因此，OwLore策略性地为异常值较多的层分配更高的采样概率，以更好地利用预训练模型的知识。  为了进一步减少微调时的内存需求，我们结合梯度低秩投影，使得每一层能以低秩方式高效训练。通过融合低秩优势和最优层别采样策略，OwLore显著优化了LLM剪枝中的内存-性能权衡。我们在多个架构，如LLaMa2、LLaMa3和Mistral上的广泛实验表明，OwLore持续优于基础方法，包括全量微调。例如，在常识推理基准上，OwLore可实现平均1.1%的精度提升，MMLU上提高3.0%，而在MT-Bench上更是有显著的10%提升，同时内存效率更高。特别地，OwLore仅需21GB内存即可对LLaMa2-7B进行微调。**|
|**2024-05-28**|**LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models**|Anthony Sarah et.al.|[2405.18377](http://arxiv.org/abs/2405.18377)|null|现代大型语言模型（LLMs）在自然语言处理、复杂推理、情感分析等任务中的卓越表现推动了它们的广泛应用。然而，这些强大的功能伴随着巨大的内存和计算成本，限制了在大多数硬件平台上的使用。为解决这一问题，我们提出了一种有效的方法，基于LLaMA2-7B进行单次微调后，通过遗传算法搜索找到更小、计算复杂度更低的网络架构。实验表明，对于某些标准基准任务，预训练的LLaMA2-7B模型实际上过于庞大且复杂。我们实现了1.5倍的模型大小缩减和1.3倍的吞吐量提升，同时保持了几乎无损的准确性。相较于某些剪枝或稀疏化技术，我们的方法在效率和效果上更为优越。最后，我们展示了量化与我们的方法相结合的效果，进一步通过量化减少了找到的网络的大小和复杂性。我们相信，本工作提供了一种自动创建可在更廉价和广泛可用硬件平台上使用的LLMs的方法。|
|**2024-05-28**|**Empowering Source-Free Domain Adaptation with MLLM-driven Curriculum Learning**|Dongjie Chen et.al.|[2405.18376](http://arxiv.org/abs/2405.18376)|**[link](https://github.com/Dong-Jie-Chen/RCL)**|**### 背景  源免费领域适应（SFDA）的目标是仅使用未标记的靶域数据来调整预训练的源模型。当前的SFDA方法在有效利用预训练知识和挖掘靶域数据潜力方面面临挑战。多模态大型语言模型（MLLMs）在理解视觉和文本信息方面表现出色，但它们应用于SFDA时存在问题，如指令执行失败、计算需求高以及在适应前性能评估困难。为了缓解这些问题，我们提出了一种新颖的框架——可靠性基于课程学习（RCL），它通过伪标签化整合多个MLLM以促进知识利用，应用于SFDA。  ### 方法  我们的框架包括：1) 可靠知识转移，2) 自我纠正，3) MLLM引导的知识扩展，以及4) 多热掩码精炼，这些方法协同作用，逐步发掘靶域未标记数据的价值。RCL在多个SFDA基准上实现了最先进的（SOTA）性能，例如在DomainNet上提升显著，达到 $\textbf{+9.4\%}$ ，证明了其在增强适应性和鲁棒性方面的有效性，同时无需访问源数据。代码可在https://github.com/Dong-Jie-Chen/RCL获取。**|
|**2024-05-28**|**Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning**|Phakphum Artkaew et.al.|[2405.18375](http://arxiv.org/abs/2405.18375)|**[link](https://github.com/PhakphumAdev/Thai-Winograd)**|常识推理是自然语言理解的重要组成部分，为此已开发出多个评估基准。然而，这些基准大多仅限于英语。创建平行基准有助于跨语言评估，从而更好地理解不同语言。本研究介绍了一个泰语版的Winograd Schema集合，这是一个专为测试泰语中的常识推理能力而设计的新数据集。我们通过邀请母语者、专业翻译和严格验证的方法，确保该系列题库能准确反映泰国语言的独特性、习语和文化引用，同时保持模糊性和常识挑战。我们对大型语言模型（如GPT-4和Claude-3-Opus）在这项基准上的性能进行了评估，结果显示尽管在英语上表现优异，但它们在泰语中的性能明显下降，这表明在多语言常识推理方面仍有待进步。|
|**2024-05-28**|**PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework**|Eshaan Agarwal et.al.|[2405.18369](http://arxiv.org/abs/2405.18369)|null|大型语言模型（LLMs）已经在各个领域带来了革命性的变化，展现出卓越的能力。它们成功的关键在于提示的概念，即指导模型生成输出。然而，手动创建提示既耗时又局限于特定领域，因此需要自动化的解决方案。本文介绍PromptWizard，一个新颖的框架，它利用LLMs迭代地合成和优化针对特定任务的提示。与现有方法不同，PromptWizard同时优化提示指令和上下文示例，以最大化模型性能。该框架通过变异指令并引入负例，逐步深化理解并保证多样性。借助一个评判者，PromptWizard进一步改进指令和示例，融入详细的推理步骤，以实现最佳表现。PromptWizard具有计算效率高、适应不同训练数据量场景以及在小型LLM上同样有效的特点。通过对8个数据集的35个任务进行严谨评估，结果显示PromptWizard明显优于现有的提示策略，证明了其在提示优化方面的高效性和可扩展性。|
|**2024-05-28**|**Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?**|Yifan Bai et.al.|[2405.18361](http://arxiv.org/abs/2405.18361)|null|随着自动驾驶（AD）任务的快速发展，基于端到端的方法，特别是视觉语言模型（VLM）的应用变得尤为重要。这些模型试图融合强大的逻辑推理和认知能力，以实现全面的端到端规划。然而，现有的VLM方法往往依赖于2D视觉分词器和大型语言模型（LLM），在处理三维几何信息方面存在不足，这对于可靠的规划至关重要。研究表明，2D分词的LLM并不能准确感知三维环境，这引发了关于VLM在自动驾驶中可靠性的质疑。  针对这一问题，我们提出了一种名为Atlas的新方法，它结合了DETR风格的3D感知器作为3D分词器，与单层线性投影器相连，巧妙地利用了三维物理世界的固有特性。这种方法允许高分辨率多视角图像的同时处理和时空建模。尽管简单，但Atlas在NuScenes数据集上的3D检测和自主驾驶规划任务中表现出色，证明了3D分词的LLM对于实现可靠自动驾驶至关重要。我们将开源代码和数据集，以供进一步研究。|
|**2024-05-28**|**Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs**|Somnath Kumar et.al.|[2405.18359](http://arxiv.org/abs/2405.18359)|null|大型语言模型（LLMs）正在全球范围内重塑众多领域，但它们在处理非拉丁字母和低资源语言时的包容性和效果仍有待提升。本文针对这一关键挑战，提出了一种无需大量训练或微调的方法来增强多语言LLMs的表现。通过系统地研究和评估各种语言在流行的问题解答（QA）数据集上的性能，我们提出了一系列新颖技术，以释放LLMs在多元语言环境中的真正潜力。我们的方法包括三个核心策略，极大地提高了多语言能力：首先，精心优化适用于多语言LLM的提示，挖掘其潜在能力，显著提升了各语言的表现。其次，我们引入了一种新的混合方法，结合了多语言嵌入的LLM检索增强生成（RAG），实现了更好的多任务性能。最后，我们开发了一种动态学习策略，实现实时根据查询动态选择最合适的提示策略、LLM模型和嵌入模型，从而最大化LLM在不同语言上的效率，超越了最佳静态和随机策略。此外，我们的方法既适用于离线配置调整，也支持在线适应，能够无缝适应新语言和数据集，显著推动了多语言理解和生成在各种语言中的进步。|
|**2024-05-28**|**MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning**|Somnath Kumar et.al.|[2405.18358](http://arxiv.org/abs/2405.18358)|null|## 背景  近期的多模态大型语言模型（MLLM）在视觉与语言融合任务上取得了显著进步。然而，它们在细致的多模态理解、复杂任务解析以及多模态信息推理方面仍存在挑战。本文提出MMCTAgent，一个旨在解决当前MLLM在复杂视觉推理任务中固有局限性的新型多模态批判性思维代理框架。MMCTAgent借鉴了人类认知过程和批判性思考的特点，通过迭代分析多模态信息、拆解问题、规划策略，并实现动态推理。  此外，MMCTAgent还融入了批判性思考元素，如对最终答案的验证和自我反思。它通过一种新颖的方法定义基于视觉的评判者，并确定特定任务的评估标准，从而提升决策能力。在多个图像理解和视频理解基准测试中，我们严谨地评估了MMCTAgent（包括带评判者的版本）的表现，结果表明它在超越基础MLLM和其他工具增强的管道方面表现出色。|
|**2024-05-27**|**Matryoshka Multimodal Models**|Mu Cai et.al.|[2405.17430](http://arxiv.org/abs/2405.17430)|null|## 背景  大型多模态模型（如LLaVA）在视觉-语言推理方面表现出色。这些模型首先将图像嵌入到大量的固定视觉令牌中，然后将它们输入到大型语言模型（LLM）。然而，这种设计在处理高分辨率图像和视频等密集视觉场景时会导致大量令牌，从而导致效率低下。尽管存在令牌剪枝/合并方法，但它们为每个图像生成单个长度的输出，无法在信息密度与效率之间灵活权衡。受到套娃玩偶概念的启发，我们提出了M3：套娃多模态模型，它学习将视觉内容表示为捕捉不同粗细粒度信息的嵌套视觉令牌集合。  ## 任务  我们的方法为LMMs带来了几个独特的优势：(1) 在测试实例中，用户可以明确控制视觉粒度，例如，根据内容的复杂性或简洁性调整用于表示图像的令牌数量；(2) M3提供了一个分析现有数据集所需粒度的框架，我们发现像COCO这样的基准只需要大约~9个视觉令牌就能获得与使用所有576个令牌相当的准确性；(3) 我们的方法为探索性能与视觉令牌长度之间的最佳权衡提供了基础，研究显示当前固定规模表示与理想上限之间存在显著差距。|
|**2024-05-27**|**NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models**|Chankyu Lee et.al.|[2405.17428](http://arxiv.org/abs/2405.17428)|null|本文介绍了一种名为NV-Embed的新型大语言模型，专门设计用于提升基于解码器的大型语言模型在文本嵌入任务中的性能，包括密集向量检索。NV-Embed通过多种架构设计和训练策略显著增强模型的灵活性和表现，同时保持其简洁性和可复现性。  在架构方面，我们引入了隐式注意力层来获取池化嵌入，这在检索和下游任务准确性上均优于平均池化或使用LLMs的最后一个<EOS> token嵌入。为了改进表示学习，我们移除了LLMs的自回归注意力掩码，在对比性训练中允许更全面的信息交互。  在训练策略上，我们采用两阶段的对比性指令调优方法。第一阶段在检索数据集上进行指令训练，利用批次内负样本和精心挑选的难例。第二阶段将各种非检索任务的数据融入指令调优，不仅提高非检索任务的准确性，还提升了检索性能。  凭借这些创新，NV-Embed仅使用公开数据就实现了前所未有的高分，达到69.32，荣登大规模文本嵌入基准（MTEB）（截至2024年5月24日）榜首，涵盖56项任务，包括检索、重排、分类、聚类和语义文本相似度。尤其值得注意的是，我们的模型在BEIR的15项检索任务中取得了最高的59.36分。NV-Embed模型的源代码将在以下网址开源：https://huggingface.co/nvidia/NV-Embed-v1。|
|**2024-05-27**|**Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model**|Kuan-Chih Huang et.al.|[2405.17427](http://arxiv.org/abs/2405.17427)|**[link](https://github.com/kuanchihhuang/reason3d)**|**随着多模态大型语言模型（LLMs）的最新进展，它们在概念推理等领域展现出巨大潜力。然而，在理解三维环境方面的应用仍相对有限。本文提出Reason3D，这是一种专为全面3D理解设计的新颖LLM。Reason3D接受点云数据和文本提示作为输入，生成文本响应和分割掩码，支持高级任务，如3D推理分割、层次搜索、表达式指代和详细掩码输出的问答。特别是，我们设计了一种分层掩码解码器，能够精确定位广阔场景中的小物体。该解码器首先生成一个粗略的位置估计，覆盖物体的大致区域，然后采用逐步细化的策略，显著提高对象识别和分割的精度。实验结果显示，Reason3D在ScanNet和Matterport3D等大规模数据集上，在3D表达式指代、3D问答和3D推理分割任务上表现出卓越性能。代码和模型已在以下链接提供：https://github.com/KuanchihHuang/Reason3D。**|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|由于实体代理需要与现实世界互动，它们必须具备全面的先验知识、长远规划能力以及快速响应速度。尽管近期基于大型语言模型（LLM）的代理表现出色，但它们仍存在一些局限性。例如，LLM的输出通常是描述性的句子，在确定具体动作时可能存在歧义。为了克服这些问题，我们提出了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归方式预测后续动作。为了训练LARM，我们开发了一种新颖的数据格式，称为自回归节点传输结构，并构建了相应的数据集。通过两阶段训练，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法所能达到的成就需要更复杂的决策链。此外，LARM的速度是最快的，比以前快6.8倍。|
|**2024-05-27**|**Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation**|Jiaming Liu et.al.|[2405.17418](http://arxiv.org/abs/2405.17418)|null|当机器人操作策略面对新任务或物体实例时，其动作性能往往不尽人意。因此，自动检测和自我纠正失败动作的能力对于实际的机器人系统至关重要。近期，多模态大型语言模型（Multimodal Large Language Models，MLLM）在视觉指令跟随方面展现出前景，并在多种任务中展现出强大的推理能力。为了将通用MLLM作为端到端的机器人代理，我们提出了Self-Corrected (SC)-MLLM，不仅使其能够预测末端执行器位置，还赋予其自主识别并纠正错误动作的能力。首先，我们通过参数效率高的微调，使MLLM具备姿态预测功能，将其转化为一个语言建模问题。在遇到执行失败时，模型能识别低层次动作错误的原因（如位置和旋转误差），并主动寻求专家的提示。根据反馈，SC-MLLM会重新思考当前失败场景，生成修正后的动作。此外，我们设计了一种连续策略学习方法，针对成功纠正的样本，提升模型对当前场景配置的适应性，减少专家干预的频率。  为了评估我们的SC-MLLM，我们在模拟和真实世界环境中进行了广泛实验。结果表明，与先前最先进的机器人MLLM（ManipLLM）相比，SC-MLLM显著提高了操作精度：在已知物体类别上从57%提升至79%，在未知新类别上从47%提升至69%。|
|**2024-05-27**|**THREAD: Thinking Deeper with Recursive Spawning**|Philip Schroeder et.al.|[2405.17402](http://arxiv.org/abs/2405.17402)|**[link](https://github.com/philipmit/thread)**|大型语言模型（LLMs）在各种场景中展现出卓越的能力，但随着上下文的长度和复杂度增加，它们仍面临挑战。为此，我们提出了Thinking Recursively and Dynamically（ThReaD）方法。ThReaD将模型生成过程构想为一个执行流程，根据上下文可以完整运行或动态地创建新线程。通过子线程，模型可以分发任务（如思考、获取信息），子线程只返回父线程所需的令牌，从而让模型能够根据需要调整产生令牌时使用的中间工作量。我们在任务解决和问答等场景中应用ThReaD，使其能递归地将给定的任务或问题分解为逐步简化的小子问题，由单独的子线程解决。我们使用少量样本学习的方式实现ThReaD，并在包括ALFWorld、TextCraft、WebShop在内的多个基准测试上评估GPT-4和GPT-3.5的表现，以及两个新基准：DataCommons QA和MIMIC-III ICU QA。实验结果显示，ThReaD在这些基准上实现了最先进的性能，相对于现有框架，即使是小型模型（如Llama-3-8b和CodeLlama-7b）也能提升10%到50%的绝对分数。|
|**2024-05-27**|**MindMerger: Efficient Boosting LLM Reasoning in non-English Languages**|Zixian Huang et.al.|[2405.17386](http://arxiv.org/abs/2405.17386)|**[link](https://github.com/cone-mt/mindmerger)**|## 任务  推理能力对于大型语言模型（LLMs）至关重要，但英语与其他非英语语言之间的差距明显。一些研究通过微调LLMs以重新学习非英语的推理能力，而另一些方法则使用外部模型（如英语翻译文本）的输出来替换非英语输入，以应对LLM理解非英语的挑战。然而，这些方法往往未能充分利用LLMs内在的推理和语言理解能力。为了更好地利用LLMs的思维和语言理解能力，我们提出了一种新方法，称为MindMerger，它将LLMs与多语言模型的外部语言理解能力相结合，以提升多语言推理性能。我们还引入了两步训练策略，首先将外部能力嵌入LLMs，然后训练外部能力和内置能力的协作使用。在三个多语言推理数据集和一个语言理解数据集上的实验表明，MindMerger始终优于所有基线，特别是在低资源语言上。在不更新LLMs参数的情况下，MGSM数据集上所有语言的平均准确率提高了6.7%，低资源语言提高了8.0%。|
|**2024-05-27**|**ReMoDetect: Reward Models Recognize Aligned LLM's Generations**|Hyunseok Lee et.al.|[2405.17382](http://arxiv.org/abs/2405.17382)|**[link](https://github.com/hyunseoklee-ai/reward_llm_detect)**|随着大型语言模型（LLMs）的卓越性能和易用性提升，它们带来的社会风险，如假新闻生成，促使开发出能检测LLM生成文本（LGT）的方法以确保安全使用。然而，由于大量LLM的存在，逐个识别它们的特点变得不切实际。因此，研究关注的是这些强大模型共有的特性，即“对齐训练”，即训练LLMs生成更符合人类偏好的文本。我们的关键发现是，随着这些对齐训练的LLMs致力于最大化人类偏好，它们生成的文本甚至比人类撰写的文本在估计偏好上更高，这使得利用偏好模型（一个训练来模拟人类偏好分布的LLM）轻易就能检测到这些文本。  基于这一发现，我们提出两种进一步增强偏好模型检测能力的训练策略：（1）持续偏好微调，使模型更偏向于识别对齐的LLG；（2）奖励模型对人/LLM混合文本的学习，即使用对齐LLM重述的人类原创文本，这是一种介于LGT和人类文本之间的偏好基准，有助于更好地学习决策边界。我们在六个文本领域和十二种对齐LLM上进行了广泛评估，结果显示我们的方法表现出最先进的性能。相关代码已在https://github.com/hyunseoklee-ai/reward_llm_detect上提供。|
|**2024-05-27**|**RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects**|Ahmed Allam et.al.|[2405.17378](http://arxiv.org/abs/2405.17378)|**[link](https://github.com/AUCOHL/RTL-Repo)**|大型语言模型在辅助进行寄存器传输级（Register Transfer Level, RTL）设计任务上展现出潜力。然而，现有的基准测试在反映真实世界RTL项目复杂性方面存在显著差距。为此，该论文提出了一项新的基准——RTL-Repo，专为评估大型语言模型在大规模RTL设计项目中的性能而设计。RTL-Repo包含了从GitHub公共仓库提取的超过4000个Verilog代码样本，每个样本都提供了对应仓库的完整上下文。我们对包括GPT-4、GPT-3.5、Starcoder2以及像VeriGen和RTLCoder这样的Verilog专用模型在内的多款最先进的模型在RTL-Repo基准上的性能进行了评估，比较它们在生成复杂项目的Verilog代码方面的表现。RTL-Repo为硬件设计社区提供了一个宝贵的资源，用于评估和比较语言模型在实际RTL设计场景中的性能，并针对复杂的多文件RTL项目专门训练Verilog代码生成。RTL-Repo是开源的，已在GitHub上公开可用。|
|**2024-05-28**|**Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models**|ShengYun Peng et.al.|[2405.17374](http://arxiv.org/abs/2405.17374)|**[link](https://github.com/shengyun-peng/llm-landscape)**|### 背景  安全校准是确保大型语言模型（LLMs）的行为符合人类偏好并避免有害行为的关键，但近期研究显示，仅使用少量精心设计的训练样本来微调模型可能导致安全性被轻易破坏。我们致力于通过探索LLM的安全景观来评估微调过程中的风险。我们发现了一个普遍存在于流行开源LLM模型参数空间中的新现象，称为“安全盆地”：随机扰动模型权重能使模型在局部区域保持原始校准模型的安全性。  ### 发现与贡献  我们的发现启发我们提出了一种新的安全度量方法——VISAGE，它通过探测模型的安全景观来评估LLM微调过程中的安全性。可视化校准模型的安全景观有助于理解微调如何使模型偏离安全盆地，从而损害安全性。此外，我们观察到系统提示在保护模型方面的重要性，这种保护甚至会传递给处于安全盆地内的扰动版本。这些从安全景观研究中得出的见解为未来LLM安全领域的研究提供了新的洞见。|
|**2024-05-24**|**Scaling Laws for Discriminative Classification in Large Language Models**|Dean Wyatte et.al.|[2405.15765](http://arxiv.org/abs/2405.15765)|null|## 背景  现代大型语言模型（LLMs）标志着机器学习模型能力的一个重大飞跃。这些模型能够对各种查询生成合理的回答，这表明它们在客户服务应用中具有潜力。然而，LLMs已被观察到存在胡言乱语的问题，这在短期内限制了它们在客户服务中的应用。为了解决这个问题，我们提出了一种系统，将语言建模任务重新构想为分类任务，以帮助客户服务代表选择最佳的模板回复。我们的目标是为客服代表提供最合适的前K个候选回复。  ## 任务描述  我们展示了离线和在线实验的结果，证明了实验系统的有效性，离线实验显示出改进，而在线实验则带来了统计显著的效果提升。此外，我们分享了通过模型参数调整进行的验证损失和前K精度的度量曲线。最后，我们讨论了模型大小、延迟和准确性之间的权衡，并展望了未来可能的应用领域。|
|**2024-05-24**|**Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias**|Andres Algaba et.al.|[2405.15739](http://arxiv.org/abs/2405.15739)|**[link](https://github.com/andresalgaba/llm_citation_patterns)**|论文摘要： 引用实践对于构建科学知识结构至关重要，但往往受到当代规范和偏见的影响。随着大型语言模型（如GPT-4）的出现，这一领域出现了新的动态。研究者首次探索了完全依赖参数知识而非基于搜索或检索增强生成的推荐引用的特性及其潜在偏见。实验使用了一组包含166篇来自AAAI、NeurIPS、ICML和ICLR的论文，这些论文在GPT-4的知识截止日期后发表，涉及3,066个引用。实验让GPT-4为匿名文本中的引用提供学术参考。结果揭示了人类和语言模型（如GPT-4）的引用模式惊人相似，但GPT-4显示出更强的高引用偏见，即使在控制了出版年份、标题长度、作者数量和会议等因素后依然存在。此外，我们发现GPT-4生成的既有和不存在引用的特性高度一致，表明模型内化了引用模式。通过分析引用图谱，显示GPT-4推荐的引用嵌入在相关引用网络中，暗示其对概念的深入理解。尽管语言模型可以辅助引用生成，但它们也可能放大现有偏见并引入新偏见，可能影响科学知识的传播。我们的结果强调了识别模型偏见的必要性，并开发平衡的方法与语言模型互动的重要性。|
|**2024-05-24**|**LM4LV: A Frozen Large Language Model for Low-level Vision Tasks**|Boyang Zheng et.al.|[2405.15734](http://arxiv.org/abs/2405.15734)|**[link](https://github.com/bytetriper/lm4lv)**|大型语言模型（LLMs）的成功催生了多模态大型语言模型（MLLMs）的研究热潮，它们正在改变计算机视觉领域的多个研究范式。尽管MLLMs在诸如视觉问答（VQA）和文本到图像等高级视觉和 Vision-and-Language 任务上表现出色，但尚无研究探讨过低级视觉任务如何从这些模型中受益。我们发现，当前大多数MLLM的设计使其对低级特征视而不见，因此在解决低级视觉任务方面存在固有限制。为此，我们提出 $\textbf{LM4LV}$ ，这是一个框架，它允许一个冻结的LLM无需任何多模态数据或先验知识就能解决一系列低级视觉任务。这突显了LLMs在低级视觉领域的强大潜力，并弥合了MLLMs与低级视觉任务之间的鸿沟。我们期望这项工作能激发对LLMs的新视角，加深对其工作机制的理解。|
|**2024-05-24**|**Optimizing Large Language Models for OpenAPI Code Completion**|Bohdan Petryshyn et.al.|[2405.15729](http://arxiv.org/abs/2405.15729)|**[link](https://github.com/BohdanPetryshyn/openapi-completion-benchmark)**|近期，大型语言模型（LLMs）在代码生成任务中的进步极大地改变了软件开发领域。尽管主流编程语言的代码补全解决方案表现出色，但它们在处理较少见的格式，如OpenAPI定义时性能欠佳。本研究评估了GitHub Copilot，一个流行的商业代码补全工具，在OpenAPI完成任务中的表现，并针对Meta开源的Code Llama模型提出了一系列针对该任务的优化策略。研究中设计了一个语义感知的OpenAPI完成基准，通过实验分析了不同提示工程和微调技术对Code Llama模型性能的影响。经过微调的Code Llama模型在正确性上达到了比GitHub Copilot高出55.2%的峰值，同时其参数数量仅为商业解决方案（基于Codex模型）的1/25。此外，研究还改进了一种广泛使用的代码填充训练方法，解决了模型在接收到小于训练时使用的上下文长度提示时的性能不足问题。|
|**2024-05-24**|**Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models**|Yue Zhang et.al.|[2405.15684](http://arxiv.org/abs/2405.15684)|null|为了弥合视觉和语言模态之间的鸿沟，多模态大型语言模型（Multimodal Large Language Models，MLLMs）通常会学习一个适配器，将视觉输入转化为大语言模型（LLMs）能理解的令牌。然而，大多数适配器生成的视觉令牌相对固定，不考虑提示中提及的具体对象。由于这些适配器对图像中的每个细节分配同等关注，且倾向于处理整个场景，这可能会增加大语言模型在处理复杂场景时的认知负荷。为此，我们提出了提示感知适配器。这类适配器设计有根据提示特定关注点动态嵌入视觉输入的能力。具体来说，提示感知适配器利用全局和局部文本特征，在粗粒度和细粒度层次上捕捉与提示最相关的视觉线索。这种方法显著提升了大语言模型理解和解释视觉内容的能力。在各种视觉问答任务中，如计数和位置推理实验中，提示感知适配器的效果得到了验证。|
|**2024-05-24**|**What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models**|Abdelrahman Abdelhamed et.al.|[2405.15668](http://arxiv.org/abs/2405.15668)|null|这篇论文探讨了如何利用大型语言模型（LLMs）进行零样本图像分类。作者提出了一种简单但有效的方法，通过将多模态LLMs应用于图像输入，生成详尽的文本表示。这些文本表示被转化为跨模态嵌入空间中的固定维特征，并结合使用于零样本分类，无需为每个数据集设计复杂的提示。研究者采用通用提示策略，而非针对每个数据集单独调整。实验结果显示，这种方法在多个数据集上表现出色，比先前方法的准确性有所提升。平均而言，在十个基准测试中，该方法比传统方法提高了4.1个百分点，尤其在ImageNet数据集上的提升达到了6.8个百分点。这表明，多模态LLMs有潜力显著增强如零样本图像分类之类的计算机视觉任务，为现有技术带来了显著的进步。|
|**2024-05-24**|**Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning**|Wenhan Chang et.al.|[2405.15662](http://arxiv.org/abs/2405.15662)|null|在人工智能时代，用户可能因隐私顾虑要求AI公司从训练数据集中删除他们的信息。作为模型所有者，重新训练模型会消耗大量计算资源，因此机器遗忘（machine unlearning）技术应运而生，以允许删除请求的训练数据或类别，同时尽量减少对模型性能的影响。然而，对于大规模复杂数据，如图像或文本，从模型中“遗忘”一个类别可能导致性能下降，因为难以确定类别与模型之间的关联。为此，我们提出使用概念（Concept）而非图像特征或文本数据中的令牌来表示要删除类别的语义信息，这有助于切断模型与类别的联系，实现彻底消除影响。  为了分析复杂数据中的概念影响，我们采用了后处理概念瓶颈模型和集成梯度技术，精确识别不同类别中的概念。然后，我们利用随机标签和目标标签的数据污染策略，提出遗忘方法。我们在图像分类模型和大型语言模型（LLMs）上测试了我们的方法，结果一致显示，提出的策略能准确地从模型中抹除目标信息，同时保持模型性能的大部分。|
|**2024-05-24**|**$$\mathbf{L^2\cdot M = C^2}$$ Large Language Models as Covert Channels... a Systematic Analysis**|Simen Gaure et.al.|[2405.15652](http://arxiv.org/abs/2405.15652)|null|近年来，大型语言模型（LLMs）因其在翻译、预测和内容生成等任务中的出色表现而备受瞩目。同时，研究界发现LLMs易受攻击，但也能增强系统的安全性。然而，这些开源的LLMs在作为掩蔽通信媒介，如支持抗审查通信方面的能力如何呢？本论文从实验角度出发，通过实证测量开源LLM模型（Llama-7B）的安全性与容量，以评估其作为掩蔽通信的有效性。尽管结果显示，基于这种模型的通道不太可能实现高实际比特率，这取决于消息长度和模型熵，但我们发现对手发现隐秘通信的可能性较低。为了使结果易于广泛参考，我们采用了一个简单且直观的方案，并假设模型是公开可用的。|
|**2024-05-24**|**LLM-based Robot Task Planning with Exceptional Handling for General Purpose Service Robots**|Ruoyu Wang et.al.|[2405.15646](http://arxiv.org/abs/2405.15646)|null|在日常生活中开发通用服务机器人的需求促使机器人必须能恰当地执行多种基础行为。近期，大规模语言模型（LLMs）的训练进步使得可以直接根据自然语言指令生成任务序列，无需额外的领域知识。然而，尽管LLMs的输出在语义上是正确的，但生成的任务计划可能并不精确地对应于可接受的动作，并且可能存在各种语言模糊性。LLM的幻觉问题对机器人任务规划构成挑战，可能导致生成的内容与现实世界事实或用户输入不符。为此，我们提出了一种基于约束LLM提示的任务规划方法，该方法可以从命令中生成可执行的动作序列。此外，我们还设计了一个异常处理模块来应对LLM幻觉问题，确保生成的结果在当前环境中是可接纳的。我们在RoboCup@Home命令生成器生成的命令上测试了我们的方法，结果显示机器人在理解和执行任务方面表现出色。|
|**2024-05-24**|**GECKO: Generative Language Model for English, Code and Korean**|Sungwoo Oh et.al.|[2405.15640](http://arxiv.org/abs/2405.15640)|null|我们介绍GECKO，一个专为韩语和英语（包括编程语言）设计的双语大语言模型（LLM）。它基于LLaMA架构，使用平衡且高质量的韩英语数据集进行预训练。本报告详述了我们在构建数据管道和训练模型过程中的一些努力。尽管GECKO的词汇量较小，但其在生成韩语和英语令牌时表现出高效性能。我们在代表性的基准测试上评估了其性能，特别是在韩国MMMLU（韩国多模态多语言理解）任务上表现优异，而在英语和代码方面则显示出适度的能力，尽管其训练的令牌数量少于专注于英语的LLMs。GECKO以宽松的许可协议对开源社区开放，我们希望它能为韩语LLM研究提供研究基线和实用见解。您可以在以下链接找到该模型：https://huggingface.co/kifai/GECKO-7B。|
|**2024-05-23**|**A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns**|Asaf Yehudai et.al.|[2405.14863](http://arxiv.org/abs/2405.14863)|null|跨领域对齐是指将一个概念从一个领域映射到另一个领域的任务。例如，询问“如果\textit{医生}是一种\textit{颜色}，它会是什么颜色？”这个看似奇特的课题旨在研究人们如何通过类别映射和对这些映射的推理来表征具体和抽象的概念。在这篇论文中，我们借鉴认知科学中的这一任务，通过行为研究评估大型语言模型（LLMs）在概念化和推理能力上的表现。我们通过提示LLMs执行跨域映射任务，并在群体和个体层面分析它们的响应。此外，我们还评估了模型对其预测进行推理的能力，通过分析和分类它们对这些映射的解释。结果显示，人类和模型的映射以及解释存在显著相似性，表明模型以与人类类似的方式表征概念。这种相似性不仅体现在模型的表示上，也体现在它们的行为中。而且，模型大多给出有效的解释，并采用与人类类似的推理路径。|
|**2024-05-23**|**Bitune: Bidirectional Instruction-Tuning**|Dawid J. Kopiczko et.al.|[2405.14862](http://arxiv.org/abs/2405.14862)|null|我们提出了一种名为Bitune的方法，该方法提升了预训练的解码器型大语言模型在指令调优方面的性能，从而在多个下游任务上实现了显著的提升。Bitune通过同时应用自回归和双向注意力到提示上，以获取更精确的查询或指令表示。我们为此引入了两组参数，并采用了参数高效微调技术来处理。这两种特征随后被组合成一个加权平均，其中权重由可训练系数决定，用于生成新的令牌。实验结果表明，Bitune在零样本设置下在常识推理、算术和语言理解任务上表现出色。大量的消融研究验证了每个组件的作用，并显示了该方法对不同PEFT技术的鲁棒性。|
|**2024-05-23**|**PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression**|Vladimir Malinovskii et.al.|[2405.14852](http://arxiv.org/abs/2405.14852)|**[link](https://github.com/vahe1994/aqlm)**|## 背景  对于大型语言模型（LLMs）的“极端”压缩，即将其参数压缩至1-2位每参数，以适应资源受限设备上的高效执行，引起了广泛关注。现有研究主要集中在改进一次性量化技术和权重表示上；然而，纯后训练方法在精度与位宽权衡方面的收益正在减少。当前最先进的量化方法，如QuIP#和AQLM，包含对部分压缩参数的小规模校准数据微调；然而，这些针对压缩权重的微调通常仅使用直通估计器（STE），STE在这种场景下的性能尚不明确。  本工作质疑在极端LLM压缩中使用STE的有效性，并系统地研究了量化感知微调策略。我们提出PV-Tuning，一个无特定架构限制的框架，它扩展并改进了现有的微调策略，并在某些受限情况下提供收敛保证。在实际应用中，当用于1-2位矢量量化时，PV-Tuning在高性能模型如Llama和Mistral上优于先前的技术。通过使用PV-Tuning，我们在2位参数的情况下首次实现了Llama 2家族模型的帕累托最优量化。|
|**2024-05-23**|**HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models**|Bernal Jiménez Gutiérrez et.al.|[2405.14831](http://arxiv.org/abs/2405.14831)|**[link](https://github.com/osu-nlp-group/hipporag)**|为了在恶劣多变的自然环境中生存，哺乳动物的大脑发展出存储大量世界知识并不断整合新信息的能力，同时避免灾难性遗忘。尽管大型语言模型（LLMs）如带有检索增强生成（RAG）的方法在处理此类任务上已取得显著成就，但它们在大规模新经验融合方面仍面临挑战。本研究中，我们提出HippoRAG，一个受人类长期记忆海马回索引理论启发的新型检索框架，旨在促进对新经验的更深、更有效集成。HippoRAG巧妙地协同LLMs、知识图谱以及个性化PageRank算法，模拟人脑皮层和海马体在记忆中的不同作用。  我们将HippoRAG与现有RAG方法在多轮问答任务中进行比较，结果显示HippoRAG显著优于当前最先进的方法，性能提升高达20%。单步检索时，HippoRAG表现出与迭代检索方法如IRCoT相当或更好的性能，同时成本节省10-30倍，速度提升6-13倍。当将HippoRAG融入IRCoT后，还能带来额外的显著增益。最后，我们展示HippoRAG能够应对现有方法难以触及的新场景。代码和数据已在<https://github.com/OSU-NLP-Group/HippoRAG>上开源。|
|**2024-05-23**|**Can LLMs Solve longer Math Word Problems Better?**|Xin Xu et.al.|[2405.14804](http://arxiv.org/abs/2405.14804)|**[link](https://github.com/xinxu-ustc/coleg-math)**|### 翻译  数学应用题（MWPs）是衡量大型语言模型（LLMs）能力的关键，但现有研究主要集中在简短背景的题目上。然而，现实生活中的数学问题往往涉及复杂情境，因此LLMs解决长篇数学应用题的能力对于其在实际场景的应用至关重要，但这一方面尚未得到充分探索。本研究首次关注Context Length Generalizability（CoLeG），即LLMs处理长篇数学应用题的能力。我们创建了Extended Grade-School Math（E-GSM）数据集，其中包含带有详细叙述的问题。为此，我们提出了两个新指标来评估LLMs在这类任务上的效能和鲁棒性。  通过对现有零样本提示方法以及商业和开源模型的考察，我们发现它们在CoLeG方面普遍存在不足。针对不同类型的LLMs，我们提出针对性的解决方案：对于专有模型，我们设计了一种新的指导性提示以减轻长文本的影响；对于开源模型，我们开发了一种数据增强任务以提升模型的适应性。我们的全面实验结果显示，我们的方法不仅在E-GSM上表现出色，而且在其他多个数学应用题基准上也展现出良好的泛化能力。  本研究的结果为未来利用LLMs处理复杂现实问题的研究提供了方向，为当前限制提出了实用解决方案，并为进一步探索模型泛化性和训练策略开辟了道路。|
|**2024-05-23**|**Lessons from the Trenches on Reproducible Evaluation of Language Models**|Stella Biderman et.al.|[2405.14782](http://arxiv.org/abs/2405.14782)|null|在自然语言处理（NLP）领域，有效评估语言模型仍然是一项未解的挑战。研究人员和工程师面临诸多方法论难题，例如模型对评估设置的敏感性、不同方法之间的比较困难，以及可重复性和透明度的缺失。本文基于三年的大型语言模型评估经验，为研究者提供指导和教训。首先，我们概述了语言模型评估中常见的问题。其次，我们阐述了应对或减轻这些问题的最佳实践。第三，我们介绍了Language Model Evaluation Harness（lm-eval）：一个开源库，旨在独立、可重复和扩展地评估语言模型，以解决这些问题。我们将介绍库的功能，并通过案例研究展示如何使用该库来缓解这些方法论关注点。|
|**2024-05-23**|**WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models**|Peng Wang et.al.|[2405.14768](http://arxiv.org/abs/2405.14768)|**[link](https://github.com/zjunlp/easyedit)**|**在大型语言模型（LLMs）中，随着世界事实的不断增长和纠正错误响应的需求，模型编辑的方法需要不断更新知识。论文的核心问题是：在编辑过程中，知识应存储在模型的哪个记忆层次更为合适。研究发现，直接修改长期记忆（模型参数）或利用工作记忆（通过检索的神经网络激活）都会导致不可逾越的三角困境——可靠性、泛化能力和局部性无法同时实现于终身编辑场景中。直接修改参数会与无关的预训练知识或先前编辑产生冲突（可靠性差、局部性不足）；而基于检索的工作记忆难以使模型理解并泛化编辑（泛化能力弱）。因此，作者提出了一个名为WISE的新方法，旨在弥合记忆之间的鸿沟。  在WISE中，设计了一种双参数内存机制，包括主内存用于存储预训练知识，侧内存用于存放编辑后的知识。仅对侧内存中的知识进行编辑，并训练一个路由器，以便根据查询决定从哪个内存中获取信息。对于持续编辑，采用了知识切片机制，将不同的编辑分布在参数的不同子空间中，然后合并到共享内存中，以避免冲突。实验结果表明，WISE在问答、幻觉生成和跨不同趋势的LLM架构（如GPT、LLaMA和Mistral）的终身模型编辑任务中表现出色，超越了先前的模型编辑方法，成功克服了上述困境。代码将在https://github.com/zjunlp/EasyEdit上发布。**|
|**2024-05-23**|**FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models**|Hongyang Yang et.al.|[2405.14767](http://arxiv.org/abs/2405.14767)|**[link](https://github.com/ai4finance-foundation/finrobot)**|**随着金融机构和专业人士越来越多地将大型语言模型（LLMs）融入工作流程，金融行业与AI社区之间仍存在显著障碍，如专有数据和专业知识。这些挑战限制了AI在提升金融任务效率方面的潜力。鉴于金融分析的重要性，我们旨在开发专门针对金融的LLM驱动工具链，并通过开源项目推动其普及，促进AI在金融决策中的广泛应用。本文介绍FinRobot，一个创新的开源AI代理平台，支持多个金融专业AI代理，每个都由LLM驱动。平台主要分为四层：1）金融AI代理层，通过构建金融Chain-of-Thought（CoT）将复杂的金融问题分解为逻辑序列；2）金融LLM算法层，根据特定任务动态配置合适的模型应用策略；3）LLMOps和DataOps层，通过训练/微调技术以及使用与任务相关的数据生成精确模型；4）多源LLM基础模型层，整合各种LLM，使上述各层可以直接访问。FinRobot旨在为专业分析师和非专业人士提供实践操作，让他们能够利用强大的AI技术进行高级金融分析。FinRobot的开源代码可在此获取：\url{https://github.com/AI4Finance-Foundation/FinRobot}。**|
|**2024-05-23**|**Evaluating Large Language Models for Public Health Classification and Extraction Tasks**|Joshua Harris et.al.|[2405.14766](http://arxiv.org/abs/2405.14766)|null|随着大型语言模型（LLMs）的快速发展，人们对其在公共卫生领域支持专家工作的潜力产生了浓厚兴趣。本研究通过结合六个外部标注的和七个内部标注的数据集，评估了LLMs在处理与健康负担、流行病学风险因素和公共卫生干预相关的文本分类和提取任务上的性能。我们首先对五个开源大模型（参数量从7亿到70亿不等）进行了零样本的上下文学习测试。结果显示，Llama-3-70B-Instruct表现出色，微-F1得分在17个任务中的15项中最高。各任务间的性能差异显著，例如，有些模型如Contact Classification的得分低于60%，而像GI疾病分类这样的任务，所有模型都能达到80%以上的微-F1。对于12个任务的子集，我们还评估了GPT-4，发现其与Llama-3-70B-Instruct的结果相当，Llama-3-70B-Instruct在其中6个任务上得分更高或持平。总体而言，根据初步结果，我们发现LLMs有可能成为公共卫生专家从各种自由文本源提取信息的有效工具，有助于公共卫生监测、研究和干预措施。|
|**2024-05-23**|**Large language models can be zero-shot anomaly detectors for time series?**|Sarah Alnegheimish et.al.|[2405.14755](http://arxiv.org/abs/2405.14755)|**[link](https://github.com/sintel-dev/sigllm)**|近期的研究表明，大型语言模型能够执行多种任务，包括时间序列预测。这些模型的灵活性使其适用于众多应用。本文提出一项新颖的研究，探讨大型语言模型在复杂的时间序列异常检测任务中的性能。对于语言模型而言，这涉及识别输入序列（或多个部分）中的异常点，以及处理时间序列数据而非传统的文本输入。我们介绍了sigllm，一个专为时间序列异常检测设计的大型语言模型框架。该框架包含将时间序列转换为文本的模块，以及端到端的流程，用于引导语言模型进行异常检测。我们试验了两种测试大型语言模型能力的方法：一是直接提示模型指出输入中的异常元素；二是利用语言模型的预测能力来辅助检测过程。  我们在11个来自不同来源的数据集上评估了我们的框架，使用了10种不同的管道。结果显示，预测方法在所有11个数据集中都显著优于提示方法，尤其是在F1分数上。尽管大型语言模型能够发现异常，但目前的深度学习模型在性能上仍占优，其表现比大型语言模型高出30%。|
|**2024-05-21**|**Reducing Transformer Key-Value Cache Size with Cross-Layer Attention**|William Brandon et.al.|[2405.12981](http://arxiv.org/abs/2405.12981)|null|## 翻译  键值缓存对于加速Transformer架构的自回归大型语言模型（LLMs）的解码至关重要。然而，随着序列长度增加和批量大小增大，存储键值缓存所需的内存可能会变得难以承受。自从Transformer诞生以来，两个最有效的内存减小策略是多查询注意力（MQA）及其推广，群组查询注意力（GQA）。MQA和GQA通过让多个查询头共享单个键/值头，显著减少了不同键/值头的数量，同时对准确性影响较小。本文展示了如何进一步发展MQA，即在相邻层之间也共享键和值头，我们将其称为跨层注意力（CLA）。实验表明，使用CLA，可以在保持接近原始MQA精度的同时，将键值缓存的大小再减少2倍。我们在从头训练10亿参数和30亿参数模型的实验中验证了这一点，结果表明，CLA在内存与准确性之间的权衡上提供了优于传统MQA的帕累托改进，使得更长的序列长度和更大的批量大小下的推理成为可能。|
|**2024-05-21**|**Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale**|Shriram Chennakesavalu et.al.|[2405.12961](http://arxiv.org/abs/2405.12961)|**[link](https://github.com/rotskoff-group/llm-era)**|在化学空间中的搜索是一个极具挑战性的问题，因为可能的分子数量随着原子数量呈组合级增长。大型自回归模型通过学习化学化合物数据库已经产生了强大的生成器，但我们仍然缺乏有效策略来生成具有特定性质的分子。这个问题与大型语言模型的“对齐”问题相似，尽管在许多化学任务中，我们有一个明确且易于评估的奖励函数。本文介绍了一种名为能量排名对齐（ERA）的算法，它利用明确的奖励函数构建了一个梯度优化目标，用于调整自回归策略。理论上，我们发现该算法与Proximal Policy Optimization（PPO）和Direct Preference Optimization（DPO）密切相关，但其最小化器收敛于一个理想的吉布斯-玻尔兹曼分布，奖励函数扮演了能量角色。此外，该算法具有高度可扩展性，无需强化学习，并且在每对样本的偏好观察次数较少时，相对于DPO表现出色。  我们将这种方法应用于分子变压器的对齐，以生成具有外部指定属性的分子，并发现它能稳健地进行搜索，探索化学空间的多样化部分。虽然我们的重点在于化学搜索，但我们在一个AI监督的任务上也取得了优秀结果，表明该方法是可扩展且通用的。|
|**2024-05-21**|**Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models**|Zhangyue Yin et.al.|[2405.12939](http://arxiv.org/abs/2405.12939)|**[link](https://github.com/yinzhangyue/AoR)**|## 背景 近期，Chain-of-Thought提示的进展极大地推动了大型语言模型（LLMs）在复杂推理任务中的突破。当前研究通过采样多种推理路径并根据答案频率进行ensemble，提高了LLMs的推理性能。然而，这种方法在正确答案处于少数的情况时失效。我们发现这是制约LLMs推理能力的关键因素，仅凭预测答案无法解决这个问题。为此，我们提出了一个层次化的推理聚合框架AoR（推理聚合），它依据推理链条的评估来选择答案。此外，AoR引入了动态采样策略，根据任务复杂度调整推理链条的数量。  ## 任务 一系列复杂推理任务的实验结果显示，AoR相较于主流ensemble方法表现出色。进一步分析表明，AoR不仅适用于各种LLMs，而且在与现有方法的性能天花板比较中，达到了更优秀的水平。|
|**2024-05-21**|**Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs**|Bilgehan Sel et.al.|[2405.12933](http://arxiv.org/abs/2405.12933)|null|大型语言模型在诸如总结、算术推理和问答等任务上表现出色。然而，在道德推理和伦理决策方面，尤其是在涉及多个利益相关者的复杂情景中，它们面临严峻挑战。本文提出了一种名为Skin-in-the-Game（SKIG）的框架，旨在通过从不同利益相关者角度审视决策的后果，提升语言模型在道德推理中的能力。SKIG的核心机制是模拟行动的责任感，结合同理心练习和风险评估，对提高其有效性至关重要。我们使用专有和开源语言模型在各种道德推理基准上验证SKIG的表现，并通过深入的消融分析探究其关键组件。|
|**2024-05-21**|**Code-mixed Sentiment and Hate-speech Prediction**|Anjali Yadav et.al.|[2405.12929](http://arxiv.org/abs/2405.12929)|**[link](https://github.com/matejklemen/sentiment-hate-speech-with-code-mixed-models)**|在多语言环境中，混合代码（code-mixed discourse）指的是单文本中融合多种语言的现象，尤其是在官方语言多元的国家的非正式交流中常见。随着大型语言模型在自然语言处理任务中的主导地位提升，我们针对代码混合语境的研究也随之展开。首先，我们特别设计了四款新的英语-印地语和英语-斯洛文尼亚双语预训练遮罩语言模型，以适应非正式语言。接着，我们对各种类型的模型——包括单语、双语、少量语言和大规模多语言模型——在社交媒体文本的情感分析和攻击性语言检测等任务上的性能进行了评估。结果显示，最有效的分类器是针对社交媒体文本的专业化双语和多语言模型，随后是非专业的大规模多语言和单语模型，而大型生成模型的表现并不突出。对于涉及情感的问题，模型在处理代码混合数据时总体上略优于非代码混合数据。|
|**2024-05-21**|**Streamlining Software Reviews: Efficient Predictive Modeling with Minimal Examples**|Tim Menzies et.al.|[2405.12920](http://arxiv.org/abs/2405.12920)|**[link](https://github.com/timm/ez)**|该论文提出了一项新的软件分析挑战任务。在这个被称为“软件审查”的过程中，一组SME（主题专家）会评审软件行为示例，以建议如何改进软件的运行。由于SME的时间通常非常有限，理想的状况是，该团队仅通过查看少量具有高度信息价值的示例就能完成优化任务。为了支持这个审查过程，研究探索了训练预测模型的方法，该模型能够预测某个专家是否会喜欢或不喜欢下一个示例。这种预测模型可以与SME合作，引导他们探索所有示例，同时在专家离开后，模型也可以作为代理，处理新出现的案例，以应对专家们的忙碌。  在31个案例研究中（涵盖了从软件流程的高层决策到视频编码软件配置的低层决策），我们展示了仅使用12到30个标签就能建立这样的预测模型。据我们所知，仅凭少数示例（不依赖大型语言模型）就能取得这样的成果，在当前尚属罕见。遵循开放科学的原则，我们将在<https://github.com/timm/ez/tree/Stable-EMSE-paper>提供所有的代码和数据，以便他人能复制、验证或在此基础上进一步改进这些结果。|
|**2024-05-21**|**G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation**|Xingyuan Pan et.al.|[2405.12915](http://arxiv.org/abs/2405.12915)|**[link](https://github.com/xypan0/G-DIG)**|大型语言模型（LLMs）在通用场景中展现出显著能力，通过指令微调，它们能够与人类在多种任务上协同。然而，指令数据的多样性和质量是指令微调面临的两大挑战。为此，本论文提出了一种新颖的基于梯度的方法，用于自动选择机器翻译中的高质量和多样化的指令微调数据。我们的核心创新在于分析单个训练样例如何在训练过程中影响模型。通过结合影响力函数和一小部分高质量种子数据，我们选择对模型产生积极影响的样例作为高质量数据。此外，为了增加数据多样性，我们通过聚类其梯度并重采样，最大化它们对模型产生的影响多样性。在WMT22和FLORES翻译任务上的广泛实验验证了我们方法的优越性，深入分析进一步证实了其效果和泛化能力。|
|**2024-05-21**|**An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation**|Zhiyu Tan et.al.|[2405.12914](http://arxiv.org/abs/2405.12914)|**[link](https://github.com/llm-conditioned-diffusion/llm-conditioned-diffusion.github.io)**|一个关键的先决条件是准确理解文本输入，这对于忠实的文本到图像生成至关重要。现有的方法利用CLIP模型的文本编码器来表示提示。然而，预训练的CLIP模型仅能处理英文，且其文本编码器的模型容量相对有限。相比之下，大型语言模型（LLMs）支持多语言输入，能够处理更长的上下文，并提供更优秀的文本表示。本文研究了使用LLMs作为文本编码器以提升文本到图像生成中的语言理解能力。然而，从头开始训练包含LLMs的文本到图像生成模型需要大量的计算资源和数据。  为此，我们提出了一种三阶段训练流程，有效地整合现有文本到图像模型与LLMs，同时保持高效的训练。特别地，我们设计了一个轻量级适配器，使得能够快速使用LLMs生成的文本表示来训练文本到图像模型。大量的实验表明，我们的模型不仅支持多语言输入，还能处理更长的上下文，而且在图像生成质量上表现出色。|
|**2024-05-21**|**Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment**|Holli Sargeant et.al.|[2405.12910](http://arxiv.org/abs/2405.12910)|**[link](https://github.com/AhmedIzzidien/TopicLLM)**|**该论文关注法律分析中的一个重要空白，通过构建和应用一种新颖的判例主题分类法，对英国的简易判决案件进行了探索。利用精心挑选的简易判决案例数据集，我们利用大型语言模型Claude 3 Opus研究功能性话题和趋势。结果显示，Claude 3 Opus在主题分类上的准确率为87.10%，揭示了不同法律领域中简易判决的明显模式。由于英国的判例法并未原始标注关键词或提供主题过滤选项，这项研究不仅深化了我们对简易判决主题本质的理解，还展示了传统方法与人工智能驱动分类方法结合的可能性。因此，本文提供了英国法律的新通用分类框架。这项工作的意义为司法行政领域的进一步研究和计算法学研究方法论讨论奠定了基础。**|
|**2024-05-21**|**Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents**|San Kim et.al.|[2405.12900](http://arxiv.org/abs/2405.12900)|null|近期，大规模语言模型（LLMs）和各种有效的训练方法的兴起推动了开放领域对话系统的发展。然而，这些模型中的毒性问题对用户体验构成重大挑战。本文提出了一种创新的训练算法——对抗式直接偏好优化（ADPO），它是在直接偏好优化（DPO）的基础上改进的。ADPO旨在训练模型增加对优选回复的概率分布，同时降低对使用有毒控制令牌生成的不安全回复的概率。研究显示，ADPO能够增强模型抵御有害对话的能力，同时尽量减少性能下降。此外，我们证明ADPO提供了比传统DPO更为稳定的训练流程。据我们所知，这是首次将有害数据直接融入生成模型的DPO变体，从而减少了人工创建安全对话数据的需求。|
|**2024-05-20**|**Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning**|Guanglin Zhou et.al.|[2405.12217](http://arxiv.org/abs/2405.12217)|**[link](https://github.com/jameszhou-gl/icl-distribution-shift)**|**近期的研究表明，大型多模态模型（LMMs）在应对自然分布变化时表现出极高的鲁棒性，常常超越先前的基准。然而，领域特定的适应仍然是必要的，尤其是在医疗等专业领域。鉴于LMMs庞大的参数空间使其微调不切实际，本研究聚焦于探索上下文学习（ICL）作为一种增强LMM适应性的有效方法。我们发现，ICL的成功在很大程度上依赖于示例的选择，这与大型语言模型类似，但对面临分布变化的LMMs提出了独特挑战。为此，我们评估了一种无监督的ICL方法——TopKNearestPR，该方法通过特征相似性进行最近示例搜索来选择示例。研究揭示了这种方法在处理分布转移场景下的视觉编码器缺陷对其效果的限制。  为解决这些问题，我们提出了一种新颖的方法——InvariantSelectPR，它利用类条件对比不变性（CCI）来提升预训练视觉编码器的稳健性。CCI通过增强不同类别间的区分度并确保对领域特定变化的不变性，提高了编码器识别和检索最有信息价值示例的能力。这种方法有助于引导LMM适应新的查询样本，即使在不同的分布下也是如此。实验结果显示，InvariantSelectPR显著提高了LMM的适应性，在Camelyon17和HAM10000基准数据集上的7-shot任务中，分别实现了34.2%和16.9%的准确率提升，相对于零-shot性能，这是显著的进步。**|
|**2024-05-20**|**MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark**|Hongwei Liu et.al.|[2405.12209](http://arxiv.org/abs/2405.12209)|**[link](https://github.com/open-compass/mathbench)**|**随着大型语言模型（LLMs）的最新进展在数学领域取得了显著进步，传统的数学基准如GSM8k在全面评价这些模型的数学能力方面存在局限。为了弥补这一不足，我们提出了MathBench，这是一个全新基准，旨在严格评估大型语言模型的数学能力。MathBench覆盖广泛的数学学科，对理论理解和实际问题解决能力进行详尽评估。它分为五个阶段，从基础算术到大学数学，结构上设计用于考察模型在不同深度知识的理解。每个阶段包括理论问题和应用题，以衡量模型的数学熟练度及其在实际情境中应用概念的能力。MathBench的目标是提升对LLMs数学能力的评价，提供对其知识理解水平和问题解决技能的细致视角，同时支持双语环境。该项目已发布在https://github.com/open-compass/MathBench。**|
|**2024-05-20**|**Developers' Perceptions on the Impact of ChatGPT in Software Development: A Survey**|Thiago S. Vaillant et.al.|[2405.12195](http://arxiv.org/abs/2405.12195)|**[link](https://github.com/gpt-impact/Paper-content)**|随着大型语言模型（如ChatGPT）的不断发展，其强大的自然语言处理能力和广泛应用引起了广泛关注。尽管人工智能（AI）与软件工程（SE）的融合趋势日益明显，但关于这种融合如何影响软件开发实践和认知的研究仍显不足。为了揭示将AI驱动工具，如ChatGPT，融入软件开发过程的影响和挑战，我们进行了一项调查，针对207名软件开发者进行了研究。调查内容包括ChatGPT对软件质量、生产力以及开发者工作满意度的影响，同时还探讨了他们对未来ChatGPT应用的预期、对可能的工作岗位替代的担忧，以及对监管措施的看法。|
|**2024-05-20**|**CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models**|Haoxiang Shi et.al.|[2405.12174](http://arxiv.org/abs/2405.12174)|null|该论文介绍了一个名为CT-Eval的中文文本转表格数据集，旨在衡量大语言模型在非英语语言环境下的文本转表格任务性能。由于现有英文文本转表格数据集主要面向英语，CT-Eval填补了这一空白，选择了一种流行的多学科中文在线百科作为来源，涵盖了28个领域以保证数据多样性。为了减少数据虚构（hallucination）问题，研究者首先训练了一个语言模型来识别并过滤掉存在虚构问题的样本，然后人工标注验证集和测试集中的错误。最终，CT-Eval包含了大约88,600个任务样本。通过CT-Eval，研究者评估了开源和闭源大语言模型（如GPT-4）的表现，结果显示零-shot模式下这些模型与人类判断仍有显著差距。经过微调后，开源模型在文本转表格能力上有了显著提升，大幅超越了GPT-4。总之，CT-Eval不仅为评估和理解现有大语言模型的中文文本转表格能力提供了有价值的工具，也为提升这类模型在这项任务上的性能提供了宝贵资源。|
|**2024-05-20**|**Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging**|Xiaobo Liang et.al.|[2405.12163](http://arxiv.org/abs/2405.12163)|**[link](https://github.com/dropreg/fennec)**|**随着大型语言模型的迅速发展，它们在众多现实任务中的应用日益广泛，主要目标是符合人类的意图。然而，理解人类意图的复杂性使得依赖于耗时的人工评估成为必要。为了缓解这一问题，我们探讨了利用开源大型语言模型作为评估者的趋势，特别是在GPT-4的流行背景下。我们提出了一种名为\textbf{Fennec}的框架，专注于\textbf{F}ine-grained \textbf{E}valuation（细致评估）和\textbf{N}eeded \textbf{E}xtension（必要扩展）通过分支（Branching）和连接（Bridging）。分支操作将评估任务分解为不同维度和粒度，从而减轻评估挑战。同时，连接操作融合了多样化的训练数据集，增加了评估任务的多样性。实验结果显示，我们的7B模型在各种常用基准上的\textit{一致性}和\textit{一致同意}性能均优于开源的更大规模评估模型，接近GPT-4的表现。我们利用模型的精细校正功能改进多个模型响应，结果显示，这种优化提升了响应质量，在MT-Bench上提高了1-2分。我们的代码已在GitHub上开源\footnote{\url{https://github.com/dropreg/Fennec}}。**|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-20**|**MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning**|Ting Jiang et.al.|[2405.12130](http://arxiv.org/abs/2405.12130)|**[link](https://github.com/kongds/mora)**|**低秩适应是大型语言模型中流行的参数高效微调方法。在这篇论文中，我们研究了低秩更新（如LoRA实现）的影响。我们的发现指出，这种机制可能限制了大语言模型学习和记忆新知识的能力。受此启发，我们提出了一种新的方法MoRA，它利用平方矩阵实现高秩更新，同时保持与LoRA相同的可训练参数数量。为此，我们引入了相应的非参数运算器，以降低输入维度并增加输出维度处理平方矩阵。这些运算器确保权重能无缝融入到大语言模型中，使得我们的方法能够像LoRA一样部署。我们在五个任务上进行了全面评估：指令调整、数学推理、连续预训练、记忆以及预训练。在内存密集型任务上，我们的方法优于LoRA，并在其他任务上表现出相当的性能。**|
|**2024-05-20**|**Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation**|Zhankui He et.al.|[2405.12119](http://arxiv.org/abs/2405.12119)|null|大型语言模型（LLMs）正在通过出色地索引项目内容、理解复杂的对话上下文并生成相关项目标题，革新了对话推荐系统。然而，控制推荐项目的分布仍是一个挑战，导致在针对对话推荐平台的快速变化的数据分布，如项目流行度上，性能欠佳。在对话推荐中，LLMs通过自回归方式生成项目标题（作为多个令牌），这使得获取和控制所有项目推荐变得困难。因此，我们提出了一种名为“重索引-然后适应”（Reindex-Then-Adapt，RTA）的框架，它将多令牌项目标题转换为单个令牌于LLMs内，随后调整这些单令牌项目标题的概率分布。RTA框架结合了LLMs理解和复杂查询的优势，以及传统推荐系统（RecSys）在对话推荐中有效控制推荐项目分布的能力。实验结果表明，我们的框架在三个不同的对话推荐数据集和两种适应设置下，展示了改进的准确性指标。|
|**2024-05-20**|**Imp: Highly Capable Large Multimodal Models for Mobile Devices**|Zhenwei Shao et.al.|[2405.12107](http://arxiv.org/abs/2405.12107)|**[link](https://github.com/milvlg/imp)**|**尽管大型语言模型（LLMs）和大型多模态模型（LMMs）在开放世界多模态理解方面展现出惊人的能力，但它们通常参数量大、计算需求高，限制了在资源受限环境中的应用。为了应对这一问题，研究人员已经提出了一系列轻量级LMM，旨在在有限规模（如30亿参数）下最大化性能。然而，这些方法多数仅关注设计空间的单一或两个方面，对影响模型能力的关键设计选择尚未进行全面探讨。  本文系统地研究了轻量级LMM的设计，包括模型架构、训练策略和训练数据。根据我们的研究结果，我们构建了一套名为Imp的高性能LMM家族，覆盖20亿到40亿参数规模。尤其值得注意的是，我们的Imp-30亿模型在与同类规模的现有轻量级模型相比时持续领先，并超越了130亿参数规模的最新LMM状态。通过低精度量化和分辨率降低技术，Imp模型能够在高通骁龙8Gen3移动芯片上实现高速部署，每秒处理大约13个令牌的推理速度。**|
|**2024-05-20**|**DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction**|Hao Chen et.al.|[2405.12100](http://arxiv.org/abs/2405.12100)|null|## 背景 数学世界问题修正（MWPC）是一个专门针对解决数学问题过程中错误推理的修正任务。本文利用大语言模型（LLMs）的进步，关注两点：（1）区分数学推理与错误修正；（2）探索策略以提升LLMs在数学领域的错误修正能力，以应对MWPC任务。我们注意到，在实时教育中，帮助学生识别错误比单纯提供正确答案更为关键。然而，当前研究往往侧重于获取精确的解题答案，而非纠正可能的错误。因此，我们调整了研究范式，表明提升数学推理能力并不等同于精通错误修正。同时，我们提出了一种名为诊断导向提示（DOP）的新方法，旨在促进LLMs在错误修正方面表现出色。实验结果显示，DOP表现出卓越性能，彰显其重要性。我们强调，在数学教育中，对出色修正者的需要超过了对熟练推理者的追求。代码和数据可在<https://github.com/ChenhaoEcnuCS/Reason-Correct>获取。|
|**2024-05-17**|**A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers**|Kaiyu Huang et.al.|[2405.10936](http://arxiv.org/abs/2405.10936)|**[link](https://github.com/kaiyuhwang/mllm-survey)**|**随着大型语言模型（LLMs）的快速发展，在自然语言处理领域展现出显著的多语言能力，引起了学术界和业界的广泛关注。为了减少潜在的歧视并提升技术的通用性和可访问性，对于多语言技术的发展至关重要。尽管LLMs取得了突破，但对多语言场景的深入研究仍显不足。因此，迫切需要一份全面的综述，总结近期的方法、进展、局限性和可能的解决方案。本文旨在从多个角度审视LLMs在多语言环境中的应用。我们首先回顾了预训练语言模型研究的历史演变。接着，我们探讨了LLMs的多语言特性，包括训练和推理方法、模型安全、跨领域与文化适应以及数据集使用。我们还分析了这些方面面临的挑战，并提出可能的解决策略。此外，我们指出了未来的研究方向，以进一步提升LLMs的多语言性能。本综述旨在帮助研究界应对多语言问题，提供一个关于基于LLMs的多语言自然语言处理核心概念、关键技术及最新进展的全面理解。**|
|**2024-05-17**|**The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks**|Lucius Bushnaq et.al.|[2405.10928](http://arxiv.org/abs/2405.10928)|**[link](https://github.com/apolloresearch/rib)**|### 概述  机械解释性目标是通过逆向工程理解神经网络的行为。然而，现有方法在解析神经网络激活方面面临挑战，因为缺乏对激活的分解，使得单个神经元或模型组件无法清晰对应于独特的特征或功能。为此，我们提出了一种新颖的可解释性方法——局部交互基（Local Interaction Basis，LIB）。LIB旨在通过消除无关激活和交互，识别计算特征。该方法摒弃无意义的激活方向，并使基础与相邻层间雅可比矩阵的奇异向量对齐。同时，它根据特征对后续计算的重要性进行缩放，生成一个显示模型中所有计算相关特性和交互的图谱。  我们在模块加法和CIFAR-10模型上评估了LIB的有效性，结果表明，相比于主成分分析，LIB能识别出更多计算相关的特征，并呈现出更稀疏的交互。然而，在应用于语言模型时，LIB并未显著提高可解释性或交互稀疏度。因此，我们得出结论，尽管LIB是一种有前景的理论驱动方法，但当前形式并不适用于大型语言模型。|
|**2024-05-17**|**COGNET-MD, an evaluation framework and dataset for Large Language Model benchmarks in the medical domain**|Dimitrios P. Panagoulias et.al.|[2405.10893](http://arxiv.org/abs/2405.10893)|null|这篇技术论文阐述了COGNET-MD，一个专为医疗领域设计的大型语言模型评估的新基准。我们提出了一种评分框架，旨在评估语言模型理解医学文本的能力，并且设计了一系列难度分级的多项选择题（MCQ）数据库。这个数据库由多个医疗领域的专家合作创建，以反映当前医学趋势，确保安全、实用和适用性。初期版本包含了精神科、牙科、肺病学、皮肤科和内分泌学等领域的题目，但会持续扩展，未来还会加入更多医学学科。|
|**2024-05-17**|**Application of Artificial Intelligence in Schizophrenia Rehabilitation Management: Systematic Literature Review**|Hongyi Yang et.al.|[2405.10883](http://arxiv.org/abs/2405.10883)|null|该综述旨在系统地评估人工智能（AI）在精神分裂症患者康复管理中的现状和前景，以及其对康复过程的影响。我们从2012年至现在筛选了70项研究，重点关注机器学习、深度学习、强化学习等技术在心理健康干预和管理中的应用、技术类别、产品和数据类型，如生态瞬时评估、行为和语音数据的分析。结果显示，AI在症状监测、复发风险预测和康复治疗中具有广泛的应用潜力。此外，本研究还探讨了基于AI的新兴产品、技术和分析方法，如社交媒体分析、严肃游戏和大型语言模型在康复中的潜在挑战和未来发展方向。总的来说，这篇论文系统回顾了AI在精神分裂症康复管理中的应用，并为未来的研究路径提供了有价值的见解和建议。|
|**2024-05-17**|**The Future of Large Language Model Pre-training is Federated**|Lorenzo Sani et.al.|[2405.10853](http://arxiv.org/abs/2405.10853)|null|## 背景  生成式预训练大型语言模型（LLMs）因其在众多任务上的出色表现而备受瞩目，这得益于它们所接受的海量训练数据。根据已建立的规模法则，LLMs未来性能的提升在很大程度上依赖于我们能够利用的计算和数据资源。联邦学习（FL）有可能释放全球大部分未充分利用的数据和计算能力，这些是当前以数据中心为中心的LLM训练方法所忽视的。本文提出了一种稳健、灵活且可复现的FL方法，旨在促进机构间的大规模协作，共同训练LLMs，从而动员更多的计算和数据资源，甚至可能达到或超越中心化的性能。  ## 任务  我们的工作展示了一种FL训练方法，它能够在有限资源下扩展到百亿元级的联邦LLM，使得拥有丰富数据的实体能够成为预训练LLMs的主导力量，而不是仅让计算资源丰富的机构独占鳌头。这种方法强调了联邦训练的规模效益，并为实现这一目标提供了一种实用路径。|
|**2024-05-17**|**Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities**|Hao Zhou et.al.|[2405.10825](http://arxiv.org/abs/2405.10825)|null|随着大型语言模型（LLMs）因其卓越的理解和推理能力而备受瞩目，它们在各个领域取得了显著进步，尤其在第六代（6G）通信技术的推动下展现出人工智能通用性（AGI）的潜力。本研究旨在全面概述LLM赋能的电信网络。首先，我们概述了LLMs的基础，包括模型架构、预训练、微调、推理与应用、模型评估，以及在电信部署中的运用。接着，我们将探讨LLM支持的关键技术和电信应用，涉及生成、分类、优化和预测问题。生成应用包括电信领域知识、代码和网络配置自动生成。基于LLM的分类任务涵盖网络安全、文本、图像和流量分类。此外，我们介绍了利用LLMs的自动化优化技术，如强化学习的奖励函数设计和口语强化学习。对于预测问题，LLMs可用于时间序列预测和多模态电信预测。最后，我们指出了LLM赋能电信网络所面临的挑战，并展望了未来的研究方向。|
|**2024-05-17**|**ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios**|Markus Bayer et.al.|[2405.10808](http://arxiv.org/abs/2405.10808)|null|主动学习旨在通过优先处理最能提升学习效果的实例来减少标注工作量。然而，许多主动学习策略面临“冷启动”问题，即在初期需要大量数据才能发挥效能，这限制了它们在预训练模型（如BERT）上的应用，这些模型在少量样本情况下已表现良好。为此，我们提出了一种新颖的主动学习方法——ActiveLLM，它利用大型语言模型（如GPT-4、Llama 3和Mistral Large）进行实例选择。实验证明，ActiveLLM显著提高了BERT分类器在少量样本情况下的性能，超越了传统主动学习方法和SetFit等少数样本学习方法。此外，ActiveLLM还能扩展到非少量样本场景，支持迭代选择，从而帮助其他主动学习策略克服冷启动难题。结果表明，ActiveLLM为改善不同学习环境中的模型性能提供了有前景的解决方案。|
|**2024-05-17**|**Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings**|Albert Sawczyn et.al.|[2405.10745](http://arxiv.org/abs/2405.10745)|null|### 翻译  知识密集型任务对机器学习（ML）技术提出了严峻挑战。通常采用的方法，如大型语言模型（LLMs），在处理这类任务时往往存在局限性。然而，人们已经努力通过知识图谱（KG）来弥补这些不足，尤其是通过将小规模的领域特定KG与通用KG相结合。尽管KG在知识表示方面具有优势，但构建它们的成本可能阻碍了广泛的研究和应用。为此，我们提出了一种框架，旨在通过链接到大规模通用KG来提升小型领域特定KG嵌入的学习性能。实验结果显示，这种方法带来了显著的提升，例如，Hits@10指标最高提高了44%。这一相对未被充分探索的研究方向有望促进KG在知识密集型任务中的更频繁运用，从而产生更为稳健、可靠的ML解决方案，它们相较于流行但易出错的LLM方法更具可靠性。关键词：知识图谱、知识图谱补全、实体对齐、表示学习、机器学习|
|**2024-05-17**|**Efficient Multimodal Large Language Models: A Survey**|Yizhang Jin et.al.|[2405.10739](http://arxiv.org/abs/2405.10739)|**[link](https://github.com/lijiannuist/efficient-multimodal-llms-survey)**|**在过去一年里，多模态大型语言模型（Multimodal Large Language Models，MLLMs）在诸如视觉问答、视觉理解和推理等任务上展现出卓越性能。然而，这些模型的庞大规模和高昂的训练与推理成本限制了它们在学术界和工业界的广泛应用。因此，研究高效且轻量级的MLLM具有巨大的潜力，特别是在边缘计算环境中。本综述全面系统地回顾了当前高效MLLM的研究现状。我们概述了代表性高效模型的发展历程，总结了有效结构和策略的研究状态，以及其实用应用。最后，我们讨论了当前高效MLLM研究的局限，并展望了有前景的未来发展方向。如需更多信息，请参考我们的GitHub仓库：https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey。**|
|**2024-05-17**|**INDUS: Effective and Efficient Language Models for Scientific Applications**|Bishwaranjan Bhattacharjee et.al.|[2405.10725](http://arxiv.org/abs/2405.10725)|null|大型通用语言模型在自然语言处理任务上表现出色。然而，先前的研究表明，针对特定领域的训练数据可以使模型在专业任务上表现更佳。为此，我们开发了INDUS，一套专为地球科学、生物学、物理学、太阳物理、行星科学和天文学领域设计的定制化语言模型。这些模型基于精心挑选的科学语料库，包括：（1）一个使用领域专用词汇和数据集训练的编码器，用于提升自然语言理解任务的表现；（2）一个基于对比学习的通用文本嵌入模型，利用多源数据集进行训练，以优化信息检索任务；（3）通过知识蒸馏技术缩小规模的模型，适用于对延迟和资源有限的应用。此外，我们创建了三个新的科学基准数据集：CLIMATE-CHANGE-NER（实体识别）、NASA-QA（抽取式问答）和NASA-IR（信息检索），以推动跨学科领域的研究进展。最后，实验结果显示，我们的模型在新任务和相关领域现有基准任务上均优于通用编码器（如RoBERTa）和现有的领域特定编码器（如SciBERT）。|
|**2024-05-16**|**UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models**|Sahel Sharifymoghaddam et.al.|[2405.10311](http://arxiv.org/abs/2405.10311)|**[link](https://github.com/castorini/unirag)**|## 背景  近期，多模态（MM）大型语言模型（LLMs）已经解锁了许多需要多模态理解（如图像描述或视觉问答）和生成（如文本引导的图像生成或编辑）复杂任务。为了进一步提升MM-LLMs的输出质量，我们提出了一种模型通用的UniRAG技术，它在推理阶段将相关检索信息添加到提示中，作为少量样例。与普遍认为检索增强（RA）主要改进罕见实体的生成或理解不同，我们在MSCOCO数据集上对包括GPT4、Gemini-Pro在内的专有模型以及Llava、LaVIT和Emu2等开源小型模型进行了评估，结果显示，这些模型在输入提示通过MM检索器（如UniIR模型）增强后，显著提高了生成质量。|
|**2024-05-16**|**4D Panoptic Scene Graph Generation**|Jingkang Yang et.al.|[2405.10305](http://arxiv.org/abs/2405.10305)|**[link](https://github.com/jingkang50/psg4d)**|**我们生活在一个三维空间中，同时通过第四维时间向前推进。为了使人工智能能够全面理解这种4D环境，我们提出了一种新的表示形式——4D全景场景图（PSG-4D），它将动态4D世界中的原始视觉数据抽象为节点和边，节点代表具有精确位置和状态信息的实体，边捕捉时间关系。为了促进在这一新领域的研究，我们构建了一个丰富的注释PSG-4D数据集，包含3000个RGB-D视频，总计100万帧，每帧都带有4D全景分割掩码以及详细的动态场景图标签。我们为此任务提出了一种名为PSG4DFormer的Transformer模型，该模型能够预测全景分割掩码，沿时间轴跟踪掩码，并通过关系组件生成相应的场景图。在新数据集上的大量实验表明，我们的方法为未来的PSG-4D研究提供了一个强大的基准。最后，我们展示了如何通过将大型语言模型融入我们的PSG-4D系统来实现动态场景理解的一个实际应用示例。**|
|**2024-05-16**|**HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models**|Rhea Sanjay Sukthanker et.al.|[2405.10299](http://arxiv.org/abs/2405.10299)|**[link](https://github.com/automl/hw-aware-llm-bench)**|**随着语言模型的规模不断扩大，对硬件指标（如延迟、能耗、GPU内存使用和性能）之间的权衡需求日益增长。人们正在寻求为不同语言模型配置建立帕累托前沿，以在指定硬件限制下找到最优模型。然而，对多种架构在多台设备上的全面训练和评估在计算上是不可行的。为此，我们提出了HW-GPT-Bench，这是一个基于硬件感知的语言模型代理基准，利用神经架构搜索（NAS）中的权重共享技术，在一个模型中高效地训练包含不同规模语言模型的超网络。我们在13种设备上对这些模型进行了性能剖析，考虑了5种硬件指标和3种不同的模型规模。最后，我们通过8种不同的多目标NAS算法展示了HW-GPT-Bench的可用性，并评估了由此产生的帕累托前沿的质量。我们的目标是推动和加速大型语言模型的多目标方法，如NAS和结构化剪枝的研究。**|
|**2024-05-16**|**Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction**|Jianhao Chen et.al.|[2405.10288](http://arxiv.org/abs/2405.10288)|**[link](https://github.com/jianhaochen-nju/tsdre)**|**摘要：**  事实抽取对于构建知识图谱至关重要。随着对时间相关事实在下游任务中的需求增长，出现了时间性事实抽取的任务。本文特别关注从自然语言文本中提取时间性事实。先前的研究未能妥善处理复杂句子中时间与事实对应关系的建立难题。为解决这一挑战，我们提出了一种基于时间线的句子分解策略，利用大语言模型（LLMs）进行上下文学习，以实现对事实相关时间线的精细理解。然而，直接使用LLMs进行时间性事实抽取的性能并不理想。因此，我们引入了TSDRE方法，将LLMs的分解能力融入到小型预训练语言模型（PLMs）的传统微调过程中。  为了支持评估，我们构建了一个复杂的时序事实抽取数据集ComplexTRED。实验结果显示，TSDRE在HyperRED-Temporal和ComplexTRED数据集上实现了最先进的性能。|
|**2024-05-16**|**Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers**|Tuo Zhang et.al.|[2405.10276](http://arxiv.org/abs/2405.10276)|null|近年来，许多研究旨在通过策略性提示提升大型语言模型（LLMs）的效能。特别是优化通过prompting（OPRO）方法表现出顶尖性能，它利用LLMs作为优化器，目标是寻找能最大化任务准确性的指令。本论文重新审视了OPRO在小型LLMs（如LaMa-2系列和Mistral 7B）上的自动化提示效果。我们的研究表明，对于小型LLMs，OPRO的效果有限，因为其有限的推理能力限制了优化潜力。因此，我们建议未来的自动提示工程应同时考虑模型能力和计算成本。针对小型LLMs，我们推荐直接提供明确阐述目标和方法的指令，作为稳健的提示基线，以确保在当前研究中实现高效且有效的提示设计。|
|**2024-05-16**|**Keep It Private: Unsupervised Privatization of Online Text**|Calvin Bao et.al.|[2405.10260](http://arxiv.org/abs/2405.10260)|**[link](https://github.com/csbao/kip-privatization)**|**## 背景  作者身份混淆技术有望通过自动重写文本来保护网络通信中的个人隐私。然而，在自然语言处理（NLP）文献中，这些技术的评估大多局限在狭小场景下，主要依赖于表面的编辑操作，可能导致输出不自然。本研究提出了一种自动文本私密化框架，通过强化学习对大型语言模型进行微调，以生成兼顾准确、连贯和隐私的重写。我们在大规模的英语Reddit帖子测试集上进行了详尽的评估，该数据集由68,000名作者撰写，包含短到中等长度的文本。我们探讨了在不同评估条件下，如作者简介长度和作者识别策略，性能的变化。我们的方法在自动化指标和人工评估中保持高文本质量，并成功地规避了几种自动作者识别攻击。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|**[link](https://github.com/activevisionlab/awesome-llm-3d)**|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动人工智能体在空间理解与交互方面的发展。研究覆盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的结合，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。此外，我们还简要回顾了其他结合三维和语言的方法。本文的元分析显示了显著的进步，但也指出了挖掘3D-LLMs全部潜力所需的创新方法的必要性。因此，本文旨在为未来的研究方向提供指导，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本调查，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-16**|**A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks**|Xuanfan Ni et.al.|[2405.10251](http://arxiv.org/abs/2405.10251)|null|近期的研究已评估了大型语言模型（LLMs）在常识推理、数学推理和代码生成等方面的能力。然而，据我们所知，尚无专门针对自然语言生成（NLG）任务的深入研究，这是衡量模型优秀程度的关键标准。因此，本论文旨在全面评估知名且性能出色的LLMs，包括ChatGPT、ChatGLM、基于T5的模型、基于LLaMA的模型和Pythia模型，在对话生成和文本总结等NLG任务中的表现。我们选择了涵盖英语和中文的数据集，并设计了一种共同的评估框架，包括输入模板和后处理策略。研究结果报告了自动评分，同时进行了详细分析。|
|**2024-05-16**|**IntelliExplain: Enhancing Interactive Code Generation through Natural Language Explanations for Non-Professional Programmers**|Hao Yan et.al.|[2405.10250](http://arxiv.org/abs/2405.10250)|null|大型语言模型（LLMs）在根据自然语言描述自动生成可执行代码方面展现出巨大潜力，特别是通过互动功能，用户可以通过迭代反馈指导模型。然而，当前的互动方式往往假设用户具备调试源代码的专业知识，对非专业程序员不太友好。这使得使互动代码生成对不同编程水平的个体更易于使用成为一个挑战。为解决这个问题，我们提出了IntelliExplain，这是一种创新的人机交互范式，通过让用户通过自然语言解释与源代码互动，提升非专业人士的体验。用户通过提供他们发现错误的自然语言纠正反馈，来指导系统修订代码，直到用户对系统的代码解释感到满意。我们的用户研究显示，使用IntelliExplain的用户在Text-to-SQL和Python代码生成任务中的成功率分别比纯GPT-3.5提高了11.6%和25.3%，同时所需时间分别减少了39.0%和15.6%。|
|**2024-05-16**|**CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations**|Jiahao Zhao et.al.|[2405.10212](http://arxiv.org/abs/2405.10212)|**[link](https://github.com/CAS-SIAT-XinHai/CPsyExam)**|在这篇论文中，我们提出了一种创新的心理学基准测试——CPsyExam，它源于中国语言考试的问题。CPsyExam旨在分别强调心理学知识和案例分析的重要性，认识到将心理学知识应用于实际情境的价值。从22,000个问题库中，我们精选了4,000个来构建该基准，确保了主题的均衡覆盖，并包含了各种案例分析方法的多样性。此外，我们对一系列现有的大型语言模型（LLMs）进行了评估，包括开源和API基础的模型。实验和分析结果显示，CPsyExam是一个有效的确立语言模型对心理学理解能力的基准，同时支持在不同粒度上比较这些模型。|

<p align=right>(<a href=#updated-on-20250406>back to top</a>)</p>

